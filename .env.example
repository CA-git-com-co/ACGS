# =============================================================================
# ACGS-PGP Centralized Configuration
# =============================================================================

# Environment Configuration
ENVIRONMENT=development                                # development, production, testing
DEBUG=false                                           # Enable debug mode
TEST_MODE=false                                       # Enable test mode

# Database Configuration
DATABASE_URL=postgresql+asyncpg://acgs_user:acgs_password@postgres_db:5432/acgs_pgp_db
TEST_DATABASE_URL=postgresql+asyncpg://test_user:test_password@localhost:5433/test_acgs_db
DB_ECHO_LOG=false                                     # Enable SQL query logging

# Service URLs (External/Development)
AUTH_SERVICE_URL=http://localhost:8000
AC_SERVICE_URL=http://localhost:8001
INTEGRITY_SERVICE_URL=http://localhost:8002
FV_SERVICE_URL=http://localhost:8003
GS_SERVICE_URL=http://localhost:8004
PGC_SERVICE_URL=http://localhost:8005

# Internal Service URLs (Docker Container Communication)
AUTH_SERVICE_INTERNAL_URL=http://auth_service:8000
AC_SERVICE_INTERNAL_URL=http://ac_service:8001
INTEGRITY_SERVICE_INTERNAL_URL=http://integrity_service:8002
FV_SERVICE_INTERNAL_URL=http://fv_service:8003
GS_SERVICE_INTERNAL_URL=http://gs_service:8004
PGC_SERVICE_INTERNAL_URL=http://pgc_service:8005

# API Configuration
API_VERSION=v1
BACKEND_CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Security Configuration
JWT_SECRET_KEY=your-secret-key-change-in-production-please-use-strong-random-key
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# External Services
LLM_API_ENDPOINT=http://mock_llm_service/generate

# AI Model Configuration for ACGS-PGP Operations
# Primary model for policy synthesis and governance operations
ACGS_PRIMARY_LLM_MODEL=claude-3-7-sonnet-20250219
ACGS_RESEARCH_LLM_MODEL=sonar-pro
ACGS_FALLBACK_LLM_MODEL=claude-3-5-sonnet

# Model-specific settings
ACGS_LLM_MAX_TOKENS=64000
ACGS_LLM_TEMPERATURE=0.2
ACGS_RESEARCH_TEMPERATURE=0.1

# Enable specific AI models for testing and research
ENABLE_GEMINI_2_5_FLASH=true                          # Enable Google Gemini 2.5 Flash for testing
ENABLE_DEEPSEEK_R1=true                               # Enable DeepSeek-R1 for research operations
ENABLE_BIAS_DETECTION_LLM=true                        # Enable LLM-based bias detection
ENABLE_GROQ_LLAMA_MODELS=true                         # Enable Groq-hosted Llama models for testing

# Groq Model Configuration for ACGS-PGP Testing
GROQ_MODEL_NAME=llama-3.3-70b-versatile              # Default Groq model for testing
GROQ_TESTING_MODEL_VERSATILE=llama-3.3-70b-versatile # Large versatile model for comprehensive testing
GROQ_TESTING_MODEL_MAVERICK=meta-llama/llama-4-maverick-17b-128e-instruct  # Mid-size model with extended context
GROQ_TESTING_MODEL_SCOUT=meta-llama/llama-4-scout-17b-16e-instruct         # Efficient model for rapid testing

# Monitoring and Logging
LOG_LEVEL=INFO                                        # DEBUG, INFO, WARNING, ERROR, CRITICAL
METRICS_ENABLED=true
PROMETHEUS_PORT=9090

# =============================================================================
# TaskMaster AI API Keys (Required to enable respective provider)
# =============================================================================
ANTHROPIC_API_KEY="your_anthropic_api_key_here"       # Required: Format: sk-ant-api03-...
PERPLEXITY_API_KEY="your_perplexity_api_key_here"     # Optional: Format: pplx-...
OPENAI_API_KEY="your_openai_api_key_here"             # Optional, for OpenAI/OpenRouter models. Format: sk-proj-...
GOOGLE_API_KEY="your_google_api_key_here"             # Required for Google Gemini 2.5 Flash model
MISTRAL_API_KEY="your_mistral_key_here"               # Optional, for Mistral AI models.
XAI_API_KEY="YOUR_XAI_KEY_HERE"                       # Optional, for xAI AI models.
AZURE_OPENAI_API_KEY="your_azure_key_here"            # Optional, for Azure OpenAI models (requires endpoint in .taskmasterconfig).
OLLAMA_API_KEY="your_ollama_api_key_here"             # Optional: For remote Ollama servers that require authentication.

# HuggingFace Configuration for DeepSeek-R1
HUGGINGFACE_API_KEY="your_huggingface_api_key_here"   # Required for DeepSeek-R1 model access
HUGGINGFACE_API_ENDPOINT="https://api-inference.huggingface.co/models"  # HuggingFace Inference API endpoint

# OpenRouter Configuration (Alternative for DeepSeek models)
OPENROUTER_API_KEY="your_openrouter_api_key_here"     # Optional: For OpenRouter DeepSeek models

# Groq Configuration for Llama Models
GROQ_API_KEY="your_groq_api_key_here"                 # Required for Groq-hosted Llama models (llama-3.3-70b-versatile, llama-4-maverick-17b, llama-4-scout-17b)

# =============================================================================
# LangGraph Workflow Configuration
# =============================================================================

# LangGraph Infrastructure
LANGGRAPH_REDIS_URL=redis://langgraph_redis:6379      # Redis URL for LangGraph state management
LANGGRAPH_POSTGRES_URL=postgresql://acgs_user:acgs_password@postgres_db:5432/acgs_pgp_db  # PostgreSQL for workflow persistence

# Multi-Model LLM Configuration for LangGraph
GEMINI_API_KEY="your_gemini_api_key_here"             # Required for Gemini models in LangGraph workflows
GROQ_API_KEY="your_groq_api_key_here"                 # Required for Groq Llama models in LangGraph workflows
XAI_API_KEY="your_xai_api_key_here"                   # Required for xAI Grok models in LangGraph workflows

# Constitutional Governance Thresholds
CONSTITUTIONAL_FIDELITY_THRESHOLD=0.85                # Minimum constitutional compliance score (0.0-1.0)
POLICY_QUALITY_THRESHOLD=0.80                         # Minimum policy quality score (0.0-1.0)
BIAS_DETECTION_THRESHOLD=0.15                         # Maximum acceptable bias score (0.0-1.0)

# Workflow Iteration Limits
MAX_SYNTHESIS_LOOPS=3                                 # Maximum policy synthesis iterations
MAX_REFINEMENT_ITERATIONS=5                           # Maximum amendment refinement iterations
MAX_CORRECTION_ATTEMPTS=3                             # Maximum error correction attempts

# LangGraph Monitoring and Alerting
ENABLE_PERFORMANCE_MONITORING=true                    # Enable LangGraph performance monitoring
ENABLE_CONSTITUTIONAL_ALERTS=true                     # Enable constitutional compliance alerts
ALERT_WEBHOOK_URL="your_webhook_url_here"             # Webhook URL for critical alerts

# LangGraph Debug and Development
LANGGRAPH_DEBUG_MODE=false                            # Enable LangGraph debug mode
LANGGRAPH_LOG_LEVEL=INFO                              # Log level for LangGraph workflows
ENABLE_WORKFLOW_TRACING=true                          # Enable detailed workflow tracing

# =============================================================================
# WINA (Weight Informed Neuron Activation) Configuration
# =============================================================================

# WINA Core Configuration
WINA_ENABLED=true                                     # Enable WINA optimization
WINA_TARGET_SPARSITY=0.6                             # Target sparsity ratio (0.0-1.0)
WINA_GFLOPS_REDUCTION_TARGET=0.5                     # Target GFLOPs reduction ratio (0.0-1.0)
WINA_ACCURACY_THRESHOLD=0.95                         # Minimum acceptable accuracy (0.0-1.0)
WINA_MODE=inference_only                             # WINA operation mode (inference_only, training_aware, constitutional_guided)
WINA_SPARSITY_STRATEGY=layer_specific                # Sparsity strategy (global_uniform, layer_specific, adaptive_dynamic)

# WINA Feature Toggles
WINA_ENABLE_SVD=true                                 # Enable SVD-based transformation
WINA_ENABLE_GATING=true                              # Enable dynamic runtime gating
WINA_ENABLE_MONITORING=true                          # Enable performance monitoring
WINA_ENABLE_CONSTITUTIONAL=true                      # Enable constitutional compliance checking

# WINA Integration Configuration
WINA_GS_ENGINE_OPT=true                              # Enable GS Engine LLM optimization
WINA_EC_LAYER_OPT=true                               # Enable EC Layer oversight optimization
WINA_PGC_ENHANCEMENT=true                            # Enable PGC enforcement enhancement
WINA_CONSTITUTIONAL_UPDATES=true                     # Enable constitutional principle updates

# WINA Performance Monitoring
WINA_METRICS_INTERVAL=60                             # Metrics collection interval in seconds
WINA_PROMETHEUS_METRICS=true                         # Enable Prometheus metrics export
WINA_DETAILED_LOGGING=true                           # Enable detailed performance logging

# WINA Advanced Configuration
WINA_SVD_RANK_REDUCTION=0.8                         # SVD rank reduction ratio (0.0-1.0)
WINA_GATING_THRESHOLD=0.1                            # Gating threshold for neuron activation
WINA_CACHE_TRANSFORMED_WEIGHTS=true                  # Cache transformed weights for efficiency
WINA_PARALLEL_PROCESSING=true                        # Enable parallel processing
WINA_BATCH_OPTIMIZATION=true                         # Enable batch optimization

# =============================================================================
# MCP Server Configuration (for development tools)
# =============================================================================

# GitHub Integration
GITHUB_PERSONAL_ACCESS_TOKEN="your_github_pat_here"  # Required for GitHub MCP server integration

# Search and Browser Tools
BRAVE_API_KEY="your_brave_api_key_here"              # Required for Brave Search MCP server
POWER_API_KEY="your_power_api_key_here"              # Required for Power MCP server