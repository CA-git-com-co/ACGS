"""
policy_synthesizer.py

This module defines the PolicySynthesizer, responsible for generating,
suggesting, and refining policies (both constitutional principles and
operational rules) using various techniques, including LLMs and potentially
formal synthesis methods.

Classes:
    PolicySynthesizer: Orchestrates policy synthesis.
    LLMPolicyGenerator: Uses an LLM to generate policy suggestions.
    # Future: FormalPolicySynthesizer, RuleBasedRefiner, etc.
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union

from integrations.alphaevolve_engine.core.constitutional_principle import (
    ConstitutionalPrinciple,
)
from integrations.alphaevolve_engine.core.operational_rule import OperationalRule
from integrations.alphaevolve_engine.services.llm_service import (
    LLMService,
    get_llm_service,
)
from integrations.alphaevolve_engine.utils.logging_utils import setup_logger

# from alphaevolve_gs_engine.services.validation.syntactic_validator import SyntacticValidator
# from alphaevolve_gs_engine.services.validation.semantic_validator import SemanticValidator, SemanticTestCase
# from alphaevolve_gs_engine.services.validation.safety_validator import SafetyValidator, SafetyAssertion
# Other validators can be added for iterative refinement.

logger = setup_logger(__name__)


class PolicySynthesisInput:
    """
    Represents the input for a policy synthesis task.

    Attributes:
        synthesis_goal (str): A natural language description of the desired policy's objective.
        policy_type (str): "constitutional_principle" or "operational_rule".
        existing_policies (Optional[List[Union[ConstitutionalPrinciple, OperationalRule]]]):
            Relevant existing policies that the new policy should be consistent with or build upon.
        constraints (Optional[List[str]]): Specific constraints or requirements for the new policy.
        context_data (Optional[Dict[str, Any]]): Additional contextual information.
        desired_format (str): "rego", "natural_language_structured", etc.
        target_id (Optional[str]): If refining an existing policy, its ID.
    """

    def __init__(
        self,
        synthesis_goal: str,
        policy_type: str,  # "constitutional_principle" or "operational_rule"
        desired_format: str = "rego",
        existing_policies: Optional[
            List[Union[ConstitutionalPrinciple, OperationalRule]]
        ] = None,
        constraints: Optional[List[str]] = None,
        context_data: Optional[Dict[str, Any]] = None,
        target_id: Optional[str] = None,
    ):  # For refinement
        self.synthesis_goal = synthesis_goal
        if policy_type not in ["constitutional_principle", "operational_rule"]:
            raise ValueError(
                "policy_type must be 'constitutional_principle' or 'operational_rule'."
            )
        self.policy_type = policy_type
        self.desired_format = desired_format
        self.existing_policies = existing_policies if existing_policies else []
        self.constraints = constraints if constraints else []
        self.context_data = context_data if context_data else {}
        self.target_id = target_id

    def __repr__(self) -> str:
        return (
            f"PolicySynthesisInput(goal='{self.synthesis_goal[:50]}...', "
            f"type='{self.policy_type}', format='{self.desired_format}')"
        )


class PolicySuggestion:
    """
    Represents a policy suggestion generated by a synthesizer.

    Attributes:
        suggested_policy_code (str): The generated policy code (e.g., Rego).
        explanation (str): An explanation of how the policy meets the goal.
        confidence_score (Optional[float]): Synthesizer's confidence in the suggestion.
        source_synthesizer (str): Name of the synthesizer that generated this.
        metadata (Optional[Dict[str, Any]]): Additional metadata.
    """

    def __init__(
        self,
        suggested_policy_code: str,
        explanation: str,
        source_synthesizer: str,
        confidence_score: Optional[float] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        self.suggested_policy_code = suggested_policy_code
        self.explanation = explanation
        self.source_synthesizer = source_synthesizer
        self.confidence_score = confidence_score
        self.metadata = metadata if metadata else {}

    def __repr__(self) -> str:
        return (
            f"PolicySuggestion(source='{self.source_synthesizer}', "
            f"confidence={self.confidence_score or 'N/A'}, "
            f"code_snippet='{self.suggested_policy_code[:100].strip()}...')"
        )


class PolicySynthesizer(ABC):
    """
    Abstract base class for policy synthesizers.
    """

    @abstractmethod
    def synthesize_policy(
        self, s_input: PolicySynthesisInput
    ) -> Optional[PolicySuggestion]:
        """
        Generates or refines a policy based on the synthesis input.

        Args:
            s_input (PolicySynthesisInput): The requirements for the policy.

        Returns:
            Optional[PolicySuggestion]: The suggested policy, or None if synthesis fails.
        """


class LLMPolicyGenerator(PolicySynthesizer):
    """
    Uses a Large Language Model (LLM) to generate policy suggestions.
    """

    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        logger.info(
            f"LLMPolicyGenerator initialized with LLM service: {type(llm_service).__name__}"
        )

    def _construct_prompt(self, s_input: PolicySynthesisInput) -> str:
        """Constructs a detailed prompt for the LLM."""
        prompt = f"You are an expert AI policy engineer. Your task is to generate a new {s_input.policy_type}."
        if s_input.target_id:
            prompt += f" This is a refinement of an existing policy with ID '{s_input.target_id}'."
        prompt += f"\nThe primary goal for this policy is: {s_input.synthesis_goal}\n"

        prompt += f"The desired output format for the policy code is: {s_input.desired_format}.\n"
        if s_input.desired_format == "rego":
            prompt += (
                "Ensure the Rego code is syntactically correct, follows best practices, "
                "and includes a clear package declaration (e.g., `package system.generated_policies`).\n"
            )

        if s_input.existing_policies:
            prompt += "\nConsider the following existing policies for context and consistency:\n"
            for i, policy in enumerate(s_input.existing_policies):
                policy_id = getattr(
                    policy,
                    "principle_id",
                    getattr(policy, "rule_id", f"UnknownPolicy_{i}"),
                )
                policy_desc = getattr(policy, "description", "N/A")
                prompt += f"- Policy ID {policy_id}: {policy_desc}\n"  # Could include code snippets too

        if s_input.constraints:
            prompt += "\nThe new policy MUST adhere to the following constraints:\n"
            for constraint in s_input.constraints:
                prompt += f"- {constraint}\n"

        if s_input.context_data:
            prompt += "\nAdditional context to consider:\n"
            for key, value in s_input.context_data.items():
                prompt += (
                    f"- {key}: {str(value)[:200]}\n"  # Truncate long context values
                )

        prompt += "\nPlease provide the generated policy code and a brief explanation of how it achieves the goal."
        prompt += "\nStructure your response as follows:\n"
        prompt += (
            "POLICY_CODE_START\n[Your generated policy code here]\nPOLICY_CODE_END\n"
        )
        prompt += "EXPLANATION_START\n[Your explanation here]\nEXPLANATION_END\n"

        # Optional: Add a request for confidence score if the LLM supports it or can be prompted for it.
        # prompt += "CONFIDENCE_SCORE_START\n[A score from 0.0 to 1.0 indicating your confidence]\nCONFIDENCE_SCORE_END\n"

        return prompt

    def _parse_llm_response(
        self, llm_output: str
    ) -> Tuple[Optional[str], Optional[str], Optional[float]]:
        """Parses the LLM's response to extract code, explanation, and confidence."""
        policy_code = None
        explanation = None
        confidence = None  # Not reliably extracted without specific LLM features

        try:
            code_match = re.search(
                r"POLICY_CODE_START\n(.*?)POLICY_CODE_END", llm_output, re.DOTALL
            )
            if code_match:
                policy_code = code_match.group(1).strip()

            explanation_match = re.search(
                r"EXPLANATION_START\n(.*?)EXPLANATION_END", llm_output, re.DOTALL
            )
            if explanation_match:
                explanation = explanation_match.group(1).strip()

            # Example for confidence (if LLM was prompted and provided it)
            # confidence_match = re.search(r"CONFIDENCE_SCORE_START\n(.*?)\nCONFIDENCE_SCORE_END", llm_output, re.DOTALL)
            # if confidence_match:
            #     try:
            #         confidence = float(confidence_match.group(1).strip())
            #     except ValueError:
            #         logger.warning(f"Could not parse confidence score from LLM output: {confidence_match.group(1).strip()}")

        except Exception as e:
            logger.error(
                f"Error parsing LLM response: {e}. Raw output: {llm_output[:300]}",
                exc_info=True,
            )
            return None, None, None  # Return None for all if parsing fails badly

        if not policy_code and not explanation:  # If nothing was parsed
            logger.warning(
                f"Could not parse policy code or explanation from LLM response. Using full output as explanation. Output: {llm_output[:300]}"
            )
            # Fallback: Use the whole output as explanation if parsing fails completely, no code.
            return None, llm_output, None

        return policy_code, explanation, confidence

    def synthesize_policy(
        self, s_input: PolicySynthesisInput
    ) -> Optional[PolicySuggestion]:
        prompt = self._construct_prompt(s_input)
        logger.debug(
            f"LLM Prompt for policy synthesis ({s_input.policy_type}, goal: '{s_input.synthesis_goal}'):\n{prompt}"
        )

        try:
            # Using generate_text for now. If LLM supports structured output (JSON mode),
            # that would be more robust for parsing policy_code and explanation.
            # This might require a model that supports JSON mode and careful prompting.
            # For instance, asking the LLM to return a JSON object:
            # {"policy_code": "...", "explanation": "...", "confidence_score": 0.X}
            # llm_response_raw = self.llm_service.generate_structured_output(
            #     prompt,
            #     output_format={"policy_code": "string", "explanation": "string", "confidence_score": "float"}
            # )
            # policy_code = llm_response_raw.get("policy_code")
            # explanation = llm_response_raw.get("explanation")
            # confidence = llm_response_raw.get("confidence_score")

            # Using simple text generation and parsing for broader compatibility:
            llm_raw_text_response = self.llm_service.generate_text(
                prompt, max_tokens=2048, temperature=0.5
            )  # Adjust tokens/temp as needed

            policy_code, explanation, confidence = self._parse_llm_response(
                llm_raw_text_response
            )

            if not policy_code:
                logger.error(
                    f"LLM failed to generate policy code for goal: {s_input.synthesis_goal}. Raw response: {llm_raw_text_response[:500]}"
                )
                # Fallback to using the raw response as explanation if code is missing
                if not explanation:
                    explanation = f"LLM response did not yield parseable code: {llm_raw_text_response[:500]}"
                return PolicySuggestion(
                    suggested_policy_code="",  # Empty code
                    explanation=explanation,
                    source_synthesizer=type(self).__name__,
                    confidence_score=0.0,  # Low confidence if no code
                )

            if not explanation:  # If code is there but no explanation
                explanation = "LLM generated policy code but explanation was not parsable or provided."
                logger.warning(
                    explanation + f" Raw response: {llm_raw_text_response[:500]}"
                )

            suggestion = PolicySuggestion(
                suggested_policy_code=policy_code,
                explanation=explanation,
                confidence_score=confidence,  # May be None
                source_synthesizer=type(self).__name__,
            )
            logger.info(
                f"LLM generated policy suggestion for goal: '{s_input.synthesis_goal}'. "
                f"Code snippet: {policy_code[:100].strip()}..."
            )
            return suggestion

        except Exception as e:
            logger.error(
                f"Error during LLM policy synthesis for goal '{s_input.synthesis_goal}': {e}",
                exc_info=True,
            )
            return None


# Example Usage:
if __name__ == "__main__":
    import re  # For parsing in the main example, if not using the class's internal parsing

    # Use MockLLMService for example to avoid actual API calls
    mock_llm = get_llm_service("mock")

    # Initialize the synthesizer
    llm_synthesizer = LLMPolicyGenerator(llm_service=mock_llm)

    # --- Example 1: Synthesize a new Operational Rule in Rego ---
    print("\n--- Example 1: New Operational Rule (Rego) ---")
    op_rule_input = PolicySynthesisInput(
        synthesis_goal="Create an operational rule that denies access to users from 'external_domain.com' "
        "if they try to access resources tagged as 'confidential'.",
        policy_type="operational_rule",
        desired_format="rego",
        constraints=[
            "The rule must be in a package named 'company.access_control'.",
            "The denial reason should be logged (conceptually).",
        ],
        context_data={"relevant_tags": ["confidential", "internal", "public"]},
    )

    op_rule_suggestion = llm_synthesizer.synthesize_policy(op_rule_input)

    if op_rule_suggestion:
        print(f"Suggestion Source: {op_rule_suggestion.source_synthesizer}")
        print(
            f"Confidence: {op_rule_suggestion.confidence_score if op_rule_suggestion.confidence_score is not None else 'N/A'}"
        )
        print("Suggested Policy Code (Rego):")
        print(op_rule_suggestion.suggested_policy_code)
        print("\nExplanation:")
        print(op_rule_suggestion.explanation)
        # Basic assertion for mock - mock LLM usually includes "Mock response" and "Rego code"
        assert (
            "Mock response" in op_rule_suggestion.explanation
            or "mock policy" in op_rule_suggestion.suggested_policy_code.lower()
        )
        assert (
            "package company.access_control" in op_rule_suggestion.suggested_policy_code
        )  # Check if package constraint was met
    else:
        print("Policy synthesis failed for the operational rule.")

    # --- Example 2: Synthesize a Constitutional Principle (Natural Language) ---
    print("\n--- Example 2: New Constitutional Principle (Natural Language) ---")
    # For this, the MockLLMService might not give a great "natural language" policy.
    # A real LLM would be better. We'll adjust the prompt slightly for the mock.
    cp_input = PolicySynthesisInput(
        synthesis_goal="Draft a constitutional principle ensuring AI systems prioritize human well-being "
        "and safety above operational efficiency.",
        policy_type="constitutional_principle",
        desired_format="structured_natural_language",  # LLM should output text
        existing_policies=[],  # No existing policies for this simple example
        constraints=["The principle should be concise and unambiguous."],
    )
    # Adjust prompt for mock LLM to guide it better for non-Rego
    original_construct_prompt = llm_synthesizer._construct_prompt

    def mock_construct_prompt_for_nl(s_input: PolicySynthesisInput):
        base_prompt = original_construct_prompt(s_input)
        if s_input.desired_format == "structured_natural_language":
            return base_prompt.replace(
                "Ensure the Rego code is syntactically correct",
                "Ensure the principle is clearly articulated in natural language",
            ).replace("package system.generated_policies", "a clear title or heading")
        return base_prompt

    llm_synthesizer._construct_prompt = (
        mock_construct_prompt_for_nl  # Temporarily override
    )

    cp_suggestion = llm_synthesizer.synthesize_policy(cp_input)

    llm_synthesizer._construct_prompt = original_construct_prompt  # Restore original

    if cp_suggestion:
        print(f"Suggestion Source: {cp_suggestion.source_synthesizer}")
        print("Suggested Principle Text:")
        print(cp_suggestion.suggested_policy_code)  # This will be natural language text
        print("\nExplanation:")
        print(cp_suggestion.explanation)
        assert (
            "Mock response" in cp_suggestion.explanation
            or "mock policy" in cp_suggestion.suggested_policy_code.lower()
        )
        assert (
            "human well-being" in cp_suggestion.suggested_policy_code.lower()
            or "human well-being" in cp_suggestion.explanation.lower()
        )  # Check if goal was addressed
    else:
        print("Policy synthesis failed for the constitutional principle.")

    # --- Example 3: Refining an existing policy (conceptual) ---
    print("\n--- Example 3: Refine an Existing Policy (Conceptual) ---")
    refinement_input = PolicySynthesisInput(
        target_id="OPR001_Old",
        synthesis_goal="Modify the existing operational rule 'OPR001_Old' to also include a check "
        "for 'high_sensitivity' resource tag, in addition to 'confidential'.",
        policy_type="operational_rule",
        desired_format="rego",
        # existing_policies could include the actual code of OPR001_Old if helpful
        constraints=[
            "Maintain existing functionality for 'confidential' tag.",
            "Update package name to 'company.access_control_v2'",
        ],
    )

    ref_suggestion = llm_synthesizer.synthesize_policy(refinement_input)
    if ref_suggestion:
        print("Refined Policy Code Suggestion:")
        print(ref_suggestion.suggested_policy_code)
        print("\nExplanation:")
        print(ref_suggestion.explanation)
        assert (
            "package company.access_control_v2" in ref_suggestion.suggested_policy_code
        )
        assert (
            "high_sensitivity" in ref_suggestion.suggested_policy_code
            or "high_sensitivity" in ref_suggestion.explanation
        )
    else:
        print("Policy refinement failed.")

    print("\nPolicy synthesizer examples completed.")
