#!/usr/bin/env python3
"""
ACGS Phase 3 Production Infrastructure Setup Script
Sets up production environment with monitoring, alerting, and deployment validation.

Production Requirements:
- 16GB+ RAM, 100GB+ SSD storage
- Blue-green deployment strategy
- Prometheus/Grafana/AlertManager monitoring
- SSL/TLS configuration and firewall hardening
- Alerting thresholds: 25ms latency, 80% cache hit rate, 85% memory/80% CPU
"""

import argparse
import asyncio
import json
import logging
import os
import subprocess
import sys
from datetime import UTC, datetime

import psutil

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/phase3_production_setup.log"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


class ACGSProductionInfrastructure:
    def __init__(self):
        self.services = {
            "auth_service": {"port": 8000, "name": "Auth Service"},
            "ac_service": {"port": 8001, "name": "AC Service"},
            "integrity_service": {"port": 8002, "name": "Integrity Service"},
            "fv_service": {"port": 8003, "name": "FV Service"},
            "gs_service": {"port": 8004, "name": "GS Service"},
            "pgc_service": {"port": 8005, "name": "PGC Service"},
        }

        self.production_requirements = {
            "min_memory_gb": 16,
            "min_storage_gb": 100,
            "min_cpu_cores": 4,
            "max_latency_ms": 25,
            "min_cache_hit_rate": 0.80,
            "max_memory_usage": 0.85,
            "max_cpu_usage": 0.80,
        }

        self.monitoring_config = {
            "prometheus_port": 9090,
            "grafana_port": 3002,
            "alertmanager_port": 9093,
            "metrics_scrape_interval": "15s",
            "alert_evaluation_interval": "15s",
        }

        self.results = {
            "setup_start": datetime.now(UTC).isoformat(),
            "infrastructure_checks": {},
            "monitoring_setup": {},
            "security_hardening": {},
            "deployment_validation": {},
            "production_readiness": {},
            "overall_status": "PENDING",
        }

    def check_system_requirements(self) -> bool:
        """Check if system meets production requirements."""
        logger.info("üîç Checking production system requirements...")

        checks = {}
        all_passed = True

        # Check CPU cores
        cpu_cores = psutil.cpu_count(logical=False)
        cpu_passed = cpu_cores >= self.production_requirements["min_cpu_cores"]
        checks["cpu_cores"] = {
            "required": self.production_requirements["min_cpu_cores"],
            "actual": cpu_cores,
            "passed": cpu_passed,
        }
        all_passed &= cpu_passed
        logger.info(
            f"CPU Cores: {cpu_cores} (Required: ‚â•{self.production_requirements['min_cpu_cores']}) - {'‚úÖ PASSED' if cpu_passed else '‚ùå FAILED'}"
        )

        # Check memory
        memory_info = psutil.virtual_memory()
        memory_gb = memory_info.total / (1024**3)
        memory_passed = memory_gb >= self.production_requirements["min_memory_gb"]
        checks["memory_gb"] = {
            "required": self.production_requirements["min_memory_gb"],
            "actual": round(memory_gb, 1),
            "passed": memory_passed,
        }
        all_passed &= memory_passed
        logger.info(
            f"Memory: {memory_gb:.1f}GB (Required: ‚â•{self.production_requirements['min_memory_gb']}GB) - {'‚úÖ PASSED' if memory_passed else '‚ùå FAILED'}"
        )

        # Check disk space
        disk_info = psutil.disk_usage("/")
        disk_gb = disk_info.free / (1024**3)
        disk_passed = disk_gb >= self.production_requirements["min_storage_gb"]
        checks["storage_gb"] = {
            "required": self.production_requirements["min_storage_gb"],
            "actual": round(disk_gb, 1),
            "passed": disk_passed,
        }
        all_passed &= disk_passed
        logger.info(
            f"Available Storage: {disk_gb:.1f}GB (Required: ‚â•{self.production_requirements['min_storage_gb']}GB) - {'‚úÖ PASSED' if disk_passed else '‚ùå FAILED'}"
        )

        # Check Docker availability
        try:
            result = subprocess.run(
                ["docker", "--version"], capture_output=True, text=True, timeout=10
            )
            docker_passed = result.returncode == 0
            checks["docker"] = {
                "required": True,
                "actual": docker_passed,
                "passed": docker_passed,
                "version": result.stdout.strip() if docker_passed else "Not available",
            }
            all_passed &= docker_passed
            logger.info(
                f"Docker: {'‚úÖ AVAILABLE' if docker_passed else '‚ùå NOT AVAILABLE'}"
            )
        except Exception as e:
            checks["docker"] = {
                "required": True,
                "actual": False,
                "passed": False,
                "error": str(e),
            }
            all_passed = False
            logger.error(f"Docker: ‚ùå NOT AVAILABLE - {e}")

        self.results["infrastructure_checks"] = checks
        return all_passed

    def setup_monitoring_stack(self) -> bool:
        """Set up Prometheus, Grafana, and AlertManager monitoring."""
        logger.info("üìä Setting up monitoring stack...")

        monitoring_results = {}

        # Create monitoring configuration directories
        try:
            os.makedirs("monitoring/prometheus", exist_ok=True)
            os.makedirs("monitoring/grafana", exist_ok=True)
            os.makedirs("monitoring/alertmanager", exist_ok=True)

            # Create Prometheus configuration
            prometheus_config = {
                "global": {
                    "scrape_interval": self.monitoring_config[
                        "metrics_scrape_interval"
                    ],
                    "evaluation_interval": self.monitoring_config[
                        "alert_evaluation_interval"
                    ],
                },
                "scrape_configs": [
                    {
                        "job_name": "acgs-services",
                        "static_configs": [
                            {
                                "targets": [
                                    f"localhost:{port}"
                                    for port in [8000, 8001, 8002, 8003, 8004, 8005]
                                ]
                            }
                        ],
                        "metrics_path": "/metrics",
                        "scrape_interval": "15s",
                    },
                    {
                        "job_name": "node-exporter",
                        "static_configs": [{"targets": ["localhost:9100"]}],
                    },
                ],
                "rule_files": ["alert_rules.yml"],
                "alerting": {
                    "alertmanagers": [
                        {
                            "static_configs": [
                                {
                                    "targets": [
                                        f'localhost:{self.monitoring_config["alertmanager_port"]}'
                                    ]
                                }
                            ]
                        }
                    ]
                },
            }

            # Save Prometheus config
            import yaml

            with open("monitoring/prometheus/prometheus.yml", "w") as f:
                yaml.dump(prometheus_config, f, default_flow_style=False)

            monitoring_results["prometheus_config"] = {
                "status": "created",
                "path": "monitoring/prometheus/prometheus.yml",
            }
            logger.info("‚úÖ Prometheus configuration created")

        except Exception as e:
            monitoring_results["prometheus_config"] = {
                "status": "failed",
                "error": str(e),
            }
            logger.error(f"‚ùå Failed to create Prometheus config: {e}")

        # Create AlertManager configuration
        try:
            alertmanager_config = {
                "global": {
                    "smtp_smarthost": "localhost:587",
                    "smtp_from": "alerts@acgs.local",
                },
                "route": {
                    "group_by": ["alertname"],
                    "group_wait": "10s",
                    "group_interval": "10s",
                    "repeat_interval": "1h",
                    "receiver": "web.hook",
                },
                "receivers": [
                    {
                        "name": "web.hook",
                        "webhook_configs": [
                            {
                                "url": "http://localhost:5001/webhook",
                                "send_resolved": True,
                            }
                        ],
                    }
                ],
            }

            with open("monitoring/alertmanager/alertmanager.yml", "w") as f:
                yaml.dump(alertmanager_config, f, default_flow_style=False)

            monitoring_results["alertmanager_config"] = {
                "status": "created",
                "path": "monitoring/alertmanager/alertmanager.yml",
            }
            logger.info("‚úÖ AlertManager configuration created")

        except Exception as e:
            monitoring_results["alertmanager_config"] = {
                "status": "failed",
                "error": str(e),
            }
            logger.error(f"‚ùå Failed to create AlertManager config: {e}")

        # Create alert rules
        try:
            alert_rules = {
                "groups": [
                    {
                        "name": "acgs-alerts",
                        "rules": [
                            {
                                "alert": "HighLatency",
                                "expr": 'http_request_duration_seconds{quantile="0.95"} > 0.025',
                                "for": "2m",
                                "labels": {"severity": "warning"},
                                "annotations": {
                                    "summary": "High latency detected",
                                    "description": "95th percentile latency is above 25ms for {{ $labels.instance }}",
                                },
                            },
                            {
                                "alert": "HighMemoryUsage",
                                "expr": "node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < 0.15",
                                "for": "5m",
                                "labels": {"severity": "warning"},
                                "annotations": {
                                    "summary": "High memory usage",
                                    "description": "Memory usage is above 85% for {{ $labels.instance }}",
                                },
                            },
                            {
                                "alert": "ServiceDown",
                                "expr": "up == 0",
                                "for": "1m",
                                "labels": {"severity": "critical"},
                                "annotations": {
                                    "summary": "Service is down",
                                    "description": "Service {{ $labels.instance }} is down",
                                },
                            },
                        ],
                    }
                ]
            }

            with open("monitoring/prometheus/alert_rules.yml", "w") as f:
                yaml.dump(alert_rules, f, default_flow_style=False)

            monitoring_results["alert_rules"] = {
                "status": "created",
                "path": "monitoring/prometheus/alert_rules.yml",
            }
            logger.info("‚úÖ Alert rules created")

        except Exception as e:
            monitoring_results["alert_rules"] = {"status": "failed", "error": str(e)}
            logger.error(f"‚ùå Failed to create alert rules: {e}")

        self.results["monitoring_setup"] = monitoring_results
        return all(
            result.get("status") == "created" for result in monitoring_results.values()
        )

    def create_production_docker_compose(self) -> bool:
        """Create production Docker Compose configuration."""
        logger.info("üê≥ Creating production Docker Compose configuration...")

        try:
            docker_compose_prod = {
                "version": "3.8",
                "services": {
                    "prometheus": {
                        "image": "prom/prometheus:latest",
                        "container_name": "acgs-prometheus-prod",
                        "ports": [f'{self.monitoring_config["prometheus_port"]}:9090'],
                        "volumes": [
                            "./monitoring/prometheus:/etc/prometheus",
                            "prometheus_data:/prometheus",
                        ],
                        "command": [
                            "--config.file=/etc/prometheus/prometheus.yml",
                            "--storage.tsdb.path=/prometheus",
                            "--web.console.libraries=/etc/prometheus/console_libraries",
                            "--web.console.templates=/etc/prometheus/consoles",
                            "--storage.tsdb.retention.time=200h",
                            "--web.enable-lifecycle",
                        ],
                        "restart": "unless-stopped",
                    },
                    "grafana": {
                        "image": "grafana/grafana:latest",
                        "container_name": "acgs-grafana-prod",
                        "ports": [f'{self.monitoring_config["grafana_port"]}:3000'],
                        "volumes": [
                            "grafana_data:/var/lib/grafana",
                            "./monitoring/grafana:/etc/grafana/provisioning",
                        ],
                        "environment": [
                            "GF_SECURITY_ADMIN_PASSWORD=acgs_admin_2024",
                            "GF_USERS_ALLOW_SIGN_UP=false",
                        ],
                        "restart": "unless-stopped",
                    },
                    "alertmanager": {
                        "image": "prom/alertmanager:latest",
                        "container_name": "acgs-alertmanager-prod",
                        "ports": [
                            f'{self.monitoring_config["alertmanager_port"]}:9093'
                        ],
                        "volumes": ["./monitoring/alertmanager:/etc/alertmanager"],
                        "command": [
                            "--config.file=/etc/alertmanager/alertmanager.yml",
                            "--storage.path=/alertmanager",
                            "--web.external-url=http://localhost:9093",
                        ],
                        "restart": "unless-stopped",
                    },
                    "node-exporter": {
                        "image": "prom/node-exporter:latest",
                        "container_name": "acgs-node-exporter-prod",
                        "ports": ["9100:9100"],
                        "volumes": [
                            "/proc:/host/proc:ro",
                            "/sys:/host/sys:ro",
                            "/:/rootfs:ro",
                        ],
                        "command": [
                            "--path.procfs=/host/proc",
                            "--path.rootfs=/rootfs",
                            "--path.sysfs=/host/sys",
                            "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)",
                        ],
                        "restart": "unless-stopped",
                    },
                },
                "volumes": {"prometheus_data": {}, "grafana_data": {}},
                "networks": {"acgs-monitoring": {"driver": "bridge"}},
            }

            import yaml

            with open("docker-compose.monitoring.yml", "w") as f:
                yaml.dump(docker_compose_prod, f, default_flow_style=False)

            logger.info("‚úÖ Production Docker Compose configuration created")
            return True

        except Exception as e:
            logger.error(f"‚ùå Failed to create Docker Compose config: {e}")
            return False

    def validate_production_deployment(self) -> bool:
        """Validate production deployment readiness."""
        logger.info("üéØ Validating production deployment readiness...")

        validation_results = {}
        all_passed = True

        # Check service health
        import requests

        healthy_services = 0
        for _service_id, config in self.services.items():
            try:
                response = requests.get(
                    f"http://localhost:{config['port']}/health", timeout=5
                )
                if response.status_code == 200:
                    healthy_services += 1
                    logger.info(f"‚úÖ {config['name']} - Healthy")
                else:
                    logger.warning(
                        f"‚ö†Ô∏è {config['name']} - Unhealthy (Status: {response.status_code})"
                    )
            except Exception as e:
                logger.error(f"‚ùå {config['name']} - Error: {e}")

        service_health_passed = healthy_services == len(self.services)
        validation_results["service_health"] = {
            "healthy_services": healthy_services,
            "total_services": len(self.services),
            "passed": service_health_passed,
        }
        all_passed &= service_health_passed

        # Check system performance
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_percent = psutil.virtual_memory().percent

        cpu_passed = cpu_percent < (self.production_requirements["max_cpu_usage"] * 100)
        memory_passed = memory_percent < (
            self.production_requirements["max_memory_usage"] * 100
        )

        validation_results["system_performance"] = {
            "cpu_percent": cpu_percent,
            "memory_percent": memory_percent,
            "cpu_passed": cpu_passed,
            "memory_passed": memory_passed,
        }
        all_passed &= cpu_passed and memory_passed

        logger.info(
            f"CPU Usage: {cpu_percent:.1f}% (Target: <{self.production_requirements['max_cpu_usage']*100:.0f}%) - {'‚úÖ PASSED' if cpu_passed else '‚ùå FAILED'}"
        )
        logger.info(
            f"Memory Usage: {memory_percent:.1f}% (Target: <{self.production_requirements['max_memory_usage']*100:.0f}%) - {'‚úÖ PASSED' if memory_passed else '‚ùå FAILED'}"
        )

        self.results["deployment_validation"] = validation_results
        return all_passed

    def generate_production_readiness_report(self) -> bool:
        """Generate comprehensive production readiness report."""
        logger.info("üìã Generating production readiness report...")

        readiness_score = 0.0
        max_score = 4.0  # 4 main categories

        # Infrastructure checks (25%)
        if (
            self.results.get("infrastructure_checks", {})
            .get("cpu_cores", {})
            .get("passed", False)
        ):
            readiness_score += 0.25
        if (
            self.results.get("infrastructure_checks", {})
            .get("memory_gb", {})
            .get("passed", False)
        ):
            readiness_score += 0.25
        if (
            self.results.get("infrastructure_checks", {})
            .get("storage_gb", {})
            .get("passed", False)
        ):
            readiness_score += 0.25
        if (
            self.results.get("infrastructure_checks", {})
            .get("docker", {})
            .get("passed", False)
        ):
            readiness_score += 0.25

        # Monitoring setup (25%)
        monitoring_setup = self.results.get("monitoring_setup", {})
        monitoring_score = sum(
            1
            for result in monitoring_setup.values()
            if result.get("status") == "created"
        )
        readiness_score += (monitoring_score / max(len(monitoring_setup), 1)) * 1.0

        # Service health (25%)
        deployment_validation = self.results.get("deployment_validation", {})
        if deployment_validation.get("service_health", {}).get("passed", False):
            readiness_score += 1.0

        # System performance (25%)
        if deployment_validation.get("system_performance", {}).get("cpu_passed", False):
            readiness_score += 0.5
        if deployment_validation.get("system_performance", {}).get(
            "memory_passed", False
        ):
            readiness_score += 0.5

        readiness_percentage = (readiness_score / max_score) * 100
        production_ready = readiness_percentage >= 90.0

        self.results["production_readiness"] = {
            "readiness_score": readiness_score,
            "max_score": max_score,
            "readiness_percentage": readiness_percentage,
            "production_ready": production_ready,
            "requirements_met": {
                "infrastructure": all(
                    check.get("passed", False)
                    for check in self.results.get("infrastructure_checks", {}).values()
                ),
                "monitoring": len(
                    [
                        r
                        for r in self.results.get("monitoring_setup", {}).values()
                        if r.get("status") == "created"
                    ]
                )
                >= 3,
                "services": self.results.get("deployment_validation", {})
                .get("service_health", {})
                .get("passed", False),
                "performance": (
                    self.results.get("deployment_validation", {})
                    .get("system_performance", {})
                    .get("cpu_passed", False)
                    and self.results.get("deployment_validation", {})
                    .get("system_performance", {})
                    .get("memory_passed", False)
                ),
            },
        }

        self.results["overall_status"] = "READY" if production_ready else "NOT_READY"

        logger.info(f"Production Readiness Score: {readiness_percentage:.1f}%")
        logger.info(f"Production Ready: {'‚úÖ YES' if production_ready else '‚ùå NO'}")

        return production_ready

    async def setup_production_infrastructure(self) -> bool:
        """Run complete production infrastructure setup."""
        logger.info("üöÄ Starting ACGS Phase 3 Production Infrastructure Setup")
        logger.info("=" * 80)

        try:
            # Step 1: Check system requirements
            if not self.check_system_requirements():
                logger.error(
                    "‚ùå System requirements not met. Cannot proceed with production setup."
                )
                return False

            # Step 2: Set up monitoring stack
            self.setup_monitoring_stack()

            # Step 3: Create production Docker Compose
            self.create_production_docker_compose()

            # Step 4: Validate deployment
            self.validate_production_deployment()

            # Step 5: Generate readiness report
            production_ready = self.generate_production_readiness_report()

            # Step 6: Save final report
            self.save_final_report()

            return production_ready

        except Exception as e:
            logger.error(f"‚ùå Production infrastructure setup failed: {str(e)}")
            self.results["overall_status"] = "ERROR"
            self.results["error_message"] = str(e)
            return False

    def save_final_report(self):
        """Save comprehensive production setup report."""
        self.results["setup_end"] = datetime.now(UTC).isoformat()

        # Save detailed results to file
        report_file = f"logs/phase3_production_setup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, "w") as f:
            json.dump(self.results, f, indent=2)

        logger.info("=" * 80)
        logger.info("üéâ ACGS Phase 3 Production Infrastructure Setup Complete!")
        logger.info("=" * 80)
        logger.info(
            f"Overall Status: {'‚úÖ READY' if self.results['overall_status'] == 'READY' else '‚ùå NOT READY'}"
        )

        if self.results["overall_status"] == "READY":
            logger.info("üöÄ Production infrastructure is ready for deployment!")
            logger.info("Next steps:")
            logger.info(
                "  1. Start monitoring stack: docker-compose -f docker-compose.monitoring.yml up -d"
            )
            logger.info(
                "  2. Access Grafana: http://localhost:3002 (admin/acgs_admin_2024)"
            )
            logger.info("  3. Access Prometheus: http://localhost:9090")
            logger.info("  4. Deploy services with blue-green strategy")
        else:
            logger.info(
                "‚ö†Ô∏è Production infrastructure setup incomplete. Review requirements."
            )

        logger.info(f"Detailed Report: {report_file}")


async def main():
    parser = argparse.ArgumentParser(
        description="ACGS Phase 3 Production Infrastructure Setup"
    )
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="Only validate current setup without changes",
    )

    args = parser.parse_args()

    infrastructure = ACGSProductionInfrastructure()

    if args.validate_only:
        logger.info("üîç Running validation-only mode...")
        success = infrastructure.validate_production_deployment()
        infrastructure.generate_production_readiness_report()
        infrastructure.save_final_report()
    else:
        success = await infrastructure.setup_production_infrastructure()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())
