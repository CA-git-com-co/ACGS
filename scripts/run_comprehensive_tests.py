#!/usr/bin/env python3
"""
Comprehensive Test Runner for ACGS-1 Services

Executes comprehensive test suite across all 8 services with coverage reporting.
Generates detailed reports and validates >80% coverage target.

Usage:
    python scripts/run_comprehensive_tests.py [options]

Options:
    --unit-only: Run only unit tests
    --integration-only: Run only integration tests
    --performance: Include performance tests
    --coverage-report: Generate detailed coverage report
    --html-report: Generate HTML coverage report
    --json-report: Generate JSON coverage report
    --fail-under: Minimum coverage percentage (default: 80)
"""

import argparse
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Test configuration
TEST_CONFIG = {
    "target_coverage": 80,
    "services": ["auth", "ac", "integrity", "fv", "gs", "pgc", "ec", "research"],
    "test_categories": ["unit", "integration", "performance", "security"],
    "timeout_seconds": 300,
    "parallel_workers": 4,
}

# Coverage targets by service
SERVICE_COVERAGE_TARGETS = {
    "auth": 85,
    "ac": 80,
    "integrity": 75,
    "fv": 70,
    "gs": 80,
    "pgc": 85,
    "ec": 75,
    "research": 70,
}


class TestRunner:
    """Comprehensive test runner for ACGS-1 services."""

    def __init__(self, args):
        self.args = args
        self.project_root = PROJECT_ROOT
        self.test_results = {}
        self.coverage_results = {}
        self.start_time = None
        self.end_time = None

    def setup_environment(self):
        """Setup test environment and dependencies."""
        print("ğŸ”§ Setting up test environment...")

        # Set environment variables
        os.environ["TESTING"] = "true"
        os.environ["PYTHONPATH"] = str(self.project_root)
        os.environ["CONSTITUTIONAL_HASH"] = "cdd01ef066bc6cf2"

        # Create test directories
        test_dirs = [
            "tests/reports",
            "tests/logs",
            "tests/coverage",
        ]

        for test_dir in test_dirs:
            (self.project_root / test_dir).mkdir(parents=True, exist_ok=True)

        print("âœ… Test environment setup complete")

    def run_unit_tests(self) -> dict[str, Any]:
        """Run unit tests for all services."""
        print("ğŸ§ª Running unit tests...")

        unit_test_commands = [
            "python -m pytest tests/unit/ -v --tb=short",
            "python -m pytest tests/unit/services/ -v --tb=short",
        ]

        if self.args.coverage_report:
            unit_test_commands = [
                cmd + " --cov=services --cov=src --cov-report=term-missing"
                for cmd in unit_test_commands
            ]

        results = {}
        for i, cmd in enumerate(unit_test_commands):
            print(f"  Running unit test batch {i+1}/{len(unit_test_commands)}")
            result = self._run_command(cmd)
            results[f"unit_batch_{i+1}"] = result

        return results

    def run_integration_tests(self) -> dict[str, Any]:
        """Run integration tests."""
        print("ğŸ”— Running integration tests...")

        integration_commands = [
            "python -m pytest tests/integration/test_comprehensive_service_integration.py -v --tb=short -m integration",
            "python -m pytest tests/integration/ -v --tb=short -k 'not performance'",
        ]

        results = {}
        for i, cmd in enumerate(integration_commands):
            print(f"  Running integration test batch {i+1}/{len(integration_commands)}")
            result = self._run_command(cmd)
            results[f"integration_batch_{i+1}"] = result

        return results

    def run_performance_tests(self) -> dict[str, Any]:
        """Run performance tests."""
        if not self.args.performance:
            print("â­ï¸  Skipping performance tests (use --performance to include)")
            return {}

        print("âš¡ Running performance tests...")

        performance_commands = [
            "python -m pytest tests/unit/ -v --tb=short -m performance",
            "python -m pytest tests/integration/ -v --tb=short -m performance",
        ]

        results = {}
        for i, cmd in enumerate(performance_commands):
            print(f"  Running performance test batch {i+1}/{len(performance_commands)}")
            result = self._run_command(cmd)
            results[f"performance_batch_{i+1}"] = result

        return results

    def generate_coverage_report(self) -> dict[str, Any]:
        """Generate comprehensive coverage report."""
        if not self.args.coverage_report:
            return {}

        print("ğŸ“Š Generating coverage report...")

        coverage_commands = [
            "python -m pytest tests/ --cov=services --cov=src --cov-report=json --cov-report=term-missing",
        ]

        if self.args.html_report:
            coverage_commands.append(
                "python -m pytest tests/ --cov=services --cov=src --cov-report=html:tests/coverage/html"
            )

        results = {}
        for i, cmd in enumerate(coverage_commands):
            print(f"  Running coverage analysis {i+1}/{len(coverage_commands)}")
            result = self._run_command(cmd)
            results[f"coverage_batch_{i+1}"] = result

        # Parse coverage results
        coverage_file = self.project_root / "coverage.json"
        if coverage_file.exists():
            with open(coverage_file) as f:
                coverage_data = json.load(f)
                self.coverage_results = self._parse_coverage_data(coverage_data)
        else:
            print(
                "coverage.json not found. Run tests with coverage to generate it."
            )

        return results

    def _run_command(self, command: str) -> dict[str, Any]:
        """Run a shell command and capture results."""
        start_time = time.time()

        try:
            result = subprocess.run(
                command.split(),
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=TEST_CONFIG["timeout_seconds"],
            )

            end_time = time.time()
            duration = end_time - start_time

            return {
                "command": command,
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "duration": duration,
                "success": result.returncode == 0,
                "timestamp": datetime.utcnow().isoformat(),
            }

        except subprocess.TimeoutExpired:
            return {
                "command": command,
                "return_code": -1,
                "stdout": "",
                "stderr": f"Command timed out after {TEST_CONFIG['timeout_seconds']} seconds",
                "duration": TEST_CONFIG["timeout_seconds"],
                "success": False,
                "timestamp": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            return {
                "command": command,
                "return_code": -1,
                "stdout": "",
                "stderr": str(e),
                "duration": 0,
                "success": False,
                "timestamp": datetime.utcnow().isoformat(),
            }

    def _parse_coverage_data(self, coverage_data: dict) -> dict[str, Any]:
        """Parse coverage data and calculate service-specific coverage."""
        total_coverage = coverage_data.get("totals", {}).get("percent_covered", 0)

        service_coverage = {}
        files = coverage_data.get("files", {})

        for service in TEST_CONFIG["services"]:
            service_files = [
                file_path
                for file_path in files.keys()
                if f"/{service}_service/" in file_path or f"/{service}/" in file_path
            ]

            if service_files:
                total_statements = sum(
                    files[file_path]["summary"]["num_statements"]
                    for file_path in service_files
                )
                covered_statements = sum(
                    files[file_path]["summary"]["covered_lines"]
                    for file_path in service_files
                )

                service_coverage[service] = {
                    "coverage_percent": (
                        (covered_statements / total_statements * 100)
                        if total_statements > 0
                        else 0
                    ),
                    "total_statements": total_statements,
                    "covered_statements": covered_statements,
                    "target": SERVICE_COVERAGE_TARGETS.get(service, 80),
                    "meets_target": (
                        (covered_statements / total_statements * 100)
                        >= SERVICE_COVERAGE_TARGETS.get(service, 80)
                        if total_statements > 0
                        else False
                    ),
                }

        return {
            "total_coverage": total_coverage,
            "service_coverage": service_coverage,
            "meets_overall_target": total_coverage >= self.args.fail_under,
        }

    def generate_report(self):
        """Generate comprehensive test report."""
        print("ğŸ“‹ Generating comprehensive test report...")

        report = {
            "test_run_info": {
                "timestamp": datetime.utcnow().isoformat(),
                "duration": (
                    (self.end_time - self.start_time)
                    if self.start_time and self.end_time
                    else 0
                ),
                "target_coverage": self.args.fail_under,
                "services_tested": TEST_CONFIG["services"],
            },
            "test_results": self.test_results,
            "coverage_results": self.coverage_results,
            "summary": self._generate_summary(),
        }

        # Save JSON report
        if self.args.json_report:
            report_file = (
                self.project_root / "tests/reports/comprehensive_test_report.json"
            )
            with open(report_file, "w") as f:
                json.dump(report, f, indent=2)
            print(f"ğŸ“„ JSON report saved to: {report_file}")

        # Print summary
        self._print_summary(report["summary"])

        return report

    def _generate_summary(self) -> dict[str, Any]:
        """Generate test run summary."""
        total_tests = 0
        passed_tests = 0
        failed_tests = 0

        for _category, results in self.test_results.items():
            for _batch, result in results.items():
                if result["success"]:
                    # Parse test counts from stdout (simplified)
                    stdout = result["stdout"]
                    if "passed" in stdout:
                        # Extract test counts (this is a simplified parser)
                        passed_tests += 10  # Placeholder
                        total_tests += 10
                else:
                    failed_tests += 1
                    total_tests += 1

        return {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": (
                (passed_tests / total_tests * 100) if total_tests > 0 else 0
            ),
            "coverage_summary": self.coverage_results,
            "overall_success": (
                failed_tests == 0
                and self.coverage_results.get("meets_overall_target", False)
            ),
        }

    def _print_summary(self, summary: dict[str, Any]):
        """Print test run summary."""
        print("\n" + "=" * 80)
        print("ğŸ¯ ACGS-1 COMPREHENSIVE TEST RESULTS SUMMARY")
        print("=" * 80)

        print(
            f"ğŸ“Š Tests: {summary['total_tests']} total, {summary['passed_tests']} passed, {summary['failed_tests']} failed"
        )
        print(f"âœ… Success Rate: {summary['success_rate']:.1f}%")

        if self.coverage_results:
            total_coverage = self.coverage_results.get("total_coverage", 0)
            print(f"ğŸ“ˆ Overall Coverage: {total_coverage:.1f}%")

            if self.coverage_results.get("service_coverage"):
                print("\nğŸ” Service Coverage Breakdown:")
                for service, coverage in self.coverage_results[
                    "service_coverage"
                ].items():
                    status = "âœ…" if coverage["meets_target"] else "âŒ"
                    print(
                        f"  {status} {service.upper()}: {coverage['coverage_percent']:.1f}% (target: {coverage['target']}%)"
                    )

        overall_success = summary["overall_success"]
        status_icon = "âœ…" if overall_success else "âŒ"
        print(
            f"\n{status_icon} Overall Result: {'SUCCESS' if overall_success else 'NEEDS IMPROVEMENT'}"
        )

        if not overall_success:
            print("\nğŸ’¡ Recommendations:")
            if summary["failed_tests"] > 0:
                print("  - Fix failing tests before proceeding")
            if not self.coverage_results.get("meets_overall_target", True):
                print(
                    f"  - Increase test coverage to meet {self.args.fail_under}% target"
                )

        print("=" * 80)

    def run(self):
        """Run comprehensive test suite."""
        self.start_time = time.time()

        print("ğŸš€ Starting ACGS-1 Comprehensive Test Suite")
        print(f"ğŸ“… Timestamp: {datetime.utcnow().isoformat()}")
        print(f"ğŸ¯ Coverage Target: {self.args.fail_under}%")
        print("-" * 80)

        try:
            # Setup environment
            self.setup_environment()

            # Run test categories
            if not self.args.integration_only:
                self.test_results["unit"] = self.run_unit_tests()

            if not self.args.unit_only:
                self.test_results["integration"] = self.run_integration_tests()

            self.test_results["performance"] = self.run_performance_tests()

            # Generate coverage report
            self.test_results["coverage"] = self.generate_coverage_report()

            self.end_time = time.time()

            # Generate final report
            report = self.generate_report()

            # Exit with appropriate code
            if report["summary"]["overall_success"]:
                print("\nğŸ‰ All tests passed and coverage targets met!")
                sys.exit(0)
            else:
                print("\nâš ï¸  Some tests failed or coverage targets not met")
                sys.exit(1)

        except KeyboardInterrupt:
            print("\nâ¹ï¸  Test run interrupted by user")
            sys.exit(130)

        except Exception as e:
            print(f"\nğŸ’¥ Test run failed with error: {e}")
            sys.exit(1)


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="ACGS-1 Comprehensive Test Runner")

    parser.add_argument("--unit-only", action="store_true", help="Run only unit tests")
    parser.add_argument(
        "--integration-only", action="store_true", help="Run only integration tests"
    )
    parser.add_argument(
        "--performance", action="store_true", help="Include performance tests"
    )
    parser.add_argument(
        "--coverage-report", action="store_true", help="Generate coverage report"
    )
    parser.add_argument(
        "--html-report", action="store_true", help="Generate HTML coverage report"
    )
    parser.add_argument(
        "--json-report", action="store_true", help="Generate JSON test report"
    )
    parser.add_argument(
        "--fail-under", type=int, default=80, help="Minimum coverage percentage"
    )

    args = parser.parse_args()

    # Enable coverage report by default if HTML or JSON report requested
    if args.html_report or args.json_report:
        args.coverage_report = True

    runner = TestRunner(args)
    runner.run()


if __name__ == "__main__":
    main()
