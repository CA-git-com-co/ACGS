# NVIDIA LLM Router Configuration
# This configuration defines routing policies and model mappings for the ACGS-PGP platform

# Global Configuration
global:
  version: "1.0.0"
  environment: "${ENVIRONMENT:-development}"
  log_level: "${LLM_ROUTER_LOG_LEVEL:-INFO}"
  debug_mode: "${LLM_ROUTER_DEBUG_MODE:-false}"
  
# NVIDIA API Configuration
nvidia_api:
  base_url: "${NVIDIA_API_BASE_URL:-https://integrate.api.nvidia.com/v1}"
  api_key: "${NVIDIA_API_KEY}"
  timeout_seconds: "${REQUEST_TIMEOUT_SECONDS:-30}"
  max_retries: 3
  retry_delay_seconds: 1

# Model Definitions
models:
  # Premium Tier - High-performance models for critical tasks
  premium:
    - name: "nvidia/llama-3.1-nemotron-70b-instruct"
      endpoint: "/chat/completions"
      max_tokens: 4096
      temperature: 0.1
      capabilities: ["constitutional_analysis", "policy_synthesis", "complex_reasoning"]
      cost_tier: "high"
      latency_target_ms: 2000
      
    - name: "nvidia/llama-3.1-405b-instruct"
      endpoint: "/chat/completions"
      max_tokens: 4096
      temperature: 0.1
      capabilities: ["constitutional_analysis", "policy_synthesis", "complex_reasoning", "multi_modal"]
      cost_tier: "premium"
      latency_target_ms: 3000

  # Standard Tier - Balanced performance for general tasks
  standard:
    - name: "nvidia/llama-3.1-70b-instruct"
      endpoint: "/chat/completions"
      max_tokens: 2048
      temperature: 0.2
      capabilities: ["general_analysis", "policy_review", "content_generation"]
      cost_tier: "medium"
      latency_target_ms: 1500
      
    - name: "nvidia/llama-3.1-8b-instruct"
      endpoint: "/chat/completions"
      max_tokens: 2048
      temperature: 0.2
      capabilities: ["general_analysis", "content_generation", "summarization"]
      cost_tier: "medium"
      latency_target_ms: 1000

  # Efficient Tier - Fast models for simple tasks
  efficient:
    - name: "nvidia/llama-3.1-8b-instruct"
      endpoint: "/chat/completions"
      max_tokens: 1024
      temperature: 0.3
      capabilities: ["summarization", "classification", "simple_qa"]
      cost_tier: "low"
      latency_target_ms: 500
      
    - name: "nvidia/mistral-7b-instruct-v0.3"
      endpoint: "/chat/completions"
      max_tokens: 1024
      temperature: 0.3
      capabilities: ["summarization", "classification", "simple_qa"]
      cost_tier: "low"
      latency_target_ms: 400

# Task-Based Routing Policies
task_routing:
  # Constitutional AI Service Tasks
  constitutional_analysis:
    preferred_models: ["nvidia/llama-3.1-nemotron-70b-instruct", "nvidia/llama-3.1-405b-instruct"]
    fallback_models: ["nvidia/llama-3.1-70b-instruct"]
    min_confidence_threshold: 0.95
    require_audit_trail: true
    
  constitutional_compliance:
    preferred_models: ["nvidia/llama-3.1-nemotron-70b-instruct"]
    fallback_models: ["nvidia/llama-3.1-70b-instruct"]
    min_confidence_threshold: 0.90
    require_audit_trail: true
    
  # Governance Synthesis Service Tasks
  policy_synthesis:
    preferred_models: ["nvidia/llama-3.1-405b-instruct", "nvidia/llama-3.1-nemotron-70b-instruct"]
    fallback_models: ["nvidia/llama-3.1-70b-instruct"]
    min_confidence_threshold: 0.92
    require_multi_model_consensus: true
    
  policy_review:
    preferred_models: ["nvidia/llama-3.1-70b-instruct", "nvidia/llama-3.1-nemotron-70b-instruct"]
    fallback_models: ["nvidia/llama-3.1-8b-instruct"]
    min_confidence_threshold: 0.85
    
  # Policy Governance Compliance Tasks
  compliance_enforcement:
    preferred_models: ["nvidia/llama-3.1-nemotron-70b-instruct"]
    fallback_models: ["nvidia/llama-3.1-70b-instruct"]
    min_confidence_threshold: 0.90
    require_audit_trail: true
    
  violation_detection:
    preferred_models: ["nvidia/llama-3.1-70b-instruct", "nvidia/llama-3.1-8b-instruct"]
    fallback_models: ["nvidia/mistral-7b-instruct-v0.3"]
    min_confidence_threshold: 0.80
    
  # General Tasks
  content_generation:
    preferred_models: ["nvidia/llama-3.1-70b-instruct", "nvidia/llama-3.1-8b-instruct"]
    fallback_models: ["nvidia/mistral-7b-instruct-v0.3"]
    min_confidence_threshold: 0.75
    
  summarization:
    preferred_models: ["nvidia/llama-3.1-8b-instruct", "nvidia/mistral-7b-instruct-v0.3"]
    fallback_models: ["nvidia/llama-3.1-70b-instruct"]
    min_confidence_threshold: 0.70
    
  classification:
    preferred_models: ["nvidia/llama-3.1-8b-instruct", "nvidia/mistral-7b-instruct-v0.3"]
    fallback_models: ["nvidia/llama-3.1-70b-instruct"]
    min_confidence_threshold: 0.75

# Complexity-Based Routing Policies
complexity_routing:
  # High Complexity - Multi-step reasoning, constitutional analysis
  high:
    complexity_indicators:
      - "constitutional"
      - "multi-step"
      - "reasoning"
      - "analysis"
      - "synthesis"
      - "governance"
    min_token_count: 500
    preferred_tier: "premium"
    fallback_tier: "standard"
    
  # Medium Complexity - Policy review, content analysis
  medium:
    complexity_indicators:
      - "review"
      - "evaluate"
      - "compare"
      - "assess"
      - "policy"
    min_token_count: 200
    max_token_count: 500
    preferred_tier: "standard"
    fallback_tier: "efficient"
    
  # Low Complexity - Simple tasks, summarization
  low:
    complexity_indicators:
      - "summarize"
      - "classify"
      - "extract"
      - "list"
      - "simple"
    max_token_count: 200
    preferred_tier: "efficient"
    fallback_tier: "standard"

# Load Balancing Configuration
load_balancing:
  strategy: "least_connections"  # Options: round_robin, least_connections, weighted_round_robin
  health_check_interval_seconds: 30
  max_concurrent_requests_per_model: 10
  circuit_breaker:
    failure_threshold: 5
    recovery_timeout_seconds: 60
    half_open_max_calls: 3

# Fallback Configuration
fallback:
  enable_fallback: true
  max_fallback_attempts: 3
  fallback_delay_seconds: 1
  default_fallback_model: "nvidia/llama-3.1-8b-instruct"
  emergency_fallback_model: "nvidia/mistral-7b-instruct-v0.3"

# Caching Configuration
caching:
  enable_response_caching: true
  cache_ttl_seconds: 300
  max_cache_size_mb: 100
  cache_key_strategy: "content_hash"
  exclude_tasks: ["constitutional_analysis", "compliance_enforcement"]

# Monitoring and Metrics
monitoring:
  enable_metrics: true
  metrics_port: 9092
  health_check_port: 8082
  log_requests: true
  log_responses: false  # Set to true for debugging, false for production
  performance_tracking:
    track_latency: true
    track_throughput: true
    track_error_rates: true
    track_model_selection: true

# Security Configuration
security:
  enable_request_validation: true
  max_request_size_mb: 10
  rate_limiting:
    requests_per_minute: 100
    burst_size: 20
  api_key_rotation:
    enable_rotation: true
    rotation_interval_days: 30
    warning_days_before_expiry: 7

# ACGS-PGP Integration
acgs_integration:
  enable_constitutional_routing: "${ENABLE_CONSTITUTIONAL_ROUTING:-true}"
  constitutional_model_tier: "${CONSTITUTIONAL_MODEL_TIER:-premium}"
  policy_synthesis_model: "${POLICY_SYNTHESIS_MODEL:-nvidia/llama-3.1-nemotron-70b-instruct}"
  audit_trail:
    enable_audit_logging: true
    audit_log_level: "INFO"
    include_request_content: false  # For privacy compliance
    include_response_metadata: true
  service_integration:
    auth_service_url: "${AUTH_SERVICE_INTERNAL_URL:-http://auth_service:8000}"
    ac_service_url: "${AC_SERVICE_INTERNAL_URL:-http://ac_service:8001}"
    gs_service_url: "${GS_SERVICE_INTERNAL_URL:-http://gs_service:8004}"
    pgc_service_url: "${PGC_SERVICE_INTERNAL_URL:-http://pgc_service:8005}"
