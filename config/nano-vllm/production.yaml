# Nano-vLLM Production Configuration
service:
  name: "nano-vllm-reasoning"
  version: "1.0.0"
  environment: "production"

models:
  nvidia_acerreason:
    model_path: "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF"
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_model_len: 32768
    port: 8000
    specialties: ["governance", "accountability"]
    
  microsoft_phi4:
    model_path: "microsoft/Phi-4"
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.6
    max_model_len: 16384
    port: 8001
    specialties: ["ethics", "fairness"]

constitutional:
  principles_file: "/app/constitutional/principles.yaml"
  compliance_threshold: 0.85
  reasoning_depth: "deep"
  require_citations: true

performance:
  max_concurrent_requests: 20
  request_timeout: 120
  health_check_interval: 30
  
logging:
  level: "INFO"
  format: "structured"
  file: "/app/logs/nano-vllm-production.log"

monitoring:
  metrics_enabled: true
  prometheus_port: 9090
  health_endpoint: "/health"
