# ACGS-2 Consolidated Testing Pipeline
# Constitutional Hash: cdd01ef066bc6cf2
#
# This workflow consolidates and replaces:
# - test.yml, testing.yml, comprehensive-testing.yml
# - acgs-comprehensive-testing.yml, e2e-tests.yml, test-coverage.yml
# - acgs-e2e-testing.yml, test-automation-enhanced.yml, test-monitoring.yml

name: üß™ ACGS-2 Testing Comprehensive

on:
  push:
    branches: [main, master, develop]
    paths:
      - '**.py'
      - '**.rs'
      - '**.js'
      - '**.ts'
      - '**.go'
      - 'tests/**'
      - 'services/**'
      - 'requirements*.txt'
      - 'Cargo.toml'
      - 'package*.json'
      - 'pyproject.toml'
      
  pull_request:
    branches: [main, master, develop]
    paths:
      - '**.py'
      - '**.rs'
      - '**.js'
      - '**.ts'
      - '**.go'
      - 'tests/**'
      - 'services/**'
      - 'requirements*.txt'
      - 'Cargo.toml'
      - 'package*.json'
      - 'pyproject.toml'
      
  schedule:
    - cron: '0 4 * * *'  # Daily at 4 AM for comprehensive testing
    
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - unit
          - integration
          - e2e
          - performance
          - all

permissions:
  contents: read
  checks: write
  pull-requests: write
  id-token: write

env:
  CONSTITUTIONAL_HASH: cdd01ef066bc6cf2
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  RUST_VERSION: '1.81.0'
  MIN_COVERAGE_THRESHOLD: 85

jobs:
  # =============================================================================
  # UNIT TESTING - Fast, Isolated Tests
  # =============================================================================
  unit-tests:
    name: üß™ Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        test-group:
          - services-core
          - services-platform
          - shared-components
          - infrastructure
          
    outputs:
      coverage_percentage: ${{ steps.coverage.outputs.percentage }}
      tests_passed: ${{ steps.test-results.outputs.passed }}
      
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Python with Caching
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Cache Python Dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            .venv
          key: python-test-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            python-test-${{ env.PYTHON_VERSION }}-
            
      - name: Install Dependencies
        run: |
          echo "üì¶ Installing test dependencies..."
          pip install --upgrade pip wheel setuptools
          
          # Install security-hardened requirements
          if [ -f "requirements-security.txt" ]; then
            pip install -r requirements-security.txt
          elif [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
          fi
          
          # Install test dependencies
          pip install pytest pytest-cov pytest-asyncio pytest-xdist pytest-mock pytest-timeout
          pip install coverage[toml] pytest-html pytest-json-report
          
      - name: Constitutional Compliance Check
        run: |
          echo "üèõÔ∏è Validating constitutional compliance in tests..."
          
          # Check for constitutional hash in test files
          hash_count=$(find tests/ -name "*.py" | xargs grep -l "$CONSTITUTIONAL_HASH" | wc -l || echo "0")
          
          if [ "$hash_count" -gt 0 ]; then
            echo "‚úÖ Constitutional compliance detected in test files"
          else
            echo "‚ö†Ô∏è Warning: No constitutional compliance references in test files"
          fi
          
      - name: Run Unit Tests - ${{ matrix.test-group }}
        run: |
          echo "üß™ Running unit tests for ${{ matrix.test-group }}..."
          
          # Determine test path based on matrix group
          case "${{ matrix.test-group }}" in
            "services-core")
              test_path="tests/unit/services/core/"
              source_path="services/core/"
              ;;
            "services-platform")
              test_path="tests/unit/services/platform_services/"
              source_path="services/platform_services/"
              ;;
            "shared-components")
              test_path="tests/unit/services/shared/"
              source_path="services/shared/"
              ;;
            "infrastructure")
              test_path="tests/unit/infrastructure/"
              source_path="infrastructure/"
              ;;
            *)
              test_path="tests/unit/"
              source_path="services/"
              ;;
          esac
          
          # Run tests with coverage if path exists
          if [ -d "$test_path" ]; then
            pytest $test_path \
              --cov=$source_path \
              --cov-report=xml:coverage-${{ matrix.test-group }}.xml \
              --cov-report=term-missing \
              --cov-report=html:htmlcov-${{ matrix.test-group }} \
              --junit-xml=junit-${{ matrix.test-group }}.xml \
              --html=report-${{ matrix.test-group }}.html \
              --self-contained-html \
              -v \
              --tb=short \
              -n auto \
              --timeout=300 \
              --maxfail=10
          else
            echo "‚ö†Ô∏è Test path $test_path not found, skipping"
          fi
          
      - name: Calculate Coverage
        id: coverage
        if: always()
        run: |
          echo "üìä Calculating test coverage..."
          
          if [ -f "coverage-${{ matrix.test-group }}.xml" ]; then
            # Extract coverage percentage from XML
            coverage_pct=$(python -c "
            import xml.etree.ElementTree as ET
            try:
                tree = ET.parse('coverage-${{ matrix.test-group }}.xml')
                root = tree.getroot()
                coverage = root.attrib.get('line-rate', '0')
                print(int(float(coverage) * 100))
            except:
                print('0')
            ")
            
            echo "percentage=$coverage_pct" >> $GITHUB_OUTPUT
            echo "Coverage for ${{ matrix.test-group }}: $coverage_pct%"
            
            # Check coverage threshold
            if [ "$coverage_pct" -lt "$MIN_COVERAGE_THRESHOLD" ]; then
              echo "‚ö†Ô∏è Coverage below threshold: $coverage_pct% < $MIN_COVERAGE_THRESHOLD%"
            else
              echo "‚úÖ Coverage meets threshold: $coverage_pct% >= $MIN_COVERAGE_THRESHOLD%"
            fi
          else
            echo "percentage=0" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No coverage data available"
          fi
          
      - name: Test Results Summary
        id: test-results
        if: always()
        run: |
          echo "üìã Test results summary for ${{ matrix.test-group }}..."
          
          if [ -f "junit-${{ matrix.test-group }}.xml" ]; then
            # Parse JUnit XML for test counts
            tests=$(python -c "
            import xml.etree.ElementTree as ET
            try:
                tree = ET.parse('junit-${{ matrix.test-group }}.xml')
                root = tree.getroot()
                tests = root.attrib.get('tests', '0')
                failures = root.attrib.get('failures', '0')
                errors = root.attrib.get('errors', '0')
                print(f'Tests: {tests}, Failures: {failures}, Errors: {errors}')
                passed = int(failures) == 0 and int(errors) == 0
                print(f'Passed: {passed}')
            except Exception as e:
                print(f'Error parsing: {e}')
                print('Passed: False')
            ")
            
            echo "$tests"
            passed=$(echo "$tests" | grep "Passed:" | cut -d' ' -f2)
            echo "passed=$passed" >> $GITHUB_OUTPUT
          else
            echo "passed=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-${{ matrix.test-group }}
          path: |
            coverage-${{ matrix.test-group }}.xml
            junit-${{ matrix.test-group }}.xml
            report-${{ matrix.test-group }}.html
            htmlcov-${{ matrix.test-group }}/
            
      - name: Upload Coverage to Codecov
        uses: codecov/codecov-action@v4
        if: always()
        with:
          file: coverage-${{ matrix.test-group }}.xml
          flags: unit-tests,${{ matrix.test-group }}
          name: codecov-${{ matrix.test-group }}

  # =============================================================================
  # INTEGRATION TESTING - Service Interaction Tests
  # =============================================================================
  integration-tests:
    name: üîó Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    timeout-minutes: 30
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: acgs_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Python with Caching
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install Dependencies
        run: |
          echo "üì¶ Installing integration test dependencies..."
          pip install --upgrade pip
          
          if [ -f "requirements-security.txt" ]; then
            pip install -r requirements-security.txt
          elif [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
          fi
          
          pip install pytest pytest-asyncio httpx docker
          
      - name: Setup Test Environment
        run: |
          echo "üîß Setting up integration test environment..."
          
          # Set environment variables for testing
          export DATABASE_URL="postgresql://postgres:test_password@localhost:5432/acgs_test"
          export REDIS_URL="redis://localhost:6379/0"
          export CONSTITUTIONAL_HASH="${{ env.CONSTITUTIONAL_HASH }}"
          export TESTING=true
          
          # Create test database schemas if needed
          echo "Database URL: $DATABASE_URL"
          echo "Redis URL: $REDIS_URL"
          
      - name: Run Integration Tests
        env:
          DATABASE_URL: postgresql://postgres:test_password@localhost:5432/acgs_test
          REDIS_URL: redis://localhost:6379/0
          CONSTITUTIONAL_HASH: ${{ env.CONSTITUTIONAL_HASH }}
          TESTING: true
        run: |
          echo "üîó Running integration tests..."
          
          if [ -d "tests/integration" ]; then
            pytest tests/integration/ \
              --junit-xml=junit-integration.xml \
              --html=report-integration.html \
              --self-contained-html \
              -v \
              --tb=short \
              --timeout=600 \
              --maxfail=5
          else
            echo "‚ö†Ô∏è No integration tests found, skipping"
          fi
          
      - name: Upload Integration Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            junit-integration.xml
            report-integration.html

  # =============================================================================
  # END-TO-END TESTING - Full System Tests
  # =============================================================================
  e2e-tests:
    name: üåê End-to-End Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.event_name == 'schedule' || github.event.inputs.test_suite == 'e2e' || github.event.inputs.test_suite == 'all'
    timeout-minutes: 45
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Docker Compose Environment
        run: |
          echo "üê≥ Setting up Docker Compose environment..."
          
          # Create test environment file
          cat > .env.test << EOF
          CONSTITUTIONAL_HASH=${{ env.CONSTITUTIONAL_HASH }}
          POSTGRES_DB=acgs_test
          POSTGRES_USER=postgres
          POSTGRES_PASSWORD=test_password
          REDIS_URL=redis://redis:6379/0
          TESTING=true
          DEBUG=true
          EOF
          
          # Start services with docker-compose
          if [ -f "docker-compose.test.yml" ]; then
            docker-compose -f docker-compose.test.yml up -d
          elif [ -f "infrastructure/docker/docker-compose.acgs.yml" ]; then
            docker-compose -f infrastructure/docker/docker-compose.acgs.yml up -d
          else
            echo "‚ö†Ô∏è No docker-compose test configuration found"
          fi
          
          # Wait for services to be ready
          echo "‚è≥ Waiting for services to be ready..."
          sleep 30
          
      - name: Setup Python for E2E Tests
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install E2E Test Dependencies
        run: |
          pip install --upgrade pip
          pip install pytest pytest-asyncio httpx requests docker-compose
          
      - name: Run E2E Tests
        run: |
          echo "üåê Running end-to-end tests..."
          
          if [ -d "tests/e2e" ]; then
            pytest tests/e2e/ \
              --junit-xml=junit-e2e.xml \
              --html=report-e2e.html \
              --self-contained-html \
              -v \
              --tb=short \
              --timeout=900 \
              --maxfail=3
          else
            echo "‚ö†Ô∏è No E2E tests found, running basic health checks"
            
            # Basic health checks
            echo "ü©∫ Running basic health checks..."
            
            # Check if services are responding
            services=("constitutional-ai:8001" "integrity:8002" "api-gateway:8010")
            
            for service in "${services[@]}"; do
              service_name=$(echo $service | cut -d: -f1)
              port=$(echo $service | cut -d: -f2)
              
              echo "Checking $service_name on port $port..."
              curl -f "http://localhost:$port/health" || echo "‚ö†Ô∏è $service_name health check failed"
            done
          fi
          
      - name: Collect Service Logs
        if: always()
        run: |
          echo "üìã Collecting service logs..."
          
          # Create logs directory
          mkdir -p e2e-logs
          
          # Collect docker-compose logs
          if command -v docker-compose &> /dev/null; then
            docker-compose logs > e2e-logs/docker-compose.log 2>&1 || true
          fi
          
          # Collect individual container logs
          for container in $(docker ps --format "table {{.Names}}" | tail -n +2); do
            echo "Collecting logs for $container..."
            docker logs $container > e2e-logs/${container}.log 2>&1 || true
          done
          
      - name: Cleanup E2E Environment
        if: always()
        run: |
          echo "üßπ Cleaning up E2E environment..."
          
          if [ -f "docker-compose.test.yml" ]; then
            docker-compose -f docker-compose.test.yml down -v
          elif [ -f "infrastructure/docker/docker-compose.acgs.yml" ]; then
            docker-compose -f infrastructure/docker/docker-compose.acgs.yml down -v
          fi
          
      - name: Upload E2E Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            junit-e2e.xml
            report-e2e.html
            e2e-logs/

  # =============================================================================
  # PERFORMANCE TESTING - Load and Performance Tests
  # =============================================================================
  performance-tests:
    name: ‚ö° Performance Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.event_name == 'schedule' || github.event.inputs.test_suite == 'performance' || github.event.inputs.test_suite == 'all'
    timeout-minutes: 30
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install Performance Test Tools
        run: |
          echo "üìà Installing performance test tools..."
          pip install --upgrade pip
          pip install pytest pytest-benchmark locust httpx
          
          if [ -f "requirements-security.txt" ]; then
            pip install -r requirements-security.txt
          elif [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
          fi
          
      - name: Run Performance Tests
        run: |
          echo "‚ö° Running performance tests..."
          
          if [ -d "tests/performance" ]; then
            # Run pytest-benchmark tests
            pytest tests/performance/ \
              --benchmark-only \
              --benchmark-json=benchmark-results.json \
              --junit-xml=junit-performance.xml \
              -v \
              --tb=short
              
            echo "‚úÖ Performance tests completed"
          else
            echo "‚ö†Ô∏è No performance tests found"
          fi
          
      - name: Performance Thresholds Check
        run: |
          echo "üìä Checking performance thresholds..."
          
          if [ -f "benchmark-results.json" ]; then
            # Extract performance metrics
            python -c "
            import json
            
            with open('benchmark-results.json', 'r') as f:
                data = json.load(f)
            
            for benchmark in data.get('benchmarks', []):
                name = benchmark['name']
                stats = benchmark['stats']
                mean = stats['mean']
                
                print(f'{name}: {mean:.4f}s average')
                
                # Check against P99 target (<5ms = 0.005s)
                if mean > 0.005:
                    print(f'‚ö†Ô∏è Warning: {name} exceeds 5ms target')
                else:
                    print(f'‚úÖ {name} meets performance target')
            "
          fi
          
      - name: Upload Performance Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            benchmark-results.json
            junit-performance.xml

  # =============================================================================
  # TEST REPORTING - Comprehensive Test Results Summary
  # =============================================================================
  test-summary:
    name: üìä Test Summary Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests]
    if: always()
    timeout-minutes: 10
    
    steps:
      - name: Download All Test Artifacts
        uses: actions/download-artifact@v4
        
      - name: Generate Comprehensive Test Report
        run: |
          echo "üìä Generating comprehensive test report..."
          
          echo "# üß™ ACGS-2 Testing Report" >> $GITHUB_STEP_SUMMARY
          echo "**Constitutional Hash:** \`${{ env.CONSTITUTIONAL_HASH }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Test Run:** ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## üéØ Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status | Coverage |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|----------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result }} | ${{ needs.unit-tests.outputs.coverage_percentage }}% |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| E2E Tests | ${{ needs.e2e-tests.result }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-tests.result }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## üìà Quality Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- **Overall Coverage:** ${{ needs.unit-tests.outputs.coverage_percentage }}%" >> $GITHUB_STEP_SUMMARY
          echo "- **Coverage Threshold:** ${{ env.MIN_COVERAGE_THRESHOLD }}%" >> $GITHUB_STEP_SUMMARY
          echo "- **Constitutional Compliance:** ‚úÖ Validated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Coverage threshold check
          coverage=${{ needs.unit-tests.outputs.coverage_percentage }}
          if [ "$coverage" -ge "$MIN_COVERAGE_THRESHOLD" ]; then
            echo "- **Coverage Status:** ‚úÖ Meets threshold" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Coverage Status:** ‚ùå Below threshold" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Testing completed as part of ACGS-2 constitutional compliance framework*" >> $GITHUB_STEP_SUMMARY
          
      - name: Test Results Validation
        run: |
          echo "‚úÖ Test summary generation completed"
          echo "Constitutional Hash: ${{ env.CONSTITUTIONAL_HASH }}"
          echo "All test suites processed and documented"