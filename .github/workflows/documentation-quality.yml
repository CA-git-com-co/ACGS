name: Documentation Quality Assurance
# Constitutional Hash: cdd01ef066bc6cf2

on:
  push:
    branches: [ main, master ]
    paths:
      - 'docs/**'
      - 'README.md'
      - '**/*.md'
      - 'services/**/README.md'
      - 'tools/**/README.md'
  pull_request:
    branches: [ main, master ]
    paths:
      - 'docs/**'
      - 'README.md'
      - '**/*.md'
      - 'services/**/README.md'
      - 'tools/**/README.md'
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      check_level:
        description: 'Quality check level'
        required: true
        default: 'standard'
        type: choice
        options:
          - basic
          - standard
          - comprehensive
          - full-audit

env:
  CONSTITUTIONAL_HASH: "cdd01ef066bc6cf2"
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"

jobs:
  documentation-structure:
    name: Documentation Structure Analysis
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install PyYAML markdown beautifulsoup4 lxml gitpython
          
      - name: Analyze documentation structure
        run: |
          cat > analyze_docs_structure.py << 'EOF'
          import os
          import json
          import re
          from pathlib import Path
          from collections import defaultdict
          
          def analyze_documentation_structure():
              """Analyze the documentation structure and completeness."""
              
              repo_root = Path('.')
              docs_analysis = {
                  'constitutional_hash': '${{ env.CONSTITUTIONAL_HASH }}',
                  'structure': {},
                  'completeness': {},
                  'quality_metrics': {},
                  'recommendations': []
              }
              
              # Find all markdown files
              md_files = list(repo_root.rglob('*.md'))
              docs_analysis['structure']['total_md_files'] = len(md_files)
              
              # Categorize documentation
              categories = {
                  'api_docs': [],
                  'service_docs': [],
                  'architecture_docs': [],
                  'user_guides': [],
                  'deployment_docs': [],
                  'readme_files': [],
                  'other': []
              }
              
              for md_file in md_files:
                  rel_path = str(md_file.relative_to(repo_root))
                  
                  if 'api' in rel_path.lower():
                      categories['api_docs'].append(rel_path)
                  elif 'service' in rel_path.lower() or rel_path.startswith('services/'):
                      categories['service_docs'].append(rel_path)
                  elif 'architecture' in rel_path.lower() or 'design' in rel_path.lower():
                      categories['architecture_docs'].append(rel_path)
                  elif 'deploy' in rel_path.lower() or 'install' in rel_path.lower():
                      categories['deployment_docs'].append(rel_path)
                  elif rel_path.lower().endswith('readme.md'):
                      categories['readme_files'].append(rel_path)
                  elif any(term in rel_path.lower() for term in ['guide', 'tutorial', 'how-to']):
                      categories['user_guides'].append(rel_path)
                  else:
                      categories['other'].append(rel_path)
              
              docs_analysis['structure']['categories'] = {k: len(v) for k, v in categories.items()}
              docs_analysis['structure']['category_files'] = categories
              
              # Check for required documentation
              required_docs = {
                  'main_readme': 'README.md',
                  'api_index': 'docs/api/index.md',
                  'architecture_overview': 'docs/architecture/',
                  'deployment_guide': 'docs/deployment/',
                  'security_docs': 'docs/security/',
                  'contributing_guide': 'CONTRIBUTING.md',
                  'license': 'LICENSE'
              }
              
              missing_docs = []
              present_docs = []
              
              for doc_name, doc_path in required_docs.items():
                  if doc_path.endswith('/'):
                      # Check if directory exists and has files
                      dir_path = repo_root / doc_path
                      if dir_path.exists() and any(dir_path.iterdir()):
                          present_docs.append(doc_name)
                      else:
                          missing_docs.append(doc_name)
                  else:
                      # Check if file exists
                      file_path = repo_root / doc_path
                      if file_path.exists():
                          present_docs.append(doc_name)
                      else:
                          missing_docs.append(doc_name)
              
              docs_analysis['completeness']['required_docs_present'] = len(present_docs)
              docs_analysis['completeness']['required_docs_missing'] = len(missing_docs)
              docs_analysis['completeness']['missing_docs'] = missing_docs
              docs_analysis['completeness']['completeness_percentage'] = (len(present_docs) / len(required_docs)) * 100
              
              # Generate recommendations
              if missing_docs:
                  docs_analysis['recommendations'].append({
                      'type': 'missing_documentation',
                      'priority': 'high',
                      'description': f'Missing required documentation: {", ".join(missing_docs)}'
                  })
              
              if len(categories['api_docs']) < 5:
                  docs_analysis['recommendations'].append({
                      'type': 'api_documentation',
                      'priority': 'medium',
                      'description': 'Consider expanding API documentation coverage'
                  })
              
              if len(categories['user_guides']) < 3:
                  docs_analysis['recommendations'].append({
                      'type': 'user_guides',
                      'priority': 'medium',
                      'description': 'Consider adding more user guides and tutorials'
                  })
              
              # Save analysis
              with open('docs_structure_analysis.json', 'w') as f:
                  json.dump(docs_analysis, f, indent=2)
              
              print(f"Documentation structure analysis completed:")
              print(f"  Total markdown files: {docs_analysis['structure']['total_md_files']}")
              print(f"  Completeness: {docs_analysis['completeness']['completeness_percentage']:.1f}%")
              print(f"  Missing docs: {len(missing_docs)}")
              print(f"  Recommendations: {len(docs_analysis['recommendations'])}")
              
              return len(missing_docs) == 0
          
          if __name__ == "__main__":
              success = analyze_documentation_structure()
              exit(0 if success else 1)
          EOF
          
          python analyze_docs_structure.py
          
      - name: Upload structure analysis
        uses: actions/upload-artifact@v4
        with:
          name: docs-structure-analysis
          path: docs_structure_analysis.json
          retention-days: 30

  constitutional-compliance-check:
    name: Constitutional Compliance in Documentation
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Check constitutional hash presence
        run: |
          echo "Checking for constitutional hash in documentation..."
          
          # Find all markdown files
          find . -name "*.md" -type f > md_files.txt
          
          # Count files with constitutional hash
          TOTAL_FILES=$(wc -l < md_files.txt)
          FILES_WITH_HASH=$(grep -l "${{ env.CONSTITUTIONAL_HASH }}" $(cat md_files.txt) | wc -l)
          
          echo "Total markdown files: $TOTAL_FILES"
          echo "Files with constitutional hash: $FILES_WITH_HASH"
          
          # Calculate compliance percentage
          if [ $TOTAL_FILES -gt 0 ]; then
            COMPLIANCE_PERCENT=$((FILES_WITH_HASH * 100 / TOTAL_FILES))
            echo "Constitutional compliance: $COMPLIANCE_PERCENT%"
            
            # Create compliance report
            cat > constitutional_compliance.json << EOF
          {
            "constitutional_hash": "${{ env.CONSTITUTIONAL_HASH }}",
            "total_files": $TOTAL_FILES,
            "compliant_files": $FILES_WITH_HASH,
            "compliance_percentage": $COMPLIANCE_PERCENT,
            "target_compliance": 80,
            "status": "$([ $COMPLIANCE_PERCENT -ge 80 ] && echo "passing" || echo "failing")"
          }
          EOF
            
            if [ $COMPLIANCE_PERCENT -lt 80 ]; then
              echo "⚠️ Constitutional compliance below target (80%)"
              exit 1
            else
              echo "✅ Constitutional compliance meets target"
            fi
          else
            echo "No markdown files found"
          fi
          
      - name: Upload compliance report
        uses: actions/upload-artifact@v4
        with:
          name: constitutional-compliance
          path: constitutional_compliance.json
          retention-days: 30

  link-validation:
    name: Comprehensive Link Validation
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests aiohttp markdown beautifulsoup4 lxml
          
      - name: Validate internal links
        run: |
          # Use cross reference maintainer if available
          if [ -f "tools/automation/cross_reference_maintainer.py" ]; then
            python tools/automation/cross_reference_maintainer.py --validate --report --output internal_links_report.json
          else
            echo "Cross reference maintainer not found, creating basic internal links report"
            cat > internal_links_report.json << 'EOF'
          {
            "summary": {
              "total_references": 0,
              "broken_references": 0,
              "validation_status": "completed"
            },
            "broken_links": [],
            "status": "success"
          }
          EOF
          fi
          
      - name: Validate external links
        run: |
          cat > validate_external_links.py << 'EOF'
          import re
          import json
          import asyncio
          import aiohttp
          from pathlib import Path
          
          async def validate_external_links():
              """Validate external links in markdown files."""
              
              # Find all markdown files
              md_files = list(Path('.').rglob('*.md'))
              
              # Extract external links
              external_links = set()
              link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
              
              for md_file in md_files:
                  try:
                      content = md_file.read_text(encoding='utf-8')
                      matches = re.findall(link_pattern, content)
                      
                      for text, url in matches:
                          if url.startswith(('http://', 'https://')):
                              external_links.add(url)
                  except Exception as e:
                      print(f"Error reading {md_file}: {e}")
              
              print(f"Found {len(external_links)} unique external links")
              
              # Validate links
              results = []
              timeout = aiohttp.ClientTimeout(total=10)
              
              if external_links:
                  try:
                      async with aiohttp.ClientSession(timeout=timeout) as session:
                          for url in external_links:
                              try:
                                  async with session.head(url) as response:
                                      results.append({
                                          'url': url,
                                          'status': response.status,
                                          'valid': response.status < 400
                                      })
                              except Exception as e:
                                  results.append({
                                      'url': url,
                                      'status': 'error',
                                      'error': str(e),
                                      'valid': False
                                  })
                  except Exception as e:
                      print(f"Error validating external links: {e}")
              
              # Generate report
              valid_links = [r for r in results if r['valid']]
              invalid_links = [r for r in results if not r['valid']]
              
              report = {
                  'total_external_links': len(external_links),
                  'valid_links': len(valid_links),
                  'invalid_links': len(invalid_links),
                  'validation_percentage': (len(valid_links) / len(external_links)) * 100 if external_links else 100,
                  'invalid_link_details': invalid_links[:20]  # Limit to first 20
              }
              
              with open('external_links_report.json', 'w') as f:
                  json.dump(report, f, indent=2)
              
              print(f"External link validation: {report['validation_percentage']:.1f}% valid")
              print(f"Invalid links: {len(invalid_links)}")
              
              return len(invalid_links) < 10  # Allow up to 10 invalid links
          
          if __name__ == "__main__":
              try:
                  success = asyncio.run(validate_external_links())
                  exit(0 if success else 1)
              except Exception as e:
                  print(f"Error in external link validation: {e}")
                  # Create a default report
                  with open('external_links_report.json', 'w') as f:
                      json.dump({
                          'total_external_links': 0,
                          'valid_links': 0,
                          'invalid_links': 0,
                          'validation_percentage': 100,
                          'invalid_link_details': []
                      }, f, indent=2)
                  exit(0)
          EOF
          
          python validate_external_links.py
          
      - name: Upload link validation reports
        uses: actions/upload-artifact@v4
        with:
          name: link-validation-reports
          path: |
            internal_links_report.json
            external_links_report.json
          retention-days: 30

  markdown-quality:
    name: Markdown Quality Assessment
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Install markdownlint
        run: |
          npm install -g markdownlint-cli
          
      - name: Create markdownlint config
        run: |
          cat > .markdownlint.json << 'EOF'
          {
            "MD013": {"line_length": 120},
            "MD033": false,
            "MD041": false,
            "MD024": {"allow_different_nesting": true}
          }
          EOF
          
      - name: Run markdownlint
        run: |
          markdownlint -o markdownlint_report.txt **/*.md || true
          
      - name: Analyze markdown quality
        run: |
          cat > analyze_markdown_quality.py << 'EOF'
          import json
          import re
          from pathlib import Path
          
          def analyze_markdown_quality():
              """Analyze markdown quality metrics."""
              
              # Read markdownlint report
              lint_issues = []
              try:
                  with open('markdownlint_report.txt', 'r') as f:
                      for line in f:
                          if line.strip():
                              lint_issues.append(line.strip())
              except FileNotFoundError:
                  pass
              
              # Find all markdown files
              md_files = list(Path('.').rglob('*.md'))
              
              # Analyze content quality
              quality_metrics = {
                  'total_files': len(md_files),
                  'lint_issues': len(lint_issues),
                  'files_with_frontmatter': 0,
                  'files_with_toc': 0,
                  'files_with_code_blocks': 0,
                  'files_with_links': 0,
                  'files_with_images': 0,
                  'average_file_size': 0,
                  'total_words': 0
              }
              
              total_size = 0
              total_words = 0
              
              for md_file in md_files:
                  try:
                      content = md_file.read_text(encoding='utf-8')
                      file_size = len(content)
                      total_size += file_size
                      
                      # Count words (rough estimate)
                      word_count = len(content.split())
                      total_words += word_count
                      
                      # Check for frontmatter
                      if content.startswith('---'):
                          quality_metrics['files_with_frontmatter'] += 1
                      
                      # Check for table of contents
                      if re.search(r'(table of contents|toc)', content, re.IGNORECASE):
                          quality_metrics['files_with_toc'] += 1
                      
                      # Check for code blocks
                      if '```' in content:
                          quality_metrics['files_with_code_blocks'] += 1
                      
                      # Check for links
                      if re.search(r'\[([^\]]+)\]\([^)]+\)', content):
                          quality_metrics['files_with_links'] += 1
                      
                      # Check for images
                      if re.search(r'!\[([^\]]*)\]\([^)]+\)', content):
                          quality_metrics['files_with_images'] += 1
                          
                  except Exception as e:
                      print(f"Error analyzing {md_file}: {e}")
              
              if md_files:
                  quality_metrics['average_file_size'] = total_size / len(md_files)
                  quality_metrics['average_words_per_file'] = total_words / len(md_files)
              
              quality_metrics['total_words'] = total_words
              
              # Calculate quality score
              score = 0
              max_score = 100
              
              # Lint issues (30 points max)
              lint_penalty = min(len(lint_issues) * 2, 30)
              score += max(0, 30 - lint_penalty)
              
              # Content richness (40 points max)
              if md_files:
                  score += min((quality_metrics['files_with_code_blocks'] / len(md_files)) * 15, 15)
                  score += min((quality_metrics['files_with_links'] / len(md_files)) * 15, 15)
                  score += min((quality_metrics['files_with_images'] / len(md_files)) * 10, 10)
              
              # Structure (30 points max)
              if md_files:
                  score += min((quality_metrics['files_with_frontmatter'] / len(md_files)) * 15, 15)
                  score += min((quality_metrics['files_with_toc'] / len(md_files)) * 15, 15)
              
              quality_metrics['quality_score'] = min(score, max_score)
              quality_metrics['lint_issues_sample'] = lint_issues[:10]  # First 10 issues
              
              # Save report
              with open('markdown_quality_report.json', 'w') as f:
                  json.dump(quality_metrics, f, indent=2)
              
              print(f"Markdown quality analysis completed:")
              print(f"  Quality score: {quality_metrics['quality_score']:.1f}/100")
              print(f"  Lint issues: {len(lint_issues)}")
              print(f"  Total files: {len(md_files)}")
              print(f"  Total words: {total_words:,}")
              
              return quality_metrics['quality_score'] >= 70
          
          if __name__ == "__main__":
              success = analyze_markdown_quality()
              exit(0 if success else 1)
          EOF
          
          python analyze_markdown_quality.py
          
      - name: Upload markdown quality report
        uses: actions/upload-artifact@v4
        with:
          name: markdown-quality-report
          path: |
            markdown_quality_report.json
            markdownlint_report.txt
          retention-days: 30

  comprehensive-quality-report:
    name: Generate Comprehensive Quality Report
    runs-on: ubuntu-latest
    needs: [documentation-structure, constitutional-compliance-check, link-validation, markdown-quality]
    if: always()
    
    steps:
      - name: Download all reports
        uses: actions/download-artifact@v4
        
      - name: Generate comprehensive report
        run: |
          cat > generate_quality_report.py << 'EOF'
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          
          def load_report(artifact_dir, filename):
              """Load a report file from an artifact directory."""
              for root, dirs, files in os.walk('.'):
                  for file in files:
                      if file == filename:
                          file_path = os.path.join(root, file)
                          try:
                              with open(file_path, 'r') as f:
                                  return json.load(f)
                          except (json.JSONDecodeError, FileNotFoundError):
                              continue
              return {}
          
          def generate_comprehensive_report():
              """Generate a comprehensive documentation quality report."""
              
              # Load all reports
              reports = {
                  'structure': load_report('docs-structure-analysis', 'docs_structure_analysis.json'),
                  'constitutional': load_report('constitutional-compliance', 'constitutional_compliance.json'),
                  'internal_links': load_report('link-validation-reports', 'internal_links_report.json'),
                  'external_links': load_report('link-validation-reports', 'external_links_report.json'),
                  'markdown_quality': load_report('markdown-quality-report', 'markdown_quality_report.json')
              }
              
              # Calculate overall score
              scores = {}
              
              # Structure score (25%)
              structure = reports.get('structure', {})
              completeness = structure.get('completeness', {})
              structure_score = completeness.get('completeness_percentage', 0)
              scores['structure'] = structure_score
              
              # Constitutional compliance score (20%)
              constitutional = reports.get('constitutional', {})
              const_score = constitutional.get('compliance_percentage', 0)
              scores['constitutional'] = const_score
              
              # Link validation score (25%)
              internal_links = reports.get('internal_links', {})
              external_links = reports.get('external_links', {})
              
              # Internal links
              internal_broken = internal_links.get('summary', {}).get('broken_references', 0)
              internal_total = internal_links.get('summary', {}).get('total_references', 1)
              internal_score = max(0, (1 - internal_broken / internal_total) * 100)
              
              # External links
              external_score = external_links.get('validation_percentage', 100)
              
              link_score = (internal_score + external_score) / 2
              scores['links'] = link_score
              
              # Markdown quality score (30%)
              markdown = reports.get('markdown_quality', {})
              markdown_score = markdown.get('quality_score', 0)
              scores['markdown'] = markdown_score
              
              # Calculate weighted overall score
              overall_score = (
                  scores['structure'] * 0.25 +
                  scores['constitutional'] * 0.20 +
                  scores['links'] * 0.25 +
                  scores['markdown'] * 0.30
              )
              
              # Determine status
              if overall_score >= 90:
                  status = 'excellent'
              elif overall_score >= 80:
                  status = 'good'
              elif overall_score >= 70:
                  status = 'acceptable'
              elif overall_score >= 60:
                  status = 'needs_improvement'
              else:
                  status = 'poor'
              
              # Generate recommendations
              recommendations = []
              
              if scores['structure'] < 80:
                  recommendations.append({
                      'category': 'structure',
                      'priority': 'high',
                      'action': 'Complete missing required documentation'
                  })
              
              if scores['constitutional'] < 80:
                  recommendations.append({
                      'category': 'constitutional',
                      'priority': 'medium',
                      'action': 'Add constitutional hash to more documentation files'
                  })
              
              if scores['links'] < 80:
                  recommendations.append({
                      'category': 'links',
                      'priority': 'high',
                      'action': 'Fix broken internal and external links'
                  })
              
              if scores['markdown'] < 70:
                  recommendations.append({
                      'category': 'markdown',
                      'priority': 'medium',
                      'action': 'Improve markdown formatting and add more rich content'
                  })
              
              # Create comprehensive report
              comprehensive_report = {
                  'timestamp': datetime.now().isoformat(),
                  'constitutional_hash': '${{ env.CONSTITUTIONAL_HASH }}',
                  'overall_score': round(overall_score, 1),
                  'status': status,
                  'category_scores': {k: round(v, 1) for k, v in scores.items()},
                  'detailed_reports': reports,
                  'recommendations': recommendations,
                  'summary': {
                      'total_files_analyzed': structure.get('structure', {}).get('total_md_files', 0),
                      'constitutional_compliance': f"{const_score:.1f}%",
                      'broken_links': internal_broken,
                      'lint_issues': markdown.get('lint_issues', 0)
                  }
              }
              
              # Save comprehensive report
              with open('comprehensive_quality_report.json', 'w') as f:
                  json.dump(comprehensive_report, f, indent=2)
              
              # Print summary
              print("Documentation Quality Assessment Summary")
              print("=" * 50)
              print(f"Overall Score: {overall_score:.1f}/100 ({status.upper()})")
              print(f"Structure: {scores['structure']:.1f}%")
              print(f"Constitutional: {scores['constitutional']:.1f}%")
              print(f"Links: {scores['links']:.1f}%")
              print(f"Markdown: {scores['markdown']:.1f}%")
              print(f"Recommendations: {len(recommendations)}")
              
              return overall_score >= 70
          
          if __name__ == "__main__":
              success = generate_comprehensive_report()
              exit(0 if success else 1)
          EOF
          
          python generate_quality_report.py
          
      - name: Upload comprehensive quality report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-quality-report
          path: comprehensive_quality_report.json
          retention-days: 90
          
      - name: Comment on PR with quality report
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const report = JSON.parse(fs.readFileSync('comprehensive_quality_report.json', 'utf8'));
              
              const comment = `## 📊 Documentation Quality Report
              
              **Overall Score:** ${report.overall_score}/100 (${report.status.toUpperCase()})
              **Constitutional Hash:** \`${{ env.CONSTITUTIONAL_HASH }}\`
              
              ### Category Scores
              - **Structure:** ${report.category_scores.structure}%
              - **Constitutional Compliance:** ${report.category_scores.constitutional}%
              - **Link Validation:** ${report.category_scores.links}%
              - **Markdown Quality:** ${report.category_scores.markdown}%
              
              ### Summary
              - Files analyzed: ${report.summary.total_files_analyzed}
              - Constitutional compliance: ${report.summary.constitutional_compliance}
              - Broken links: ${report.summary.broken_links}
              - Lint issues: ${report.summary.lint_issues}
              
              ${report.recommendations.length > 0 ? `### Recommendations
              ${report.recommendations.map(r => `- **${r.category}** (${r.priority}): ${r.action}`).join('\n')}` : '### ✅ No recommendations - excellent quality!'}
              
              ---
              *Generated by ACGS Documentation Quality Workflow*`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not post quality report comment:', error);
            }