# 2406.05534_Online-DPO-Online-Direct-Preference-Optimization-w
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 2406.05534_Online-DPO-Online-Direct-Preference-Optimization-w.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2

---

## Page 1

Online DPO: Online Direct Preference Optimization

with Fast-Slow Chasing

Biqing Qi1,2, Pengfei Li3, Fangyuan Li1, Junqi Gao3,∗, Kaiyan Zhang2, Bowen Zhou2,∗

1Department of Control Science and Engineering, Harbin Institute of Technology,

2 Department of Electronic Engineering, Tsinghua University,

3 School of Mathematics, Harbin Institute of Technology

{qibiqing7,lipengfei0208,jacklee19900212,gjunqi97}@gmail.com,

zhang-ky22@mails.tsinghua.edu.cn, {zhoubowen}@tsinghua.edu.cn

Abstract

Direct Preference Optimization (DPO) improves the alignment of large language

models (LLMs) with human values by training directly on human preference

datasets, eliminating the need for reward models. However, due to the presence of

cross-domain human preferences, direct continual training can lead to catastrophic

forgetting, limiting DPO’s performance and efficiency. Inspired by intraspecific

competition driving species evolution, we propose a Online Fast-Slow chasing

DPO (OFS-DPO) for preference alignment, simulating competition through fast

and slow chasing among models to facilitate rapid adaptation. Specifically, we

first derive the regret upper bound for online learning, validating our motivation

with a min-max optimization pattern. Based on this, we introduce two identical

modules using Low-rank Adaptive (LoRA) with different optimization speeds

to simulate intraspecific competition, and propose a new regularization term to

guide their learning. To further mitigate catastrophic forgetting in cross-domain

scenarios, we extend the OFS-DPO with LoRA modules combination strategy,

resulting in the Cross domain Online Fast-Slow chasing DPO (COFS-DPO). This

method leverages linear combinations of fast modules parameters from different

task domains, fully utilizing historical information to achive continual value align-

ment. Experimental results show that OFS-DPO outperforms DPO in in-domain

alignment, while COFS-DPO excels in cross-domain continual learning scenarios.

1

Introduction

To better align Large Language Models (LLMs) with human values and prevent harmful responses,

Reinforcement Learning from Human Feedback (RLHF) is commonly used in fine-tuning LLMs

[1, 2, 3]. However, the complexity progess and the dependence on reward models in RLHF limit

its practicality and efficiency [3]. To address these issues, Direct Preference Optimization (DPO)

[4] has emerged as an efficient alternative. The DPO employs supervised training directly based on

preference data, eliminating the need for complex frameworks or specific reward models [5], thereby

simplifying model optimization for preference alignment and improving training efficiency.

However, DPO is designed for supervised training on local offline data and does not adapt well to

online preference data streams [5]. Additionally, DPO cannot leverage historical information, leading

to catastrophic forgetting of the original task domain during cross-domain preference alignment

and resulting in overall performance degradation [6, 7]. The cycle of forgetting and relearning

significantly increases resource consumption, including computational costs and the need for re-

collecting annotated data, making DPO disadvantageous in resource-constrained scenarios.

∗Corresponding authors.

Preprint. Under review.

arXiv:2406.05534v1  [cs.AI]  8 Jun 2024

## Page 2

Current online learning methods for human preference alignment either involve constructing reward

models, which increases resource consumption (e.g., CPPO [8]), or rely on feedback generated by

LLMs, which lacks a flexible modular design to ensure efficient learning and memory retention

[5, 9]. In this paper, inspired by the intraspecific competition theory [10, 11], we design competitive

components with consistent optimization objectives and integrate them into the online learning to

enhance the model’s ability to adapt to continual changes. Our method retains the resource-efficient

characteristics of DPO while improving its capability to handle continuously incoming streams of

preference data.

To incorporate the concept of intraspecific competition into online preference alignment learning, we

first derive the regret bounds for online learning methods [12, 13]. We discover that these bounds

include a min-max term similar to the objective function in Generative Adversarial Networks (GANs)

[14]. The key difference is that, in our case, the min and max terms share the same optimization

objective, closely reflecting the intraspecific competition observed in nature [10, 11], thus validating

our motivation.

Furthermore, to maintain consistency with the original DPO method’s adherence to human values and

to prevent the policy model significantly deviating from the reference model, we retain the objective

function of the original DPO. Building on this foundation, we instantiate fast and slow modules using

LoRA [15] and introduce a regularization term to measure the preference probability gap between

the fast and slow modules, guiding the learning of these modules. Consequently, we propose the

Online Fast-Slow Chasing DPO (OFS-DPO) for in-domain tasks. We theoretically demonstrate that

OFS-DPO achieves a lower empirical regret bound, supported by more stable gradient optimization

and faster convergence.

To extend OFS-DPO to cross-domain preference alignment environments, we propose the Cross-

domain Oline Fast-Slow Chasing DPO (COFS-DPO). Specifically, using OFS-DPO, we derive

the optimal fast modules for two task domains and maintain domain-specific memories. Drawing

inspiration from the human brain’s capacity [16, 17, 18] for continual learning via the interplay

of modular memories, our method encapsulates this mechanism through combination of LoRAs.

Additionally, inspired by the conclusion of the equivalence between data shift and model parameter

shift [19], we theoretically derive a lower regret bound to demonstrate the effectiveness of COFS-

DPO. In experimental evaluations, our proposed OFS-DPO outperforms DPO and other competitive

methods in in-domain scenarios, including controlled sentiment generation, summarization, and

single-turn dialogue tasks. In cross-domain scenarios, we demonstrate that our proposed COFS-

DPO significantly surpasses competitive baselines in the summarization task. In summary, our

contributions are as follows:

• We propose OFS-DPO, a simple and effective method based on fast-slow LoRA modules

from the novel perspective of intraspecific competition. This method introduces a regular-

ization term to measure and guide the preference probability gap between the modules.

• To extend OFS-DPO to cross-domain scenarios, we propose COFS-DPO, which jointly

optimizes the linear combination of the optimal fast modules from different tasks. This

method achieves performance comparable to theoretically optimal model parameters across

the entire task domain while maintaining strong memory retention in individual domains.

• We validate our proposed OFS-DPO and COFS-DPO through a theoretical analysis of the

regret bounds, demonstrating their improved gradient stability and faster convergence speed.

2

Related Works

Direct Preference Optimization. The DPO aims to replace human feedback-based reinforcement

learning and has found widespread application in various downstream tasks due to its resource-

efficient advantages. For instance, in the multimodal domain, DPO has been beneficial for tasks such

as text-to-image generation using diffusion models [20], text-to-action generation [21], text-to-audio

conversion [22], video instruction following [23], and translation tasks leveraging LLMs[24]. How-

ever, DPO still has limitations that constrain its practical utility. Consequently, various improvement

strategies rooted in DPO have emerged. For example, the MODPO [25] is proposed to address the

requirements of multiple alignment objectives by balancing their weights. Additionally, the DPOP

model [26] introduces an enhanced DPO objective function to mitigate accuracy degradation in

preference datasets with smaller edit distances, thereby improving DPO’s performance on specific

2

## Page 3

tasks. Inspired by these advancements, we develop the online versions of the DPO, OFS-DPO,

and COFS-DPO methods. Continual Learning. In continual learning within in-domain tasks, the

need for swift adaptation to dynamic data streams has been emphasized [27]. In contrast, con-

tinual learning in cross-domain scenarios faces the significant challenge of preserving previously

learned task features while accommodating new ones to prevent catastrophic forgetting [28]. Current

strategies to address these challenges fall into several categories: regularization-based methods,

replay-based techniques, and domain generalization methods [29, 28]. Regularization-based methods

[6, 30, 31, 32, 33] incorporate regularization terms to balance the integration of new and old knowl-

edge, assessing the significance of various features. Replay-based methods [34, 35, 36] mitigate

forgetting issues in cross-domain scenarios by leveraging retained past data or experiences, thus

they require resources such as memory. Additionally, ongoing researches on domain generalization

[37, 38, 39] aim to identify feature representations that extend beyond the training distribution while

maintaining satisfactory performance on current tasks. However, these methods typically struggle

when faced with substantial shifts cross-domain distributions [38, 39].

3

Methodology

3.1

Preliminaries

In an standard online setting, we consider the distribution of data corresponding to specific human

preference tasks as D, updated within T time steps. In cross-domain scenarios, we differentiate

different tasks as D1, T1 and D2, T2. The sequence (x1, x2, x3, . . . , xT ) represents samples within

time T. Each data point in our task setup consists of three parts: xi = (z, yw, yl), where z is the

prompt statement for the task, and yw and yl represent the desired and undesired model preferences

given z, respectively. To ensure a fair comparison, we adhere to the settings established in [8],

concentrating on two domain configurations. Let H be the hypothesis class defined on distribution D,

with hi ∈H denoting a hypothesis function belonging to the hypothesis class at the i-th time step.

The model parameter is denoted by θ, with θi representing the model parameters at the i-th time step.

The objective function of the DPO is denoted by l(θ, x).

3.2

Motivation and Theoretical Analysis

To better understand the feasibility of the intraspecific competition motivation, we first conduct

a theoretical analysis of the difference between online learning methods and the offline optimal

decision by regret definition [40]. We begin by precisely defining online expected regret to prevent

any conceptual ambiguities.

Definition 3.2.1. (Expected Regret)

R(T) = Ex∼D

1

## T



## T

## X

t=1

l(ht, xt) −minh∈H

## T

## X

t=1

l(h, xt)



.

It is worth noting that D actually represents the distribution of task sequences in online learning,

namely D = (D1, D2, . . . , DT ), and x = (x1, x2, . . . , xT ) ∼D. Similar to the discussion of the

regret upper bound in [41], we provide a similar lemma and derive the regret upper bound.

Lemma 3.2.1. In online learning methods, there exists a regret upper bound that includes a minimax

term:

## R(T) ≤O(

p

ln(|H′|)/T) + Ex∼D

1

## T



minh′∈H′maxh∈H(

## T

## X

t=1

l(h′

t, xt) −

## T

## X

t=1

l(h, xt))



,

(1)

where the first term is the regret against the best h′ ∈H′ and H′ is an infinite hypothesis class to

approximate H, so the second term captures how well H′ approximates H.

The detailed proof of lemma 3.2.1 is in Appendix A.1.Let h ∈H be the optimal choice in the

theoretical hypothesis space. By introducing another module H′′ to approximate H, we can derive

the following inequality:

3

## Page 4

## R(T) ≤O(

p

ln(|H′|)/T) + Ex∼D

1

## T



minh′∈H′maxh′′∈H′′(

## T

## X

t=1

l(h′

t, xt) −

## T

## X

t=1

l(h′′, xt))



+ Ex∼D

1

## T



minh′′∈H′′maxh∈H(

## T

## X

t=1

l(h′′

t , xt) −

## T

## X

t=1

l(h, xt))



.

(2)

According to Equation 2, the first term is slightly affected by the learning method. Therefore, we

further constrain the expected regret by minimizing the two min-max terms on the right-hand side

of the inequality. The relationship between the min term and the max term closely aligns

with the competitive dynamics within natural populations, as both aim to achieve a smaller

cumulative loss. To approximate h′ and h′′ respectively, we introduce two modules for simulation:

a fast module and a slow module. These modules pursue each other to approximate the best

offline optimal decision h [40] ultimately optimizing the min-max term, as illustrated in Figure 1.

Figure 1: The framework of the

OFS-DPO. In the upper section, F-

Module and S-Module dynamically

adjust during training, while the ref-

erence model remains fixed. The

lower section illustrates the frame-

work of the original DPO.

3.3

In Domain: Online Fast-Slow Chasing DPO

OFS-DPO Objective Function.

Based on the above anal-

ysis, we understand that introducing fast and slow learning

modules to simulate intraspecific competition can enhance the

model’s adaptability to changes, thereby improving its ability

to handle continuously evolving data. However, integrating

these modules renders the original objective function of the

DPO [4] insufficient for training these new components. To

address this issue, we need to design new objective functions.

Essentially, our ultimate goal aligns with that of the DPO: to

ensure the model better conforms to human value preferences

while not deviating excessively from the reference model [4].

Therefore, we retain the DPO objective function as a primary

component of our new objective. The DPO objective function

measures the optimization gap between the learning model and

the reference model. To adapt it to our new setup, we replace

πθ in the DPO objective function with πθF and πθS for the fast

and slow modules, respectively, and denote these modules as

F-module and S-module for convenience. By constructing the

regularization term that measures the preference probability

gap between the fast and slow modules, we introduce a chase

between the modules to promote efficient optimization. Specifically, we propose the following

objectives:

LDP O−new(θF ) = LDP O(θF ) + αLDP O−F S,

(3)

LDP O−new(θS) = LDP O(θS) −αLDP O−F S,

(4)

In the training phase, we optimize eq. (3) and (4) alternately at different frequencies. Here

LDP O(θ) = −E(x,yw,yl)∼D



logσ



βlog πθ(yw|x)

πref(yw|x) −βlog πθ(yl|x)

πref(yl|x)



,

(5)

LDP O−F S = −E(x,yw,yl)∼D



logσ



βlogπθF (yw|x)

πθS(yw|x) −βlogπθF (yl|x)

πθS(yl|x)



,

(6)

where π denotes the preference policy probability function, and σ(·) represents the logistic function.

We have LDP O ∈(a, b) = (0, ln 2). The coefficient α ∈(0, 1) is the regularization term coeffi-

cient, and θF and θS represent the parameters of the F-module and S-module, respectively. The

corresponding gradients become:

gF

t = ∇θF LDP O−new(θF ); gS

t = ∇θSLDP O−new(θS).

(7)

## Ofs-Dpo.

Based on the above analysis, we proceed to formally construct OFS-DPO. Specifically,

incorporating diverse LoRA modules into the reference model, we initialize F-module and S-module

respectively. Using the update gradients mentioned in eq. (7), we update the parameters θF and

4

## Page 5

θS, respectively. Every k time steps, we swap the roles of the F-module and the S-module if

LDPO(θF ) > LDPO(θS), to ensure that the module with the best performance is designated as the

F-module. Otherwise, we maintain the current setup and proceed to the next update cycle, thereby

achieving the fast-slow chasing effect during training. The method is summarized as Algorithm 1 in

Appendix B. For the instantiation of the method, we require an appropriate tool to create fast-slow

modules with different optimization speeds, allowing for flexible dynamic switching during training.

LoRA [15], currently the most commonly used efficient fine-tuning strategy for LLMs, perfectly

meets our requirements and mitigates the cost increase associated with introducing two modules.

Theoretical analysis of OFS-DPO.

To further validate the theoretical effectiveness of our proposed

OFS-DPO, we employ regret analysis to demonstrate that our method can achieve a lower empirical

regret bound more rapidly in in-domain tasks. We first present the empirical distribution of the regret.

Definition 3.3.1. (Experience of Regret)

## R(T) = 1

## T

## T

## X

i=1

[l(θT , xi) −l(θ∗, xi)],

(8)

where θ∗≜argminθ

1

## T

## Pt

i=1 l(θ, xi), xi ∼D, i ∈{1, 2, · · · , T} and D is the data distribution.

In our method, at regular update intervals, we compare LDP O(θF ) and LDP O(θS) to ensure that

the better-performing model is designated as the F-module. For convenience, let θ and w represent

the parameters of the F-module and S-module, respectively. Consequently, the optimal module

parameters at each time step should satisfy the following condition:

ˆθi = argminθ(l(θi, xi), l(wi, xi)).

(9)

Continuing, we can express the empirical regret of the OFS-DPO in the following form:

## ˆR(T) =

## T

## X

i=1

1

T [l(ˆθT , xi) −l(θ∗, xi)].

(10)

Before conducting a detailed quantitative analysis of the empirical regret 10, we give some bounded-

ness assumptions for the gradient at current step and task-specific module parameters.

Assumption 3.3.1. (Gradient boundedness) Denote gi = ∇θl(θi, xi), i = 1, 2, · · · , T, then

||gi||2 ≤G, where G is a positive constant.

Assumption 3.3.2. (Model parameters boundedness) Suppose ||θn −θm||2 ≤d, ||wn −wm||2 ≤

d, ∀n, m ∈(1, ...T), where d is a positive constant.

Under assumptions 3.3.1 and 3.3.2, we can derive the following theorem.

Theorem 3.3.1. Within proposed OFS-DPO, a lower empirical regret bound can be attained, with a

probability 1 −δ, where δ = 2(T −1)δ0 −(T −1)(2T −3)(δ0)2[1 −δ0]2T −4, and δ0 ∈(0, 1).

R(T) ≥l(θ1, x1) −1

## T

## T

## X

i=1

l(θ∗, xi) −



2 −1

## T + (1 −1

T )1{mode=F S}



Gd −2(1 −1

T )ln2

r

−lnδ0

2 ,

(11)

where 1{mode=F S} represents whether to introduce fast and slow modules.

From Theorem 3.3.1, we observe that with the introduction of the fast-slow mode, i.e., 1{mode=F S} =

1, the right-hand side of Inequality 11 decreases further by (1 −1

T )Gd. This indicates that our

proposed OFS-DPO achieves a lower bound on empirical regret. More detailed proofs can be found

in Appendix A.2.

More Stable Gradient.

Building upon a superior lower bound on empirical regret, we further

demonstrate through a proposition that incorporating the LDP O−F S regularization term results in

more stable gradient information.

Proposition 3.3.1. As training progresses, ∀ϵ

>

0 such that ∇θLDP O(θ)

<

ϵ, while

∇θF LDP O−new(θF ) > ϵ. In other words, the original DPO experiences significantly diminish

gradients as training continues, leading to a lack of update momentum. Introducing the LDP O−F S

regularization term can address this issue.

5

## Page 6

Figure 2: The framework of the COFS-DPO. Instantiate the fast-slow modules with LoRAs

separately in different task domains to obtain the optimal LoRA module in each domain. Subsequently,

we seek the optimal linear combination (β1, β2) across all task domains.

The above proposition explains, from the perspective of the model update mechanism, why OFS-DPO

can achieve better performance than the original DPO. A key reason is that our method maintains

more sustained gradient update momentum.

3.4

Cross Domain: Online Fast Slow Chasing DPO

## Cofs-Dpo.

Furthermore, we extend OFS-DPO to cross-domain scenarios. The main distinction

between the COFS-DPO and the in-domain setting lies in balancing the importance of information

obtained from different task domains. This necessitates preserving and integrating historical data from

various domains in a specific manner to mitigate catastrophic forgetting in cross-domain scenarios.

Inspired by the human brain’s ability [16, 17, 18] to achieve continual learning through the interaction

of modular memories, we model this process using the combination of LoRAs. Thus, we achieve the

COFS-DPO method to retain crucial historical information across tasks, as illustrated in Figure 2.

Specifically, in the cross domain scenario, we first use the OFS-DPO to obtain the final F-modules for

two task domains, retaining a random subset of domain-specific memories M1 and M2. Subsequently,

we compute the optimal combination of F-modules over the joint memory distribution (M1, M2) to

achieve the best performance. The detailed procedure is outlined in Algorithm 2 in the Appendix B.

Theoretical analysis of COFS-DPO.

Next, we provide a theoretical analysis to demonstrate

the effectiveness of the proposed COFS-DPO. To ensure clarity, we first standardize the use of

symbols: let s(k)

i

represent the sample at the i-th moment from distribution Dk, where k = 1, 2 and

i = 0, 1, 2, . . . , Tk. Let θi represent the model parameters at moment i. Specifically, θ∗denotes

the optimal parameters for the overall task distribution, θ(1) denotes the optimal parameters for

distribution D1, and θ(2) denotes the optimal parameters for distribution D2. Consider describing

the relationship between these parameters in an incremental manner: θ∗= θ0 + ∆θ∗, θ(1) =

θ0 + ∆θ(1), θ(2) = θ0 + ∆θ(2).

Building upon these symbol definitions and the previously established lower bound on single-task

regret (Equation 3.3.1), we can derive Theorem 3.4.1 for dual-task scenarios. This foundation also

enables the extension to cross domain scenarios.

Definition 3.4.1. (Experience regret of cross-domain tasks: Dual-Task Regret)

## R(T1, T2) = 1

## T1

## T1

## X

i=1

l(θT1, s(1)

i ) + 1

## T2

## T2

## X

j=1

l(θT2, s(2)

j ) −1

## T1

## T1

## X

i=1

l(θ∗, s(1)

i ) −1

## T2

## T2

## X

j=1

l(θ∗, s(2)

j ).

(12)

Given the definition of dual-task regret, we derive a lower regret bound similar to the in-domain case.

Theorem 3.4.1. Under the settings of this paper, the dual-task regret also has a lower empirical

regret bound.

R(T1, T2) ≥l1(s(1)

1 , s(2)

## 1 ) −B(T1, T2) −2(1 −T1 + T2

## T1T2

)c

## −[6 −T1 + T2

## T1T2

## + (2 −T1 + T2

## T1T2

)1{mode=F S}]Gd,

(13)

6

## Page 7

where B(T1, T2) =

1

## T1

## Pt1

i=1 l(θ(1), s(1)

i ) +

1

## T2

## Pt2

j=1 l(θ(2), s(2)

j ), l1(s(1)

1 , s(2)

1 ) = l(θ1, s(1)

1 ) +

l(θ2, s(2)

1 ), c = max{ln2

q

−lnδ1

2 , ln2

q

−lnδ2

2 }, where δ1, δ2 ∈(0, 1), 1{mode=F S} represents

whether to introduce fast- slow modules.

Theorem 3.4.1 shows that with the introduction of the fast-slow mode, the right side of Inequality 13

is further reduced by (2 −T1+T2

T1T2 )Gd. This indicates that COFS-DPO, like OFS-DPO, also achieves

a lower empirical regret bound.

4

Experiments

Table 1: GPT-4 win rates for three in-domain tasks. We set the rank of LoRA between 16 and

256 to observe its impact on the model fine-tuning results. We report results over 5 trials. More

implementation details are available in the Appendix C.

LoRA rank

Method

Controlled Sentiment Generation

Summarization

Single-turn Dialogue

16

## Sft

0.307±0.042

0.128±0.036

0.132±0.033

## Dpo

0.488±0.037

0.194±0.014

0.192±0.012

## Ppo

0.358±0.056

0.136±0.048

0.147±0.052

OFS-DPO (Ours)

0.568±0.034

0.223±0.033

0.231±0.014

256

## Sft

0.313±0.035

0.138±0.036

0.138±0.032

## Dpo

0.494±0.032

0.211±0.028

0.203±0.024

## Ppo

0.364±0.039

0.148±0.044

0.155±0.042

OFS-DPO (Ours)

0.571±0.043

0.234±0.021

0.253±0.017

4.1

Experimental settings

The experiments are primarily divided into two parts. The first part aims to validate the effectiveness

of the OFS-DPO in adapting to online data streams of the in-domain tasks. The second part focuses

on evaluating the COFS-DPO’s capability to retain model memory in cross-domain scenarios.

In-Domain Task Setting. In the online in-domain preference alignment, we maintain consistent

experimental settings with DPO [4], validating our method’s effectiveness across three tasks: con-

trolled sentiment generation, summarization, and single-turn dialogue. For the controlled sentiment

generation task, we use the IMDB dataset. Initially, we fine-tune the GPT-2 LLM [42] on the IMDB

training dataset to obtain the Supervised Fine-Tuning (SFT) [43] model. Subsequently, using positive

and negative reviews from the dataset as preference data, we further fine-tune the SFT model using

the OFS-DPO to align it with human value preferences. In the summarization task, we use the TL;DR

Summarize dataset [1] with human preferences. We fine-tune the GPT-J [44] on the summarization

dataset as the SFT model and then conduct online value preference alignment using our method. For

single-turn dialogue, we fine-tune the Pythia-2.8B model2 on the Anthropic Helpful and Harmless

dialogue dataset3 as the SFT model. Subsequently, we train the SFT model with the OFS-DPO.

In the evaluation process, we introduce PPO and DPO, along with the corresponding SFT models

for each task, as baseline models to compare against our method under identical training settings.

Specifically, alignment with the previous studies[4, 5, 8, 45], we use GPT-4 as a surrogate human

evaluator to assess the quality of model-generated content and compare it with preferences extracted

from authentic datasets. The resulting win rate serves as a metric to quantify the effectiveness of

model alignment. More detailed experimental configurations are provided in the appendix C.

Cross-Domain Task Setting. Based on the experimental setup in CPPO [8], we design our cross-

domain experiment named "Summary." We utilize two datasets: the Reddit TL;DR dataset for SFT

and the human preference dataset 4 provided by CarperAI for RLHF. To validate our method’s

continual learning capability, we partition these datasets into two domains based on post types:

"relationships" and "others." We denote the "relationships" domain as Task-1, comprising a single

category, while the "others" domain is denoted as Task-2, consisting of the remaining 28 categories.

To ensure a fair comparison with CPPO, we adopt the same experimental settings. We train a GPT-2

2https://huggingface.co/EleutherAI/pythia-2.8b

3https://huggingface.co/datasets/Anthropic/hh-rlhf

4https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons

7

## Page 8

Table 2: The main evaluation results of cross-domain tasks are presented. Hyperparameters for

CPPO can be defined in two ways: heuristic or learnable. For our comparison with COFS-DPO, we

use heuristic CPPO (CPPOH). Experiments were conducted using GPT2-s and LLaMA3. The rank

of LoRA was set to 16 and 256. We report results over 5 trials. More implementation details are

available in the Appendix C.

Model

LoRA rank

Method

Task-1

Final

rPMS1(↑)

Rouge1(↑)

rPMS1(↑)

Rouge1(↑)

## Sfr(↓)

rPMS2(↑)

Rouge2(↑)

GPT2-s

16

## Dpo [4]

5.750±0.124

0.228±0.009

5.773±0.132

0.230±0.008

-0.023±0.012

4.785±0.121

0.167±0.019

## Ppo+Ewc [6]

4.932±0.117

0.203±0.011

4.983±0.108

0.207±0.014

-0.051±0.014

4.505±0.118

0.137±0.014

PPO+LwF [32]

4.890±0.122

0.199±0.021

4.953±0.118

0.202±0.010

-0.063±0.004

4.533±0.114

0.128±0.008

## Ppo+Tfcl [49]

4.934±0.148

0.217±0.018

4.988±0.220

0.217±0.013

-0.054±0.010

4.524±0.113

0.135±0.012

## Pc[50]

4.811±0.210

0.204±0.031

4.845±0.164

0.216±0.008

-0.034±0.013

4.574±0.120

0.149±0.011

## Hn-Ppo [51]

4.945±0.151

0.218±0.013

4.992±0.211

0.204±0.011

-0.047±0.012

4.531±0.147

0.136±0.012

## Nlpo [52]

4.931±0.121

0.203±0.022

4.987±0.136

0.208±0.033

-0.056±0.007

4.482±0.124

0.136±0.019

## Cppo-H [8]

5.059±0.212

0.211±0.012

5.410±0.189

0.213±0.010

-0.351±0.015

4.629±0.127

0.165±0.017

COFS-DPO (Ours)

5.756±0.212

0.230±0.010

6.398±0.224

0.234±0.007

-0.642±0.014

5.641±0.151

0.174±0.021

256

## Dpo [4]

5.754±0.183

0.230±0.010

5.766±0.215

0.230±0.013

-0.012±0.009

4.793±0.131

0.168±0.019

## Ppo+Ewc [6]

4.944±0.161

0.208±0.011

4.997±0.172

0.211±0.015

-0.053±0.007

4.505±0.249

0.137±0.010

PPO+LwF [32]

4.907±0.198

0.203±0.015

4.969±0.167

0.210±0.013

-0.062±0.007

4.562±0.154

0.151±0.007

## Ppo+Tfcl [49]

5.068±0.185

0.213±0.012

5.142±0.231

0.204±0.013

-0.074±0.011

4.563±0.214

0.145±0.014

## Pc[50]

4.923±0.245

0.207±0.023

4.981±0.178

0.237±0.016

-0.058±0.011

4.558±0.214

0.119±0.008

## Hn-Ppo [51]

5.072±0.235

0.214±0.011

5.153±0.234

0.208±0.019

-0.081±0.009

4.575±0.212

0.157±0.012

## Nlpo [52]

5.113±0.261

0.209±0.013

5.201±0.246

0.206±0.020

-0.088±0.011

4.501±0.196

0.146±0.014

## Cppo-H [8]

5.270±0.284

0.211±0.016

5.618±0.258

0.217±0.021

-0.348±0.014

4.664±0.188

0.134±0.016

COFS-DPO (Ours)

5.816±0.248

0.234±0.012

6.430±0.245

0.241±0.013

-0.614±0.023

5.664±0.213

0.178±0.017

Llama3

16

## Dpo [4]

5.809±0.215

0.110±0.013

5.816±0.236

0.113±0.015

-0.007±0.014

5.473±0.267

0.099±0.016

## Ppo+Ewc [6]

5.076±0.221

0.102±0.016

5.160±0.186

0.113±0.027

-0.084±0.012

5.050±0.271

0.102±0.011

PPO+LwF [32]

5.007±0.232

0.103±0.012

5.109±0.245

0.110±0.012

-0.102±0.016

4.601±0.169

0.111±0.014

## Ppo+Tfcl [49]

5.082±0.231

0.108±0.017

5.171±0.235

0.113±0.009

-0.089±0.009

4.624±0.123

0.112±0.011

## Pc[50]

5.012±0.271

0.094±0.016

5.104±0.244

0.112±0.013

-0.092±0.010

4.573±0.239

0.082±0.014

## Hn-Ppo [51]

5.098±0.258

0.098±0.014

5.168±0.198

0.106±0.019

-0.070±0.015

4.627±0.211

0.127±0.010

## Nlpo [52]

5.018±0.219

0.087±0.013

5.101±0.272

0.108±0.012

-0.083±0.008

4.594±0.222

0.110±0.016

## Cppo-H [8]

5.121±0.214

0.096±0.011

5.449±0.261

0.101±0.009

-0.328±0.021

5.318±0.264

0.089±0.013

COFS-DPO (Ours)

5.867±0.220

0.115±0.014

7.285±0.288

0.159±0.012

-1.418±0.026

7.094±0.301

0.139±0.010

256

## Dpo [4]

5.801±0.271

0.114±0.023

5.805±0.243

0.114±0.012

-0.004±0.007

5.477±0.264

0.099±0.010

## Ppo+Ewc [6]

5.121±0.215

0.101±0.016

5.220±0.275

0.105±0.014

-0.099±0.017

5.216±0.248

0.115±0.010

PPO+LwF [32]

5.107±0.214

0.098±0.013

5.201±0.237

0.110±0.010

-0.094±0.014

5.203±0.235

0.108±0.011

## Ppo+Tfcl [49]

5.172±0.233

0.109±0.012

5.263±0.269

0.116±0.035

-0.091±0.015

5.278±0.249

0.094±0.031

## Pc[50]

4.893±0.198

0.101±0.023

4.980±0.251

0.107±0.012

-0.087±0.022

4.995±0.276

0.056±0.005

## Hn-Ppo [51]

5.168±0.314

0.111±0.018

5.235±0.341

0.109±0.014

-0.067±0.022

5.280±0.361

0.096±0.021

## Nlpo [52]

5.096±0.277

0.092±0.019

5.167±0.301

0.108±0.024

-0.071±0.014

5.236±0.267

0.038±0.012

## Cppo-H [8]

5.322±0.255

0.102±0.011

5.657±0.248

0.097±0.014

-0.335±0.022

5.351±0.257

0.060±0.007

COFS-DPO (Ours)

5.895±0.269

0.116±0.011

7.508±0.285

0.163±0.014

-1.613±0.022

7.317±0.275

0.143±0.014

Small (GPT2-s) model 5 with 124M parameters using Task-1 data from the Reddit TL;DR dataset for

5 epochs as the SFT model. Additionally, we train the LLaMA3 (8B) model [46] for further testing.

We fine-tune the SFT model combined with LoRA on the Task-1 data of the human preference

dataset to obtain θ(1). Then, we fine-tune it on the Task-2 data after initializing the model to obtain

θ(2). We retain a small amount of data from different tasks to provide COFS-DPO for the combined

optimization of ∆θ(1) and ∆θ(2).

To evaluate model alignment performance, alignment with the previous works [8, 47], we fine-tune

the GPT-J (6.7B) model on the entire human preferences dataset as a reference preference model

(rPM). We use ROUGE [48] and rPM scores (rPMS) [8] to measure the model’s alignment with

current data and use SFR metric [8] to measure forgetting rate of old data. Table 7 in Appendix C

presents the evaluation metrics for each task.

4.2

Evaluation results on in-domain tasks

Table 1 presents the performance of the OFS-DPO compared to traditional DPO, PPO, and SFT

models under an online in-domain task data stream. Using GPT-4 as a human proxy [4, 5, 8, 45],

we evaluate the model-generated content against actual preference data. The results indicate that

the OFS-DPO consistently outperforms the DPO and PPO. Specifically, in the controlled emotion

generation task, OFS-DPO achieves approximately an 8% improvement in win rate across different

LoRA ranks. In the single-turn dialogue task, it demonstrates an improvement of approximately 5%

in win rates. This shows the superior alignment effectiveness of the OFS-DPO in various tasks.

4.3

Evaluation results on cross-domain tasks

Table 2 presents the results of continual learning for the summarization task based on human

preference datasets. In this experiment, we used GPT2-s and LLaMA3 as our fundamental models.

After training on Task 1, we evaluated the model’s performance on Task 1 using rPMS and Rouge

metrics. We then continue training on Task 2 and re-evaluate the model’s performance on both Task 1

and Task 2. The ability of the model to overcome catastrophic forgetting was assessed by examining

5https://huggingface.co/openai-community/gpt2

8

## Page 9

Figure 3: All ablation results are based on IMDB. From left to right: Win rates with different

choices of the regularization coefficient α; win rates comparing OFS-DPO and PPO under varying

learning rate multipliers between fast-slow modules; the influence of batch size and the contrast

period k between fast-slow modules on win rates; and kernel density estimates of the loss gradients

from the original DPO and the OFS-DPO during the training process.

the changes in rPMS for Task 1 before and after training on Task 2. The results indicate that when

using GPT2-s as the base model, the COFS-DPO achieves an SFR metric of around -0.6, which

significantly surpasses the memory retention performance of all baselines. When using Llama3 as the

base model, the corresponding SFR metric is around -1.5, nearly twice as good as the best-performing

PPO variant. This shows the superior memory retention capabilities of our method.

4.4

Ablation Studies

To validate the impact of the coefficient α in the regularization term of our objective function on

the win rate of models in the controlled sentiment generation task, we designed experiments with α

ranging from 0 to 0.9. The results, illustrated in the left panel of Figure 3, indicate that the model’s

win rate remains stable around 50%, even in the least favorable scenario (α = 0), demonstrating the

stability of our method. The second panel from the left in Figure 3 investigates the effect of varying

the learning rate multiples between fast-slow modules on training effectiveness. Across various LoRA

rank settings, OFS-DPO significantly outperform the baseline PPO, maintaining a lead of at least

10% even in the worst-case scenario. This suggests that our method is robust across different learning

rate configurations. As illustrated in the second panel from the right in Figure 3, increasing the batch

size and the contrastive update period between fast-slow modules leads to a more stable win rate for

our models. This demonstrates the positive impact of these adjustments on model performance. The

rightmost panel in Figure 3 shows that the gradient norms of the OFS-DPO exhibit more sustained

stability compared to those of the original DPO loss. This provides experimental evidence of our

method’s superiority in maintaining gradient stability, further supporting its overall effectiveness.

Table 3: Results of COFS-DPO with different sam-

ple sizes.

Sample Num

rPMS1

## Sfr

rPMS2

100

5.988

-0.242

5.156

200

6.180

-0.434

5.340

500

6.398

-0.652

5.641

1000

6.367

-0.621

5.590

2000

6.312

-0.641

5.605

To investigate the impact of retaining specific

domain samples on the memory retention ca-

pability of models in cross-domain tasks, we

designed validation experiments with total sam-

ple sizes of 100, 200, 500, and 1000 for two

tasks, as shown in Table 3. The results indicate

that when the total sample size increases to 500,

the COFS-DPO achieves optimal retention of

historical preference information. Beyond this

sample size, further increases do not enhance the

method’s performance. This finding suggests that within our proposed framework, it is unnecessary

to retain a large number of specific domain samples to achieve excellent results.

5

Conclusion

In this work, inspired by intraspecific competition theory, we propose a simple and effective OFS-

DPO, which leverages the competition between fast and slow modules under the same objective

to achieve continual preference learning, with theoretical guarantees in terms of regret bounds and

gradient stability. Furthermore, to extend OFS-DPO to cross domain settings, we introduce COFS-

DPO, which achieves preference alignment by replacing the optimal LoRA for a task domain with a

linear combination of LoRAs from different domains, validated both experimentally and theoretically.

Our work demonstrates that proposed methods based on intraspecific competition provide new

insights and solutions for online human preference alignment tasks and have the potential for broad

applicability across multi domains.

9

## Page 10

References

[1] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec

Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.

Advances in Neural Information Processing Systems, 33:3008–3021, 2020.

[2] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep

reinforcement learning from human preferences. Advances in neural information processing

systems, 30, 2017.

[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn

Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless

assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,

2022.

[4] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and

Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.

Advances in Neural Information Processing Systems, 36, 2024.

[5] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexan-

dre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. Direct

language model alignment from online ai feedback, 2024.

[6] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,

Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.

Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of

sciences, 114(13):3521–3526, 2017.

[7] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Expe-

rience replay for continual learning. Advances in neural information processing systems, 32,

2019.

[8] Han Zhang, Yu Lei, Lin Gui, Min Yang, Yulan He, Hui Wang, and Ruifeng Xu. CPPO:

Continual learning for reinforcement learning with human feedback. In The Twelfth International

Conference on Learning Representations, 2024.

[9] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop,

Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human

feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.

[10] Daniel I Bolnick.

Intraspecific competition favours niche width expansion in drosophila

melanogaster. Nature, 410(6827):463–466, 2001.

[11] Barbara L Thorne, Nancy L Breisch, and Mario L Muscedere. Evolution of eusociality and

the soldier caste in termites: influence of intraspecific competition and accelerated inheritance.

Proceedings of the National Academy of Sciences, 100(22):12808–12813, 2003.

[12] Naman Agarwal, Brian Bullins, Elad Hazan, Sham Kakade, and Karan Singh. Online control

with adversarial disturbances. In International Conference on Machine Learning, pages 111–119.

## Pmlr, 2019.

[13] Elad Hazan, Sham Kakade, and Karan Singh. The nonstochastic control problem. In Algorithmic

Learning Theory, pages 408–421. PMLR, 2020.

[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil

Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications

of the ACM, 63(11):139–144, 2020.

[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,

Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv

preprint arXiv:2106.09685, 2021.

[16] Brenden M Lake and Marco Baroni. Human-like systematic generalization through a meta-

learning neural network. Nature, 623(7985):115–121, 2023.

[17] Gido M van de Ven, Hava T Siegelmann, and Andreas S Tolias. Brain-inspired replay for

continual learning with artificial neural networks. Nature Communications, 11(1), 2020.

[18] Biqing Qi, Xingquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, and Bowen Zhou.

Interactive continual learning: Fast and slow thinking. CoRR, abs/2403.02628, 2024.

10

## Page 11

[19] Zhimeng Stephen Jiang, Xiaotian Han, Hongye Jin, Guanchu Wang, Rui Chen, Na Zou, and Xia

Hu. Chasing fairness under distribution shift: A model weight perturbation approach. Advances

in Neural Information Processing Systems, 36, 2024.

[20] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam,

Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using

direct preference optimization, 2023.

[21] Massimiliano Pappa, Luca Collorone, Giovanni Ficarra, Indro Spinelli, and Fabio Galasso.

Modipo: text-to-motion alignment via ai-feedback-driven direct preference optimization. arXiv

preprint arXiv:2405.03803, 2024.

[22] Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and

Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct

preference optimization. arXiv preprint arXiv:2404.09956, 2024.

[23] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu,

Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of

video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258,

2024.

[24] Guangyu Yang, Jinghong Chen, Weizhe Lin, and Bill Byrne. Direct preference optimization for

neural machine translation with minimum bayes risk decoding. arXiv preprint arXiv:2311.08380,

2023.

[25] Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao.

Beyond one-preference-fits-all alignment: Multi-objective direct preference optimization, 2023.

[26] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White.

Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint

arXiv:2402.13228, 2024.

[27] Jiangwei Xie, Shipeng Yan, and Xuming He. General incremental learning with domain-aware

categorical representations. In Proceedings of the IEEE/CVF Conference on Computer Vision

and Pattern Recognition, pages 14351–14360, 2022.

[28] Christian Simon, Masoud Faraki, Yi-Hsuan Tsai, Xiang Yu, Samuel Schulter, Yumin Suh,

Mehrtash Harandi, and Manmohan Chandraker. On generalizing beyond domains in cross-

domain continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision

and Pattern Recognition (CVPR), pages 9265–9274, June 2022.

[29] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual

learning: Theory, method and application. IEEE Transactions on Pattern Analysis and Machine

Intelligence, 2024.

[30] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte-

laars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European

conference on computer vision (ECCV), pages 139–154, 2018.

[31] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian

walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of

the European conference on computer vision (ECCV), pages 532–547, 2018.

[32] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern

analysis and machine intelligence, 40(12):2935–2947, 2017.

[33] Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil, Cordelia Schmid, and Karteek

Alahari. End-to-end incremental learning. In Proceedings of the European conference on

computer vision (ECCV), pages 233–248, 2018.

[34] Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. Lamol: Language modeling for lifelong

language learning. arXiv preprint arXiv:1909.03329, 2019.

[35] Xialei Liu, Chenshen Wu, Mikel Menta, Luis Herranz, Bogdan Raducanu, Andrew D Bagdanov,

Shangling Jui, and Joost van de Weijer. Generative feature replay for class-incremental learning.

In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition

Workshops, pages 226–227, 2020.

[36] Biqing Qi, Junqi Gao, Xingquan Chen, Dong Li, Jianxing Liu, Ligang Wu, and Bowen Zhou.

Contrastive augmented graph2graph memory interaction for few shot continual learning. CoRR,

abs/2403.04140, 2024.

11

## Page 12

[37] Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to

interpolation and extrapolation in domain generalization. In Gustau Camps-Valls, Francisco

J. R. Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on

Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research,

pages 2641–2657. PMLR, 28–30 Mar 2022.

[38] Riccardo Volpi, Diane Larlus, and Grégory Rogez. Continual adaptation of visual representa-

tions via domain randomization and meta-learning. In Proceedings of the IEEE/CVF Conference

on Computer Vision and Pattern Recognition, pages 4443–4453, 2021.

[39] Jogendra Nath Kundu, Rahul Mysore Venkatesh, Naveen Venkat, Ambareesh Revanur, and

R Venkatesh Babu. Class-incremental domain adaptation. In Computer Vision–ECCV 2020:

16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII 16, pages

53–69. Springer, 2020.

[40] Peng Zhao and Lijun Zhang. Improved analysis for dynamic regret of strongly convex and

smooth functions. In Learning for Dynamics and Control, pages 48–59. PMLR, 2021.

[41] Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty. Smoothed analysis of online and

differentially private learning. Advances in Neural Information Processing Systems, 33:9203–

9215, 2020.

[42] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[43] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,

Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.

arXiv preprint arXiv:1909.08593, 2019.

[44] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language

Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.

[45] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring the use of large

language models for reference-free text quality evaluation: A preliminary empirical study. arXiv

preprint arXiv:2304.00723, 2023.

[46] AI@Meta. Llama 3 model card. 2024.

[47] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.

In International Conference on Machine Learning, pages 10835–10866. PMLR, 2023.

[48] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization

branches out, pages 74–81, 2004.

[49] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),

June 2019.

[50] Christos Kaplanis, Murray Shanahan, and Claudia Clopath. Policy consolidation for continual

reinforcement learning. arXiv preprint arXiv:1902.00255, 2019.

[51] Philemon Schöpf, Sayantan Auddy, Jakob Hollenstein, and Antonio Rodriguez-Sanchez.

Hypernetwork-ppo for continual reinforcement learning. In Deep Reinforcement Learning

Workshop NeurIPS 2022, 2022.

[52] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa,

Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for

natural language processing?: Benchmarks, baselines, and building blocks for natural language

policy optimization. 2022.

[53] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning

and an application to boosting. Journal of computer and system sciences, 55(1):119–139, 1997.

12

## Page 13

## A

Mathematical Derivations

## A.1

The Proof of Lemma 3.2.1

Proof. To derive our theorem, we first present a necessary lemma on regret bounds.

Lemma A.1.1. [41] Consider an algorithm A that uses Hedge [53] on a finite hypothesis class H′

instead of H, with the expected regret is defined as

## E[Regret(A, D)] = 1

T Es∼D

## " T

## X

t=1

errst (ht) −min

h∈H

## T

## X

t=1

errst(h)

#

,

(14)

where errs(h) = 1{h(s)̸=y}. The expected regret has the upper bound below:

## E[Regret(A, D)] ≤O

p

ln (|H′|) /T



+ 1

T Es∼D

"

max

h∈H min

h′∈H′

## T

## X

t=1

1{h(st)̸=h′(st)}

#

.

(15)

Note that our regret R(T) differs from REGRET(A, D) in the aforementioned theorem by at most

a constant factor of ln 2, which represents the maximum value of l(θ, x). Therefore, the term

## O

p

ln (|H′|) /T



in eq. (15) still holds. Leveraging Lemma A.1.1, we can derive the following

result:

R(T) =Ex∼D

1

## T



## T

## X

t=1

l(ht, xt) −minh∈H

## T

## X

t=1

l(h, xt)



=Ex∼D

1

## T



## T

## X

t=1

l(ht, xt) −minh′∈H′

## T

## X

t=1

l(h′, xt)



+ 1

## T



minh′∈H′

## T

## X

t=1

l(h′

t, xt) −minh∈H

## T

## X

t=1

l(h, xt)



## ≤O(

p

ln(H′)/T) + Ex∼D

1

## T



minh′∈H′maxh∈H(

## T

## X

t=1

l(h′

t, xt) −

## T

## X

t=1

l(h, xt))



.

(16)

## A.2

The Proof of Theorem 3.3.1

Proof. Vanilla DPO:

l(θt−k, xt−k) −l(θt, xt) = l(θt−k, xt−k) −l(θt, xt−k) + l(θt, xt−k) −l(θt, xt)

≤gt−k(θt−k −θt) + l(θt, xt−k) −l(θt, xt),

(17)

where k is a positive integer. According to assumptions 3.3.1 and 3.3.2, we have |gt−k(θt−k −θt)| ≤

Gd; l(θT , xi) −l(θi, xi) ≥−Gd, on the other hand,

l(θt, xt−k) −l(θt, xt) = l(θt, xt−k) −E[l(θt)] + E[l(θt)] −l(θt, xt),

(18)

where l(θt, x1), l(θt, x2), . . . , l(θt, xT ) ∼

iid. l(θt), l(θt) is a distribution of objective function condi-

tioned on the parameters θt. According to the Hoeffding’s inequality, ∃δ0 ∈(0, 1) s.t.

## P

l(θt, xt−k) −E[l(θt)] ≤ln2

r

−lnδ0

2

!

≥1 −δ0,

(19)

likewise, we have

## P

E[l(θt)] −l(θt, xt) ≥ln2

r

−lnδ0

2

!

≤δ0,

(20)

1

## Page 14

which equals to

## P

l(θt, xt) −E[l(θt)] ≤ln2

r

−lnδ0

2

!

≥1 −δ0.

(21)

substituting into the right side of eq. (17), we have

l(θt−k, xt−k) −l(θt, xt) ≤Gd + 2ln2

r

−lnδ0

2

(22)

holds with probability (1 −δ0)2. Furthermore, we can obtain:

l(θi, xi) ≥l(θ1, x1) −(Gd + 2ln2

r

−lnδ

2 ).

(23)

Denote δ = 2(T −1)δ0 −(T −1)(2T −3)(δ0)2[1 −δ0]2T −4, we have the following inequality

holds with probability 1 −δ:

## R(T) = 1

## T



## T

## X

i=1

[l(θT , xi) −l(θi, xi) + l(θi, xi) −l(θ∗, xi)]



≥−Gd + l(θ1, x1) −1

## T

## T

## X

i=1

l(θ∗, xi) −(1 −1

T )(Gd + 2ln2

r

−lnδ0

2 )

= l(θ1, x1) −1

## T

## T

## X

i=1

l(θ∗, xi) −(2 −1

T )Gd −2(1 −1

T )ln2

r

−lnδ0

2 .

(24)

After introducing fast and slow modules:

l(θs−k, xs−k) −l(θs, xs)

≤l(θs−k, xs−k) −l(θs, xs−k) + l(θs, xs−k) −min(l(θs, xs), l(ws, xs))

≤gs−k(θs−k −θs) + l(θs, xs−k) −l(θs, xs) + l(ws, xs) −|l(θs, xs) −l(ws, xs)|

2

≤Gd + l(θs, xs−k) −l(θs, xs) + l(θs, xs) −l(ws, xs)

2

+ |l(θs, xs) −l(ws, xs)|

2

≤l(θs, xs−k) −l(θs, xs) + 2Gd.

(25)

Similar to the process in eq. (18) - eq. (23), we have the derivations from eq. (26) - eq. (30):

l(θs, xs−k) −l(θs, xs) = l(θs, xs−k) −E[l(θs)] + E[l(θs)] −l(θs, xs),

(26)

where l(θs, x1), l(θs, x2), . . . , l(θs, xT ) ∼

iid. l(θs), l(θs) is a distribution of objective function condi-

tioned on the parameters θs. According to Hoeffding’s inequality,

## P

l(θs, xs−k) −E[l(θs)] ≤ln2

r

−lnδ0

2

!

≥1 −δ0,

(27)

## P

E[l(θs)] −l(θs, xs) ≥ln2

r

−lnδ0

2

!

≤δ0,

(28)

thus P

l(θs, xs) −E[l(θs)] ≤ln2

r

−lnδ0

2

!

≥1 −δ0.

(29)

Hence, we can draw a similar conclusion.

## P

"

l(θs, xs−k) −l(θs, xs) ≤2ln2

r

−lnδ0

2

#

≥(1 −δ0)2.

(30)

Therefore, in the setting of OFS-DPO, the eq. (25) gives l(θs−k, xs−k) −l(θs, xs) ≤2Gd + 2c with

probability (1 −δ0)2. Then the follwing equation establishes with probability 1 −δ

ˆR(T) ≥l(θ1, x1) −1

## T

## T

## X

i=1

l(θ∗, xi) −(3 −1

T )Gd −2(1 −1

T )ln2

r

−lnδ0

2 .

(31)

Hence the OFS-DPO algorithm has a smaller lower bound of regret than vanilla DPO.

2

## Page 15

## A.3

The Proof of Proposition 3.3.1

Proof. Consider the objective function of conventional DPO method:

LDP O(θ) = −E(x,yw,yl)∼D[logσ(βlog πθ(yw|x)

πref(yw|x) −βlog πθ(yl|x)

πref(yl|x))],

(32)

where its gradient is

∇θLDP O(θ) =

−βE(x,yw,yl)∼D[σ(ˆrθ(x, yl) −ˆrθ(x, yw))][∇θlogπθ(yw|x) −∇θlogπθ(yl|x)],

(33)

where ˆrθ(x, y) ≜β πθ(y|x)

πref (y|x).

(i)In DPO method, πθ(yl|x) →0, πθ(yw|x) →1. Therefore, ∀ϵ > 0, ∃N > 0, C > 0, and

N + C > ln(1−ϵ/ϵ)

β

, s.t.,

log πθ(yl|x)

πref(yl|x) < −N,

log πθ(yw|x)

πref(yw|x) > C.

(34)

Note that the coefficient of DPO gradient tends towards an exceedingly small value, i.e.,

σ(ˆrθ(x, yl) −ˆrθ(x, yw)) ≤σ(β(−N −C)) =

1

1 + eβ(N+C) ≤ϵ.

(35)

(ii)In OFS-DPO method, LDP O−F S is incorporated as a regularization term, influencing the

acquired gradients during model iteration.

LDP O−new(θF ) = LDP O(θF ) + αLDP O−F S.

(36)

Its gradient can be calculated as follows:

∇θF LDP O−new(θF )

= −βE(x,yw,yl)∼D(σ(ˆrθF (x, yl) −ˆrθF (x, yw))[∇θF logπθF (yw|x) −∇θF logπθF (yl|x)]

|

{z

}

first term

+2ασ(ˆrF S(x, yl) −ˆrF S(x, yw))[∇θF logπθF (yw|x) −∇θF logπθF (yl|x)]

|

{z

}

second term

,

(37)

where ˆrθF (x, y) ≜πθF (y|x)

πref (y|x), ˆrF S(x, y) ≜πθF (y|x)

πθS (y|x) .

The first term of the eq. (37) is equivalent to the form of the original DPO objective function. As

elucidated in the analysis of DPO method, this term gradually diminishes towards zero with the

proceed of training, i.e.,σ(ˆrθ(x, yl) −ˆrθ(x, yw)) ≤ϵ.

In the training stage, F-module and S-module converge to the same objective: πθF (yl|x) →0,

πθF (yw|x) →1, πθS(yl|x) →0, πθS(yw|x) →1. For ϵ > 0, ∃δ1 > 0, s.t., πθF (yw|x) −

πθS(yw|x) = πθS(yl|x) −πθF (yl|x) < δ1.

Choose δ1 = πθS(yl|x)(1 −eln2+ ln(ϵ/1−ϵ)

β

), and there are the following results:

πθF (yw|x)

πθS(yw|x) ∈(1, 2)

⇒πθF (yl|x)

πθS(yl|x) +

δ1

πθS(yl|x) > 1

⇒πθF (yl|x)

πθS(yl|x) > eln2+ ln(ϵ/(1−ϵ))

β

,

(38)

3

## Page 16

substituting into σ(ˆrF S(x, yl) −ˆrF S(x, yw)), we have

σ(ˆrF S(x, yl) −ˆrF S(x, yw))

=σ(βlogπθF (yl|x)

πθS(yl|x) −βlogπθF (yw|x)

πθS(yw|x) )

>σ(ln(

ϵ

1 −ϵ))

=ϵ.

(39)

## A.4

The Proof of Theorem 3.4.1

Proof. Before proceeding with our formal proof, we first present some necessary theorems [19]:

Theorem A.4.1. Given source and target datasets with probability distribution PS and PT , there

exists data perturbation so that the training loss of any neural network l(θ, ·) for target distribution

equals that for source distribution with data perturbation, i.e.,

Ex∼PT [l (θ, x)] = Eδx(x)Ex∼PS [l (θ, x + δx(x))] .

(40)

Theorem A.4.2. Considering the source dataset with distribution PS, suppose the source dataset

is perturbed with data perturbation δ, and the loss of the neural network is given by l(θ, ·). In the

general case, there exists a model weight perturbation ∆θ such that the training loss on the perturbed

source dataset is the same as the training loss with the model weight perturbation ∆θ on the source

distribution:

Eδx(x)Ex∼PS[l(θ, x + δx(x))] = Ex∼PS[l(θ + ∆θ, x)].

(41)

By the Definition 3.4.1, the empirical regret in cross-domain scenarios can be expressed as follows:

## R(T1, T2)

=[ 1

## T1

## T1

## X

i=1

l(θT1, s(1)

i ) + 1

## T2

## T2

## X

j=1

l(θT2, s(2)

j )] −[ 1

## T1

## T1

## X

i=1

l(θ∗, s(1)

i ) + 1

## T2

## T2

## X

j=1

l(θ∗, s(2)

j )]

=[ 1

## T1

## T1

## X

i=1

l(θT1, s(1)

i ) + 1

## T2

## T2

## X

j=1

l(θT2, s(2)

j )] −[ 1

## T1

## T1

## X

i=1

l(θ(β), s(1)

i ) + 1

## T2

## T2

## X

j=1

l(θ(β)), s(2)

j )]

+ [ 1

## T1

## T1

## X

i=1

l(θ(β), s(1)

i ) + 1

## T2

## T2

## X

j=1

l(θ(β), s(2)

j )] −[ 1

## T1

## T1

## X

i=1

l(θ∗, s(1)

i ) + 1

## T2

## T2

## X

j=1

l(θ∗, s(2)

j )]

= L(θT1, θT2) −L(θ(β), θ(β))

|

{z

}

first term

+ L(θ(β), θ(β)) −L(θ∗, θ∗)

|

{z

}

second term

,

(42)

where θ(β) = θ0 + β∆θ(1) + (1 −β)∆θ(2). For the first term of R(T1, T2), L(θT1, θT2) −

L(θ(β), θ(β)), we have:

L(θT1, θT2) −L(θ(1), θ(2))

|

{z

}

first term

+ L(θ(1), θ(2)) −L(θ(β), θ(β))

|

{z

}

second term

.

(43)

The first term of eq. (43) can be written as :

A1 ≜L(θT1, θT2) −L(θ(1), θ(2))

= 1

## T1

## T1

## X

i=1

(l(θT1, s(1)

i ) −l(θ(1), s(1)

i )) + 1

## T2

## T2

## X

j=1

(l(θT2, s(1)

j ) −l(θ(2), s(2)

j ))].

(44)

4

## Page 17

Using the result of theorem 3.3.1, there exist δ1, δ2 ∈(0, 1), s.t.

A1 ≥l(θ1, s(1)

1 ) −1

## T1

## T1

## X

i=1

l(θ(1), s(1)

i ) −



2 −1

## T1

+ (1 −1

## T1

)1{mode=F S}



Gd

−2(1 −1

## T1

) ln 2

r

−ln δ1

2

+ l(θ1, s(2)

1 ) −1

## T2

## T2

## X

j=1

l(θ(2), s(2)

j )

−



2 −1

## T2

+ (1 −1

## T2

)1{mode=F S}



Gd −2(1 −1

## T2

) ln 2

r

−ln δ2

2

≥l1(s(1)

1 , s(2)

## 1 ) −B(T1, T2) −[4 −T1 + T2

## T1T2

## + (2 −T1 + T2

## T1T2

)1{mode=F S}]Gd

## −2(1 −T1 + T2

## T1T2

)c,

(45)

here B(T1, T2) =

1

## T1

## Pt1

i=1 l(θ(1), s(1)

i ) +

1

## T2

## Pt2

j=1 l(θ(2), s(2)

j ), l1(s(1)

1 , s(2)

1 ) = l(θ1, s(1)

1 ) +

l(θ1, s(2)

1 ), c = max{ln2

q

−lnδ1

2 , ln2

q

−lnδ2

2 }, δ1, δ2 ∈(0, 1) are constants, and 1{mode=F S}

represents whether to introduce fast and slow models. Meanwhile, the second term of eq. (43) can be

denoted as A2:

A2 ≜L(θ(1), θ(2)) −L(θ(β), θ(β))

= 1

## T1

## T1

## X

i=1

(l(θ(1), s(1)

i ) −l(θ(β), s(1)

i )) + 1

## T2

## T2

## X

j=1

(l(θ(β), s(2)

j ) −l(θ(2), s(2)

j )).

(46)

For convenience of derivation, we make the following symbol conventions:

δθ(1)(β) ≜∆θ(1) −∆θ(β) = (1 −β)(∆θ(1) −∆θ(2)),

(47)

δθ(2)(β) ≜∆θ(2) −∆θ(β) = β(∆θ(2) −∆θ(1)).

(48)

According to theorem A.4.2 and theorem A.4.1,

EδD1(x)Ex∼Dl(θ(β), x + δD1(x)) = Ex∼Dl(θ(β) + δθ(1)(β), x).

(49)

Perform the first-order Taylor expansion on both sides of the eq. (49), we have

EδD1(x)Ex∼D[l(θ(β), x) + ∇xl(θ(β), ˆx1)δD1(x)] = Ex∼D[l(θ(β), x) + ∇θl(ˆθ(1)(β), x)δθ(1)(β)].

(50)

Hence, we can obtain

δθ(1)(β) = (Ex∼D∇θl(ˆθ(1)(β), x))−1EδD1(x)Ex∼D∇xl(θ(β), ˆx1)δD1(x),

δθ(2)(β) = (Ex∼D∇θl(ˆθ(2)(β), x))−1EδD2(x)Ex∼D∇xl(θ(β), ˆx2)δD2(x).

And A2 can be represented as:

## A2 = 1

## T1

## T1

## X

i=1

∇θl(ˆθ(1)(β), s(1)

i )δθ(1)(β) + 1

## T2

## T2

## X

j=1

∇θl(ˆθ(2)(β), s(2)

j )δθ(2)(β).

(51)

Combine eq. (45) and eq. (51) and substitute into eq. (43), with the probility (1 −δ1)(1 −δ2) holds:

A1 + A2 ≥l1(s(1)

1 , s(2)

## 1 ) −B(T1, T2)

## −[4 −T1 + T2

## T1T2

## + (2 −T1 + T2

## T1T2

)1{mode=F S}]Gd −2(1 −T1 + T2

## T1T2

)c

+ 1

## T1

## T1

## X

i=1

∇θl(ˆθ(1)(β), s(1)

i )δθ(1)(β) + 1

## T2

## T2

## X

j=1

∇θl(ˆθ(2)(β), s(2)

j )δθ(2)(β).

(52)

5

## Page 18

For the second term of R(T1, T2) i.e. L(θ(β), θ(β))−L(θ∗, θ∗), denote θ∗+δ(1) ≜θ(1), θ∗+δ(2) ≜

θ(2) (θ0 + ∆θ∗+ δ(1) = θ0 + ∆θ(1), θ0 + ∆θ∗+ δ(2) = θ0 + ∆θ(2)), we make the derivations

below:

L(θ(β), θ(β)) −L(θ∗, θ∗) = 1

## T1

## T1

## X

i=1

(l(θ(β), s(1)

i ) −l(θ∗, s(1)

i ))

+ 1

## T2

## T2

## X

j=1

(l(θ(β), s(2)

j ) −l(θ∗, s(2)

j ))

=[ 1

## T1

## T1

## X

i=1

∇θl(ˆθ(1)(β), s(1)

i ) + 1

## T2

## T2

## X

j=1

∇θl(ˆθ(2)(β), s(2)

j )](θ(β) −θ∗),

(53)

where θ(β) −θ∗= βδθ(1) + (1 −β)δθ(2). Then combined with eq. (42), we obtain

R(T1, T2) ≥l1(s(1)

1 , s(2)

## 1 ) −B(T1, T2)

## −[4 −T1 + T2

## T1T2

## + (2 −T1 + T2

## T1T2

)1{mode=F S}]Gd −2(1 −T1 + T2

## T1T2

)c

+ 1

## T1

## T1

## X

i=1

∇θl(ˆθ(1)(β), s(1)

i )δθ(1)(β) + 1

## T2

## T2

## X

j=1

∇θl(ˆθ(2)(β), s(2)

j )δθ(2)(β)

+ [ 1

## T1

## T1

## X

i=1

∇θl(ˆθ(1)(β), s(1)

i ) + 1

## T2

## T2

## X

j=1

∇θl(ˆθ(2)(β), s(2)

j )](θ(β) −θ∗).

(54)

For all summation terms over distribution D1, we can derive the lower bound below:

## T1

## X

i=1

∇θl



ˆθ(1)(β), s(1)

i



δθ(1)(β) +

## T1

## X

i=1

∇θl



ˆθ(1), s(1)

i

 

βδθ(1) + (1 −β)δ(2)

=

## T1

## X

i=1

∇θl



ˆθ(1)(β), s(1)

i



δθ(1)(β) −

## T1

## X

i=1

∇θl



ˆθ(1), s(1)

i



δθ(1)(β)

+

## T1

## X

i=1

∇θl



ˆθ(1), s(1)

i



δθ(1)(β) +

## T1

## X

i=1

∇θl



ˆθ(1), s(1)

i

 

βδθ(1) + (1 −β)δθ(2)

=

## T1

## X

i=1

∇θl



ˆθ(1)(β), s(1)

i



δθ(1)(β) −

## T1

## X

i=1

∇θl



ˆθ(1), s(1)

i



δθ(l)(β)

+

## T1

## X

i=1

∇θl



ˆθ(1), s(1)

i

 

∆θ(1) −∆θ∗

≥−T1Gd +

## T1

## X

i=1

∇θl



ˆθ(1), s(1)

i

 

∆θ(1) −∆θ∗

≥−T1Gd.

(55)

The last inequality holds because ∇θl(ˆθ(1), s(1)

i )(∆θ(1) −∆θ∗) can be viewed as the inner product of

two vectors. Specifically, ∇θl(ˆθ(1), s(1)

i )(∆θ(1) −∆θ∗) = m1m2γ, where m1 = ∥∇θl(ˆθ(1), s(1)

i )∥,

m2 = ∥∆θ(1) −∆θ∗∥, and γ represents the cosine of the angle between these two vectors. We

note that ∇θl(ˆθ(1), s(1)

i ) points to the optimal parameter on distribution D1, while (∆θ(1) −∆θ∗)

represents the direction from the optimal parameter on the dual-task distribution to the opti-

mal parameter on distribution D1. In theory, the angle between these two vectors is less than

90 degrees, i.e., ∇θl(ˆθ(1), s(1)

i )(∆θ(1) −∆θ∗) > 0. Similarly, there are analogous properties

for all summation terms over distribution D2 that ensure PT2

j=1 ∇θl



ˆθ(2)(β), s(2)

j



δθ(2)(β) +

6

## Page 19

## Pt2

j=1 ∇θl



ˆθ(2), s(2)

j

  βδθ(2) + (1 −β)δθ(2)

≥−T2Gd holds true. Hence,

R(T1, T2) ≥l1(s(1)

1 , s(2)

## 1 ) −B(T1, T2)

## −[4 −T1 + T2

## T1T2

## + (2 −T1 + T2

## T1T2

)1{mode=F S}]Gd −2(1 −T1 + T2

## T1T2

)c

−2Gd

=l1(s(1)

1 , s(2)

## 1 ) −B(T1, T2)

## −[6 −T1 + T2

## T1T2

## + (2 −T1 + T2

## T1T2

)1{mode=F S}]Gd −2(1 −T1 + T2

## T1T2

)c.

(56)

7

## Page 20

## B

Proposed Algorithms

The OFS-DPO algorithm is presented in Algorithm 1. Unlike DPO, we incorporate fast and slow

modules to simulate intraspecific competition, thereby accelerating the evolutionary process. The

COFS-DPO algorithm is depicted in Algorithm 2. For task-1 in the cross-domain setting, the

parameters of the fast module, θF

1 , are trained and subsequently used to initialize both the fast and

slow modules for task-2. Training then proceeds on task-2. Ultimately, the parameters of the fast

modules from both tasks, θF

1 and θF

2 , are combined with appropriate weights to form the final model.

Algorithm 1 OFS-DPO Algorithm

Input: SFT model MSF T , Data stream D, update k

Output: Fast module param θF

1: Initialize F-module(MF ), S-module(MS) with MSF T

2: for i in D do

3:

gF

t = ∇θF LDP O−new(θF )

4:

gS

t = ∇θSLDP O−new(θS)

5:

update θF , θS with gF

t , gS

t respectively

6:

if i%k == 0 then

7:

if LDP O(θS) < LDP O(θF ) then

8:

Interchange MF , MS

9:

end if

10:

end if

11: end for

Algorithm 2 COFS-DPO Algorithm

## Step 1:

Task 1

Input: MSF T , Task1 data D1, update k

Output: Fast module param θF

1

1: Initialize MF , MS with MSF T

2: for i in D1 do

3:

gF

t = ∇θF LDP O−new(θF )

4:

gS

t = ∇θSLDP O−new(θS)

5:

update MF , MS with gF

t , gS

t respectively

6:

if i%k == 0 then

7:

if LDP O(θS) < LDP O(θF ) then

8:

Interchange MF , MS

9:

end if

10:

end if

11:

Reserve data in M1 with randomness

12: end for

Task 2

Input: θF

1 , Task2 data D2, update k

Output: Fast module param θF

2

1: Initialize MF , MS with θF

1

2: for i in D2 do

3:

gF

t = ∇θF LDP O−new(θF )

4:

gS

t = ∇θSLDP O−new(θS)

5:

update MF , MS with gF

t , gS

t respectively

6:

if i%k == 0 then

7:

if LDP O(θS) < LDP O(θF ) then

8:

Interchange MF , MS

9:

end if

10:

end if

11:

Reserve data in M2 with randomness

12: end for

## Step 2:

β∗

1 ∈(0, 1), β∗

2 ∈(0, 1), Using the retained data M1, M2, by COFS-DPO, look for β∗

1, β∗

2 that

have the best performance of generalization on the overall distribution after linear combination of

the model parameter θ(β) = β∗

1θF

1 + β∗

2θF

2

## C

Experimental Details

## C.1

In-domain experiments

In the in-domain task experiments, we employ distinct models and datasets across three tasks to

assess the method’s continual learning capability. Each experiment is limited to a single epoch, and

all experiments are conducted on a single NVIDIA A800 80G GPU. The hyperparameters used in the

experiments are detailed in Table 4. The evaluation methodology for GPT-4 remains consistent with

8

## Page 21

DPO throughout the experiments. Each evaluation involves collecting 120 samples from the test set,

with the prompts used during evaluation detailed in Table 5.

Table 4: Hyperparameters of different in-domain tasks.

Hyperparameters

Controlled sentiment generation

Summarization

Single-turn dialogue

model

gpt2-large

gptj

pythia28

batch size

4

2

2

gradient accumulation steps

1

2

2

seq length

550

550

550

optimizer

adamw

adamw

adamw

slow lr

## 5.00E-07

## 5.00E-07

## 5.00E-07

betas

[0.9, 0.999]

[0.9, 0.999]

[0.9, 0.999]

eps

## 1.00E-08

## 1.00E-08

## 1.00E-08

weight decay

## 1.00E-06

## 1.00E-06

## 1.00E-06

update k

10

5

10

loss weight α

0.1

0.7

0.7

lr times

2

2

2

Table 5: Evaluation prompts for all in-domain tasks.

Task

Prompt

Controlled

sentiment

generation

Which of the following controlled sentiment generations does a better job of

generating the given text, without deviating from the text? A good generation

is both positive and logical.

prefixes: <test>

generation A: <chosen>

generation B: <model output>

FIRST provide a one-sentence comparison of the two generations, explaining

which you prefer and why. SECOND, on a new line, state only "A" or "B" to

indicate your choice. Your response should use the format:

Comparison: <one-sentence comparison and explanation>

More positive: <"A" or "B">

Summarization

Which of the following summaries does a better job of summarizing the most

important points in the given forum post, without including unimportant or ir-

relevant details? A good summary is both precise and concise.

Post: <test>

Summary A: <chosen>

Summary B: <model output>

FIRST provide a one-sentence comparison of the two summaries, explaining

which you prefer and why. SECOND, on a new line, state only "A" or "B" to

indicate your choice. Your response should use the format:

Comparison: <one-sentence comparison and explanation>

Preferred: <"A" or "B">

Single-turn

dialogue

For the following query to a chatbot, which response is more helpful?

Query: <user query>

Response A: <chosen>

Response B: <model output>

FIRST provide a one-sentence comparison of the two responses and explain

which is more helpful. SECOND, on a new line, state only "A" or "B" to in-

dicate which response is more helpful. Your response should use the format:

Comparison: <one-sentence comparison and explanation>

More helpful: <"A" or "B">

9

## Page 22

## C.2

Cross-domain experiments

In cross-domain experiments, we follow the task setting of CPPO, splitting the dataset into two task

domains to assess the method’s ability to retain old knowledge while learning new knowledge. In

both COFS-DPO and the baseline method, experiments are conducted with two models: GPT2-s and

LLaMA3. Once training is completed in both task domains, COFS-DPO aggregates the two LoRAs

through weighted fusion to construct the final model. Experiments using GPT2-s can be completed

on a single NVIDIA A800 80G GPU, whereas studies with LLaMA3 require two A800 GPUs. The

hyperparameters used in the experiments are shown in Table6. The metrics utilized for evaluation

were adapted from the CPPO setup, with rPMs and ROUGE scores calculated based on the degree of

alignment determined by the reference PM, as given in Table 7.

Table 6: The hyperparameters of various methods.

Hyperparameters

## Cppoh

## Dpo

ours

model

GPT2-s and Llama3

seq-length

550

550

550

total steps

25600

-

-

optimizer

adamw

adamw

adamw

lr

## 1.00E-05

## 5.00E-07

## 5.00E-07

betas

[0.9, 0.999]

[0.9, 0.999]

[0.9, 0.999]

eps

## 1.00E-08

## 1.00E-08

## 1.00E-08

weight-decay

## 1.00E-06

## 1.00E-06

## 1.00E-06

update k

-

-

10

loss weight α

-

-

0.7

lr times

-

-

2

Table 7: Metrics for our cross-domain tasks.

Task ID

Metric

Definition

Task-1

rPM score on task-1(rPMS1)

rPM(M1, Dtest

1

)

Rouge score on task-1(Rouge1)

Rouge(M1, Dtest

1

)

Final

rPM score on task-1(rPMS1)

rPM(Mf, Dtest

1

)

rPM score on task-2(rPMS2)

rPM(Mf, Dtest

2

)

Rouge score on task-1(Rouge1)

Rouge(Mf, Dtest

1

)

Rouge score on task-2(Rouge2)

Rouge(Mf, Dtest

2

)

Score Forgetting Ratio (SFR)

rPM(M1, Dtest

1

) −rPM(Mf, Dtest

1

)

10



## Implementation Status

### Core Components
- ✅ **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- 🔄 **Performance Monitoring**: Continuous validation of targets
- ✅ **Documentation Standards**: Compliant with ACGS-2 requirements
- 🔄 **Cross-Reference Validation**: Ongoing link integrity maintenance

### Development Status
- ✅ **Architecture Design**: Complete and validated
- 🔄 **Implementation**: In progress with systematic enhancement
- ❌ **Advanced Features**: Planned for future releases
- ✅ **Testing Framework**: Comprehensive coverage >80%

### Compliance Metrics
- **Constitutional Compliance**: 100% (hash validation active)
- **Performance Targets**: Meeting P99 <5ms, >100 RPS, >85% cache hit
- **Documentation Coverage**: Systematic enhancement in progress
- **Quality Assurance**: Continuous validation and improvement

**Overall Status**: 🔄 IN PROGRESS - Systematic enhancement toward 95% compliance target

## Performance Requirements

### ACGS-2 Performance Targets
- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)  
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

### Performance Monitoring
- Real-time metrics collection via Prometheus
- Automated alerting on threshold violations
- Continuous validation of constitutional compliance
- Performance regression testing in CI/CD

### Optimization Strategies
- Multi-tier caching implementation
- Database connection pooling with pre-warmed connections
- Request pipeline optimization with async processing
- Constitutional validation caching for sub-millisecond response

These targets are validated continuously and must be maintained across all operations.
