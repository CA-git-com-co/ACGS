# 2403.15583_U-ARE-ME-Uncertainty-Aware-Rotation-Estimation-in-
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 2403.15583_U-ARE-ME-Uncertainty-Aware-Rotation-Estimation-in-.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2

---

## Page 1

U‚ÄìARE‚ÄìME: Uncertainty-Aware Rotation

Estimation in Manhattan Environments

Aalok Patwardhan, Callum Rhodes, Gwangbin Bae, and Andrew J. Davison

Dyson Robotics Lab, Imperial College London {a.patwardhan21, c.rhodes, g.bae,

a.davison}@imperial.ac.uk

Fig. 1: U‚ÄìARE‚ÄìME provides globally consistent rotation estimates in Manhattan envi-

ronments across sequences of RGB images ‚Äì without camera intrinsics. For each frame,

we estimate the rotation from the predicted surface normals along with pixel-wise un-

certainty and enforce temporal consistency via a factor graph.

Abstract. Camera rotation estimation from a single image is a chal-

lenging task, often requiring depth data and/or camera intrinsics, which

are generally not available for in-the-wild videos. Although external sen-

sors such as inertial measurement units (IMUs) can help, they often

suffer from drift and are not applicable in non-inertial reference frames.

We present U-ARE-ME, an algorithm that estimates camera rotation

along with uncertainty from uncalibrated RGB images. Using a Man-

hattan World assumption, our method leverages the per-pixel geomet-

ric priors encoded in single-image surface normal predictions and per-

forms optimisation over the SO(3) manifold. Given a sequence of im-

ages, we can use the per-frame rotation estimates and their uncertainty

to perform multi-frame optimisation, achieving robustness and tempo-

ral consistency. Our experiments demonstrate that U-ARE-ME performs

comparably to RGB-D methods and is more robust than sparse feature-

based SLAM methods. We encourage the reader to view the accompa-

nying video at https://callum-rhodes.github.io/U-ARE-ME for a visual

overview of our method.

arXiv:2403.15583v1  [cs.CV]  22 Mar 2024

## Page 2

2

A. Patwardhan et al.

0

5

10

15

Time [s]

0

20

40

60

80

100

ARE [deg]

## U-Are-Me

## Imu

Time [s]

Estimated Horizon at ùë°= 4ùë†

## U-Are-Me

Estimated Horizon at ùë°= 4ùë†

Realsense IMU

ARE [deg]

Fig. 2: IMU sensors are prone to drift especially in non-inertial frames of reference

(e.g. inside a moving vehicle). The horizon line in each image represents the line per-

pendicular to the up-vector inferred from our proposed method (middle) and an IMU

sensor (right).

1

Introduction

Accurate estimation of camera rotation from a sequence of monocular images is

crucial for many computer vision applications, including visual odometry [20],

image stabilisation [37], and augmented reality [46]. Many solutions have been

proposed for a variety of sensor setups. For instance, the recently released Apple

Vision Pro operates using visual-inertial odometry, relying on both the cameras

and the inertial measurement units (IMUs). However, IMUs are prone to drift

and are by design not suitable for non-inertial frames of reference (see Fig. 2).

If depth measurements (paired with the input images) are available, the RGB-

D frames can be aligned ‚Äî based on photometric and geometric consistency ‚Äî

to recover their relative camera poses [9]. If the surface normal vectors in the

scene are aligned with a set of principal directions, the camera rotation can

be found by aligning the input normals (extracted from the depth maps) to

those directions [16, 41, 43]. While such approaches provide drift-free rotation

estimates with high accuracy, they cannot be applied to in-the-wild videos or

devices without a depth sensor.

This paper focuses on the most challenging setup in which only RGB input

is available. Previous attempts have focused on detecting and matching 2D im-

age features. For instance, ORB-SLAM [38] tracks sparse ORB features, while

methods like [5, 30] group line segments to identify the vanishing points and

hence the camera rotation with respect to the principal directions. However,

such methods are sensitive to image degradation (e.g. noise and motion blur)

and perform poorly in textureless environments. More importantly, they assume

known camera intrinsics ‚Äî which are often not available for in-the-wild videos.

While a neural network can be trained to regress the rotation between consec-

utive frames [6], such an approach is prone to overfitting and drift. It is also

computationally costly to train such a specialised model.

In this work, we propose to make use of the dense pixel-wise geometric pri-

ors learned by single-image surface normal estimation models. Surface normal

estimation models are efficient (e.g. [2] runs at ‚àº60 fps on a 4090 GPU) and

## Page 3

## U‚ÄìAre‚ÄìMe

3

have strong generalisation ability [2,3]. In recent years, their usefulness has been

demonstrated for various computer vision tasks, including object grasping [51],

multi-task learning [35], simultaneous localisation and mapping [33], and CAD

model alignment [28]. We explore whether such powerful front-end perception

can also be used for rotation estimation.

Similar to previous optimisation-based approaches [16, 41, 43], we assume a

certain distribution of surface normal vectors in world coordinates and optimise

for the camera rotation that would align the predicted normals to the principal

directions of the scene. While previous methods (1) used depth sensors to extract

the normal vectors and (2) were only applicable to a single image, we attempt

to remove both constraints. Two types of uncertainty arise in the process of

removing these two commonly adopted constraints.

First is the heteroscedastic aleatoric uncertainty [25] in surface normal predic-

tions. As shown in [2], surface normals predicted by a neural network ‚Äî unlike

those extracted from a depth map ‚Äî are unreliable, especially for the pixels

near object boundaries and on small objects. As these pixels should be down-

weighted in the optimisation objective, we introduce a new uncertainty-weighted

cost function and show how the uncertainty can be learned in a data-driven man-

ner.

The second type of uncertainty arises when the image contains a limited

number of principal directions. For instance, when a Manhattan World (MW) [7]

is assumed, two (or more) of the six directions (¬±X, ¬±Y, ¬±Z) should be observed

to determine the camera rotation. If only one axis is visible, any rotation around

that axis would result in an equally valid prediction. To this end, we quantify

the uncertainty around each principal axis and use it to enhance the temporal

consistency in the predictions.

To summarise, our framework alternates between two optimisation steps:

‚Äì Single-frame optimisation: We optimise the world-to-camera rotation

matrix such that the rotated principal directions are best aligned with the

predicted surface normals. We improve the accuracy and robustness by in-

troducing an uncertainty-weighted cost function.

‚Äì Multi-frame optimisation: We take the covariance matrix of rotation

around each axis ‚Äî which is readily available from the Hessian approxi-

mation in the second-order optimisation of the first step ‚Äî and use it to

jointly optimise a sliding window of previous frame rotations. We improve

the global consistency of our solution, reject outlier rotations and intuitively

handle frames that may contain limited information on certain principal

axes.

The proposed method runs at ‚àº40 fps on an NVIDIA 4090 GPU. Note that,

unlike the learning-based models that can only be used for rotation estimation,

the surface normal predictions ‚Äî from which we infer the rotation ‚Äî can be

used for other tasks, reducing the overall computational overhead.

The main strength of our approach lies in its robustness. Compared to the

methods that rely on sparse feature tracking or line segment detection, our ap-

proach is more robust to the presence of image degradation. Unlike SLAM-based

## Page 4

4

A. Patwardhan et al.

methods, our approach does not require camera intrinsics and can be applied

to a single image or in-the-wild videos, making it useful in a wider range of

scenarios.

2

Related work

Rotation estimation of a camera from single images has been extensively studied

and is generally based upon the assumption that indoor scenes exhibit inherent

structure, conforming to the MW assumption. Estimating the Manhattan Frame

(MF) ‚Äì the three principal orthogonal directions of the MW scene thus becomes

the primary focus of rotation estimation approaches. These approaches for MF

estimation broadly fall into two domains; using RGB images to extract per-

spective cues, and using 3D information such as surface normals from RGB-D

images.

Earlier work estimates the MF by considering vanishing points and lines in an

image as perspective cues [27]. The work in [29] generates several MF hypotheses

from an image, using line segments to find the best fitting model. In [15] line

segments are extracted onto a hemisphere and clustered to identify three orthog-

onal directions, however, this method is sensitive to the chosen resolution of the

hemisphere discretisation. The algorithm proposed by [13] uses line clustering to

find three vanishing points, and achieves real-time camera rotation estimation

over a sequence of images in a video. The work in [30] (ROVE) does not rely

on the MW assumption but uses sequential Bayesian filtering to jointly estimate

rotation and vanishing points. These RGB methods rely on the existence of mul-

tiple parallel lines and vanishing points, and are not robust in the presence of

noise and outliers, or in texture-less scenes.

Rotation estimation methods that use RGB-D images are more accurate and

stable as they utilise 3D information in the scene, whether this is directly through

depth camera data, or by using this data to compute the surface normals in the

scene. The approach in [41] uses point normals and perspective cues to perform

an Exhaustive Search (ES) over a set of candidate directions and uses a scoring

heuristic to estimate the MF, although this incurs a high computational cost.

Depth data from a Kinect camera was used in [47] to determine the MF of a scene

by identifying the ground, and selecting a perpendicular direction from one of

the walls. This method relies on the presence of a visible floor and multiple walls

in the image, and so is not generalisable. In [16] the MF is estimated through

non-convex optimisation by considering the sparsity constraints of MF-aligned

surface normals.

In [44] the authors argue that real-world scenes contain a Mixture of Manhat-

tan Frames (MMF), which they simultaneously estimate from surface normals

calculated from depth data. Their follow-up work ‚Äì RTMF [43] ‚Äì presented a

GPU-accelerated version for real-time applications.

These methods are often sensitive to initial conditions and cannot guarantee

global optimality, unlike the family of Branch and Bound (BnB) methods which

operate in the rotation search space [4,19,39]. These methods guarantee global

## Page 5

## U‚ÄìAre‚ÄìMe

5

optimality, but cannot be considered real-time algorithms. Real-time rotation

estimation is enabled in [24] using the BnB method by maximising the consensus

set of inliers over the search space of rotation. Surface normals are discretised on

an equi-rectangular plane to generate the Extended Gaussian Image (EGI) [21],

from which the BnB approach is used to estimate the MF.

Recently [52] proposed a novel and efficient cost function of Multiple Normal

vectors and Multiple MF Axes (MNMA) for MF estimation. The cost function

makes use of the vector dot and cross products between the scene surface normals

and the axes of the MF leading to an efficient, accurate and real-time algorithm.

All of the methods considered here have used surface normals calculated from

depth data from RGB-D images, rather than directly estimating them from an

RGB input. Estimating surface normals has traditionally been computationally

intensive, producing unreliable results. We make use of the recent advances in

fast and accurate surface normal estimation [3], and build upon [52] to enable

fast rotation estimation directly from RGB images. Unlike the previous work in

this field, our method also captures per-frame estimates of rotational uncertainty.

This enables us to perform uncertainty-aware robust multi-frame optimisation

leading to temporal consistency between frames.

3

Method

Given a monocular video, our goal is to estimate the per-frame camera rotation

relative to the world coordinates. We begin by assuming that the scene satisfies

the MW assumption [7], which is valid for a wide range of indoor/outdoor scenes.

Note that it is straightforward to extend our approach to other world assump-

tions (e.g. Mixture of Manhattan Worlds [44], Atlanta World [40], and Hong

Kong World [32]), given that the principal directions in the world coordinates

are known a priori.

Our method is named U-ARE-ME (Uncertainty-Aware Rotation Estima-

tion in Manhattan Environments), as it can be used to complement or replace the

I-M-U sensors. We leverage the recent advances in single-image surface normal

estimation and propose to infer the camera rotation by aligning the predicted

normals to the world assumption. In Sec. 3.1 we introduce a new uncertainty-

weighted optimisation objective and show how the uncertainty can be learned

from data. For real-time video applications, it is important to ensure temporal

consistency in the predictions. We explain in Sec. 3.2 the factor graph formula-

tion required to achieve this.

3.1

Uncertainty-aware rotation estimation from a single image

Suppose that a surface normal vector ni ‚ààS2 corresponding to the i-th pixel

is aligned with one of the principal directions. Then, for any Manhattan axis

r ‚àà{¬±X, ¬±Y, ¬±Z}, the angle Œ∏ = cos‚àí1(ni ¬∑ r) should be {0‚ó¶, 90‚ó¶, 180‚ó¶}. To this

end, Zhang et al. [52] introduced a cost function E(r|ni) = sin2 Œ∏ cos2 Œ∏, which

## Page 6

6

A. Patwardhan et al.

Fig. 3: (left) This figure visualises the cost function defined by a single normal vector.

The cost is minimised when the (rotated) principal axes are parallel or vertical. (mid-

dle) Here we compare the shape of different probability distributions defined over a

unit sphere. (right) This figure visualises the cost function defined by three mutually

orthogonal Manhattan axes.

is visualised in Fig. 3. We modify this cost by multiplying it by some confidence

measure Œ∫.

\labe l { e qn :met h od-c ost} E(\mathbf {r}|\mathbf {n}_i, \kappa _i) = \kappa _i \sin ^2 \theta \cos ^2 \theta

(1)

To learn Œ∫ in a data-driven manner, we define the following training loss:

\la

b el { eqn : metho d -l oss}  \mat hcal {L}(\mathbf {n}^{gt}_i|\mathbf {n}_i,\kappa _i) = C(\kappa _i) + \kappa _i \sin ^2 \theta \cos ^2 \theta

(2)

where ngt

i is the ground truth and Œ∏ is the angular error of the predicted normal

ni. C(Œ∫) should be a monotonically decreasing function of Œ∫ to prevent the

model from estimating Œ∫i = 0 for every pixel. Another thing to note is that the

second term should be defined only for 0‚ó¶‚â§Œ∏ < 45‚ó¶. Otherwise, the loss can be

minimised by increasing the error. To satisfy such constraints, we assume that

the surface normal probability distribution can be parameterised as follows:

\la

b el { eqn :

m

ethod -pdf} \ begi n  {al ig

ned}

p (

\

mathb f {n} ^{

g t

}_i|

\ ma t

h

bf {n

}_i,\ k a ppa  _i) &= \begin {cases} D(\kappa _i) \exp (- \kappa _i \sin ^2 \theta \cos ^2 \theta ) \;\;\; \text {when} \;\;\; \theta < \frac {\pi }{4}\\ D(\kappa _i) \exp (- \frac {\kappa _i}{4}) \;\;\; \text {when} \;\;\; \theta >= \frac {\pi }{4} \end {cases} \\ \text {where}\;\;\; C(\kappa _i) &= - \log D(\kappa _i). \end {aligned}

(3)

Then, L(ngt

i |ni, Œ∫i) can be interpreted as the negative log-likelihood of the

above distribution. D(Œ∫) is a monotonically increasing function of Œ∫ as the dis-

tribution should be normalised. During training, the network is encouraged to

increase the value of Œ∫ for the pixels with lower error, thereby encoding confidence

in the prediction. In Fig. 3, we provide a visual comparison of the proposed dis-

tribution against the von Mises-Fisher distribution [14] and the Angular vonMF

distribution [2]. As the analytic form for D(Œ∫) could not be found, we obtain

the values for Œ∫ ‚àà[0, 105] and fit them using natural cubic splines. We use a

lightweight convolutional encoder-decoder architecture [1] and use the training

data of [3]. See Appendix A for additional detail regarding the network training.

Using the formulation of the problem in [52], Eq. (1) can be given in terms

of a rotation matrix R ‚ààSO(3) that gives the rotation from the camera to the

## Page 7

## U‚ÄìAre‚ÄìMe

7

world frame. We then use Levenberg-Marquardt (LM) optimisation to minimise

the cost function, rewriting it in terms of the corresponding residual function

f(R) to obtain the optimal rotation R‚àó(see [52] for a full derivation).

\ mat hbf

## {R^*} &=

\a

r gmi n _

{

\mathbf {R}} E(\mathbf {R}|\mathbf {n},\kappa ) \\ &= \argmin _{\mathbf {R}} f(\mathbf {R})^\intercal f(\mathbf {R}).

(5)

From [52], the analytical Jacobian of the residual function, Jf(R), is given

with respect to a change in R i.e. exp(‚àÜœï)R, where ‚àÜœï refers to the Lie algebra

of a change in rotation about R (and therefore about r). This allows us to acquire

an approximation for the covariance Œ£R as:

\la b el {e

q:j

ac}

J _ f(\ m athbf {R}) &= \frac {\partial f(R)}{\partial \Delta \phi }\\ \label {eq:sigma} \Sigma ^{R} = H^{-1} &\approx (J_f(\mathbf {R})^\intercal J_f(\mathbf {R}))^{-1}

(7)

where H is the hessian matrix approximation from the second order optimisation.

Having obtained the optimal MW rotation Rmw = R‚àó‚ä∫and its uncertainty,

this estimate can then be used for further downstream tasks. This could be

for bootstrapping visual odometry systems, correcting drift in inertial pipelines,

rectifying images for CNNs, which are sensitive to image rotation, as well as

camera calibration to name a few. In the following section we address one such

extension, that being to estimate camera rotation across a sequence of images

to achieve a coherent trajectory.

3.2

Multi-frame rotation estimation for temporal consistency

When estimating single-frame rotation using the above method, the rotation

relative to the closest minimum of the cost function is found. The problem with

applying this method consecutively to a sequence of images is that there will be

no temporal consistency between rotations, and therefore when rotating around

any single axis, there is an inherent 90o ambiguity which will always default

to the closest rotation to identity. It is simple to naively initialise the current

rotation Rt, with the optimised rotation from the previous frame Rt‚àí1 which

removes this ambiguity and also increases convergence of optimisation. However,

two main problems arise from this initialisation. Firstly, for frames that are less

Manhattan, we have no way of loosening the MW assumption and therefore a

single frame‚Äôs rotation is only dependent on its own (potentially poorly defined)

cost function . Secondly, not all frames in a sequence may optimise to a consistent

minimum and may give erroneous rotations that initialise subsequent frames

with a bad orientation, potentially poisoning the rest of the sequence. In the

following section, we address these issues in order to extend rotation estimation

to sequences of images.

## Page 8

8

A. Patwardhan et al.

Single-frame

optimisation

(Sec. 3.1)

Multi-frame

optimisation

(Sec. 3.2)

Fig. 4: The multi-frame optimisation process. Single-frame rotation and covariance es-

timates are used to initialise a sliding window factor graph in order to provide temporal

consistency between frames and reject outlier measurements. Robust factors are shown

along orange edges on measurements. The latest frame is then used to initialise the

rotation estimate for the next frame.

Sliding window optimisation To tackle both of these issues, we implement

a sliding window optimisation that jointly considers previously estimated frame

rotations. This sliding window acts as a support for the current rotation estimate

in order to provide consistency along the sequence. To model this non-linear

joint optimisation, we employ a simple factor graph that consists of factor nodes

f ‚ààF and variables X = {xt, . . . , xt‚àín}, where t refers to the latest variable

and n defines the length of the sliding window. The joint distribution p(X) is

represented as a product of all factors in the graph which, when considering a

Gaussian factor graph, can be solved by the following minimisation:

\ hat  {\

m

a

t

hcal {X} }  = \a

rgmin _{\mathcal {X}} \sum _i{||h_i(\mathcal {X}_i) - z_i||^2_{\Sigma _i}}

(8)

where Xi represents the clique of variables corresponding to the factor fi, hi(¬∑)

represents a function that predicts a measurement based on the state of input

variables, and zi represents some observed measurement. For further details on

state estimation with factor graphs, see Dellaert et. al‚Äôs book on the subject [11].

For our problem, each variable xi ‚ààX represents an SO(3) rotation Ri ‚ààR,

and 3 types of factors exist to constrain the problem. The first factor, f z(Ri),

is a prior factor on each variable that is set to the rotation estimated from the

single-frame estimation problem, Zi ‚ààSO(3). The second factor, f s(Ri, Rj), is

a smoothness factor which enforces that adjacent frames should have a similar

rotation i.e. their difference should be the identity rotation, I3. Finally, another

prior factor f p(Ri) is added to the oldest frame in the sliding window which

represents the marginalised state of the oldest variable in the previous sliding

window optimisation, Rp. This multi-frame factor graph is shown pictorially in

Figure 4. The minimisation for our problem is therefore given by:  \label {eq:mf_minso3} \hat {\mathcal {R}} = \argmin _{\mathcal {R}} \sum _{(i,j)\in \mathcal {F}^s}{||\mathbf {R}_i^{-1}\mathbf {R}_j \ominus \mathbf {I}_3||^2_{\Sigma ^s_i}} + \sum _{i\in \mathcal {F}^z}{||\mathbf {R}_i \ominus \mathbf {Z}_i||^2_{\Sigma ^z_i}} + \sum _{i\in \mathcal {F}^p}{||\mathbf {R}_i \ominus \mathbf {R}_p||^2_{\Sigma ^p_i}}

## Page 9

## U‚ÄìAre‚ÄìMe

9

ÀÜR = arg min

## R

## X

(i,j)‚ààFs

## ||R‚àí1

i Rj ‚äñI3||2

Œ£s

i +

## X

i‚ààFz

||Ri ‚äñZi||2

Œ£z

i +

## X

i‚ààFp

||Ri ‚äñRp||2

Œ£p

i

(9)

where ‚äñdenotes the vector increment (defined on the tangent space of the

right hand variable) between two rotations via the logarithmic map. See [42]

for more details on the subject of lie theory when applied to 3D rotations. For

ease of implementation, we use the popular sensor fusion library GTSAM [10]

to optimise the multi-frame factor graph based on the definitions above.

Once ÀÜR has been computed, the latest frame‚Äôs rotation can then be fed back

into the single frame optimisation as initialisation for the subsequent frame i.e.

Rinit

t+1 = ÀÜRt ‚ààÀÜR

Robust estimation In order to reduce the influence of outlier measurements

due to dropped frames, poor normal predictions, and incorrect local minima from

the single frame optimisation process, we also apply robust factors to all prior

measurement factors f z. We leverage the Huber cost function which represents a

Gaussian energy for small residuals, but transitions to a linear function for large

residuals. This effectively dampens the influence of measurements that grossly

disagree with the smoothness model between variables. However, by maintaining

these measurements in the factor graph, a genuine large change in rotation will

be properly estimated after a few frames of consistent measurements.

The result of this robustified multi-frame optimisation is that high frequency

noise in the single frame rotation estimates is smoothed out, whilst outlier mea-

surements are repressed from poisoning the rotation estimates of subsequent

frames.

Incorporating uncertainty To address the issue of non-Manhattan frames,

the covariance estimate from Eq. (7) can be directly applied to each measure-

ment factor (Œ£z in Eq. (9)), since Eq. (6) is defined around the global MF upon

which we are optimising. This means that frames which exhibit strong Man-

hattan normals will have a greater influence on the result, stabilising rotation

estimates for non-Manhattan frames. For example, if a frame only sees normals

n in one dominant direction, ri, then the single-frame optimisation is free to

rotate around this vector. Since limn‚Üíri œÉ2

ri = ‚àû, whilst limn‚Üíri œÉ2

r‚ä•

i = 0.5, any

rotation around ri will be effectively ignored in the multi-frame optimisation,

whilst keeping the information for the perpendicular axes of rotation. Therefore

even though we are using the MW assumption, we are not strictly bound to it.

For the smoothness factors, the covariance Œ£s is set to an isotropic covariance

ŒªI3, where Œª is a tuning parameter that defines how strongly the smoothness

constraint is enforced. A high value will ensure rotations are close to each other,

whilst a low value will trust the measurement rotations more. The covariance

for the prior measurement Œ£p, is automatically defined by the marginalisation

of the last variable Rt‚àín in the previous iterations window.

## Page 10

10

A. Patwardhan et al.

4

Experiments

In Sec. 4.1, we evaluate the accuracy and robustness of U‚ÄìARE‚ÄìME and make a

comparison to the existing approaches. In Sec. 4.2, we demonstrate the versatility

of our approach by introducing potential applications: up-vector estimation and

ground segmentation.

4.1

Rotation estimation over sequences

Dataset and evaluation protocol Following previous methods [17, 26, 34],

we evaluate our method on ICL-NUIM [18] and TUM RGB-D [45] which cover

synthetic and real indoor scenes, respectively. To assess the performance in chal-

lenging real-world scenarios, we also evaluate the performance on ScanNet [8].

ScanNet images have a significant amount of noise and blur as they were cap-

tured using hand-held cameras in poorly lit environments. The scenes are also

cluttered with many objects that violate the Manhattan World assumption. We

do not discard such scenes and evaluate on all test sequences to highlight the

robustness of our method.

To measure accuracy, each frame is fed into the algorithm sequentially and the

estimated rotation is recorded after each frame. The rotations are then aligned

with the relevant ground truth so that methods that do not estimate a specific

world alignment can also be compared with the MW-based methods. The metric

used for accuracy is the average rotation error (ARE) and is given by

\ t ext {

## A

## Re} =

\t ext  {

c

os}^{-1}\left (\frac {\text {tr}({\mathbf {R}}_{\text {gt}}^{-1} \hat {\mathbf {R}})-1}{2}\right )

(10)

where ÀÜR is the rotation estimate and Rgt is the ground truth.

Baseline methods We compare our approach to various monocular rotation

estimation methods. We first choose OLRE [5] and ROVE [30] which extract

and align vanishing points. We also compare against RGB-D approaches that

align the surface normal vectors extracted from the depth measurements and

replace with our predicted normals to assess their performance in a monocular

setup. Furthermore, we also draw direct comparisons with the popular monoc-

ular SLAM system ORB-SLAM [38]. Whilst this is not a single image rotation

estimation method, it is one of the state-of-the-art methods for obtaining accu-

rate real-time odometry and will provide a challenging benchmark from which

to draw conclusions of our work. Lastly, several RGB-D methods are also used

in our comparisons so that we can give context on how accurate our system is

compared to methods that require a depth map. These are: GOME [24], Com-

pass [26] and E-Graph [34].

## Page 11

## U‚ÄìAre‚ÄìMe

11

Table 1: (top) Quantitative evaluation on ICL-NUIM and TUM RGB-D [deg]. The

best RGB method is bold and the second-best is underlined. (bottom) Our approach,

compared to ORB-SLAM, is more robust to inaccuracy in camera intrinsics. We simu-

late changes in focal length and principal point by cropping/shifting the input image.

Sequences

RGB Methods

RGB-D methods

Ours RMFE* RTMF*

## Es*

## Ovpd Reov Orb

GOME Compass E-Graph

Office 0

4.99

4.99

4.97

5.21

6.71

29.11

0.60

5.12

0.37

0.11

Office 1

3.87

89.18

44.59

3.90

√ó

34.98

√ó

√ó

0.37

0.22

Office 2

2.38

3.35

2.36

41.99

10.91

60.54

0.69

6.67

0.38

0.39

Office 3

2.72

2.84

41.98

2.87

3.41

10.67

2.53

5.57

0.38

0.24

Living 0

8.43

8.36

8.25

11.53

√ó

√ó

0.35

√ó

0.31

0.44

Living 1

3.58

91.95

3.81

14.04

3.72

26.74

√ó

8.56

0.38

0.24

Living 2

2.39

2.45

2.50

2.44

4.21

39.71

0.57

8.15

0.34

0.36

Living 3

5.38

5.64

5.58

57.62

√ó

√ó

0.84

√ó

0.35

0.36

Struc notex

4.61

4.55

4.94

7.96

11.22

√ó

√ó

4.07

1.96

4.46

Struc tex

3.03

3.10

3.18

3.14

8.21

13.73

0.37

4.71

2.92

0.60

Large cabinet

4.54

4.30

4.60

5.20

38.12

28.41

1.13

3.74

2.04

1.45

Cabinet

5.41

40.15

6.27

70.39

√ó

√ó

√ó

2.59

2.48

2.47

Long office

5.62

5.78

5.98

46.74

√ó

√ó

7.86

√ó

1.75

-

Nostruc notex

6.93

54.46

27.52

30.77

√ó

√ó

√ó

√ó

√ó

-

Nostruc tex

28.94

24.16

63.62

11.58 46.18

16.45 17.42

√ó

√ó

-

* Reimplemented using our predicted normals

Crop

Ours ORB-SLAM

Shift

Ours ORB-SLAM

Original

2.39

0.57

original

2.39

0.57

5% crop

2.30

2.40

5% shift

2.31

6.36

10% crop

2.28

5.13

10% shift

3.97

14.63

15% crop

2.31

7.45

15% shift

6.03

22.32

20% crop

2.39

## X

20% shift

8.26

## X

Results from ICL-NUIM and TUM RGB-D The overall results for both

the ICL-NUIM and TUM RGB-D datasets are shown in Tab. 1. U‚ÄìARE‚ÄìME

can accurately estimate a valid trajectory of rotations in 14/15 sequences and

has on average the best accuracy of the non-SLAM based monocular methods.

Whilst RTMF, RMFE and ES sometimes show the same accuracy as the pro-

posed method, the lack of temporal consistency means that they often shift

into a globally inconsistent MF and in many of the sequences show large errors.

Comparing to the RGB-D methods, the proposed method is often better than

GOME despite only using RGB images, and whilst we are worse than Compass

and E-Graph for the synthetic ICL-NUIM sequences (which have perfect depth),

we achieve comparable accuracy in some real-world TUM sequences.

Comparing to ORB-SLAM, for the ICL-NUIM sequences in which ORB-

SLAM does not fail, we find that ORB-SLAM is significantly better than all the

RGB methods and is even on par with the RGB-D methods. However, ORB-

SLAM may lose track or be unable to initialise in texture-less scenes, meaning

that it sometimes fails to register many valid rotations. ORB-SLAM (and indeed

most other SLAM methods) also need finely calibrated camera intrinsics, and are

therefore often not suitable for running on in-the-wild sequences which may be

subject to distortion and cropping (something we are capable of handling with

## Page 12

12

A. Patwardhan et al.

0

2

4

6

8

10

12

14

Passed with error below [deg]

0

20

40

60

80

100

Cumulative no. of runs

100%

70%

80%

90%

95%

ScanNet: ORB-SLAM comparison

## U-Are-Me

## Orb-Slam

0

2

4

6

8

10

12

14

Passed with error below [deg]

0

20

40

60

80

100

ScanNet: Ablations

## U-Are-Me

U-ARE-ME ‚Äì single

U-ARE-ME ‚Äì single, no Œ∫

Fig. 5: (left) U‚ÄìARE‚ÄìME and ORB-SLAM accuracy comparison on 100 sequences

from the ScanNet dataset. Solid line shows cumulative number of runs below a certain

accuracy threshold for U‚ÄìARE‚ÄìME and dashed lines are for ORB-SLAM. Percentage

pass rate is shown, whereby at least X% of frames per sequence must contain a valid

rotation estimate (this includes any initialisation and loss of tracking). (right) Ab-

lation study experiments. The blue line shows the results of the full pipeline. ‚Äôsingle‚Äô

means that multi-frame optimisation is disabled and ‚Äôno Œ∫‚Äô means that the uncertainty

weighting in the cost function is removed.

U‚ÄìARE‚ÄìME). To demonstrate this point, Tab. 1 (bottom) shows the accuracy

comparison on the ICL-NUIM living room 2 sequence, but using increasingly

deformed images with the same camera intrinsics on both U‚ÄìARE‚ÄìME and

ORB-SLAM. Since we do not need specific camera intrinsics, we are extremely

robust to both image cropping and image shifting. Since the normal network has

been trained on ‚àº60o fov images, we actually see a minor improvement at 90%

cropping as the ICL-NUIM images have a slightly wider than ideal 67o fov.

Results from ScanNet The ScanNet suite provides a large set of real-world

sequences from which we can draw better conclusions about the generalisability

of our system. We therefore further test U‚ÄìARE‚ÄìME on all 100 test sequences

and compare to the most accurate RGB method from the previous section, ORB-

SLAM. We then perform an ablation study on each of the proposed features of

U‚ÄìARE‚ÄìME to demonstrate the benefit of each one.

Fig. 5 (left) shows the results from the comparison with ORB-SLAM. Since

ORB-SLAM is a multi-view system and therefore will never produce rotation

estimates for every single frame, we show the results of ORB-SLAM at different

success rates e.g. 70% defines that at least 70% of frames per sequence need

valid rotation estimates (the missing frames being from either initialisation or

loss of tracking). In this regard, we allow ORB-SLAM to lose tracking and do

not consider this an outright failure. The results show that U‚ÄìARE‚ÄìME has

a much higher robustness to the ScanNet sequences and will estimate rotations

with an accuracy < 10o in 80% of the sequences. Comparing this to ORB-SLAM

with a 95% frame threshold which only successfully achieves < 10o in 17% of the

sequences. For higher accuracy < 3o, we find that ORB-SLAM is more capable

in some sequences (10%) whereas the proposed method only achieves at best 3o.

## Page 13

## U‚ÄìAre‚ÄìMe

13

Table 2: Accuracy of the estimated up-vector [deg]. Note that PF [23] and CTRL-

C [31] were trained specifically to estimate the up-vector.

Sequence U‚ÄìARE‚ÄìME

## Pf

## Ctrl-C

Sequence U‚ÄìARE‚ÄìME PF CTRL-C

Living 0

7.53

9.90

12.14

Office 0

3.95

5.52

18.87

Living 1

2.67

3.17

13.74

Office 1

3.50

4.65

22.32

Living 2

1.77

4.67

9.71

Office 2

1.61

3.80

15.95

Living 3

4.01

10.84

7.67

Office 3

2.13

3.95

18.47

We argue that this is primarily caused by the accuracy of the surface normal

predictions, e.g. the state of the art [3] reports a mean normal error of 16.2o on

ScanNet, and therefore as normal predictions improve so should our results.

The ablation study Fig. 5 (right), shows that the overall reliability of the sys-

tem improves as firstly, the uncertainty weighted normals reject non-Manhattan

pixels within the image, and then more so second, the multi-frame optimisation

provides a consistent global MF which anchors the solution across the sequence.

We also see a slight improvement in the accuracy of the rotations when discount-

ing uncertain pixels.

4.2

Applications

Up-vector Estimation Estimating the ‚Äòupward‚Äô direction in a scene is im-

portant in applications that require knowledge of the scene orientation. Recent

neural network approaches have shown success in estimating camera parameters

such as the up-vector. CTRL-C [31] proposes an end-to-end transformer ap-

proach to combine detected vanishing points with learned features. Perspective

Fields (PF) [23] predicts the per-pixel information about the camera parameters,

and demonstrated the use of the up-vector for AR effects such as compositing

rainfall and 3D objects into the scene.

We compare our method against these baselines on the ICL-NUIM dataset

and report the angular difference of the up-vector in Table 2. As the baseline

methods operate on single RGB images, we perform single frame rotation es-

timation in our method for a fair comparison. Our method outperforms the

baselines which were trained on feature-rich image datasets, and is able to more

accurately estimate camera rotation in the relatively texture-less scenes from the

ICL-NUIM dataset.

Horizon estimation in a Non-inertial Reference Frame We are also mo-

tivated by the limitations of using Inertial Measurement Units (IMUs) when op-

erating within a non-inertial reference frame. We perform multi-frame rotation

estimation for a sequence of RGB images taken inside an accelerating London

bus, and perform a comparison against IMU data. For each frame we compute

the ARE between the estimated rotation and the initial rotation, and plot the

evolution over time in Fig. 2.

The camera is stationary with respect to the bus throughout the sequence and

so there should be no relative rotation. The Intel Realsense D435i camera was

## Page 14

14

A. Patwardhan et al.

(a) London

(b) Mumbai

(c) Cardiff

(d) Office

Fig. 6: Even in non-Manhattan scenes, our framework can optimise the rotation such

that the global up direction is aligned with the surface normal of the ground plane.

It can thus be used to accurately segment the ground plane, which can be useful for

robotics applications.

used to take synchronised RGB and IMU measurements which were integrated

using the complementary filter provided by the Intel Realsense SDK [22].

As the bus harshly turns our visual-only approach maintains a steady rotation

estimate, while the IMU suffers from strong accelerations causing a large error

in its measured rotation. This is seen in Fig. 2 where the horizon lines for each

method have been calculated using the up-vector derived from the estimated

rotation. We envisage that U‚ÄìARE‚ÄìME can be used to complement traditional

IMU-based applications in non-inertial reference frames.

Ground Segmentation The up-vector can be used to perform real-time ground

segmentation from RGB images, which is an important pre-processing step in

many applications including robotics [36], autonomous driving [12], and 3D ob-

ject tracking for augmented reality [46], where it can be used to seamlessly

integrate virtual objects with the real-world environment.

As U‚ÄìARE‚ÄìME produces per-pixel surface normal estimates, we can directly

use the result of the rotation estimation to segment areas of an input image

corresponding to the ground, assuming that this is aligned with the world up-

vector. Our method can be applied to real-world indoor and outdoor images to

segment the ground even when the scene is non-Manhattan, as seen in Fig. 6.

5

Conclusion

Motivated by the need to provide extrinsic rotation estimates from in-the-wild

images and sequences, we have presented U-ARE-ME, an accurate and robust

camera rotation estimator that operates on uncalibrated RGB images. The sys-

tem is capable of outputting estimates for single images and has also been ex-

tended to reliably handle multi-frame scenarios. An extensive evaluation has

been performed and it has been shown that our method is capable of providing

globally consistent multi-frame rotation estimates which rivals the performance

of similar methods that leverage accurate depth maps ‚Äì all whilst remaining real-

time. By accounting for the uncertainty and inherent ambiguity of the common

MW assumption, we are also capable of providing accurate results on scenes that

superficially appear to not contain structural regularities, and where other 2D

feature-based methods often fail.

## Page 15

## U‚ÄìAre‚ÄìMe

15

Acknowledgements

This research has been supported by the EPSRC Prosperity Partnership Award

with Dyson Technology Ltd.

References

1. Alhashim, I., Wonka, P.: High quality monocular depth estimation via transfer

learning. arXiv preprint arXiv:1812.11941 (2018) 6

2. Bae, G., Budvytis, I., Cipolla, R.: Estimating and exploiting the aleatoric uncer-

tainty in surface normal estimation. In: ICCV (2021) 2, 3, 6

3. Bae, G., Davison, A.J.: Rethinking inductive biases for surface normal estimation.

In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

(2024) 3, 5, 6, 13, 19

4. Bazin, J.C., Seo, Y., Demonceaux, C., Vasseur, P., Ikeuchi, K., Kweon, I.S., Polle-

feys, M.: Globally optimal line clustering and vanishing point estimation in man-

hattan world. pp. 638‚Äì645 (06 2012). https://doi.org/10.1109/CVPR.2012.

6247731 4

5. Bazin, J.C., Pollefeys, M.: 3-line ransac for orthogonal vanishing point detection.

In: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.

pp. 4282‚Äì4287. IEEE (2012) 2, 10

6. Cai, R., Hariharan, B., Snavely, N., Averbuch-Elor, H.: Extreme rotation estimation

using dense correlation volumes. In: Proceedings of the IEEE/CVF Conference on

Computer Vision and Pattern Recognition. pp. 14566‚Äì14575 (2021) 2

7. Coughlan, J., Yuille, A.L.: The manhattan world assumption: Regularities in scene

statistics which enable bayesian inference. In: NeurIPS (2000) 3, 5

8. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie√üner, M.: Scannet:

Richly-annotated 3d reconstructions of indoor scenes. In: Proc. Computer Vision

and Pattern Recognition (CVPR), IEEE (2017) 10

9. Dai, A., Nie√üner, M., Zollh√∂fer, M., Izadi, S., Theobalt, C.: Bundlefusion: Real-

time globally consistent 3d reconstruction using on-the-fly surface reintegration.

ACM Transactions on Graphics (ToG) 36(4), 1 (2017) 2

10. Dellaert, F., Contributors, G.: borglab/gtsam (May 2022). https://doi.org/10.

5281/zenodo.5794541, https://github.com/borglab/gtsam 9

11. Dellaert, F., Kaess, M., et al.: Factor graphs for robot perception. Foundations and

Trends¬Æ in Robotics 6(1-2), 1‚Äì139 (2017) 8

12. Deng, W., Chen, X., Jiang, J.: A staged real-time ground segmentation algorithm

of 3d lidar point cloud. Electronics 13, 841 (02 2024). https://doi.org/10.3390/

electronics13050841 14

13. Elloumi, W., Treuillet, S., Leconge, R.: Real-time camera orientation estimation

based on vanishing point tracking under manhattan world assumption. Journal of

Real-Time Image Processing (04 2014). https://doi.org/10.1007/s11554-014-

0419-9 4

14. Fisher, N.I., Lewis, T., Embleton, B.J.: Statistical Analysis of Spherical Data.

Cambridge University Press (1993) 6

15. Furukawa, Y., Curless, B., Seitz, S.M., Szeliski, R.: Manhattan-world stereo. In:

2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 1422‚Äì

1429 (2009). https://doi.org/10.1109/CVPR.2009.5206867 4

## Page 16

16

A. Patwardhan et al.

16. Ghanem, B., Thabet, A., Niebles, J.C., Heilbron, F.C.: Robust manhattan frame

estimation from a single rgb-d image. In: 2015 IEEE Conference on Computer

Vision and Pattern Recognition (CVPR). pp. 3772‚Äì3780 (2015). https://doi.

org/10.1109/CVPR.2015.7299001 2, 3, 4

17. Guo, R., Peng, K., Zhou, D., Liu, Y.: Robust visual compass using hybrid features

for indoor environments. Electronics 8(2), 220 (2019) 10

18. Handa, A., Whelan, T., McDonald, J., Davison, A.: A benchmark for RGB-D

visual odometry, 3D reconstruction and SLAM. In: IEEE Intl. Conf. on Robotics

and Automation, ICRA. Hong Kong, China (May 2014) 10

19. Hartley, R.I., Kahl, F.: Global optimization through searching rotation space and

optimal estimation of the essential matrix. In: 2007 IEEE 11th International Con-

ference on Computer Vision. pp. 1‚Äì8 (2007). https://doi.org/10.1109/ICCV.

2007.4408896 4

20. He, M., Zhu, C., Huang, Q., Ren, B., Liu, J.: A review of monocular visual odom-

etry. The Visual Computer 36(5), 1053‚Äì1065 (2020) 2

21. Horn, B.: Extended gaussian images. Proceedings of the IEEE 72(12), 1671‚Äì1686

(1984). https://doi.org/10.1109/PROC.1984.13073 5

22. Intel: Intel/realsense, https://www.intelrealsense.com/sdk-2/ 14

23. Jin, L., Zhang, J., Hold-Geoffroy, Y., Wang, O., Matzen, K., Sticha, M., Fouhey,

D.F.: Perspective fields for single image camera calibration. CVPR (2023) 13

24. Joo, K., Oh, T.H., Kim, J., Kweon, I.S.: Robust and globally optimal manhattan

frame estimation in near real time. IEEE Transactions on Pattern Analysis and

Machine Intelligence 41(3), 682‚Äì696 (2019). https://doi.org/10.1109/TPAMI.

2018.2799944 5, 10

25. Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for

computer vision? In: NeurIPS (2017) 3

26. Kim, P., Coltin, B., Kim, H.J.: Indoor rgb-d compass from a single line and plane.

In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-

nition. pp. 4673‚Äì4680 (2018) 10

27. Ko≈°eck√°, J., Zhang, W.: Video compass. In: Heyden, A., Sparr, G., Nielsen, M.,

Johansen, P. (eds.) Computer Vision ‚Äî ECCV 2002. pp. 476‚Äì490. Springer Berlin

Heidelberg, Berlin, Heidelberg (2002) 4

28. Langer, F., Bae, G., Budvytis, I., Cipolla, R.: Sparc: Sparse render-and-compare

for cad model alignment in a single rgb image. arXiv preprint arXiv:2210.01044

(2022) 3

29. Lee, D.C., Hebert, M., Kanade, T.: Geometric reasoning for single image structure

recovery. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition.

pp. 2136‚Äì2143 (2009). https://doi.org/10.1109/CVPR.2009.5206872 4

30. Lee, J.K., Yoon, K.J.: Real-time joint estimation of camera orientation and vanish-

ing points. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition

(CVPR). pp. 1866‚Äì1874 (2015). https://doi.org/10.1109/CVPR.2015.7298796

2, 4, 10

31. Lee, J., Go, H., Lee, H., Cho, S., Sung, M., Kim, J.: CTRL-C: Camera calibration

TRansformer with Line-Classification. In: Proceedings of the IEEE/CVF Interna-

tional Conference on Computer Vision (ICCV) (2021) 13

32. Li, H., Zhao, J., Bazin, J.C., Kim, P., Joo, K., Zhao, Z., Liu, Y.H.: Hong kong

world: Leveraging structural regularity for line-based slam. IEEE Transactions on

Pattern Analysis and Machine Intelligence (2023) 5

33. Li, Y., Brasch, N., Wang, Y., Navab, N., Tombari, F.: Structure-slam: Low-drift

monocular slam in indoor environments. IEEE Robotics and Automation Letters

5(4), 6583‚Äì6590 (2020) 3

## Page 17

## U‚ÄìAre‚ÄìMe

17

34. Li, Y., Tombari, F.: E-graph: Minimal solution for rigid rotation with extensibility

graphs. In: European Conference on Computer Vision. pp. 306‚Äì322. Springer (2022)

10

35. Liu, S., Fan, L., Johns, E., Yu, Z., Xiao, C., Anandkumar, A.: Prismer: A vision-

language model with an ensemble of experts. arXiv (2023) 3

36. Man, Y., Weng, X., Li, X., Kitani, K.: Groundnet: Monocular ground plane nor-

mal estimation with geometric consistency. Proceedings of the 27th ACM Inter-

national Conference on Multimedia (2018), https://api.semanticscholar.org/

CorpusID:199543484 14

37. Morimoto, C., Chellappa, R.: Evaluation of image stabilization algorithms. In:

Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and

Signal Processing, ICASSP‚Äô98 (Cat. No. 98CH36181). vol. 5, pp. 2789‚Äì2792. IEEE

(1998) 2

38. Mur-Artal, R., Montiel, J.M.M., Tardos, J.D.: Orb-slam: a versatile and accurate

monocular slam system. IEEE transactions on robotics 31(5), 1147‚Äì1163 (2015)

2, 10

39. Parra Bustos, A., Chin, T.J., Eriksson, A., Li, H., Suter, D.: Fast rotation search

with stereographic projections for 3d registration. IEEE Transactions on Pattern

Analysis and Machine Intelligence 38(11), 2227‚Äì2240 (2016). https://doi.org/

## 10.1109/Tpami.2016.2517636 4

40. Schindler, G., Dellaert, F.: Atlanta world: An expectation maximization framework

for simultaneous low-level edge grouping and camera calibration in complex man-

made environments. In: CVPR (2004) 5

41. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and support

inference from rgbd images. In: European Conference on Computer Vision (2012),

https://api.semanticscholar.org/CorpusID:545361 2, 3, 4

42. Sola, J., Deray, J., Atchuthan, D.: A micro lie theory for state estimation in

robotics. arXiv preprint arXiv:1812.01537 (2018) 9

43. Straub, J., Bhandari, N., Leonard, J.J., Fisher, J.W.: Real-time manhattan world

rotation estimation in 3d. In: 2015 IEEE/RSJ International Conference on Intel-

ligent Robots and Systems (IROS). pp. 1913‚Äì1920 (2015). https://doi.org/10.

## 1109/Iros.2015.7353628 2, 3, 4

44. Straub, J., Rosman, G., Freifeld, O., Leonard, J.J., Fisher, J.W.: A mixture of

manhattan frames: Beyond the manhattan world. In: 2014 IEEE Conference on

Computer Vision and Pattern Recognition. pp. 3770‚Äì3777 (2014). https://doi.

org/10.1109/CVPR.2014.488 4, 5

45. Sturm, J., Engelhard, N., Endres, F., Burgard, W., Cremers, D.: A benchmark for

the evaluation of rgb-d slam systems. In: Proc. of the International Conference on

Intelligent Robot Systems (IROS) (Oct 2012) 10

46. Tatzgern, M., Grasset, R., Kalkofen, D., Schmalstieg, D.: Transitional augmented

reality navigation for live captured scenes (04 2014). https://doi.org/10.1109/

## Vr.2014.6802045 2, 14

47. Taylor, C., Cowley, A.: Parsing indoor scenes using rgb-d imagery (07 2012). https:

//doi.org/10.15607/RSS.2012.VIII.051 4

48. YouTube: Race the tube - london parkour pov, https://www.youtube.com/watch?

v=tXMPRK2LQAE 21

49. YouTube: A walk around orison city - star citizen alpha 3.14 gameplay (no com-

mentary), https://www.youtube.com/watch?v=G3gqBaqDSaE 20

50. Youtube: Walking in the rain tokyo, japan (relaxing binaural thunderstorm sounds

for sleep) 4k asmr, https://www.youtube.com/watch?v=Et7O5-CzJZg 20

## Page 18

18

A. Patwardhan et al.

51. Zhai, G., Huang, D., Wu, S.C., Jung, H., Di, Y., Manhardt, F., Tombari, F.,

Navab, N., Busam, B.: Monograspnet: 6-dof grasping with a single rgb image. In:

2023 IEEE International Conference on Robotics and Automation (ICRA). pp.

## 1708‚Äì1714. Ieee (2023) 3

52. Zhang, Y., Ding, Y., Song, J., Li, J., Wei, H.L.: A fast manhattan frame estimation

method based on normal vectors. Journal of Field Robotics 39(5), 557‚Äì579 (2022)

5, 6, 7

## Page 19

Appendix

## A

Training details for surface normal estimator

Our surface normal estimator is trained on the meta-dataset introduced by [3].

DSINE [3] uses per-pixel ray direction as input and thus requires camera intrin-

sics. We removed this dependency and warped the training images such that

the principal point is at the center and the field-of-view is 60‚ó¶. Nonetheless, U‚Äì

ARE‚ÄìME generalises well to images taken with different intrinsics (e.g. the video

game sequence in the attached video has field-of-view of 86‚ó¶). DSINE [3] also

proposed to improve the piece-wise smoothness and crispness of the prediction

by recasting surface normal estimation as iterative rotation estimation. We also

remove this iterative process and hence improve the efficiency to give real-time

estimates. While this degrades the quality of the surface normal prediction, the

rotation estimates from our framework stay robust. U‚ÄìARE‚ÄìME robustly fuses

the per-pixel predictions by weighting them with a confidence Œ∫ and is thus

robust to mild inaccuracies in the surface normal prediction.

Our network estimates two quantities: the per-pixel surface normal vector n

and the corresponding confidence Œ∫. n is supervised with the angular loss, and

Œ∫ with the negative log-likelihood defined in Eq. 2 (in the main manuscript). All

other training protocols (e.g. batch size, number of epochs, data augmentation,

optimiser, and learning-rate schedulers) are the same as [3]. The training only

takes 9 hours on a single NVIDIA 4090 GPU. After training, Œ∫ is capped at 100

to prevent the over-confident predictions from dominating the optimisation.

## B

Demo video

We encourage the reader to view the accompanying video at https://callum-

rhodes.github.io/U-ARE-ME for a visual overview of our method.

For the reader‚Äôs convenience, the timestamps of each section in the video

are summarised in Table 3. We provide additional detailed explanations for each

section below.

## B.1

U‚ÄìARE‚ÄìME demo

We first demonstrate the operation of U‚ÄìARE‚ÄìME on the ICL-NUIM and TUM-

RGBD datasets discussed in our paper. The coordinate frame in the center of

the video depicts the orientation of the global Manhattan frame. Throughout

the demonstration, the video cycles through various visualisations of the scene:

‚Äì RGB image input to U‚ÄìARE‚ÄìME

‚Äì Predicted surface normals (using X‚ÄìY‚ÄìZ to R‚ÄìG‚ÄìB colour mapping)

‚Äì Confidence of predicted surface normals (greyscale ‚Äì white represents high

confidence)

## Page 20

20

A. Patwardhan et al.

Table 3: Timestamps for the contents of the demo video

Section

Timestamp

U‚ÄìARE‚ÄìME Demo

## Icl-Nuim

0:13

## Tum-Rgbd

0:30

Tokyo walking sequence

0:47

Video game sequence

1:05

Robustness to Dropped Frames

1:29

Applications

Non-inertial Reference Frame

1:58

Ground Segmentation

2:27

ICL-NUIM: living-room-2 This synthetic scene shows a camera moving

through a living room which is generally Manhattan in structure, but does con-

tain some features which do not agree with a Manhattan assumption (e.g. cur-

tains, lamps, and sofa cushions). Of note is the textured wall painting at 0:13

which is predicted as having the same surface normal as the wall it is on, while

the painting‚Äôs frame is predicted to have high uncertainty normals. The curtains

seen at 0:21 (a large non-Manhattan area) are also shown with high uncertainty

normals as they have a irregular geometry. As U‚ÄìARE‚ÄìME is uncertainty-aware,

it estimates rotation with a greater weighting on those surfaces that agree with

the Manhattan assumption, e.g., the walls and floor (shown in white on the

confidence images), whilst down-weighting the non-Manhattan features.

TUM-RGBD: fr-3 large cabinet This dataset contains videos captured with

a real-world hand-held camera, exhibiting some pitch and roll with unsteady

camera motion. The normals of objects in the background with fine structures

and lots of occlusion are harder to accurately predict, thus are estimated as hav-

ing surface normals with higher uncertainty. As a result, our method can produce

accurate estimates of rotation throughout the sequence by down-weighting such

uncertain normals.

Tokyo Walking Sequence We apply our method to an in-the-wild video

taken directly from YouTube [50] showing a hand-held camera viewpoint walking

through the streets of Tokyo. This is a challenging situation for rotation estima-

tion as the camera intrinsics are not known. The environment is dynamic, with

many pedestrians walking in the scene, and our method produces reliable rota-

tion estimation despite the small quantity of static Manhattan aligned objects

and buildings.

Video Game sequence In this example our method is shown to estimate

rotation on a synthetic sequence from the video game Star Citizen [49]. Once

again the camera intrinsics are not known, and this is a relatively non-Manhattan

environment. During the sequence, the camera switches between 1st person and

## Page 21

## U‚ÄìAre‚ÄìMe

21

3rd person (during which the player character takes up a significant portion

of the screen) yet rotations remain consistent with the game world. U‚ÄìARE‚Äì

ME takes this into consideration by estimating a high uncertainty on the player

model.

## B.2

Robustness to dropped frames

We compare U‚ÄìARE‚ÄìME operating with (bottom videos) and without (top

videos) multi-frame optimisation, when black frames are randomly injected into

the sequence, simulating dropped frames.

For example, at 01:50 three consecutive dropped frames are input to the non-

multi-frame estimator (top) which causes the estimate to differ significantly from

its previous estimate into a locally correct but globally inconsistent MF. Using

the proposed uncertainty-aware multi-frame optimisation however, U‚ÄìARE‚ÄìME

is seen to produce temporally consistent rotation estimates, in the presence of

dropped frames.

## B.3

Applications

Finally we demonstrate some applications of U‚ÄìARE‚ÄìME as discussed in the

paper.

Operation in a Non-inertial Reference Frame We apply U‚ÄìARE‚ÄìME to

a real-world video sequence captured from a bus with longitudinally and lat-

erally accelerating motions. We estimate the global up-vector visualised as a

horizon line (green). This is compared against the result obtained from an iner-

tial measurement unit (IMU) only, using the Realsense camera‚Äôs onboard state

estimation.

The video also shows an outward view demonstrating the motion of the bus

(left), the horizon line estimated by U‚ÄìARE‚ÄìME (middle), and the estimate

based on IMU measurements (right).

Ground Segmentation U‚ÄìARE‚ÄìME can keep track of the global up-vector,

even in complex non-Manhattan sequences with high degrees of camera rotation

without knowledge of the camera intrinsics. This can be useful in downstream

tasks such as ground segmentation, by considering only the pixels with surface

normals aligning with the global up-vector. For a video taken directly from

YouTube [48] we use U‚ÄìARE‚ÄìME to estimate the up-vector and highlight the

ground-aligned pixels in green.



## Implementation Status

### Core Components
- ‚úÖ **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- üîÑ **Performance Monitoring**: Continuous validation of targets
- ‚úÖ **Documentation Standards**: Compliant with ACGS-2 requirements
- üîÑ **Cross-Reference Validation**: Ongoing link integrity maintenance

### Development Status
- ‚úÖ **Architecture Design**: Complete and validated
- üîÑ **Implementation**: In progress with systematic enhancement
- ‚ùå **Advanced Features**: Planned for future releases
- ‚úÖ **Testing Framework**: Comprehensive coverage >80%

### Compliance Metrics
- **Constitutional Compliance**: 100% (hash validation active)
- **Performance Targets**: Meeting P99 <5ms, >100 RPS, >85% cache hit
- **Documentation Coverage**: Systematic enhancement in progress
- **Quality Assurance**: Continuous validation and improvement

**Overall Status**: üîÑ IN PROGRESS - Systematic enhancement toward 95% compliance target

## Performance Requirements

### ACGS-2 Performance Targets
- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)  
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

### Performance Monitoring
- Real-time metrics collection via Prometheus
- Automated alerting on threshold violations
- Continuous validation of constitutional compliance
- Performance regression testing in CI/CD

### Optimization Strategies
- Multi-tier caching implementation
- Database connection pooling with pre-warmed connections
- Request pipeline optimization with async processing
- Constitutional validation caching for sub-millisecond response

These targets are validated continuously and must be maintained across all operations.
