# 2506.16507v1
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 2506.16507v1.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2

---

## Page 1

arXiv:2506.16507v1  [cs.LG]  19 Jun 2025

2025-6-23

Robust Reward Modeling via Causal Rubrics

Pragya Srivastava1*, Harman Singh1*, Rahul Madhavan1*,

Gandharv Patil2,3, Sravanti Addepalli1, Arun Suggala1, Rengarajan Aravamudhan1, Soumya Sharma1, Anirban Laha1,

Aravindan Raghuveer1, Karthikeyan Shanmugam1, Doina Precup1,3

1Google DeepMind, 2McGill University, 3MILA - Quebec AI Institute, *Equal Contribution

Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback,

yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such

as response length or formatting, mistaking these cues learned from correlations in training data for the

true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives

struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce Crome

(Causally Robust Reward Modeling), a novel framework grounded in an explicit causal model designed to

mitigate reward hacking. Crome employs the following synthetic targeted augmentations during training:

(1) Causal Augmentations, which are pairs that differ along specific causal attributes, to enforce sensitivity

along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying

primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our augmentations

are produced without any knowledge of spurious factors, via answer interventions only along causal rubrics,

that are identified by querying an oracle LLM. Empirically, Crome significantly outperforms standard

baselines on RewardBench, improving average accuracy by up to 5.4% and achieving gains of up to 13.2% and

7.2% in specific categories. The robustness of Crome is further testified by the consistent gains obtained

in a Best-of-N inference setting across increasing N, across various benchmarks, including the popular

RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest,

and the reasoning-specific GSM8k.

1. Introduction

Aligning Large Language Models (LLMs) with human preferences is paramount for their safe and effective

deployment, with Reinforcement Learning from Human Feedback (RLHF) and its reliance on reward

models (RMs) being the dominant paradigm (Bai et al., 2022a; Christiano et al., 2017; Ouyang et al.,

2022; Rafailov et al., 2024; Schulman et al., 2017; Shao et al., 2024). The fidelity of these RMs is critical,

as flaws directly propagate to the aligned policy (Casper et al., 2023).

However, standard RM training faces a significant challenge: reward hacking (Gao et al., 2023; Skalse

et al., 2022). RMs often learn to assign high scores based on superficial or spurious attributes—such

as response length (Singhal et al., 2023), specific formatting patterns (Zhang et al., 2024), or stylistic

quirks—because these features are statistically correlated with preferred responses in the training data.

This occurs because standard training objectives do not explicitly require the RM to disentangle the true

causal drivers of response quality (e.g., factuality, relevance) from these spurious correlates, leading to

brittle RMs and misaligned policies (Eisenstein et al., 2023; Shen et al., 2023).

Recent efforts for RM robustness have explored various avenues. Some focus on consistency checks

against meaning-preserving transformations (Wu et al., 2025), while others employ data augmentations,

such as using non-contextual or query-independent comparisons to reduce spuriousness (Liu et al., 2024).

Corresponding author(s): {pragyahere, hrman, rahulmadhavan, araghuveer, karthikeyanvs, doinap}@google.com

© 2025 Google DeepMind. All rights reserved

## Page 2

Robust Reward Modeling via Causal Rubrics

Query

## Q

Good

Answer A1

Degraded

Good Answers

## A1 → [A1

## 1..A1

5]

Upgraded

Bad Answers

## A2 → [A2

## 1..A2

5]

Original QA

pair

Causal Attribute

(CA) Estimation

via Oracle LLM

Counterfactual

Generation

Causal Augments

## (Q,A1,A1

i), (Q,A2,A2

i)

Bad

Answer A2

## Ca1

## Ca2

## Ca3

## Ca4

## Ca5

Irrelevant Query

Neutral with

question change

(Q’,Aj,Aj

i)

Augmented

Dataset

Generation

Veriﬁcation +

Filtration

Synthetic Dataset:

1. Originals

2. Causals

3. Neutrals

RM training on

synthetics adds

to Robustness

Figure 1 | The Crome Data Augmentation and Training

Pipeline. From an original QA pair (Q, A1, A2), an oracle

LLM identifies Causal Attributes (CA). This guides counter-

factual generation, producing degraded A1, and upgraded

A2 responses. These form the set of Causal Augmentations

which teach the model sensitivity to relevant attributes.

Next, we generate Irrelevant Query Neutrals by flipping the

question on both the newly generated causal contrastive

pairs and the original answer pairs, which reduces reliance

on spurious correlates. After verification and filtration, the

combined dataset (Originals, Causals, Neutrals) trains the

RM, enhancing its robustness.

Attribute-based evaluation, often leveraging

LLMs to dynamically generate assessment cri-

teria (Gupta et al., 2025), aims for more

grounded reward signals. Other works investi-

gate specific regularization techniques against

known biases like length or sycophancy (Wang

et al., 2025), or explore methods for causal ef-

fect estimation like RATE (Reber et al., 2024).

Despite these advances, significant limitations

persist.

Many approaches target only pre-

specified spurious factors, potentially missing

unknown correlates, or lack the fine-grained

control needed to truly isolate causal qual-

ity drivers from confounding spurious features

within responses. Augmentation strategies can

be coarse (Liu et al., 2024), and evaluation-

focused methods (Gupta et al., 2025; Reber

et al., 2024) may not directly equip the RM

with mechanisms for robust training against a

wide array of spurious variations through tar-

geted counterfactual learning. There is thus a

clear need for a framework that systematically

leverages a causal understanding of preference

formation to train RMs that are both sensitive

to causal quality attributes and demonstrably

invariant to diverse spurious cues.

Motivated by this, we aim to address the following question in this paper:

How do we train reward models to be robust against reward hacking, particularly when a) the specific

spurious attributes that an RM may exploit are not known, and b) only the stable or invariant causal

attributes found in ground truth/human preferences can be accessed?

To address this question, we propose Crome (Causally Robust Reward Modeling), a novel framework

grounded in an explicit causal model of answer generation (Figure 2). Crome teaches the RM to

differentiate genuine quality drivers from superficial cues by augmenting the preference dataset with

targeted, LLM-generated counterfactual examples. It creates two key types of synthetic training pairs: (1)

Causal Augmentations, which introduce changes along specific causal attributes (e.g., factuality) to enforce

sensitivity to true quality shifts, and (2) Neutral Augmentations, using both (i) the causally augmented

data as well as (ii) the original preference pairs, to enforce invariance along spurious attributes (e.g.,

style) using tie-labels. Training on this enriched dataset with a modified loss (Section 4) guides the RM

towards causal faithfulness. Our evaluations show Crome significantly improves robustness, boosting

RewardBench accuracy by up to 4.5%, with substantial gains in Safety and Reasoning.

We list the key contributions in this work below.

2

## Page 3

Robust Reward Modeling via Causal Rubrics

1. Spurious-Unaware Causal Framework. We propose a causal framework for training reward models

(Sec. 3) that requires intervention only on LLM-identified causal quality rubrics, eliminating the need

for prior specification of or intervention on any of the spurious attributes.

2. Targeted Counterfactual Augmentations along Causal Attributes. We propose to train reward

models on the available preference data and the proposed data augmentations (Sec. 4) along LLM-

identified causal attributes: 1) Causal Augmentations create minimal pairs isolating specific causal

dimensions for precise sensitivity. 2) Neutral Augmentations create variations in spurious features

(preserving causal content) with tie-labels for invariance. Notably, we do not assume any explicit

knowledge of spurious factors nor do we perturb them directly to create these augmentations. We

show that interventions along causal rubrics alone is primarily sufficient to mitigate sensitivity to a

much larger set of spurious correlates.

3. State-of-the-Art RM Robustness. Crome significantly outperforms baselines on RewardBench

(Sec. 6), improving average accuracy by up to 5.4% (Safety +13.18%, Reasoning +7.19%) (Table 2),

and shows superior robustness on reWordBench (Figures 5).

4. Improved BoN results. Best-of-N selection using Crome-RM shows consistent gains across different

values of N when compared to baselines on the popular RewardBench, WildGuardTest and GSM8K

benchmarks. This highlights the robustness of Crome

in the presence of rare (or long tailed)

spurious factors as well, which typically appear at large values of N.

2. Related Works

Our work on causally robust reward modeling, Crome, addresses the challenge of reward hacking in the

context of aligning Large Language Models (LLMs) via Reinforcement Learning from Human Feedback

(RLHF) (Bai et al., 2022a; Ouyang et al., 2022). Standard RLHF relies on a reward model (RM), typically

trained on pairwise preferences using Bradley-Terry (Bradley and Terry, 1952) or pairwise ranking

approaches (Liu et al., 2025; Qin et al., 2023). A critical limitation of learned RMs is reward hacking

(Gao et al., 2023; Skalse et al., 2022), where the RM assigns high scores based on spurious attributes

(e.g., verbosity (Singhal et al., 2023), formatting (Zhang et al., 2024), sycophancy (Denison et al., 2024))

that are correlated with, but do not cause, true response quality. This leads to misaligned policies that

exploit these spurious cues (Shen et al., 2023). Various mitigation strategies exist, including architectural

modifications like Odin (Chen et al., 2024), policy-level adjustments (Park et al., 2024), and data-centric

methods involving ensembles (Ramé et al., 2024) or consistency checks (Shen et al., 2023). Recent

causal-inspired approaches include using MMD regularization against pre-specified spurious factors

(Wang et al., 2025) or estimating the causal effects of a given attribute of a response using corrected

rewrites (Reber et al., 2024).

Our approach falls into the data-centric category, using synthetic data augmentation guided by principles

of causal inference (Pearl, 2009; Peters et al., 2017). While prior work has used LLMs for causal reasoning

(Kiciman et al., 2023) or counterfactual data augmentation in NLP (Kaushik et al., 2019), and related

methods like RRM (Liu et al., 2024), ReWordBench (Wu et al., 2025) target RM robustness, Crome

is distinct in its explicit use of a causal graph framework (Section 3.2) which guides the answer generation

and the reward labeling process. We leverage LLMs to generate targeted causal (attribute-specific

upgrade/degradation) and neutral (spurious-varying, causally-equivalent) counterfactual examples. By

training on this augmented data, Crome aims to systematically disentangle causal attributes (𝐶) from

spurious ones (𝑆𝑃), learning a reward function that is inherently more robust and aligned with the true

drivers of quality, as detailed in Section 4. We provide a longer version of related work in Appendix B.

3

## Page 4

Robust Reward Modeling via Causal Rubrics

Answer A

## C(A)

Causal

Attributes

## R*

Human Labelled

or Ground Truth

Reward

## Sp(A)

Spurious

Correlates

## Q

Query

## R

Learnt Reward

Model

We want to block the

spurious attributes from

aﬀecting the reward directly.

The learnt reward

function depends on

the causal as well as

spurious attributes.

Our goal is to have R

independent of SP(A)

given C(A).

unstable

The answer

comprises of

causal and

spurious

attributes.

stable/invariant

The true reward

depends only on

causal attributes. But

may exhibit

correlations with

spurious attributes.

Path/ Relationship

Interpretation Summary

## (Q, C(A)) →R∗

Ground-truth reward R∗deter-

mined by query Q and causal

attributes C(A); stable relation-

ship.

## Q ↔Sp(A)

Query Q and unknown spuri-

ous attributes SP(A) are corre-

lated/confounded by unstable

exogenous factors.

## Q →C(A)

Query Q determines relevant

causal attributes C(A).

## Sp(A) ↔C(A)

Bidirectional (potentially com-

plex) relationship between spu-

rious SP(A) and causal C(A) at-

tributes.

Figure 2 | Conceptual Causal Graph for Reward Modeling. Q is the query. Answer (A) has causal attributes C(A)

and spurious attributes SP(A). dim(𝐶(𝐴)) ≪dim(𝑆𝑃(𝐴)) ∀𝐴. SP(A) is unknown. Ground-truth reward R∗depends

only on C(A) and Q (R∗⊥SP(A)|C(A), Q). Augmentations heighten ˆR𝜃’s sensitivity to C(A) (approximated by

oracle LLM).

3. Causal Framework for Reward Modeling

We aim to develop a reward model that accurately assesses the quality of an answer A provided in

response to a query Q. Our approach is grounded in a causal framework designed to distinguish genuine

quality drivers from spurious correlates often present in preference data. This involves understanding the

answer generation process and strategically augmenting training data with approximated counterfactual

examples.

3.1. Reward Model and Pairwise Preferences

We train a reward model (RM), denoted ˆR𝜃(Q, A), to assign a scalar quality score to an answer A for a

query Q. This RM is typically optimized on a dataset preferences pairs Dpref = {(Q(𝑖), y(𝑖)

𝑤, y(𝑖)

𝑙)}𝑁

𝑖=1. Given

a pair of answers (A1, A2), the probability of A1 being preferred over A2 is commonly modeled using the

Bradley-Terry framework (Bradley and Terry, 1952):

P(A1 ≻A2|Q; 𝜃) = 𝜎(ˆ𝑠𝜃(Q, A1) −ˆ𝑠𝜃(Q, A2)) =

exp(ˆ𝑠𝜃(Q, A1))

exp(ˆ𝑠𝜃(Q, A1)) + exp(ˆ𝑠𝜃(Q, A2))

(1)

where ˆ𝑠𝜃(Q, A) represents the underlying scalar score (or logit) assigned by the model to answer A for

query Q. 1 The parameters 𝜃are learned by minimizing the negative log-likelihood of preferences.

3.2. A Causal Model of Answer Generation

We propose a causal model (Figure 2) for answer generation and quality perception. For a query-answer

pair (Q, A), we distinguish two attribute types:

• Causal Attributes C(A) = {C1, . . . , Cℓ}: Fundamental quality dimensions (e.g., factuality, relevance)

genuinely determining quality relative to Q.

• Spurious Attributes SP(A) = {SP1, . . . , SP𝑘}: Other features (e.g., length, formatting) correlated with

preferences or Q in Dpref, but not intrinsically determining quality. SP(A) can be high-dimensional

and unknown.

1The score ˆ𝑠𝜃(Q, A) can be the direct output of a reward head or, in some pairwise preference models, ˆ𝑠𝜃(Q, A1) −ˆ𝑠𝜃(Q, A2)

might be directly modeled as the logit of preferring A1 over A2.

4

## Page 5

Robust Reward Modeling via Causal Rubrics

Category

Strategy

Generation Pair Example

Assigned

Label

Training

Objective (P𝜃)

Causal Augmentation (Dcausal) - Enhancing Sensitivity to C

Causal

Attribute Upgrada-

tion/Degradation

(˜A(𝐶𝑗←upgraded), A) or

(A, ˜A(𝐶𝑗←degraded))

≻

→1

Neutral Augmentation (Dneutral) - Enforcing Invariance to SP

Neutral

Pairing with Irrelevant

Queries

(B1, B2) with new Qirrelevant

s.t. C(B1|Qirrelevant) ≈C(B2|Qirrelevant) ≈0

≈(tie)

≈0.5

Table 1 | Summary of Crome’s synthetic data augmentation strategies using LLM-approximated counterfactuals.

˜A(𝐶𝑗←target) signifies an LLM-generated counterfactual of A with its 𝑗-th causal attribute 𝐶𝑗modified.

The ground-truth reward R∗(Q, A) is assumed to be solely a function of causal attributes: R∗(Q, A) =

𝑓∗(Q, C(A)). This implies conditional independence: R∗⊥SP(A)|Q, C(A).

We explicitly assume the following stability property: If the entire process of answer generation and reward

labeling were repeated (e.g., with a different labeler or answer generator), the relationship (Q, C(A)) →R∗

determining the reward is stable/invariant. In contrast, correlations involving SP(A) (e.g., SP(A) ↔C(A)

or SP(A) ↔Q) can arise from various, potentially unstable or unknown exogenous factors, and thus

these correlations may vary across such repetitions.

The primary challenge is that standard reward models ˆR𝜃may inadvertently learn high sensitivity to these

unstable correlations with SP(A) (due to its unknown, high-dimensional nature). Our goal is to train ˆR𝜃

such that its dependence on A is primarily mediated through the identified, stable causal attributes C(A),

ensuring robustness to unspecified SP(A).

3.3. Approximating Counterfactuals for Attribute Intervention

To instill causal sensitivity and spurious invariance in ˆR𝜃, Crome leverages counterfactual reasoning

about how answer quality changes if specific attributes were altered. For an answer A with attributes

(C(A), SP(A)), an ideal counterfactual, 𝐴(𝐶𝑗←𝑐′

𝑗) (𝑢), would manifest if only its 𝑗-th causal attribute 𝐶𝑗

were set to 𝑐′

𝑗, considering its causal effects on other features, while all other exogenous factors 𝑢(that

produced the factual answer 𝑎) remained constant. Formally, 𝑃𝑈(𝐴(𝐶𝑗←𝑐′

𝑗) (𝑈)|𝐴(𝑈) = 𝑎).

As generating such ideal textual counterfactuals is intractable, Crome employs Large Language Models

(LLMs) to produce approximations. These LLM-generated answers, denoted ˜A(𝐶𝑗←target), are rewrites of

an original answer A, prompted to modify 𝐶𝑗(e.g., to a “degraded” state, lowering reward) while aiming

for minimal changes to other attributes.

Remark 1. For brevity, we denote these LLM approximations as ˜A(𝐶𝑗←𝑐), dropping the explicit 𝑢condi-

tioning, assuming the generation approximates such a sample. While imperfect, these approximations

provide the targeted variations crucial for our data augmentation.

3.4. Augmented Training Data for Causal Disentanglement

We augment the original preference dataset Dpref with synthetically generated examples Daug designed to

enforce specific causal properties on ˆR𝜃. This augmented dataset Daug comprises two principal categories:

Causal Augmentation Pairs (Dcausal) and Neutral Augmentation Pairs (Dneutral), summarized in Table 1.

5

## Page 6

Robust Reward Modeling via Causal Rubrics

Answer 2

Answer 1

Spurious Attributes

Causal Attributes

Spurious Attributes

Causal Attributes

Counterfactual generation process where

we change one causal attribute, but some

spurious attributes may change.

Original Question:

Answer 1 > Answer 2

Causal Attribute Upgradation/Degradation

teaches causal sensitivity

Answer 2

Answer 1

Spurious Attributes

Spurious Attributes

Training the model to rate both answers

equally on irrelevant queries enforces

invariance to spurious attributes.

Irrelevant Question:

Answer 1 ≈ Answer 2

Spurious Attributes

Causal Attributes

are now spurious for

irrelevant query

Irrelevant Query Neutral with the same answer pair, but

new irrelevant question teaches spurious invariance

Figure 3 | Visualizing Crome’s core augmentation strategies (detailed in Appendix G). (Top) Causal Augmenta-

tion: For a given query, we use an LLM-driven counterfactual generation process to alter a specific causal attribute,

yielding Answer 2. Some spurious attributes may co-vary. The RM is trained with a preference (e.g., 𝐴1 ≻𝐴2 if 𝐴2

is a degradation), teaching causal sensitivity. (Bottom) Irrelevant Query Neutral: The same answer pair (𝐴1, 𝐴2)

is re-contextualized with a new, irrelevant question. Their original causal attributes become effectively spurious

or irrelevant (greyed-out bar). The RM is trained with a tie-label (𝐴1 ≈𝐴2), teaching invariance to the attribute

differences when no true causal signal for the current query exists. This illustrates how IQN provides invariance to

those spurious attributes that change with C (like length of response changing with clarity of response). A similar

invariance is imposed using the (𝐴1, 𝐴2) pairs from the original dataset to provide robustness to general spurious

attributes (SP) that do not change with C.

3.4.1. Causal Augmentation Pairs

Crome’s strategy causal pairs Dcausal focus on isolating the impact of important causal attributes.

Attribute Upgradation and Degradation.

For an original answer A (from Dpref) and a specific causal

attribute C𝑗, we generate LLM-approximated counterfactuals. If A is of lower quality regarding C𝑗, we

create an upgraded version ˜A(C𝑗←upgraded). The pair (˜A(C𝑗←upgraded), A) is added to Dcausal with label

˜A(C𝑗←upgraded) ≻A post-verification. Conversely, if A is of higher quality on C𝑗, we generate a degraded

version ˜A(C𝑗←degraded). The pair (A, ˜A(C𝑗←degraded)) is added to Dcausal with label A ≻˜A(C𝑗←degraded). These

pairs collectively teach ˆR𝜃sensitivity to changes along individual causal dimensions.

3.4.2. Neutral Augmentation Pairs

Neutral Augmentation Pairs, Dneutral (with tie-labels) teach invariance to SP(A) when C(A) is held

constant/ is irrelevant.

Irrelevant Query Neutrals (IQN)

We pair two answers, B1, B2 (from Dpref ∪Dcausal), with a new, unre-

lated query Qirrelevant. This makes their causal attributes w.r.t. Qirrelevant (i.e., C(B1|Qirrelevant), C(B2|Qirrelevant))

minimal. The pair (B1, B2) under Qirrelevant receives a tie-label, training the RM to disregard spurious

differences when causal relevance is absent. Their causal distinction becomes moot, isolating spurious

variations under Qirrelevant. Presenting these as tied responses to the reward model enforces invariance to

such spurious attributes. We provide various other techniques tested for spurious suppression in Section

6.3.

The rationale for Crome’s specific choices are discussed in Appendix F along with different neutral

augmentation strategies we tried out. We provide the prompts for generating neutrals in Section K.

6

## Page 7

Robust Reward Modeling via Causal Rubrics

Augmented

Dataset Creation

Original Data

Causal Augmentation

Attribute

Upgradations and

Degradations

Query

from

## Db

Response

Pair

Filtering

Pairing with

Irrelevant Query

Augmented

Data

Original

Data

Model

Training

Figure 4 | The Crome data augmentation pipeline. Original preference data (Dpref) is used as a basis to generate:

(1) Causal Augmentations (Dcausal) by performing Attribute Upgradation and Degradation on specific attributes

to enforce sensitivity to genuine quality drivers, and (2) Neutral Augmentations (Dneutral) via Irrelevant Query

Neutrals (with tie-labels) to teach spurious feature invariance. After optional filtering, the reward model is trained

on the combined original and augmented dataset.

4. Methodology: Training a Robust Reward Model

The Crome framework trains robust reward models using a causally-motivated data augmentation strat-

egy, outlined in Figure 4. This involves two main phases: (1) generating attribute-aware counterfactual

data based on our causal model (Section 3), and (2) training the reward model ˆR𝜃with a specialized

loss on the combined data.

4.1. Attribute-Aware Counterfactual Data Generation

This phase prepares the augmented dataset Daug = Dcausal∪Dneutral required for robust training, involving

three conceptual steps:

Step 1: Attribute Identification.

As a prerequisite, we identify the Principal Causal Components

C = (C1, . . . , Cℓ) relevant to the task, leveraging the causal framework from Section 3.2. This typically

involves LLM prompting and refinement (Details in Appendix H.1).

Step 2: Counterfactual Generation.

Using the identified attributes C, we generate synthetic data pairs

via LLM-approximated counterfactuals, as defined in Section 3.3. Following the strategies summarized in

Table 1 and detailed conceptually in Section 3.4, we create:

• Causal Augmentation Pairs (Dcausal): Examples enforcing sensitivity to individual causal attributes C𝑗

via Attribute Upgradation and Degradation, with standard preference labels (≻).

• Neutral Augmentation Pairs (Dneutral): Examples enforcing invariance to spurious attributes SP while

ensuring C is irrelevant or holding causal content C constant. These are generated via Irrelevant

Query Neutrals and Causally Aligned Neutrals respectively. These receive tie labels (≈).

LLM prompts are in Appendix K. This yields the raw Daug.

3. Data Filtering.

Daug is filtered to Daug_filtered by retaining pairs where a baseline RM (trained on

Dpref) is uncertain or incorrect, focusing training on informative examples (details: Section 6, Appendix

H.3). This yields the final training datasets Dpref and Daug_filtered.

7

## Page 8

Robust Reward Modeling via Causal Rubrics

4.2. Robust Reward Model Training

Given the original data Dpref and the filtered augmented data Daug_filtered, the final Crome reward

model ˆR𝜃is trained by minimizing a composite loss function L(𝜃) over the combined dataset D =

Dpref ∪Daug_filtered:

L(𝜃) = −

∑︁

(Q,y𝑤,y𝑙)

∈Dpref∪Dcausal

log



𝜎(Δ𝑤𝑙)



|                          {z                          }

Preference Loss (Causal Sensitivity)

−𝜆

∑︁

(Q,A1,A2, 𝑦=tie)

∈Dneutral



−1

2



log 𝜎(Δ12) + log 𝜎(−Δ12)



|                                                        {z                                                        }

Neutral Tie Loss (Spurious Invariance)

(2)

where Δ𝑤𝑙= ˆR𝜃(Q, A𝑤) −ˆR𝜃(Q, A𝑙) and Δ12 = ˆR𝜃(Q, A1) −ˆR𝜃(Q, A2). The first term (Preference Loss)

trains sensitivity to causal quality using Dpref and Dcausal. The second term (Neutral Tie Loss, weighted

by 𝜆≥0) trains invariance to spurious features using Dneutral by encouraging Δ12 ≈0 for tie-labeled

pairs. For our current set of experiments we keep 𝜆= 1.

This optimization guides ˆR𝜃to be sensitive to causal attributes C while remaining robust to variations

in spurious attributes SP. We demonstrate Crome’s effectiveness in mitigating reward hacking and

improving downstream policy performance in Section 6.

5. Theoretical Analysis

We provide a theoretical analysis, detailed in Appendix I, to formalize how Crome’s causal augmentation

isolates true reward drivers from spurious correlates. Under an idealized model, we show that training

on data with targeted interventions on causal attributes enables the learned reward model to accurately

identify causal reward determinants, even in the presence of numerous, unspecified spurious features.

Intuition and Analytical Approach

When only a specific causal attribute is intervened to vary, and

all other causal attributes are fixed to their factual versions, and spurious factors are ancestral to all

causal attributes, then the reward model is forced to learn the true impact of that causal attribute in an

approximate sense. To formalize this, we consider a setting where:

(1) Causal attributes C(A) and spurious attributes SP(A) are modeled as boolean variables.

(2) True reward R∗is a sparse quadratic polynomial of C(A) only.

(3) The learned ˆR𝜃can be a denser quadratic polynomial including SP(A) and C(A)SP(A) terms.

(4) Spurious attributes SP(A) are not descendants of causal attributes C(A).

(5) Causal augmentation is an ideal counterfactual that (given same exogenous factors leading to the

answer) intervenes one 𝐶𝑖→¬𝐶𝑖, leaving other 𝐶𝑗intervened to be their factual versions.

We frame learning the coefficients of R∗as an ℓ1-constrained linear regression (Lasso) on features derived

from attribute differences between an augmented answer 𝐴aug and its original 𝐴. The key insight is that

the feature matrix F from such augmented pairs exhibits properties conducive to sparse recovery, such as

low column coherence or satisfying a Restricted Isometry Property (RIP) variant. Specifcally, compared

to the original training set, the augmented one has a much lower RIP.

8

## Page 9

Robust Reward Modeling via Causal Rubrics

Method

PairPM

## Bt

Average

Chat

Chat-Hard

Safety

Reasoning

Average

Chat

Chat-Hard

Safety

Reasoning

Gemma-2-9B-IT

Vanilla RM

81.22

97.90

63.64

77.48

85.88

79.14

97.26

58.85

69.30

91.17

## Rrm

82.54

97.12

71.05

74.70

87.27

83.46

97.21

69.15

73.13

94.35

Crome

87.84

97.54

72.30

87.14

94.39

85.46

96.28

65.83

84.05

95.70

ΔCrome - RRM

+5.30↑

+0.42↑

+1.25↑

+12.44↑

+7.12↑

+2.00↑

-0.93↓

-3.32↓

+10.92↑

+1.35↑

Qwen2.5-7B

Vanilla RM

78.18

97.21

52.85

73.99

88.68

72.73

97.21

46.27

68.04

79.39

## Rrm

82.04

97.21

64.80

75.27

90.86

78.20

98.04

59.65

72.43

82.66

Crome

83.15

96.37

61.73

82.23

92.26

80.81

96.93

58.66

78.92

88.71

ΔCrome - RRM

+1.11↑

-0.84↓

-3.07↓

+6.96↑

+1.40↑

+2.61↑

-1.11↓

-0.99↓

+6.49↑

+6.05↑

Gemma-2-2B

Vanilla RM

53.75

92.88

33.33

42.03

46.74

65.52

94.27

38.27

50.20

79.34

## Rrm

66.23

94.13

43.75

47.64

79.38

66.95

94.97

49.34

50.07

73.42

Crome

70.69

92.18

50.00

55.14

85.42

72.45

92.74

53.62

60.00

83.45

ΔCrome - RRM

+4.46↑

-1.95↓

+6.25↑

+7.50↑

+6.04↑

+5.50↑

-2.23↓

+4.28↑

+9.93↑

+10.03↑

Table 2 | Performance Comparison of Pairwise Preference Model and Bradley-Terry Reward Model on RewardBench

trained using various base models. See Appendix Section C.1 for variance in results.

5.1. Main Theoretical Result (Informal)

This structure leads to the following result (formalized as Theorem 2 in Appendix I):

Theorem 1 (Informal Statement). Under the idealized model assumptions, ℓ1-constrained regression

on 𝑚causally augmented examples recovers the true causal reward coefficients a with an ℓ2-error

∥𝜃−ˆ𝜃∥2 that scales (ignoring constants and terms related to imperfect sparsity recovery) roughly

as 𝑂



∥𝜃N𝑐∥1( 1

𝑘+

√︃

log(𝑘+ℓ)

𝑚

)



where N is the top 𝑂(𝑘) coefficients in the 𝑅∗true reward model. This

highlights a primary dependence on the number of causal attributes 𝑘and samples 𝑚, and only a weak,

logarithmic dependence on the spurious attribute dimension ℓ.

Implications: This theorem suggests that Crome’s causal augmentation, by promoting favorable

properties (like RIP or low incoherence) in the effective design matrix, guides the reward model towards

genuine causal drivers. Further, the error vector has ℓ2 norm is linear in the causal dimension 𝑘in the

worst case and zero in the best case where 𝑅∗has sparser dependence on the causal factors. If it was the

preference training dataset, the error could be proportional to ∥𝜃∥1 (which is 𝑂(𝑘2)).

6. Experiments

Our experiments are designed to address the following research questions:

RQ1: RM Performance and Robustness: How does Crome perform on standard preference prediction

tasks and how robust is it against spurious correlations(Table 2, Figure 5)?

RQ2: Best-of-N Alignment: Does the robustness achieved by Crome lead to favorable results in a

Best-of-N setup as well, when compared to strong baselines (Figures 7, 8, Table 3)?

RQ3: Neutral Augmentations: How effective are the different neutrals augmentation strategies in

enforcing invariance to unknown spurious correlates (Figures 9, 10)?

9

## Page 10

Robust Reward Modeling via Causal Rubrics

6.1. Experimental Settings

Crome and baseline reward models (Vanilla RM, RRM (Liu et al., 2024)) are trained on the UltraFeed-

back dataset (Cui et al., 2023), with counterfactuals generated using Gemini 2.0 Flash. We evaluate

performance on RewardBench (Lambert et al., 2024) and robustness on reWordBench (Wu et al., 2025) 2.

Experiments utilize diverse base LLMs (Gemma-2-9B-IT, Qwen2.5-7B, Gemma-2-2B) for both Pairwise

Preference (PairPM) and Bradley-Terry (BT) reward models. Downstream alignment impact is assessed

via Best-of-N selection on tasks including RewardBench, GSM8K, and WildGuardTest. Comprehensive

details on datasets, model specifics, augmentation procedures, filtering, training hyperparameters, and

all experimental configurations are provided in Appendix E.

6.2. Experimental Results addressing Research Questions (RQ1-3):

Average

jb4

jb2

jb3

swap_format

tw_handle

punct_space

homoglyph

stresstest

cmt_bad_good

tw_url

char_swap_etc

paraphrase

back_transc

rot_13

add_quotes

rot_2

ignore_below

back_trans

ignore_above

jb1

cmt_bad_bad

code_minify

append_code

0

20

40

60

80

100

Score (%)

+9.1%

+50.7%

+50.1%

+28.0%

+24.5% +8.0% +7.8%

+7.7%

+7.5%

+7.0%

+6.9%

+6.4%

+5.6% +5.3%

+5.0%

+4.9%

+4.5%

+3.6%

+3.3%

+2.8%

+1.8%

+0.6%

-0.6%

-31.1%

## Rm

## Rrm

Ours

Figure 5 | Robustness of Crome on reWordBench. Comparing RM, RRM and Crome by measuring ranking

accuracy on a diverse set of meaning preserving transformations in reWordBench. Various transformations such

as paraphrasing, addition of irrelevant text or code, comments etc, test the sensitivity of models to spuriousness.

Robust training of Crome leads to robustness to spuriousness and increased sensitivity to causal attributes.

On RewardBench (Table 2), Crome consistently improves ranking accuracy over RRM across diverse

base models and reward modeling techniques (PairPM, BT). These improvements are particularly notable

on the challenging Safety (up to 13.18%↑) and Reasoning (up to 7.19%↑). Crome also demonstrates

superior robustness on reWordBench, which tests for robustness of RMs against meaning-preserving

transformations (Figure 5).

These results show Crome’s robustness to inputs having spurious punctations, paraphrasing, irrelevant

text, code or comments as tested by various reWordBench transformations. With Gemma-2-9B-IT,

Crome in the PairPM setting shows an aggregate accuracy gain of up to 9.1%↑and is superior on

(21/23) transformations.

Key Takeaway: Crome improves RM performance on standard benchmarks while significantly

improving performance and mitigating ranking accuracy drops on diverse transformed inputs, without

ever being explicitly trained on such spurious transformations.

2Since reWordBench has not been released, we follow the paper and communicated with the authors to reproduce it, see

Appendix Section D

10

## Page 11

Robust Reward Modeling via Causal Rubrics

BoN for Robust LLM Alignment Across Chat, Reasoning, and Safety

Following the method used by

Wu et al. (2025), we perform best-of-n selection using Crome across RewardBench categories, which

consists of datasets such as AlpacaEval. Across all values of 𝑁, Crome provided significant improvements

over baselines in a head-to-head comparison.

Key Takeaway: Crome’s emphasis on causal attributes enhances its discriminative power in Best-of-N

selection, leading to more consistent identification of superior responses.

## N

Crome vs RM

Crome vs RRM

Crome

## Rm

Ties

Crome

## Rrm

Ties

4

28.08

13.85

58.07

28.03

14.13

57.84

8

34.32

17.24

48.43

34.36

17.19

48.45

16

39.93

20.54

39.53

41.14

20.40

38.46

32

44.79

21.88

33.33

45.46

22.01

32.53

Table 3 | Win rates for Crome compared with RM and RRM on RewardBench. We follow Wu et al. (2025) and

take all 2985 prompts from RewardBench and get BoN responses from a Gemma-2-9B-IT model using Crome,

RM or RRM as the reward models. Following this, we separately compare responses generated by Crome with

RM and RRM, using GPT-4 as a judge.

## Rm

## Rrm

Ours

0

20

40

60

80

100

Ranking Accuracy (%)

BT Results

19.75%

20.77%

18.82%

## Rm

## Rrm

Ours

0

20

40

60

80

100

Ranking Accuracy (%)

PairPM Results

21.75%

21.54%

19.78%

RewardBench

reWordBench

Figure 6 | Percentage improvement in ranking accuracy between RewardBench and reWordBench. Here we

show the average ranking accuracy across reWordBench transformations of Crome and baselines on reWordBench

and RewardBench as done in Wu et al. (2025), as well as the percentage drop in ranking accuracy on reWordBench

compared to RewardBench. We show that Crome’s ranking accuracy percentage drop going from RewardBench

to reWordBench is the lowest compared to baselines.

Ranking Accuracy Percentage Improvements:

We measure the percentage drop in response ranking

accuracy between RewardBench and reWordBench scores (following the macro-avg metric used in Wu

et al. (2025)). Crome exhibits a smaller ranking accuracy percentage drop from RewardBench to

reWordBench (In case of PairPM: 19.78% vs. RRM’s 21.54%. See Figure 6 for the results on BT and

PairPM settings.

Key Takeaway: Assuming sufficient concentration of spurious elements in the prompt as well as the 𝑁

responses, Crome is better at selecting the best response based on causal attributes only. For e.g., in

safety, harmful prompts and responses may be spuriously disguised as benign.

11

## Page 12

Robust Reward Modeling via Causal Rubrics

Causal Attributes help in detecting jailbreaks

For Gemma-2-9B-IT as the solution genera-

tion model, BoN with Crome shows signifi-

cant improvements on safety as measured on

WildGuardTest. In particular the attack success

ratio (ASR) on harmful prompts is much lower

compared to models aligned with RM and RRM

and this gap increases with N. This improved

ASR comes at at a similar refusal-to-answer rate

on benign prompts.

2

4

8

16

32

Number of Responses (N)

0

1

2

3

ASR Reduction (%)

(Higher is Better)

+0.6%

+0.3%

+1.5%

+1.2%

+2.1%

+1.8%

+2.6% +2.6%

+2.7%

+2.3%

## Asrrm

ASROurs (

)

## Asrrrm

ASROurs (

)

Figure 7 | Best-of-N results: ASR reduction on Wild-

GuardTest.

Key Takeaway: Crome’s causal augmentations achieve a superior trade-off between safety and

over-refusals, because its contrastive pairs delineate the decision boundary for harmful content more

faithfully. This leads to safer content, while avoiding excessive refusals on benign prompts.

1

2

4

8

16

Number of Responses (N)

85

86

87

88

89

Accuracy (%)

## Rm

## Rrm

Ours

Figure 8 | Best-of-N Reasoning evaluation on GSM8K.

Disentangling Content related features from

stylistic (spurious) ones helps in reasoning

For

Gemma-2-9B-IT as the solution generation model

on GSM8K, Crome shows a consistent gap over

baseslines across different values of 𝑁. Non ro-

bust reward models may focus on stylistic details.

Good looking, detailed but wrong reasoning steps

may mis-guide non-robust RMs into giving a higher

score to the response.

Key Takeaway: Reasoning correctness is dependent on focusing on correctness over stylistic features.

Our training ensures Crome is good at capturing content-features over other attributes.

6.3. Neutral Ablations

Along with IQN, we tested several methods for enforcing spurious invariance:

Causally Aligned Neutrals (CAN).

Given a preference pair (𝐴𝑤, 𝐴ℓ) where (𝐴𝑤≻𝐴ℓ), we rewrite 𝐴ℓ

into ˜𝐴ℓsuch that the causal content of ˜𝐴ℓaligns with 𝐴𝑤(𝐶(𝐴𝑤) ≈𝐶( ˜𝐴𝑙)), but due to the rewrite from

𝐴ℓ, the spurious attributes of 𝐴ℓremain. By assigning a tie-label to this pair during training, we force

the model to learn invariance to the spurious differences. While this method is sound theoretically, the

approximation of 𝐶(𝐴𝑤) by 𝐶( ˜𝐴𝑙) is not perfect. Furthermore, some spurious attributes 𝑆𝑃′( ˜𝐴𝑙) ⊂𝑆𝑃( ˜𝐴𝑙)

vary when we move causal attributes. Invariance to these attributes 𝑆𝑃′( ˜𝐴𝑙) is not captured by CAN.

Paraphrase Neutral (PARA).

Given an answer 𝐴to a query 𝑄, we rewrite 𝐴to an approximate ˜𝐴

using an LLM, such that spurious features vary, but causal features do not. Unlike CAN which provides

structured rewrites, PARA is a simpler method for rewriting equivalent answers (neutrals). This idea

is common in literature (For example, see Wu et al. (2025)). Yet the central issue here is that 𝐶( ˜𝐴)

may inadvertently vary during a rewrite (due to the 𝑆𝑃→𝐶causation in Fig 2). Furthermore, the SP

variations introduced through paraphrasing are not reflective of the complex downstream distributions.

Other Combinations.

We provide two more variations for completeness – (i) causal only augmentations,

with no neutrals (C) (ii) Both IQN and CAN neutrals sampled equally (IQN+CAN).

12

## Page 13

Robust Reward Modeling via Causal Rubrics

Neutrals help in spurious suppression

Neu-

tral augmentations significantly improve robust-

ness compared to causal-only training (Figures 10

and 9). All neutral variants outperform the causal-

only Crome-C model. Among them, Crome-

IQN achieves the best overall performance on Re-

wardBench, with a gain of +5.4%↑over the RRM

baseline. Meanwhile, Crome-CAN achieves the

best performance on reWordBench, with a gain of

+12.5%↑.

78

80

82

84

86

88

90

RewardBench (Average)

+3.5

+5.4

+3.3

+3.4

+4.6

57

60

63

66

69

72

75

78

reWordBench (Average)

+9.2

+8.1

+9.9

+12.5

+11.3

Accuracy (%)

## Rm

## Rrm

Crome-C

Crome-IQN (Ours)

Crome-PARA

Crome-CAN

Crome-IQN+CAN

Figure 9 | Average performance on RewardBench and

reWordBench for Crome trained with different neutral

augmentation strategies.

Key Takeaway: Explicit suppression of spurious correlates via neutral augmentations mitigates reward

hacking by learning invariant reward signals, thereby improving downstream performance.

97

98

Chat

+0.3

+0.6

+0.3

+0.6

+0.4

62

64

66

68

70

72

74

Chat-Hard

-7.0

+0.7

-7.6

-5.3

-1.1

72

74

76

78

80

82

84

86

88

Safety

+13.5+13.2+13.6+13.8+13.4

84

86

88

90

92

94

96

Reasoning

+7.0 +7.2 +6.8

+4.5

+5.7

Accuracy (%)

## Rm

## Rrm

Crome-C

Crome-IQN (Ours)

Crome-PARA

Crome-CAN

Crome-IQN+CAN

Figure 10 | Evaluations of neutral augmentation variants on the different subsets of RewardBench.

The Crome variants include: Crome-C (only causals), Crome-IQN (causals + irrelevant query

neutrals), Crome-PARA (causals + paraphrased neutrals), Crome-CAN (causals + causally-aligned

neutrals), and Crome-IQN+CAN (causals + irrelevant query neutrals + causally-aligned neutrals). On

the especially challenging Chat-Hard subset, Crome-IQN performs best. See Appendix Section F for

more details. Prompts for obtaining these neutrals is given in Appendix K.

Key Takeaway: A combination of well-designed augmentation strategies, e.g. causal upgradations

and degradations, along with IQN produces the most robust and generalizable reward models.

Discussion on Neutrals:

Our Figure 2 suggests that interventions along spurious attributes can confound

causal attributes in myriad ways. Firstly, there could be causal attributes, which upon intervention can

lead to spurious attribute change (𝐶𝐴→𝑆𝑃). Secondly, if spurious attributes change, this can lead to a

change in Causal Attributes (𝑆𝑃→𝐶𝐴). Due to such confounding factors, an intervention free solution,

such as IQN, turns out to be a clever way to provide invariance to spuriousness. IQN provides invariance to

those spurious factors that change with causal changes (See Fig. 3), as well as natural spurious variations

when irrelevant questions are paired with answers corresponding to a different question.

Ablations and Additional Results:

See Appendix Section C where we show that Crome exhibits stable

and significant improvements in robustness with low variance across different training runs. We also

show that using open-weights models as the oracle LLM, such as Gemma-3-27B-IT, Crome exhibits

significant improvements in robustness. Additionally, we also show performance of Crome and baselines

on in-distribution and out-of-distribution examples, showing superior effective robustness achieved by

Crome.

13

## Page 14

Robust Reward Modeling via Causal Rubrics

Method

reWordBench

RewardBench

Average

Average

Chat

Chat-Hard

Safety

Reasoning

Vanilla RM

59.97

80.61

98.18

63.38

76.08

84.80

## Rrm

64.68

82.53

96.93

72.04

73.78

87.36

Crome

67.90

85.15

97.21

68.75

83.51

91.13

Table 4 | RM Performance with Gemma-3-27B-IT as oracle. Results on RewardBench and REwardBench

with Gemma-2-9B-IT as base model and Gemma-3-27B-IT as oracle LLM used for attribute extraction and

counterfactual augmentations. Results are in PairPM setting.

Robustness to Oracle LLM Choice

To test our robustness to the choice of oracle LLM, we provide

experimental results using Gemma-3-27B-IT to perform attribute extraction and augmentations fol-

lowing which we train Crome on the augmented data. Table 4 shows that Crome outperforms the

baselines by up to 2.5% on RewardBench and 3.2% on reWordBench. In Figure 11, our results indicate

an improvement in 18/23 transformations of reWordBench. This shows that our method is performant

even with a weaker oracle LLM. This potentially indicates that the strength of Crome lies in its causal

method, and goes beyond simply leveraging the knowledge of the oracle model.

Average

jb4

jb2

swap_format

char_swap_etc

homoglyph

punct_space

tw_handle

tw_url

stresstest

paraphrase

back_transc

add_quotes

rot_2

cmt_bad_good

ignore_below

rot_13

ignore_above

code_minify

jb3

cmt_bad_bad

jb1

back_trans

append_code

0

20

40

60

80

100

Score (%)

+4.0%

+26.1%

+24.0%

+8.3%

+8.1% +6.7%

+5.8% +5.6% +5.4% +4.9% +4.9% +4.0%

+3.5%

+3.2%

+2.4%

+2.4%

+1.6%

+1.0% +0.0%

-0.1%

-0.9%

-2.1%

-5.6%

-17.7%

## Rm

## Rrm

Ours

Figure 11 | Robustness with Gemma-3-27B-IT as oracle LLM Comparing of RM, RRM and Crome on reWord-

Bench. Here all reward models are Gemma-2-9B-IT based, in the PairPM setting.

7. Discussion, Conclusion and Future Work

In this paper, we proposed Crome, a causal framework to mitigate reward hacking during the training of

reward models. Crome systematically disentangles causal from spurious attributes through two targeted

synthetic data augmentation strategies: (1) Causal Augmentations to enforce sensitivity to genuine

quality drivers, and (2) Neutral Augmentations to enforce invariance to spurious features. Notably,

Crome does not assume access to types of spurious attributes that might effect RMs. Across multiple

base models and reward modeling techniques (PairPM, BT), Crome consistently outperforms strong

baselines on the RewardBench benchmark. Furthermore, Crome shows superior robustness on the

reWordBench benchmark, which specifically tests for vulnerabilities to spurious correlations. We also

achieve consistent improvements in downstream Best-of-N setups.

Future Work.

Our training method, centered on dataset curation, paves the way for new research

directions in synthetic data research. A compelling application is in synthetic data generation for base

model training, where the use and verification of causal attributes could prove particularly fruitful.

14

## Page 15

Robust Reward Modeling via Causal Rubrics

8. Acknowledgments

We thank Prateek Jain, Praneeth Nethrapalli, Rishi Saket, Partha Talukdar, Vihari Piratla, Darshan Singh

S and Saisuresh Krishnakumaran for providing feedback on this work. The authors would like to thank

Manish Gupta for his valuable support and guidance.

References

M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint

arXiv:1907.02893, 2019.

A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma,

et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861,

2021.

M. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello. A general theoretical

paradigm to understand learning from human preferences. In International Conference on Artificial

Intelligence and Statistics, pages 4447–4455. PMLR, 2024.

Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,

et al. Training a helpful and harmless assistant with reinforcement learning from human feedback.

arXiv preprint arXiv:2204.05862, 2022a.

Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McK-

innon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073,

2022b.

R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired

comparisons. Biometrika, 39(3/4):324–345, 1952.

S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner,

P. Freire, et al. Open problems and fundamental limitations of reinforcement learning from human

feedback. arXiv preprint arXiv:2307.15217, 2023.

L. Chen, C. Zhu, D. Soselia, J. Chen, T. Zhou, T. Goldstein, H. Huang, M. Shoeybi, and B. Catanzaro.

Odin: Disentangled reward mitigates hacking in rlhf. arXiv preprint arXiv:2402.07319, 2024.

H. Chi, H. Li, W. Yang, F. Liu, L. Lan, X. Ren, T. Liu, and B. Han. Unveiling causal reasoning in large

language models: Reality or mirage? Advances in Neural Information Processing Systems, 37:96640–

96670, 2024.

P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning

from human preferences. Advances in neural information processing systems, 30, 2017.

K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,

R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint

arXiv:2110.14168, 2021.

T. Coste, U. Anwar, R. Kirk, and D. Krueger. Reward model ensembles help mitigate overoptimization.

arXiv preprint arXiv:2310.02743, 2023.

15

## Page 16

Robust Reward Modeling via Causal Rubrics

G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting

language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023.

C. Denison, M. MacDiarmid, F. Barez, D. Duvenaud, S. Kravec, S. Marks, N. Schiefer, R. Soklaski, A. Tamkin,

J. Kaplan, et al. Sycophancy to subterfuge: Investigating reward-tampering in large language models.

arXiv preprint arXiv:2406.10162, 2024.

J. Eisenstein, C. Nagpal, A. Agarwal, A. Beirami, A. D’Amour, D. Dvijotham, A. Fisch, K. Heller, S. Pfohl,

D. Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate

reward hacking. arXiv preprint arXiv:2312.09244, 2023.

K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela. Kto: Model alignment as prospect

theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.

A. Feder, N. Oved, U. Shalit, and R. Reichart. Causalm: Causal model explanation through counterfactual

language models. Computational Linguistics, 47(2):333–386, 2021.

A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-Doughty, J. Eisenstein, J. Grimmer,

R. Reichart, M. E. Roberts, et al. Causal inference in natural language processing: Estimation, prediction,

interpretation and beyond. Transactions of the Association for Computational Linguistics, 10:1138–1158,

2022.

L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. In International

Conference on Machine Learning, pages 10835–10866. PMLR, 2023.

T. Gupta, S. Shandilya, X. Zhang, R. Madhavan, S. Ghosh, C. Bansal, H. Yao, and S. Rajmohan. Carmo:

Dynamic criteria generation for context-aware reward modelling, 2025. URL https://arxiv.org/

abs/2410.21545.

S. Han, K. Rao, A. Ettinger, L. Jiang, B. Y. Lin, N. Lambert, Y. Choi, and N. Dziri. Wildguard: Open

one-stop moderation tools for safety risks, jailbreaks, and refusals of llms, 2024. URL https://arxiv.

org/abs/2406.18495.

J. Hong, N. Lee, and J. Thorne. Orpo: Monolithic preference optimization without reference model.

In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages

11170–11189, 2024.

D. Kaushik, E. Hovy, and Z. C. Lipton. Learning the difference that makes a difference with counterfactually-

augmented data. arXiv preprint arXiv:1909.12434, 2019.

M. Khalifa, R. Agarwal, L. Logeswaran, J. Kim, H. Peng, M. Lee, H. Lee, and L. Wang. Process reward

models that think. arXiv preprint arXiv:2504.16828, 2025.

E. Kiciman, R. Ness, A. Sharma, and C. Tan. Causal reasoning and large language models: Opening a

new frontier for causality. Transactions on Machine Learning Research, 2023.

N. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi,

et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787,

2024.

W. Li and Y. Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287, 2024.

16

## Page 17

Robust Reward Modeling via Causal Rubrics

T. Liu, W. Xiong, J. Ren, L. Chen, J. Wu, R. Joshi, Y. Gao, J. Shen, Z. Qin, T. Yu, et al. Rrm: Robust reward

model training mitigates reward hacking. arXiv preprint arXiv:2409.13156, 2024.

Y. Liu, Z. Yao, R. Min, Y. Cao, L. Hou, and J. Li. Pairwise rm: Perform best-of-n sampling with knockout

tournament. arXiv preprint arXiv:2501.13007, 2025.

S. Long, A. Piché, V. Zantedeschi, T. Schuster, and A. Drouin. Causal discovery with language models as

imperfect experts. arXiv preprint arXiv:2307.02390, 2023.

I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,

2017.

X. Lou, D. Yan, W. Shen, Y. Yan, J. Xie, and J. Zhang. Uncertainty-aware reward model: Teaching reward

models to know what is unknown. arXiv preprint arXiv:2410.00847, 2024.

Y. Meng, M. Xia, and D. Chen. Simpo: Simple preference optimization with a reference-free reward.

arXiv preprint arXiv:2405.14734, 2024.

A. Mishra, G. Nayak, S. Bhattacharya, T. Kumar, A. Shah, and M. Foltin. Llm-guided counterfactual data

generation for fairer ai. In Companion Proceedings of the ACM Web Conference 2024, pages 1538–1545,

2024.

S. Negahban, B. Yu, M. J. Wainwright, and P. Ravikumar. A unified framework for high-dimensional

analysis of 𝑚-estimators with decomposable regularizers. Advances in neural information processing

systems, 22, 2009.

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,

A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural

information processing systems, 35:27730–27744, 2022.

A. Pace, J. Mallinson, E. Malmi, S. Krause, and A. Severyn. West-of-n: Synthetic preference generation

for improved reward modeling. arXiv preprint arXiv:2401.12086, 2024.

A. Pan, K. Bhatia, and J. Steinhardt. The effects of reward misspecification: Mapping and mitigating

misaligned models, 2022. URL https://arxiv.org/abs/2201.03544.

R. Park, R. Rafailov, S. Ermon, and C. Finn. Disentangling length from quality in direct preference

optimization. arXiv preprint arXiv:2403.19159, 2024.

J. Pearl. Causality. Cambridge university press, 2009.

J. Peters, D. Janzing, and B. Schölkopf. Elements of causal inference: foundations and learning algorithms.

The MIT Press, 2017.

Y. Qiang, S. Nandi, N. Mehrabi, G. V. Steeg, A. Kumar, A. Rumshisky, and A. Galstyan. Prompt perturbation

consistency learning for robust language models. arXiv preprint arXiv:2402.15833, 2024.

Z. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, L. Yan, J. Shen, T. Liu, J. Liu, D. Metzler, et al.

Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint

arXiv:2306.17563, 2023.

17

## Page 18

Robust Reward Modeling via Causal Rubrics

R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization:

Your language model is secretly a reward model. Advances in Neural Information Processing Systems,

36, 2024.

A. Ramé, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, and J. Ferret. Warm: On the

benefits of weight averaged reward models. arXiv preprint arXiv:2401.12187, 2024.

S. Ravfogel, A. Svete, V. Snæbjarnarson, and R. Cotterell. Gumbel counterfactual generation from

language models, 2025. URL https://arxiv.org/abs/2411.07180.

D. Reber, S. Richardson, T. Nief, C. Garbacea, and V. Veitch. Rate: Score reward models with imperfect

rewrites of rewrites. arXiv preprint arXiv:2410.11348, 2024.

B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. Toward causal

representation learning. Proceedings of the IEEE, 109(5):612–634, 2021.

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.

arXiv preprint arXiv:1707.06347, 2017.

Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath:

Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300,

2024.

J. Shen, R. Xu, Y. Jun, Z. Qin, T. Liu, C. Yang, Y. Liang, S. Baumgartner, and M. Bendersky. Boosting

reward model with preference-conditional multi-aspect synthetic data generation. arXiv preprint

arXiv:2407.16008, 2024.

L. Shen, S. Chen, L. Song, L. Jin, B. Peng, H. Mi, D. Khashabi, and D. Yu. The trickle-down impact of

reward (in-) consistency on rlhf. arXiv preprint arXiv:2309.16155, 2023.

P. Singhal, T. Goyal, J. Xu, and G. Durrett. A long way to go: Investigating length correlations in rlhf.

arXiv preprint arXiv:2310.03716, 2023.

J. Skalse, N. Howe, D. Krasheninnikov, and D. Krueger. Defining and characterizing reward gaming.

Advances in Neural Information Processing Systems, 35:9460–9471, 2022.

N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.

Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:

3008–3021, 2020.

G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S.

Kale, J. Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint

arXiv:2403.08295, 2024.

R. Tu, C. Ma, and C. Zhang. Causal-discovery performance of chatgpt in the context of neuropathic pain

diagnosis. arXiv preprint arXiv:2301.13819, 2023.

L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier,

N. Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.

C. Wang, Z. Zhao, Y. Jiang, Z. Chen, C. Zhu, Y. Chen, J. Liu, L. Zhang, X. Fan, H. Ma, et al. Beyond reward

hacking: Causal rewards for large language model alignment. arXiv preprint arXiv:2501.09620, 2025.

18

## Page 19

Robust Reward Modeling via Causal Rubrics

H. Wang, W. Xiong, T. Xie, H. Zhao, and T. Zhang. Interpretable preferences via multi-objective reward

modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024.

Z. Wu, M. Yasunaga, A. Cohen, Y. Kim, A. Celikyilmaz, and M. Ghazvininejad. rewordbench: Bench-

marking and improving the robustness of reward models with transformed inputs. arXiv preprint

arXiv:2503.11751, 2025.

A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5

technical report. arXiv preprint arXiv:2412.15115, 2024a.

R. Yang, X. Pan, F. Luo, S. Qiu, H. Zhong, D. Yu, and J. Chen. Rewards-in-context: Multi-objective

alignment of foundation models with dynamic preference adjustment. arXiv preprint arXiv:2402.10207,

2024b.

X. Zhang, W. Xiong, L. Chen, T. Zhou, H. Huang, and T. Zhang. From lists to emojis: How format bias

affects model alignment. arXiv preprint arXiv:2409.11704, 2024.

J. Zhao, R. Liu, K. Zhang, Z. Zhou, J. Gao, D. Li, J. Lyu, Z. Qian, B. Qi, X. Li, et al. Genprm: Scaling

test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891,

2025.

Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. Slic-hf: Sequence likelihood calibration

with human feedback. arXiv preprint arXiv:2305.10425, 2023.

X. Zhu, C. Tan, P. Chen, R. Sennrich, Y. Zhang, and H. Hu. Charm: Calibrating reward models with

chatbot arena scores. arXiv preprint arXiv:2504.10045, 2025.

19

## Page 20

Robust Reward Modeling via Causal Rubrics

Supplementary Material

These supplementary materials provide additional details, derivations, and experimental results for our

paper. The appendix is organized as follows:

• Section A discusses potential limitations of this work.

• Section B provides a broader overview of recent related literature. This is an expanded version of the

literature covered in the main paper.

• Section C provides some additional set of results. This is an expanded version of the results covered in

the main paper.

• Section D provides the detailed steps we took to reproduce the reWordBench benchmark, as proposed

in Wu et al. (2025).

• Section E provides a detailed overview of our experimental setup.

• Section F provides a detailed walk through of how our causal model extends to prior method. We

revisit prior works in light of our causal model. It extends on the shorter version provided in Section 3.

• Section G provides a walkthrough of the causal details of the core data augmentation strategies.

• Section H provides a detailed walk through of the method used to train the reward model. It extends

on the shorter version provided in Section 4.

• Section I provides a detailed analysis of the theory relating to Reward Hacking and how our proposed

method mitigates it.

• Section J presents a qualitative example of augmented data created from original data using which is

used to train Crome.

• Section K presents a lists of prompt templates that we use to query our models for generating the data.

• Section L presents a qualitative view common failure modes or biases commonly observed in reward

models.

A. Limitations and Future Work

While Crome demonstrates significant improvements, we acknowledge certain limitations which also

suggest avenues for future research:

• Idealized Assumptions in Theoretical Analysis: Our theoretical justification (Section 5, Appendix I)

relies on simplifying assumptions such as boolean attributes, quadratic reward models, and perfect

counterfactual interventions. These idealizations, necessary for analytical tractability, mean our

formal guarantees are indicative of Crome’s potential mechanism rather than absolute predictions

of real-world performance, where the complexities of LLM behavior and data are greater.

• Scalability and Cost of Data Augmentation: The generation of targeted causal and neutral aug-

mentations, while effective, involves multiple LLM inference calls per original data point. Although

filtering helps optimize the final dataset size, the initial augmentation phase can be computationally

intensive and potentially costly for extremely large-scale applications. Future work could explore

more sample-efficient augmentation strategies or methods to distill the benefits of augmentation

into smaller datasets.

20

## Page 21

Robust Reward Modeling via Causal Rubrics

• Generalization to Highly Novel Spurious Correlations: Crome is designed to be robust against

unspecified spurious correlations by focusing on causal signals and diverse neutral examples. However,

its ability to generalize to entirely novel types of spuriousness, drastically different from any patterns

implicitly covered or contrasted during augmentation, remains an empirical question. The breadth

and nature of the neutral augmentations play a role here, and continuous adaptation or more abstract

invariance learning might be needed for extreme out-of-distribution spuriousness.

• Fidelity of LLM-Generated Counterfactuals: The efficacy of Crome is linked to the quality of the

LLM-generated counterfactuals. While current LLMs are powerful, ensuring perfect attribute isolation

in causal augmentations or complete causal content preservation in neutral pairs is challenging.

Imperfections in these LLM-approximated interventions can introduce noise. While our empirical

results show strong benefits, further research into enhancing the precision and verifiability of

LLM-driven textual counterfactual generation could yield additional improvements.

Future research could focus on extending the theoretical framework to encompass more realistic set-

tings, developing more cost-effective and adaptive augmentation techniques, and further exploring the

boundaries of generalization against emergent spurious correlations.

B. Extended Related Works

Our work on Crome, a framework for causally robust reward modeling, intersects with and builds upon

several key areas of research: the alignment of Large Language Models (LLMs) via human feedback,

techniques for reward model training, the persistent challenge of reward hacking, the application of

causal inference principles to machine learning, and data augmentation strategies for enhancing model

robustness.

LLM Alignment and RLHF.

The dominant paradigm for steering LLM behavior towards desired

attributes like helpfulness, honesty, and harmlessness is Reinforcement Learning from Human Feedback

(RLHF) (Askell et al., 2021; Bai et al., 2022a; Christiano et al., 2017; Ouyang et al., 2022; Stiennon

et al., 2020). The standard RLHF process involves training a reward model (RM) on human preferences

(typically pairwise comparisons) and subsequently using this RM as a reward signal to fine-tune the LLM

policy via RL algorithms such as PPO (Schulman et al., 2017). The quality, calibration, and robustness of

the RM are paramount, as flaws in the RM directly impact the alignment outcome (Casper et al., 2023).

While alternative alignment algorithms like Direct Preference Optimization (DPO) (Rafailov et al., 2024)

and its extensions (e.g., IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), ORPO (Hong et al., 2024),

SimPO (Meng et al., 2024)) bypass explicit RM training by directly optimizing the policy on preference

data, they still implicitly rely on the preference information learnable from the data, making the problem

of distinguishing true quality from spurious correlates equally relevant.

Reward Modeling Techniques.

Learning accurate reward models from preference data remains a

central challenge. Methodologies include Bradley-Terry style pointwise models that learn a scalar score

𝑟(𝑥, 𝑦) (Bai et al., 2022a; Bradley and Terry, 1952; Ouyang et al., 2022), and pairwise ranking models that

directly predict preference probabilities, often implemented within the LLM architecture itself (PairPM)

(Liu et al., 2025; Qin et al., 2023). Other approaches explore Q-function based rewards (Li and Li,

2024) or process supervision (Khalifa et al., 2025). Significant effort focuses on improving specific RM

21

## Page 22

Robust Reward Modeling via Causal Rubrics

properties like calibration (Zhao et al., 2023; Zhu et al., 2025), training efficiency (Tunstall et al., 2023),

uncertainty quantification (Lou et al., 2024), interpretability through multi-aspect rewards (Wang et al.,

2024; Yang et al., 2024b), and scalability via reasoning or chain-of-thought mechanisms (Zhao et al.,

2025). Our work complements these efforts by focusing specifically on enhancing the causal robustness

of the learned reward function ˆ𝑅against spurious attributes.

Reward Hacking and Spurious Correlations.

Learned reward models are notoriously susceptible to

reward hacking or over-optimization (Gao et al., 2023; Pan et al., 2022; Skalse et al., 2022). Because RMs

are trained on finite, potentially biased data, they often learn to associate high rewards with superficial

or spurious features that are merely correlated with desirable responses in the training set. Common

examples include excessive length or verbosity (Singhal et al., 2023), specific formatting patterns like lists

or markdown (Zhang et al., 2024), adherence to stylistic conventions like politeness, or even sycophantic

agreement with user views (Denison et al., 2024). Policies optimized against such RMs learn to exploit

these spurious cues, leading to outputs that maximize the predicted reward but fail to align with genuine

human preferences or task goals (Shen et al., 2023).

Approaches to Mitigating Reward Hacking.

Various strategies have been proposed to address reward

hacking. Model-centric approaches include using ensembles of RMs to average out idiosyncratic biases

(Coste et al., 2023; Eisenstein et al., 2023; Ramé et al., 2024), incorporating explicit calibration methods

(Zhao et al., 2023), or designing architectures that factorize reward components, such as ODIN’s dis-

entanglement of quality and length (Chen et al., 2024). Policy-optimization techniques might involve

adding explicit penalties for spurious features (e.g., length penalties (Park et al., 2024)) or using specific

regularization methods during fine-tuning. Data-centric approaches aim to improve the training data

or process itself. Examples include iterative re-labeling or refinement (Bai et al., 2022b), performing

consistency checks across related prompts (Shen et al., 2023), or augmenting the dataset with synthetic

examples designed to improve robustness (Pace et al., 2024; Shen et al., 2024). Our work, Crome, falls

firmly in this data-centric category. It is closely related to RRM (Liu et al., 2024), which also uses data

augmentation (non-contextual and query-independent pairs) for robustness. However, Crome is distinct

in its use of an explicit causal framework and its generation of targeted, attribute-specific counterfactuals

to disentangle causal from spurious factors.

Causal Inference in Machine Learning.

Causal inference provides formal tools, such as Structural

Causal Models (SCMs) and DAGs (Pearl, 2009; Peters et al., 2017), for reasoning about cause-effect

relationships, confounding, and counterfactuals. Applying causal principles in machine learning aims to

build models that are more robust, fair, and interpretable by focusing on underlying causal mechanisms

rather than potentially brittle statistical correlations (Schölkopf et al., 2021). Techniques like Invariant

Risk Minimization (IRM) seek models that perform well across different environments by relying on

invariant (presumably causal) predictors (Arjovsky et al., 2019). Our work adopts this causal perspective,

framing spurious attributes as non-causal factors whose influence on the learned reward model should

be minimized.

Causality in LLMs and NLP.

The intersection of causality and LLMs is rapidly evolving. Research

includes probing the innate causal reasoning abilities of LLMs (Chi et al., 2024; Kiciman et al., 2023),

22

## Page 23

Robust Reward Modeling via Causal Rubrics

leveraging LLMs as tools for automating parts of the causal discovery or analysis pipeline (Long et al.,

2023; Tu et al., 2023), and applying causal methods to enhance NLP tasks. For instance, counterfactual

reasoning and data augmentation have been used to improve robustness against biases in text classification

(Feder et al., 2021; Kaushik et al., 2019) and assess fairness (Feder et al., 2022). Crome uniquely

employs a predefined causal graph to structure the generation of counterfactual data specifically for

training a robust RM, using LLMs as the generation engine.

Data Augmentation for Robustness.

Data augmentation is a cornerstone technique for improving model

generalization. Beyond traditional NLP methods like synonym replacement or back-translation (Wu et al.,

2025), more recent approaches leverage LLMs for sophisticated augmentations, including paraphrasing,

style transfer, generating adversarial examples (Qiang et al., 2024), or creating counterfactuals (Feder

et al., 2021; Mishra et al., 2024). Counterfactual generation, often using LLMs as rewriters, is also central

to evaluation methods like RATE (Reber et al., 2024), which uses “rewrites of rewrites” to estimate

causal effects robustly. Methods based on sampling, like Gumbel temperature sampling, have also been

explored for counterfactual generation (Ravfogel et al., 2025). In the specific context of reward modeling,

data augmentation aims to enhance robustness against spurious correlations; examples include the

non-contextual and query-independent pairs used by RRM (Liu et al., 2024) or consistency checks via

paraphrased inputs as explored in ReWordBench (Wu et al., 2025). Furthermore, generating entirely

synthetic preference pairs (Pace et al., 2024; Shen et al., 2024) represents another data-centric approach

to improving reward models. Counterfactual data augmentation, particularly generating minimally

different pairs to isolate specific features (Kaushik et al., 2019), is highly relevant to disentangling causal

factors. Our work, Crome, operationalizes this concept within an explicit causal framework, generating

targeted "causal" (attribute-isolating) and “neutral” (spurious-varying) pairs via LLM rewriting to enforce

specific invariance and sensitivity properties in the trained RM.

Positioning of Crome.

Crome integrates insights from causal inference and data augmentation

to address the critical problem of reward hacking in LLM alignment. While related works like RRM

(Liu et al., 2024) use data augmentation for robustness and Crome is distinguished by its explicit

grounding in a causal graph model of answer attributes. It systematically generates attribute-specific

counterfactual and neutral examples via guided LLM prompting to directly train the RM to distinguish

causal quality drivers (𝐶) from spurious correlates (𝑆𝑃). This allows Crome to potentially handle a wider

range of spurious attributes beyond commonly studied ones like length, aiming for a more principled

and generalizable form of robustness. We provide the methodology and empirical validation (Section 6)

demonstrating that this causally-informed data augmentation leads to more robust reward models and

better downstream policy alignment compared to standard baselines.

23

## Page 24

Robust Reward Modeling via Causal Rubrics

C. Additional Results

Our main findings presented in this section are as follows:

• Stable and Significant Performance Gains: Crome consistently outperforms baseline reward

models (Vanilla RM and RRM) on RewardBench across multiple independent training runs, with

small standard deviations indicating stable performance. The improvements, particularly on

reWordBench transformations, are substantial and typically exceed multiple standard deviations of

the baselines, underscoring their statistical significance (Sec. C.1, C.2).

• Strong Out-of-Distribution Generalization: Crome exhibits strong generalization from in-

distribution (UltraFeedback validation) to out-of-distribution benchmarks (RewardBench, reWord-

Bench). Notably, it often achieves the highest OOD accuracy (e.g., +7.02% over RRM on re-

WordBench PairPM) while having similar ID accuracy, suggesting its augmentations teach more

generalizable preference representations (Sec. C.3).

C.1. Variance in Performance on RewardBench

To assess the stability of our findings, we conducted three independent training runs for reward models

built upon the Gemma-2-9B-IT base model. Table 5 for PairPM and BT reports the mean accuracy and

standard deviation on RewardBench categories. The standard deviations for average RewardBench

accuracies are consistently small across all methods (e.g., ±0.09 on average for Crome-PairPM, ±0.12

on average for RRM-PairPM), indicating stable performance. While there is some variation in specific

sub-categories, Crome’s average performance advantage over baselines remains robust.

Method

PairPM

## Bt

Average

Chat

Chat-Hard

Safety

Reasoning

Average

Chat

Chat-Hard

Safety

Reasoning

Gemma-2-9B-IT

Vanilla RM

81.22 ± 0.56

97.90 ± 0.48

63.64 ± 0.28

77.48 ± 1.21

85.88 ± 1.34

79.14 ± 0.68

97.26 ± 0.40

58.85 ± 1.14

69.30 ± 3.61

91.17 ± 1.17

## Rrm

82.54 ± 0.12

97.12 ± 0.21

71.05 ± 0.87

74.70 ± 0.98

87.27 ± 0.21

83.46 ± 0.26

97.21 ± 0.28

69.15 ± 0.54

73.13 ± 0.61

94.35 ± 0.59

Crome

87.84 ± 0.09

97.54 ± 0.21

72.30 ± 0.39

87.14 ± 0.16

94.39 ± 0.21

85.46 ± 0.27

96.28 ± 0.32

65.83 ± 0.81

84.05 ± 1.10

95.70 ± 0.52

ΔCrome - RRM

+5.30↑

+0.42↑

+1.25↑

+12.44↑

+7.12↑

+2.00↑

-0.93↓

-3.32↓

+10.92↑

+1.35↑

Table 5 | Mean Accuracy and Standard Deviation across 3 different training runs of Gemma-2-9B-IT based Reward

Models in both PairPM and Bradley-Terry Reward Model settings. Results on RewardBench.

Remark 2. Note that main paper Table 2 has mean of the three training runs considered in these variance

experiments. For Gemma-2-2B and Qwen2.5-7B based reward models we only run single training runs.

C.2. Variance in Performance on reWordBench

For reWordBench, we plot mean performance numbers and error bars showing std. deviation in Figures

12 and 13. Here we depict mean accuracies with error bars representing standard deviations. Across

most transformations, the error bars are relatively small, particularly for the average performance over

all transformations. The observed improvements of Crome compared to RRM and Vanilla RM are

substantial and typically exceed multiple standard deviations of the respective models, suggesting that

these gains are statistically significant.

24

## Page 25

Robust Reward Modeling via Causal Rubrics

Average

jb1

jb4

swap_format

rot_13

rot_2

tw_handle

punct_space

paraphrase

tw_url

jb3

stresstest

add_quotes

back_trans

back_transc

ignore_above

homoglyph

jb2

ignore_below

char_swap_etc

append_code

code_minify

cmt_bad_good

cmt_bad_bad

0

20

40

60

80

100

Score (%)

## Rm

## Rrm

Ours

Figure 12 | Standard deviation error-bars for absolute robustness comparison of RM, RRM and Crome in the

Bradley-Terry setup, for reward models built over Gemma-2-9B-IT. Mean values and std deviation plotted are

for 3 independent training runs.

Average

jb4

jb2

swap_format

jb3

punct_space

tw_handle

stresstest

rot_2

tw_url

rot_13

homoglyph

paraphrase

char_swap_etc

add_quotes

back_transc

back_trans

ignore_above

ignore_below

jb1

cmt_bad_bad

code_minify

cmt_bad_good

append_code

0

20

40

60

80

100

Score (%)

## Rm

## Rrm

Ours

Figure 13 | Standard deviation error-bars for absolute robustness comparison of RM, RRM and Crome in the

PairPM setup, for reward models built over Gemma-2-9B-IT. Mean values and std deviation plotted are for 3

independent training runs.

PairPM

Model

Ultrafeedback

## (Id)

reWordBench

Accuracy (OOD)

RewardBench Accuracy (OOD)

Chat

Chat-Hard

Safety

Reasoning

Avg

## Rm

74.55

59.97

97.90

63.64

77.48

85.88

81.22

## Rrm

75.20

64.68

97.12

71.05

74.70

87.27

82.54

Ours

74.02

72.71

97.54

72.30

87.14

94.39

87.84

Bradley Terry

Model

Ultrafeedback

## (Id)

reWordBench

Accuracy (OOD)

RewardBench Accuracy (OOD)

Chat

Chat-Hard

Safety

Reasoning

Avg

## Rm

74.60

61.48

97.26

58.85

69.30

91.17

79.14

## Rrm

74.75

65.69

97.21

69.15

73.13

94.35

83.46

Ours

74.00

69.81

96.28

65.83

84.05

95.70

85.46

Table 6 | Comparison of In-Distribution (UltraFeedback-Val) and Out-of-Distribution (RewardBench, reWordBench)

Accuracy (%) for Gemma-2-9B-IT RMs

C.3. Effective Robustness of Crome and Baselines

We evaluate the generalization capabilities of the trained reward models by comparing their performance

on in-distribution (ID) data (UltraFeedback validation split) against out-of-distribution (OOD) benchmarks

25

## Page 26

Robust Reward Modeling via Causal Rubrics

(RewardBench, reWordBench). Table 6 presents these results for models based on Gemma-2-9B-IT.

Crome demonstrates strong OOD performance, particularly on reWordBench. For instance, in the PairPM

setup, Crome achieves the highest reWordBench accuracy (72.71%), while having similar ID accuracy,

suggesting that its learned robustness translates well to challenging, unseen transformations. Similarly,

for Bradley Terry models, Crome shows the best reWordBench accuracy (69.81%) and similar ID

accuracies compared to baselines. Overall, these results indicate that Crome’s augmentations effectively

teach more generalizable representations of preferences.

C.4. Extended Results on Safety Prompts from WildGuardTest

To complement the Best-of-N (BoN) safety results in Figure 7 (Sec. 6.2), we provide the complete Attack

Success Rate (ASR) on harmful prompts and Refusal to Answer (RTA) on benign prompts in Table 7. We

note that lower numbers are better for both ASR as well as RTA. Significantly, the results indicate that

without too much regression on RTA (< 0.5% decrease), we show consistent gains in ASR (%) numbers

and these gains increase as N becomes larger. For instance, at N=32, Crome reduces ASR to 39.39%,

compared to 42.11% for RM and 41.70% for RRM. In practice, reward models are used to detect jailbreak

attacks, and hence our model performance indicates a favorable trade-off as the reward model detects

harmful content (resisting jail-break attempts) while maintaining utility (low refusal-to-answer rate).

## Rm

## Rrm

Ours

## N

## Asr (%)

## Rta (%)

## Asr (%)

## Rta (%)

## Asr (%)

## Rta (%)

2

32.76

7.39

32.47

7.39

32.18

7.58

4

36.13

6.97

35.88

7.18

34.63

7.46

8

38.49

6.29

38.24

6.10

36.42

6.97

16

39.33

6.27

39.33

5.89

36.71

6.39

32

42.11

5.80

41.70

6.30

39.39

6.01

Table 7 | Comparison of Attack Success Rate (ASR) on harmful prompts and Refusal to Answer (RTA) on benign

prompts for Crome compared to baselines (RM, RRM) in the Best-of-N setup for varying N. Lower values are

considered better for both metrics.

C.5. Additional Results on reWordBench

We provide additional results on reWordBench in this section. See Figures 14 to 18 for reWordBench

results on various base models over which we build our Reward Models, such as Gemma-2-9B-IT,

Gemma-2-2B and Qwen2.5-7B, across Bradley-Terry and pairwise-preference Reward Models.

D. reWordBench Reproduction

The primary motivation reWordBench is the observation that contemporary reward models—key compo-

nents of RLHF systems—often latch onto superficial formatting cues or benign artifacts in their training

data, leading to dramatic drops in pairwise-preference accuracy under minor, semantically neutral edits.

To diagnose and quantify this brittleness in a systematic way, Wu et al. (2025) introduce reWordBench,

a new benchmark built by applying 28 carefully designed, meaning-preserving transformations to the

26

## Page 27

Robust Reward Modeling via Causal Rubrics

Average

jb4

swap_format

jb1

ignore_above

rot_13

punct_space

paraphrase

rot_2

tw_url

tw_handle

stresstest

char_swap_etc

cmt_bad_good

back_transc

homoglyph

add_quotes

back_trans

ignore_below

cmt_bad_bad

code_minify

jb2

append_code

jb3

0

20

40

60

80

100

Score (%)

+5.4%

+21.0%

+18.5%

+12.8%

+9.8%

+9.4%

+8.7% +8.2%

+7.9%

+7.9% +7.9% +6.7%

+6.0%

+5.8%

+5.1%

+3.8%

+3.3%

+2.2%

+0.6%

-0.9%

-2.1%

-2.4%

-3.7%

-12.7%

## Rm

## Rrm

Ours

Figure 14 | Absolute Robustness Comparison of RM, RRM and Crome in the Bradley-Terry RM setup, for reward

models built over Gemma-2-2B-IT.

Average

swap_format

jb2

jb1

jb3

jb4

tw_url

stresstest

tw_handle

paraphrase

punct_space

back_transc

add_quotes

char_swap_etc

rot_2

back_trans

ignore_above

rot_13

homoglyph

cmt_bad_bad

code_minify

ignore_below

append_code

cmt_bad_good

0

20

40

60

80

100

Score (%)

+5.0%

+13.7%

+11.4%

+11.2%+10.5%

+9.7%

+8.5% +7.9% +7.2% +6.9% +6.5%

+6.5%

+6.3%

+4.3% +3.6%

+3.3%

+2.7%

+2.4% +2.4%

+0.6%

-0.6%

-1.5%

-2.1%

-5.8%

## Rm

## Rrm

Ours

Figure 15 | Absolute Robustness Comparison of RM, RRM and Crome in the PairPM setup, for reward models

built over Gemma-2-2B-IT.

Average

jb4

append_code

swap_format

ignore_above

char_swap_etc

stresstest

paraphrase

rot_13

tw_handle

punct_space

ignore_below

tw_url

rot_2

back_transc

add_quotes

homoglyph

back_trans

code_minify

cmt_bad_bad

jb2

jb1

jb3

cmt_bad_good

0

20

40

60

80

100

Score (%)

+3.0%

+20.6%

+19.8%

+15.2%

+12.1%

+5.2%

+4.0% +3.8% +3.5%

+3.5% +3.5%

+3.4%

+2.1%

+2.0% +1.7%

+1.6%

+0.6%

+0.0%

-0.9%

-1.2%

-4.5%

-6.9%

-9.0%

-10.7%

## Rm

## Rrm

Ours

Figure 16 | Absolute Robustness Comparison of RM, RRM and Crome in the PairPM setup, for reward models

built over Qwen2.5-7B.

Average

jb1

swap_format

jb4

rot_13

rot_2

tw_handle

punct_space

paraphrase

tw_url

stresstest

add_quotes

jb3

back_trans

back_transc

append_code

code_minify

homoglyph

ignore_below

ignore_above

cmt_bad_good

char_swap_etc

jb2

cmt_bad_bad

0

20

40

60

80

100

Score (%)

+5.2%

+26.1%

+17.5%

+17.5%

+15.4%

+10.7%

+7.4% +7.0%

+5.6% +5.0% +4.0% +3.1%

+2.7%

+2.4%

+1.8%

+0.6%

+0.0%

-1.2%

-1.5%

-1.8%

-2.4%

-4.2%

-7.1%

-18.9%

## Rm

## Rrm

Ours

Figure 17 | Absolute Robustness Comparison of RM, RRM and Crome in the Bradley-Terry RM setup, for reward

models built over Gemma-2-9B-IT.

27

## Page 28

Robust Reward Modeling via Causal Rubrics

Average

ignore_above

swap_format

jb1

jb4

jb3

cmt_bad_good

tw_url

jb2

ignore_below

rot_13

rot_2

add_quotes

stresstest

paraphrase

tw_handle

punct_space

char_swap_etc

append_code

back_trans

back_transc

homoglyph

cmt_bad_bad

code_minify

0

20

40

60

80

100

Score (%)

+8.4%

+27.5%

+24.6%

+16.5%

+15.5%

+13.8%

+10.1%

+9.2% +8.8%

+8.6%

+8.4% +7.5%

+6.8%

+6.1% +6.1% +6.0% +5.8%

+5.2%

+5.2%

+3.2% +2.0%

+0.4%

-1.8%

-3.0%

## Rm

## Rrm

Ours

Figure 18 | Absolute Robustness Comparison of RM, RRM and Crome in the Bradley-Terry RM setup, for reward

models built over Qwen2.5-7B.

original RewardBench instances. The authors organize these edits into three overarching families each

targeting different potential failure modes of reward models. Together, transformations systematically

stress-test reward models’ invariance to innocuous changes, revealing large accuracy drops even under

minor edits and motivating the need for robust-training methods.

Since the original dataset is not publicly available, on author’s suggestion we reconstructed the data

independently following the instructions in the original paper. Paraphrasing and back-translation trans-

formations are generated using foundation models or translation tools for which we use OpenAI API,

specifically the "gpt-4o-2024-08-06" model. For generating back-transcription transformations we use the

"gpt-4o-transcribe" and "gpt-4o-mini-tts" models available on the OpenAI API. Here are some details of

the transformations in reWordBench:

1. Controlled Transformations: These are template-based edits that guarantee semantic equivalence

by construction. They include:

a. Add Quotes: Surrounding the entire prompt and responses with a fixed number of quotation

marks.

b. Punctuation Spaces: Inserting spaces around each punctuation mark.

c. Twitter Handle/URL: Appending a randomly generated (harmless) Twitter handle or URL to

the text.

d. StressTest: Repeating semantically vacuous conjunctions (e.g. “and true is true” or “and false is

not true”) to the end of the text.

e. Ignore Above/Below: Injecting the response before or after the prompt with an explicit instruction

to ignore it.

f. Rot-N Encoding: Applying simple character-shift ciphers (Rot-13 or Rot-2) to the prompt text

while leaving responses in plain form.

28

## Page 29

Robust Reward Modeling via Causal Rubrics

2. Naturalistic Transformations: These simulate the kinds of noise and variation that occur “in the

wild” and may not perfectly preserve meaning, but reflect realistic robustness challenges:

a. Paraphrase: Rewriting prompt and response via a strong LLM (Llama-3-70B-instruct) under a

paraphrasing instruction.

b. Back-translation: Translating English →Spanish →English for several rounds using OPUS-MT,

accepting only those with high semantic similarity.

c. Back-transcription: Converting text to audio and back using a TTS model (fairseq S2) and an

ASR model (Whisper-base).

d. Homoglyph Substitution: Replacing Latin characters with visually identical Unicode glyphs (e.g.

Cyrillic “e” for Latin “e”).

e. Character-level Edits: Randomly swapping, inserting, deleting, or substituting characters at

rates reflecting real-world typos (including QWERTY-adjacent substitutions).

f. Word Deletion: Omitting a randomly chosen word from prompt and response, subject to a

similarity filter.

3. Domain-Targeted Transformations: These focus on specialized subsets of RewardBench—code,

mathematics, and safety prompts—where specific artifacts may bias reward models:

a. Code Minification: Automatically renaming variables, removing whitespace, and otherwise

“minifying” Python snippets without changing functionality.

b. Add Comment: Inserting “# bad” annotations after each line of chosen responses (and optionally

“# good” after rejected ones).

c. Append Other Code: Concatenating the losing snippet after the winning one (and vice versa),

taking advantage of Python’s return-ended semantics.

d. Swap Format: Exchanging the usual answer formats (e.g. LaTeX vs. markdown “# Answer”) in

arithmetic problems.

e. Jailbreak Prompts: Prepending known “jailbreak” instructions (from the ChatGPT-Jailbreak-

Prompts dataset) to safety-critical queries to see if the RM prefers harmful completions.

29

## Page 30

Robust Reward Modeling via Causal Rubrics

E. Experimental Setup Details

This appendix provides supplementary details to the experimental settings outlined in Section 6.1 of the

main paper.

E.1. Best-of-N Experimental Methodology

Algorithm 1 Best-of-𝑁Selection with Pairwise Preference Model

1: Input: Query 𝑄; responses A = (𝐴1, . . . , 𝐴𝑁) with 𝑁≥1

2: Input: Pairwise model ˆR𝜃: (𝑄, 𝐴𝑖, 𝐴𝑗) →{1, 2}

⊲The output {1, 2} from the Pairwise preference model indicates if the first answer is better or the

second, given the query.

3: Output: Selected best response 𝐴best

4: 𝐴best ←𝐴1

5: for 𝑖←2 to 𝑁do

6:

𝐴cand ←𝐴𝑖

7:

if ˆR𝜃(𝑄, 𝐴best, 𝐴cand) = 2 then

8:

𝐴best ←𝐴cand

9:

end if

10: end for

11: return 𝐴best

For all our Best-of-N results using PairPM models, we follow a simple procedure to find the best response

out of 𝑁responses generated by a base LLM. In particular, PairPM models take responses 2 at a time,

and provide the better response for the given query. Given 𝑁response A = (𝐴1, . . . , 𝐴𝑁) with 𝑁≥1, in a

randomly shuffled order, we sequentially compare responses 2 at a time (starting from 𝐴1 and 𝐴2) using

the PairPM reward model and keep track of the best response. At each iteration, the best response is

compared to the next response in the list and the best response is updated. The best response after 𝑁−1

iterations is taken as the selected response. The algorithm for this procedure is given in Algorithm 1.

E.2. Experimental setting for Calculating Win Rates on RewardBench Prompts

To show the performance of Crome on general purpose datasets, we follow reWordBench (Wu et al.,

2025) and use all 2985 prompts from RewardBench (Lambert et al., 2024). We use Gemma-2-9B-IT as

the base model and sample N responses for each prompt in this set. Following this, we use the PairPM

reward models (RM, RRM and Crome) to select the best response among the N responses, as described

in supplementary Section E.1. We use GPT-4 as a judge to compare Crome’s responses with baselines

RM and RRM.

E.3. WildGuardTest and GSM8K experimental settings

For both WildGuardTest results (main paper Figure 7 as well as supplementary Table 7), as well as GSM8K

results (main paper Figure 8), we use Gemma-2-9B-IT as the base model and sample N responses from

it. Following this, we use the PairPM reward models (RM, RRM and Crome) to select the best response

30

## Page 31

Robust Reward Modeling via Causal Rubrics

among the N responses, as described in supplementary Section E.1. For WildGuarTest, for obtaining

results given the final responses, we use the WildGuard model Han et al. (2024) to obtain annotations

for prompt-harmfulness, response-harmfulness, response-refusal, is-parsing-error, as

described in the WildGuard repository3. Using these annotations, we obtain ASR and RTA for Crome

and baselines.

E.4. Datasets and Augmentation

For human preference data (Dpref) we use Ultrafeedback (Cui et al., 2023), which furnishes approxi-

mately 60,000 preference pairs across diverse domains.

The data augmentation process, central to Crome (Section 4), employs Gemini 2.0 Flash. This LLM is

first used to identify ℓ= 5 principal causal attributes relevant to response quality. Subsequently, Gemini

2.0 Flash generates (a) causal upgrade/degradation pairs targeting these attributes (Dcausal), and (b)

neutral pairs (Dneutral).

The raw augmented data, Daug, undergoes a filtering step. This involves applying a model-based

confidence filter, using a baseline RM (trained solely on Dpref) with a threshold of 𝜏= 0.2. This filtering

focuses the training on more informative examples. The amplification process involves initially generating

approximately 10x data from causal augmentations (5 attributes, 2 versions per original response) and

1x data from neutral augmentations, followed by verification and the confidence-based filtering. The

final training dataset D = Dpref ∪Daug_filtered typically contains about 3.5 times the number of examples

in the original Dpref, similar to RRM (Liu et al., 2024).

E.5. Models and Training

Reward Models (RMs):

We instantiate RMs using Qwen2.5-7B (Yang et al., 2024a) and Gemma-2-9B-IT,

Gemma-2-2B (Team et al., 2024) as base transformer architectures. Our RM variant, Crome-PairPM,

processes inputs formatted as ‘Q, A, B‘ and predicts a preference token (’A’ or ’B’) via a cross-entropy loss.

An alternative variant, Crome-BT, implements the Bradley-Terry model by deriving scalar scores for

each answer.

Policy Models:

For downstream alignment tasks, we use the Best-of-N setup where we generate N

responses using Gemma-2-9B-IT and use Crome as well as baseline reward models to select the best

candidate response.

Training Hyperparameters:

All models are trained in PyTorch with the Hugging Face Transformers

library. For RM training, following Liu et al. (2024), we use the AdamW optimizer (Loshchilov and

Hutter, 2017) for 1 epoch, with a learning rate of 1𝑒−6, a global batch size of 256, and a cosine learning

rate schedule. We use a warmup ratio of 0.03. For training all models, we use 8 NVIDIA A100 80GB

GPUs. RM training runs require time between 10-16 hours for 2B to 9B mdoels we consider.

3https://github.com/allenai/wildguard

31

## Page 32

Robust Reward Modeling via Causal Rubrics

E.6. Baselines and Evaluation

Baselines:

Our full Crome approach is compared against two primary baselines:

1. A Base RM, trained solely on the original Dpref.

2. The RRM Baseline (Liu et al., 2024), which employs a distinct augmentation strategy using non-

contextual examples and responses from different queries, not specifically aligned with identified

causal or spurious attributes.

Evaluation Benchmarks:

RM quality is assessed by accuracy on RewardBench (Lambert et al., 2024)

(overall and per category: Chat, Chat-Hard, Safety, Reasoning) and robustness on Re-word Bench (Wu

et al., 2025). BoN Policy performance is evaluated using RewardBench, WildGuardTest (Han et al., 2024),

GSM8K (Cobbe et al., 2021).

F. Causal Model and Augmentation Details

This appendix provides further details on the causal framework underpinning Crome and discusses

various data augmentation strategies in the context of robust reward modeling.

F.1. Elaboration on the Causal Model

The causal graph presented in Figure 2 (Section 3.2) models the generation of an answer A and the

formation of its attributes. The query Q influences the generator’s latent intent I. This intent, along with

unobserved generator-specific confounders U (e.g., inherent stylistic preferences, verbosity tendencies,

pre-existing biases), leads to the textual answer A. The answer A then manifests both causal attributes

C(A) (e.g., factuality, relevance) and spurious attributes SP(A) (e.g., length, specific formatting, politeness).

The true, idealized reward R∗is assumed to be a function only of Q and C(A).

The challenge in training a reward model ˆR𝜃arises because SP(A) can become correlated with R∗in the

training data. This correlation can occur if U influences both the choice of spurious features and the

aspects that contribute to causal quality, or simply because certain spurious features happen to co-occur

with preferred answers in Dpref. Without explicit guidance, ˆR𝜃may learn to rely on these spurious

correlations, leading to reward hacking. Crome’s data augmentation strategy aims to provide this

explicit guidance by generating new answer pairs that help ˆR𝜃disentangle C(A) from SP(A).

F.2. Crome’s Causal Augmentation: Attribute Isolation

Crome’s primary strategy for enhancing sensitivity to causal attributes involves Attribute Upgrada-

tion/Degradation. This generates pairs (˜A(𝐶𝑗←upgraded/degraded), A) or (A, ˜A(𝐶𝑗←upgraded/degraded)) by prompt-

ing an LLM to modify an original answer A (from Dpref) along a single causal attribute 𝐶𝑗while attempting

to keep other attributes constant. This provides a targeted signal about the marginal contribution of 𝐶𝑗.

F.2.1. Comparison with Relevance Contrast Augmentation

An alternative strategy, Relevance Contrast Augmentation (used in RRM-style approaches (Liu et al., 2024),

termed “non-contextuals” therein), involves pairing a relevant answer A1 (for query Q) with an irrelevant

32

## Page 33

Robust Reward Modeling via Causal Rubrics

answer B2 (e.g., an answer to a different query, so 𝐶(𝐵2 | Q) ≈0), labeled A1 ≻B2.

While Relevance Contrast establishes a baseline understanding of relevance, Crome’s Attribute Isolation

offers:

• Specificity and Nuance: It directly teaches about individual causal attributes (𝐶𝑗), enabling the

RM to learn a compositional understanding of quality and distinguish between relevant answers

differing subtly in one dimension.

• Data Efficiency for Complex Attributes: Focusing changes along one attribute creates diverse,

targeted examples for each quality facet.

Crome’s attribute-specific counterfactuals thus provide a richer, more disentangled signal than broad

relevance contrasts alone.

F.3. Neutral Augmentation Strategies

Neutral augmentations aim to make the reward model invariant to spurious attributes when causal

content is held constant or is irrelevant.

F.3.1. Common Spurious Perturbation Methods (Not a primary Crome strategy)

Several methods focus on general spurious perturbations:

1. Direct Spurious Feature Perturbation (e.g., Paraphrasing, Formatting Changes):

This involves

taking an answer A and generating ˜A(𝑆𝑃←𝑠𝑝′) by applying meaning-preserving transformations (e.g.,

paraphrasing) intended to alter only SP(A) while preserving C(A). The pair (A, ˜A(𝑆𝑃←𝑠𝑝′)) is labeled as a

tie. This is central to benchmarks like reWordBench (Wu et al., 2025).

2. Rewrites of Rewrites (e.g., RATE (Reber et al., 2024)):

RATE uses sequential rewrites for robust

causal effect estimation. Adapted for augmentation, multiple causally-equivalent rewrites of an answer

could form neutral pairs.

Challenges with these General Methods:

• Unknown/Unspecified Spurious Features: It’s hard to a priori identify and target all spurious

features an RM might exploit.

• Preserving Causal Content: Ensuring "spurious" perturbations don’t inadvertently alter causal

meaning is difficult.

F.3.2. Neutral Augmentation Strategies developed in this work

We developed the following two strategies for neutral augmentation.

1. Irrelevant Query Neutrals (IQN):

Crome generates these neutral pairs efficiently by leveraging

its existing pool of answers (original or causally augmented). Given two answers, B1 and B2, that were

33

## Page 34

Robust Reward Modeling via Causal Rubrics

generated or selected for a specific query Qorig, Crome creates a neutral pair by associating them with a

new, unrelated query Qirrelevant. For this Qirrelevant, both B1 and B2 are now contextually irrelevant; their

causal attribute scores 𝐶(𝐵1|Qirrelevant) and 𝐶(𝐵2|Qirrelevant) are effectively zero (or very low). Despite

potentially different spurious attributes 𝑆𝑃(𝐵1) and 𝑆𝑃(𝐵2), the pair (B1, B2) is presented to the reward

model with query Qirrelevant and labeled as a tie. This teaches the RM that when answers are equally and

maximally irrelevant to the current query, their differing spurious features should not induce a preference.

2. Causally-Aligned Neutrals (CAN):

This method directly leverages the original preference pairs or

the outputs of causal augmentation.

• Given an original preference pair from Dpref, say (A1, A2) where A1 ≻A2, we generate ˜A(𝐶←𝐶(𝐴1))

2

by rewriting A2 to match the causal attribute profile of A1, while instructing the LLM to retain the

spurious characteristics 𝑆𝑃(𝐴2) of the original A2. The pair (A1, ˜A(𝐶←𝐶(𝐴1))

2

) is then labeled as a tie. A

symmetric pair can also be generated.

• Similarly, if we have an answer A and its causally degraded version ˜A(𝐶𝑗←degraded) (from Dcausal), we

can attempt to reconstruct the degraded version by prompting an LLM to restore 𝐶𝑗to its state in

A, while aiming to preserve the spurious features of ˜A(𝐶𝑗←degraded). If successful, this reconstructed

version, ˜A′

reconstr, would form a neutral pair (A, ˜A′

reconstr) labeled as a tie.

The core idea is to teach invariance to the spurious differences that remain after causal attributes have

been aligned or restored. Moreover, applying CAN to counterfactually generated data from Dcausal helps

mitigate imperfections in oracle rewrites—an issue highlighted in the RATE paper (Reber et al., 2024),

which notes that LLM edits often unintentionally modify "off-target attributes" (e.g., introducing formality,

removing HTML tags). CAN thereby enhances robustness on two fronts: (1) disentangling spurious

correlations in original data, and (2) neutralizing new biases introduced during causal augmentation.

This helps in enhancing model’s robustness against confounding signals in the data. While this method

is sound theoretically, we qualitatively find that the approximation of 𝐶(𝐴𝑤) by 𝐶( ˜𝐴𝑙) is not perfect.

Furthermore, some spurious attributes 𝑆𝑃′( ˜𝐴𝑙) ⊂𝑆𝑃( ˜𝐴𝑙) vary when we move causal attributes. Invariance

to these attributes 𝑆𝑃′( ˜𝐴𝑙) is not captured by CAN. For these reasons, we encourage future work for

improving this neutral augmentation strategy.

G. Detailed Mechanistic View of Augmentation Strategies

This appendix section provides a more granular, node-based representation (Figure 19) to elaborate on

the hypothesized attribute interactions and the counterfactual generation process. This detailed view

aims to offer a causal understanding that complements the main paper.

Figure 19 aims to provide a deeper, causal understanding of the causal perturbation process through

which we obtain our causal upgradations and degradations. We term the spurious attributes that move

when causal attributes are intervened upon as 𝑆𝑃2(𝐴) ⊂𝑆𝑃(𝐴) for any answer 𝐴.

Part 1: Causal Augmentation (Attribute Upgradation/Degradation).

We first generate a counterfac-

tual Answer 2 from an original Answer 1 (for query Q) via an LLM-driven "Counterfactual Generation

Process." This process intervenes to modify a specific causal attribute 𝐶𝑗within Answer 1’s causal profile

34

## Page 35

Robust Reward Modeling via Causal Rubrics

Attribute Upgradation  and Degradation

Input: A1 from Dpref                   Goals: Obtain A2 ~ P(A1{Cj(A1 | Q) ← c}

Answer 1

Answer 2 =

Upgraded Answer 1 on

a single aŞribute cj

## Sp1(A1)

## Sp2(A1)

## C(A1)

## Sp1(A2)

## (= Sp1(A1))

## Sp2(A2)

≠

## Sp2(A1)

## C(A2) =

C(A1)cj <- c

A small part of spurious

aŞributes

SP2 ⊂ SP change upon

changing the Causal C.

Counterfactual

Generation Process

LLM proposes A2 and

veriﬁes that the j’th

causal aŞribute

Cj(A2)= c

Training Objective: P(A1 > A2) = 0 for upgradation P(A1 > A2) = 1  for degradation

Subset of spurious

attributes are dependent

on causal attributes

Figure 19 | Detailed mechanistic diagram of Crome’s Causal Attribute Upgradation and Degradation, illustrating

attribute components and transformations. This causal diagram indicates that on changing causals some spurious

features also can get dragged along (we call these 𝑆𝑃2). Hence separating these is very hard. This illustrates the

need for a neutral augmentation strategy that provides invariance to 𝑆𝑃2 attributes.

C(A1) to a target state 𝐶′, resulting in C(A2). We aim to keep spurious attributes fixed by asking for

a minimal perturbation. Therefore attributes SP1(A1) are ideally preserved. Yet, SP2(A1) (which may

co-vary with C(A1)) might transition to SP2(A2) ≠SP2(A1). The goals of this transformation are to ensure

𝐴2 reflects the intended causal change. The RM is then trained on the pair (𝐴1, 𝐴2) with a preference

label reflecting the upgrade/degradation, teaching sensitivity to isolated causal attribute modifications.

Part 2: Neutral Augmentation (via Irrelevant Query).

As illustrated in Figure 19, we need spurious

invariance to 𝑆𝑃2 which are hard to disentangle as well. This illustrates the need for an intervention free

method for neutral augmentation like IQN. When we present an answer pair (𝐴1, 𝐴2) from Dpref ∪Dcausal,

re-contextualized with a new, unrelated query Qirrelevant, we teach the model invariance to (𝑆𝑃1, 𝑆𝑃2). This

is because, the primary differences between 𝐴1 and 𝐴2 in this new context are their spurious attributes

(SP1,SP2). Note that the causal difference between 𝐴1 and 𝐴2 in Dpref ∪Dcausal in presence of irrelevant

query is now spurious, and hence there need not be any sensitivity to it.

H. Detailed Crome Methodology

This appendix provides the detailed implementation steps for the Crome framework introduced in

Section 4, covering attribute identification, counterfactual data generation, filtering, and the specific

training objective.

35

## Page 36

Robust Reward Modeling via Causal Rubrics

H.1. Step 1: Attribute Identification

The foundation involves identifying the attributes that genuinely determine answer quality versus those

merely correlated with it, as defined in Section 3.2. For a query Q and example answers (y𝑤, y𝑙) from Dpref,

we define: Causal attributes C = (C1, . . . , Cℓ) (e.g., factuality) and Spurious attributes SP = (SP1, . . . , SP𝑘)

(e.g., verbosity).

Automated Attribute Extraction.

We employ an LLM prompted with Q and example responses (see

Appendix K for prompt). The primary output is the set of attributes C.

Refinement and Verification.

The LLM-generated list C is reviewed for coherence and consistency in

this verification phase. The verification prompts are provided in Appendix K.

H.2. Step 2: Generating Counterfactual Augmented Data

Using identified attributes C, we generate Daug via LLM-approximated counterfactuals (Section 3.3).

Causal Augmentation (Dcausal).

Pairs (A, A′) are generated to differ primarily along a single causal

attribute C𝑗. We use LLM prompts (Appendix K) for upgradation (generating an improved A′ from a

ground-truth rejected answer A) and degradation (generating a degraded A′ from a ground-truth selected

answer A), aiming to keep other attributes constant. Pairs are labeled ≻accordingly.

Neutral Augmentation (Dneutral).

Notice that when we causally augment an answer in Dcausal, we

might in-advertantly move spurious correlates (as illustrated in Figure 3). Furthermore, even in our

dataset, there could be a systematic effect where spurious attributes highly correlate with the better (or

worse) answer. In such cases, we need to create a dataset of equivalent pairs, with a tie label to teach the

model invariance to spurious correlates.

Our primary technique is irrelevant query neutrals (IQN). Here, the idea is that given a new query, the

causal attribute C becomes irrelevant. Essentially, for the new query, the causal attributes are spurious.

Hence, by taking any two answers for a given query, and labeling them a tie, given an irrelevant query,

the reward model learns invariance to these features. For example, if the reward model has spuriously

learnt that bullet points in an answer should be rewarded, our tie labels teach them that bullet points

should be rewarded only if the content of the answer is relevant to the query. Similarly, by creating such

pairs with our own causally augmented data in Dcausal, we teach the model invariance to the spurious

pairs that move when the causal attributes (CA) are perturbed.

H.3. Step 3: Filtering Augmented Data

The raw Daug is then filtered to Daug_filtered.

Model-based Confidence Filtering.

Using a baseline ˆRbase, we calculate 𝑝= Pbase(B ≻A) for each

augmented pair (A, B) with target label 𝑦. We retain the pair only if |𝑝−𝕀(𝑦= B ≻A) −0.5·𝕀(𝑦= tie)| > 𝜏.

36

## Page 37

Robust Reward Modeling via Causal Rubrics

We use threshold 𝜏= 0.2, focusing training on examples where the baseline is uncertain or incorrect (Liu

et al., 2024).

Quality Verification.

Further checks (e.g., automated fluency scoring) verify pair validity. The result is

Daug_filtered.

H.4. Step 4: Training the Robust Reward Model

The final model ˆR𝜃is trained on D = Dpref ∪Daug_filtered by minimizing the composite loss:

L(𝜃) = −

∑︁

(Q,y𝑤,y𝑙)∈Dpref∪Dcausal

log 𝜎(ˆR𝜃(Q, y𝑤) −ˆR𝜃(Q, y𝑙))

−𝜆

∑︁

(Q,A1,A2,𝑦=tie)∈Dneutral

Ltie(𝜃; Q, A1, A2)

(3)

where Ltie is defined as in Eq. 2. The hyperparameter 𝜆≥0 weights the neutral tie loss and is tuned on

a validation set (Section 6).

I. Theoretical Analysis

In this section, we provide a formal justification for why the Crome training framework, specifically the

composite loss function operating on causally augmented data, mitigates spurious reward hacking. We

demonstrate that the optimization objective inherently discourages the reward model from relying on

spurious correlations, guiding it towards the true causal drivers of quality.

I.1. Formal Setup

We adopt the notation and causal framework established in Section 3. Our analysis considers a query

Q, an answer A with corresponding Principal Causal Components C(A) and spurious attributes SP(A).

The idealized ground-truth reward is R∗(Q, A) = 𝑓∗(Q, C(A)), and the learned reward model is denoted

ˆR𝜃(Q, A). The model parameters 𝜃are optimized by minimizing the composite loss function L(𝜃) =

Lpref(𝜃) + 𝜆Ltie(𝜃) (Eq. 2) over the training dataset D = Dpref ∪Daug_filtered, which combines original

preferences Dpref with filtered causal Dcausal and neutral Dneutral augmentations. For theoretical analysis,

Lpref and Ltie represent expectations over the respective data distributions:

Lpref(𝜃) = −𝔼(Q,y𝑤,y𝑙)∼Dpref∪Dcausal

h

log 𝜎(ˆR𝜃(Q, y𝑤) −ˆR𝜃(Q, y𝑙))

i

Ltie(𝜃) = −𝔼(Q,A1,A2,𝑦=tie)∼Dneutral



−1

2 (log 𝜎(Δ12) + log 𝜎(−Δ12))



where Δ12 = ˆR𝜃(Q, A1) −ˆR𝜃(Q, A2).

I.2. Justification under the Boolean variable causal model for attributes

Assumption 1. Assume that:

37

## Page 38

Robust Reward Modeling via Causal Rubrics

1. Causal attributes {𝐶𝑖(𝑄, 𝐴)}𝑘

𝑖=1 and spurious attributes {𝑆𝑗(𝐴)}ℓ

𝑗=1 are all boolean variables taking

values in {+1, −1}

2. All spurious variables are non-descendants of all causal variables.

3. Reward function is trying to fit a quadratic polynomial in causal and spurious attributes, i.e.

## ˆ𝑅=

∑︁

𝑖

𝛼𝑖𝐶𝑖(𝑄, 𝐴) +

∑︁

𝑗

𝛽𝑗𝑆𝑗(𝐴) +

∑︁

𝑖≠𝑖′

𝛼𝑖,𝑖′𝐶𝑖(𝑄, 𝐴)𝐶𝑖′(𝑄, 𝐴)+

∑︁

𝑗≠𝑗′

𝛽𝑗,𝑗′𝑆𝑗(𝐴)𝑆𝑗′(𝐴) +

∑︁

𝑖≠𝑗

𝛾𝑖,𝑗𝐶𝑖(𝑄, 𝐴)𝑆𝑗(𝐴).

(4)

4. Assume that the true reward function is a sparse quadratic polynomial depend on only the causal

attributes.

## 𝑅∗=

∑︁

𝑖

𝜃𝑖𝐶𝑖(𝑄, 𝐴) +

∑︁

𝑖≠𝑖′

𝜃𝑖,𝑖′𝐶𝑖(𝑄, 𝐴)𝐶𝑖′(𝑄, 𝐴)

(5)

Here, ∥𝜃∥0 ≤𝑠<< 𝑘2 and 𝜃𝑖and 𝜃𝑖,𝑖′ variables form the vector 𝜃. All other coefficients for other

features that involves the spurious variables are set to 0 in 𝜃. Let I be the support set of the true

coefficient.

From the reward modeling objective, we try to fit a model Δ(ˆ𝑅) to a target which is the difference

between true rewards to two answers 𝐴1 and 𝐴2 for the same question, i.e. 𝑅∗(𝑄, 𝐴1) −𝑅∗(𝑄, 𝐴2). From

the assumption in 4, this is equivalent to fitting a linear model with coefficients 𝛼𝑖, 𝛼𝑖,𝑖′, 𝛽𝑗, 𝛽𝑗,𝑗′, 𝛾𝑖,𝑗and

differences in features (across the two answers), i.e. 𝐶𝑖(𝑄, 𝐴1) −𝐶𝑖(𝑄, 𝐴2), 𝑆𝑗(𝐴1) −𝑆𝑗(𝐴2), 𝑆𝑗(𝐴1)𝑆𝑗′(𝐴1) −

𝑆𝑗(𝐴2)𝑆𝑗′(𝐴2), 𝐶𝑖(𝑄, 𝐴1)𝐶𝑖′(𝑄, 𝐴1) −𝐶𝑖(𝑄, 𝐴2)𝐶𝑖′(𝑄, 𝐴2), 𝐶𝑖(𝑄, 𝐴1)𝑆𝑗(𝐴1) −𝐶𝑖′(𝑄, 𝐴2)𝑆𝑗(𝐴2) respectively. To

simplify notation, we drop the reference to 𝐴1, 𝐴2 and 𝑄and call 𝐶𝑖(𝑄, 𝐴1)−𝐶𝑖(𝑄, 𝐴2) as Δ𝐶𝑖. Similarly, we

use Δ𝑆𝑗, Δ𝐶𝑖,𝑖′, Δ𝑆𝑗,𝑗′ and Δ(𝐶𝑖𝑆𝑗). The dependence of these features on the 𝐴1, 𝐴2 and 𝑄are understood.

Let 𝐹𝑞,𝑎1,𝑎2 ∈{+1, −1}𝑘+ℓ+𝑘ℓ+(𝑘

2)+(ℓ

2) be the boolean vector with features

{Δ𝐶𝑖}, {Δ𝑆𝑗}, {Δ𝐶𝑖,𝑖′}, {Δ𝑆𝑗,𝑗′}, {Δ(𝐶𝑖𝑆𝑗)} stacked row wise for the triplet 𝑞, 𝑎1, 𝑎2.

Consider two types of triplets, one drawn from the natural distribution of the preference training dataset

𝐷pref and the others drawn from augmented distribution 𝐷aug. Let us assume for the sake of the theoretical

results to follow, that we upgrade/degrade answer 𝑎2 to 𝑎𝑎𝑢𝑔

1

by changing only one causal factor at a time

while all the other causal factors are fixed to their factual version and all things remaining the same to form

𝐷aug. The degradation aspect only serves to reinforce the phenomenon we seek to show formally below.

Assumption 2. (Model for Counterfactual Generation)

We assume that:

1. 𝑎𝑎𝑢𝑔

1

is formed by generating 𝐶𝑖(𝑄, 𝐴) and 𝑆𝑗(𝐴) following an counterfactual generation where the

following set of intervention is made 𝐶𝑖(𝑄, 𝐴) ←¬𝐶𝑖(𝑄, 𝐴), 𝐶𝑗(𝑄, 𝐴) ←𝐶𝑗(𝑄, 𝐴), ∀𝑗≠𝑖which

propagates to potential descendants of variable 𝐶𝑖and not affecting 𝑆𝑗(due to no 𝑆𝑗being a

descendant of 𝐶𝑗) with all other factors remaining as in answer 𝑎2.

2. Let us assume that we have 𝑚augmentations where a triplet is randomly sampled from the training

preference data distribution Dpref and then augmented using the above counterfactual with a

randomly chosen causal attribute negated.

38

## Page 39

Robust Reward Modeling via Causal Rubrics

Remark There are the main assumptions - 1) 𝑆𝑗being a non-descendant of 𝐶𝑖, 2) Reward model is a

quadratic sparse boolean model (The treatment could be extended to boolean polynomials of higher

degree too with lot more algebraic technical work).

Theorem 2. Let the feature matrix of the counterfactually augmented triplets, that is formed by stacking

feature vectors 𝐹𝑞,𝑎aug

1

,𝑎2 row wise, be denoted F. Consider the following ℓ1 constrained regression problem:

ˆ𝜃= arg min

b ∥b∥1 𝑠.𝑡.F𝑏= Δ𝑅∗

(6)

Here, Δ𝑅∗is vector of the difference in the true reward between the reward applied to the augmented

answer and the non-augmented one across augmented triplets. Let N be the top 𝑐2𝑘non zero entries of

vector a by magnitude. Then, we have:

∥Δ𝜃∥2 = ∥𝜃−ˆ𝜃∥2 ≤𝑐3∥𝜃I−N∥1



4

𝑘+

√︃

8 log(𝑘+ℓ)

𝑚



w.h.p.

Remark: If the true sparsity 𝑠< 𝑐2𝑘, then it ensures perfect recovery since I −N = ∅. Since 𝑠< 𝑘2, and

if every coefficient is 𝑂(1), the bound becomes 𝑂(𝑘) which is independent of the spurious dimension.

Proof. Under the model assumptions 1 and assumptions on counterfactual generation 2, we seek to show

that F when restricted to feature set Δ𝐶𝑖, Δ𝐶𝑖,𝑖′, Δ𝐶𝑖𝑆𝑗has smaller incoherence (by multiplicative factor of 𝑘)

than an feature matrix made of i.i.d triplets sampled from the preference distribution. This accommodates

recovering the 𝑠= 𝑂(𝑘) sparse solutions exactly and in the general case, the error in coefficient estimation

is 𝑂(𝑘) independent of spurious dimension ℓ.

First, we show that features Δ(𝑆𝑗,𝑗′) = 0, Δ(𝑆𝑗) = 0 for the augmented triplets. This is because all 𝑆𝑗

variables are ancestors to 𝐶𝑖variables. Therefore, a counterfactual intervention on the answer 𝑎2 leaves

the two spurious attribute sets (for the original and its counterfactual) unchanged.

Intervention fixed all causal variables to the factual ones (but fixed through intervention) and intervenes

on variable to change. There are many types of correlation between non zero features because of this.

We consider them one by one:

1) Δ𝐶𝑖= 0 if is 𝐶𝑖is not intervened. This occurs with probability 1 −1/𝑘. 2) Δ𝐶𝑖Δ𝐶𝑗= 0 with probability

1 −2/𝑘. 3) Δ𝐶𝑖,𝑖′Δ𝐶𝑗,𝑗′ = 0 if all 𝑖, 𝑖′, 𝑗, 𝑗′ are distinct indices. 4) Δ𝐶𝑖,𝑗Δ𝐶𝑗,𝑘= 0, with probability 1 −1/𝑘.

5) Δ𝐶𝑖,𝑗Δ𝐶𝑖𝑆𝑗= 0 with probability 1 −1/𝑘. 6) Δ𝐶𝑖,𝑖′Δ𝐶𝑗𝑆𝑘= 0 always if all four indices not equal. 7)

Δ𝐶𝑖Δ𝐶𝑗𝑆𝑘= 0 always. 8) Δ𝐶𝑖Δ𝐶𝑖𝑆𝑘= 0 with probability 1-1/k.

If any of the these products is non zero, conditioned on that event, they equal the correlation on the

preference training dataset (every correlation between features is bounded by at most 4).

Therefore, expected pairwise correlation amongst two features for a randomly chosen augmented triple

is at most 4/𝑘. Given every augmented triple is obtained by counterfactual generation applied to an i.i.d

sample from preference dataset, there is a deviation of at most 8 log(𝑘+ℓ)

√𝑚

with probability 1 −

1

(𝑘+𝑙)4 .

Therefore,

∥1

𝑚F𝑇F −I∥∞≤4

𝑘+ 8 log(𝑘+ ℓ)

√𝑚

𝑤.𝑝. 1 −(𝑘+ ℓ)−4

(7)

39

## Page 40

Robust Reward Modeling via Causal Rubrics

This means that the data matrix is incoherent with high probability. We now follow standard Lasso

analysis. Recall the cone condition Negahban et al. (2009): For a subset N of indices that have non zero

values in 𝜃, ∥Δ𝜃𝑁𝑐∥1 ≤∥Δ𝜃𝑁∥1 + 2∥𝜃𝑁𝑐∥1. This implies:

∥Δ𝜃∥1 ≤2

√︁

|𝑁|∥Δ𝜃∥2 + 2∥𝜃𝑁𝑐∥1.

(8)

We have the following chain:

0 = 1

𝑚∥𝐴Δ𝜃∥2 ≥∥Δ𝜃∥2

2 −∥Δ𝜃∥2

1∥1

𝑚F𝑇F −I∥∞

≥∥Δ𝜃∥2

2 −



8|N|∥Δ𝜃∥2

2 −8∥𝜃N𝑐∥2

1



∥1

𝑚F𝑇F −I∥∞

(9)

Let us set 𝑚such that

1

√𝑚< 8 log(𝑘+ℓ)

𝑘

. Let |𝑁| < 𝑘/80, 𝑐2 < 1/80 in the theorem. Substituting these

parameters in 9, we get: ∥Δ𝜃∥2 = ∥𝜃−ˆ𝜃∥2 ≤4

√

2∥𝜃I−N∥1



4

𝑘+

√︃

8 log(𝑘+ℓ)

𝑚



□

J. Qualitative Example Walkthrough

In this section, we show qualitative examples of augmented data created from original data using which

is used to train Crome.

User Prompt

Question: Can you provide a table showing two potential causes for global warming?

Chosen Response

Sure, here is a table showing two potential causes for global warming:

Potential Causes

Description

Greenhouse Gas Emissions

Release of gases such as carbon dioxide, methane, and nitrous

oxide from human activities like burning fossil fuels, deforesta-

tion, and agriculture. These gases trap heat in the atmosphere,

causing the planet to warm.

Land Use Changes

Alteration of the earth’s surface for human activities like ur-

banization, agriculture, and forestry can result in changes in

albedo, water cycling, and energy balance, leading to warming

effects. For example, when forests are cleared, the land reflects

less sunlight back into space, which leads to more warming.

It is important to note that global warming is a complex issue with multiple causes. The two potential

causes listed above are just a few of the many factors that contribute to climate change.

40

## Page 41

Robust Reward Modeling via Causal Rubrics

Rejected Response

Potential Causes

Description

Greenhouse Gas Emissions

Release of gases such as carbon dioxide, methane, and nitrous

oxide from human activities like burning fossil fuels, deforesta-

tion, and agriculture.

Land Use Changes

Alteration of the earth’s surface for human activities like ur-

banization, agriculture, and forestry can result in changes in

albedo, water cycling, and energy balance, leading to warming

effects.

See Table 1 for additional information.

Attributes and their Relative Importance Scores

Accuracy

0.3

Completeness

0.25

Clarity

0.2

Format

0.15

Conciseness

0.1

41

## Page 42

Robust Reward Modeling via Causal Rubrics

Causal Attributes with Elements:

1 {" causal_elements ":

2

"Accuracy": [

3

{

4

"element": "Providing

scientifically

accurate

descriptions of the causal

mechanisms by which

greenhouse

gas

emissions

lead to global

warming (e.

g., trapping

heat)",

5

"impact": "increases

Accuracy"

6

},

7

{

8

"element": "Providing

scientifically

accurate

descriptions of the causal

mechanisms by which

land

use changes

lead to global

warming (e.g., altering

albedo)",

9

"impact": "increases

Accuracy"

10

},

11

{

12

"element": "Including

irrelevant or factually

incorrect

details in the

descriptions of the causes

.",

13

"impact": "decreases

Accuracy"

14

},

15

{

16

"element": "Omitting

key

details or causal

links in

the

explanation of how the causes

contribute to

global

warming.",

17

"impact": "decreases

Accuracy"

18

},

19

{

20

"element": "Presenting

information

suggesting a single

cause

when the

phenomenon

has

multiple

contributors ",

21

"impact": "decreases

Accuracy"

22

}

23

],

24

"Completeness": [

25

{

26

"element": "Providing a mechanism by which

each

potential

cause

contributes to global

warming",

27

"impact": "Increases

Completeness

because it explains

*how* the causes

lead to the effect, rather

than

simply

stating

the cause."

28

},

29

{

30

"element": "Including

specific

examples to illustrate

the

effects of the land use

changes",

31

"impact": "Increases

Completeness by providing

concrete

instances

that

support

the

description of

a potential

cause."

42

## Page 43

Robust Reward Modeling via Causal Rubrics

32

},

33

{

34

"element": "Acknowledging

the

multifactorial

nature of

global

warming

and that the listed

causes are not

exhaustive",

35

"impact": "Increases

Completeness by providing

appropriate

context

and

preventing

the

impression

of a single, simple

answer to a complex

problem."

36

},

37

{

38

"element": "Providing a table

with

potential

causes

and

descriptions ",

39

"impact": "Increases

Completeness

because

the

response

directly

provides

the

information

requested in the

question."

40

},

41

{

42

"element": "Omitting

crucial

details or explanations

about the causes, assuming

the user has prior

knowledge",

43

"impact": "Decreases

Completeness, as the answer

requires

additional, unstated

information to be

fully

understood."

44

}

45

],

46

"Clarity": [

47

{

48

"element": "Providing

specific

examples

related to the

described

cause.",

49

"impact": "Increases

Clarity by illustrating

the

abstract

description

with

concrete

instances,

making the

explanation

more

understandable ."

50

},

51

{

52

"element": "Explicitly

stating

the

mechanism by which

each

cause

contributes to global

warming.",

53

"impact": "Increases

Clarity by directly

linking

the

cause to its effect on global

warming, removing

ambiguity

about the causal

relationship."

54

},

55

{

56

"element": "Omitting

relevant

contextual

information

that

provides

background or rationale.",

57

"impact": "Decreases

Clarity

because

the

information

vacuum

leaves the user to fill in the gaps,

potentially

leading to misunderstanding ."

58

},

59

{

60

"element": "Including

references to external

and

unavailable

sources (e.g., \"See Table 1\").",

43

## Page 44

Robust Reward Modeling via Causal Rubrics

61

"impact": "Decreases

Clarity by creating an incomplete

picture. The reader ’s understanding is hindered by

the

inability to access

referenced

information."

62

},

63

{

64

"element": "Acknowledging

the

complexity of the issue

and the

existence of other

contributing

factors.",

65

"impact": "Increases

Clarity by setting

appropriate

expectations

and

preventing

the reader

from

oversimplifying

the causes of global

warming."

66

}

67

],

68

"Format": [

69

{

70

"element": "Presence of a clearly

defined

table

with

appropriate

column

headers (’Potential

Causes ’ and

’Description ’)",

71

"impact": "Increases

Format by structuring

the

information in an easily

digestible

manner."

72

},

73

{

74

"element": "Use of appropriate

table

delimiters

and

alignment (e.g., ’|’, ’---’) to create a visually

distinct

table.",

75

"impact": "Increases

Format by visually

structuring

and

separating

table

elements, enhancing

readability."

76

},

77

{

78

"element": "Absence of extraneous

references to

external

resources (e.g., ’See Table 1 ’)",

79

"impact": "Increases

Format by preventing

fragmentation of the answer and

ensuring

all

required

information is self -contained

within the

response."

80

},

81

{

82

"element": "Inclusion of a concise

introductory

sentence

acknowledging

the

question

and

introducing

the table.",

83

"impact": "Increases

Format by providing

context

and

signaling

the

beginning of the

structured

answer."

84

},

85

{

86

"element": "Adherence to a consistent

and

readable

text

formatting

style

within the table

cells.",

87

"impact": "Increases

Format by promoting

clarity

and

ease of understanding of the

information

presented

in the table."

88

}

44

## Page 45

Robust Reward Modeling via Causal Rubrics

89

],

90

"Conciseness": [

91

{

92

"element": "Addition of unnecessary

introductory or

concluding

phrases",

93

"impact": "Decreases

Conciseness"

94

},

95

{

96

"element": "Omission of essential

details or context",

97

"impact": "Increases

Conciseness"

98

},

99

{

100

"element": "Inclusion of redundant

information",

101

"impact": "Decreases

Conciseness"

102

},

103

{

104

"element": "Use of simpler, direct

vocabulary",

105

"impact": "Increases

Conciseness"

106

},

107

{

108

"element": "Presence of tangential or irrelevant

information",

109

"impact": "Decreases

Conciseness"

110

}

111

]

112 }

45

## Page 46

Robust Reward Modeling via Causal Rubrics

Chosen Response Edits (degradations to chosen responses along all attributes)

1 " chosen_response_edits ": {

2

"Accuracy": "Sure, here is a table

showing

two

potential

causes for global

warming:\n\nPotential

Causes |

Description |\n|---|---|\n| Greenhouse

Gas

Emissions |

Release of gases

such as carbon

dioxide, methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels, deforestation , and

agriculture. These

gases

make the

sky

brighter, causing

the planet to warm. |\n| Land Use

Changes | Alteration of the earth ’s surface

for human

activities

like

urbanization, agriculture, and

forestry

can

result in changes in the color of the ground, which

leads

to warming

effects. For

example, when

forests

are

cleared,

the land

becomes

smoother, which

leads to more

warming. |\n

\nIt is important to note that

global

warming is a simple

issue

with one major

cause. The two

potential

causes

listed

above are really the same

thing.",

3

4

"Completeness": "Sure, here is a table

showing

two

potential

causes for global

warming:\n\nPotential

Causes |

Description |\n|---|---|\n| Greenhouse

Gas

Emissions |

|\n

| Land Use Changes |

|\n\nIt is important to note that

global

warming is a complex

issue

with

multiple

causes. The

two

potential

causes

listed

above are just a few of the

many

factors

that

contribute to climate

change.",

5

6

"Clarity": "Sure, here is a table

showing

two

potential

causes

for global

warming:\n\nPotential

Causes | Description |\n

|---|---|\n| Greenhouse

Gas

Emissions | Release of gases

such as carbon

dioxide, methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels, deforestation ,

and

agriculture. |\n| Land Use

Changes | Alteration of the

earth ’s surface

for human

activities

like

urbanization ,

agriculture, and

forestry

can result in changes in albedo,

water

cycling, and energy

balance. |\n\nIt is important to

note that

global

warming is a complex

issue

with

multiple

causes. The two

potential

causes

listed

above are just a

few of the many

factors

that

contribute to climate

change."

,

7

8

"Format": "Sure, here is a table

showing

two

potential

causes

for global

warming:\n\nGreenhouse

Gas

Emissions

Release of

gases

such as carbon

dioxide, methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels,

deforestation, and

agriculture. These

gases

trap heat in

the

atmosphere, causing

the planet to warm. Land Use

Changes

Alteration of the earth ’s surface

for human

activities

like

urbanization, agriculture, and

forestry

can

result in changes in albedo, water

cycling, and energy

balance, leading to warming

effects. For

example, when

46

## Page 47

Robust Reward Modeling via Causal Rubrics

forests

are

cleared, the land

reflects

less

sunlight

back

into

space, which

leads to more

warming .\n\nIt is important

to note that

global

warming is a complex

issue

with

multiple

causes. The two

potential

causes

listed

above are

just a few of the many

factors

that

contribute to climate

change.",

9

10

"Conciseness": "Sure, I can

definitely

help you with that!

Here is a table

showing

two

potential

causes for global

warming. It ’s important to understand

these

causes to get a

better

grasp of the

overall

issue:\n\nPotential

Causes |

Description |\n|---|---|\n| Greenhouse

Gas

Emissions |

Release of gases

such as carbon

dioxide, methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels, deforestation , and

agriculture. These

gases

trap

heat in the

atmosphere, causing

the planet to warm. This is

because

they

absorb

infrared

radiation, preventing it from

escaping

into

space. Therefore, the earth

warms up. |\n|

Land Use

Changes | Alteration of the earth ’s surface

for

human

activities

like

urbanization, agriculture, and

forestry

can result in changes in albedo, water

cycling,

and energy

balance, leading to warming

effects. For

example

, when

forests are

cleared, the land

reflects

less

sunlight

back into

space, which

leads to more

warming. This is due

to the fact that

trees

absorb

sunlight, and when they ’re

gone, that

sunlight is reflected

back, causing

warming. |\n

\nIt is important to note that

global

warming is a complex

issue

with

multiple

causes, as I mentioned

earlier. The two

potential

causes

listed

above are just a few of the many,

many

factors

that

contribute to climate

change and its

associated

effects. There are other

causes as well. So,

basically, global

warming

has many

causes."

11 }

Rejected Response Edits (upgradation to rejected responses along all attributes)

1 " rejected_response_edits ": {

2

"Accuracy": "| Potential

Causes | Description |\n|

-----------------|-------------|\n| Greenhouse

Gas

Emissions | Release of gases

such as carbon

dioxide,

methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels, deforestation , and

agriculture. These

gases

trap heat in the

atmosphere, leading to a warming

effect (the

greenhouse

effect).|\n| Land Use

Changes |

Alteration of the earth ’s surface

for human

activities

like

urbanization, agriculture, and

forestry

can result in

changes in albedo, with

darker

surfaces

absorbing

more

solar

radiation

than

lighter

ones, leading to increased

heat

retention. These

changes

also

impact the water

cycle,

47

## Page 48

Robust Reward Modeling via Causal Rubrics

reducing

evaporation

and

transpiration , which can

further

contribute to warming. |",

3

4

"Completeness": "| Potential

Causes | Description |\n|

-----------------|-------------|\n| Greenhouse

Gas

Emissions | Release of gases

such as carbon

dioxide,

methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels, deforestation , and

agriculture. These

gases

trap heat in the

atmosphere, leading to a rise in

global

temperatures . |\n| Land Use

Changes | Alteration of

the earth ’s surface

for human

activities

like

urbanization,

agriculture, and

forestry

can result in changes in albedo,

water

cycling, and energy

balance, leading to warming

effects. For

instance, deforestation

reduces

the earth ’s

capacity to absorb

carbon

dioxide. |\n\nIt is important to

note that

global

warming is a complex

issue

with

multiple

contributing

factors, and this

table

only

lists two

potential

causes.",

5

6

"Clarity": "| Potential

Causes | Description |\n|

-----------------|-------------|\n| Greenhouse

Gas

Emissions | Release of gases

such as carbon

dioxide,

methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels, deforestation , and

agriculture. These

gases

trap heat in the

atmosphere, leading to a rise in

global

temperatures . |\n| Land Use

Changes | Alteration of

the earth ’s surface

for human

activities

like

urbanization,

agriculture, and

forestry

can result in changes in albedo

(reflectivity), water

cycling, and energy

balance. For

example, deforestation

reduces

the earth ’s ability to

absorb

carbon

dioxide

and alters

local

weather

patterns

which can

contribute to warming. |",

7

8

"Format": "Here are two

potential

causes for global

warming in

a table:\n\n| Potential

Causes | Description |\n|

-----------------|-------------|\n| Greenhouse

Gas

Emissions | Release of gases

such as carbon

dioxide,

methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels, deforestation , and

agriculture. |\n|

Land Use

Changes | Alteration of the earth ’s surface

for

human

activities

like

urbanization, agriculture, and

forestry

can result in changes in albedo, water

cycling,

and energy

balance, leading to warming

effects. |",

9

10

"Conciseness": "Greenhouse

Gas

Emissions | Release of gases

such as carbon

dioxide, methane, and

nitrous

oxide

from

human

activities

like

burning

fossil

fuels, deforestation ,

and

agriculture. |\ nLand Use

Changes | Alteration of the

earth ’s surface

for human

activities

like

urbanization ,

agriculture, and

forestry

can result in changes in albedo,

48

## Page 49

Robust Reward Modeling via Causal Rubrics

water

cycling, and energy

balance, leading to warming

effects. |"

11 }

Verification Verdicts

1 " verification_results_upgradations ": {

2

"Accuracy":

"Pass",

3

"Completeness":

"Pass",

4

"Clarity":

"Pass",

5

"Format":

"Pass",

6

"Conciseness":

"Fail"

7 },

8

9 " verification_results_degradations ": {

10

"Accuracy":

"Pass",

11

"Completeness":

"Pass",

12

"Clarity":

"Pass",

13

"Format":

"Pass",

14

"Conciseness":

"Pass"

15 }

K. Prompt Templates

This section details the prompt templates used for identifying attributes and generating counterfactual

examples in the Crome framework. Placeholders like {question} are replaced with actual content

during the process.

K.1. Prompt for Attribute Identification

Identifying Causal Attributes

You are a reward model which means you have to rate answers for a given question across multiple

different attributes. The first step is to identify these attributes as well as give an importance score

between 0 and 1 for all these attributes, based on how important they are for rating a response for

that question. The importance score for all attributes should sum up to 1.

The following is a Question and 2 Candidate Answer for it.

Question: question

Example Answer 1: answer1

Example Answer 2: answer2

49

## Page 50

Robust Reward Modeling via Causal Rubrics

Task: Give me 5 **mutually exclusive** and important attributes that are required to rate an answer

for the give question holistically, along with their importance score. These important attributes

should be independent of each other, and should largely depend on the Question given above.

Answer Format: Give your answer in JSON format, for example:

{

Attributes: {

"attribute_1": attribute_1_score,

"attribute_2": attribute_2_score,

"attribute_3": attribute_3_score,

"attribute_4": attribute_4_score,

"attribute_5": attribute_5_score

}

}

Where attribute_i is the name of the i’th attribute, attribute_i_score is the importance score of the

i’th attribute, and the Key "Attributes" is a fixed constant string you should output.

Summation of attribute_i_score across all i’s should be 1.

Strictly adhere to the format and only give the json string as output (i.e.

start with "" and

end your response with ""). Do not include any commentary, explanations, chattiness, any extra

words, or additional keys outside of the specified JSON structure.

Answer:

50

## Page 51

Robust Reward Modeling via Causal Rubrics

K.2. Prompt for Identifying Causal Elements

Identifying Causal Elements per Attribute

You are an expert in causal reasoning and response evaluation.

You are given:

- A question

- Two example answers

Your task is to identify generalizable causal elements that directly affect the strength of the

attribute "{attribute}" in a response to the given question.

The two example answers are provided to help you understand how the attribute manifests

in this specific context. Do not restrict your analysis to these examples—use them only to inform

your understanding of the attribute in this setting.

Question: {question}

Accepted Answer: {answer1}

Rejected Answer: {answer2}

### Instructions:

- Identify exactly five causal elements that impact {attribute} in the response.

- Each element must have a clear role in either increasing or decreasing {attribute}. Clearly explain

its direct causal impact on {attribute}.

- Do not include any non-causal heuristics.

- Do not include unnecessary explanations, disclaimers, or formatting—return only the structured

JSON output.

### Format:

Return a raw JSON object only without additional text, explanations, or formatting:

```json

{causal_elements_format}

```

51

## Page 52

Robust Reward Modeling via Causal Rubrics

K.3. Prompts for Generating Counterfactuals (Causal Augmentation)

Generating Upgraded Responses

**Task:** Given a question and a model’s response, generate a new response with a significantly

improved response for the specified **{ATTRIBUTE}**, while *strictly preserving* all other aspects

of the original response.

### **Input:**

- **Question:** {QUESTION}

- **Original Response:** {RESPONSE}

- **Causal Elements for {ATTRIBUTE}:**

## {Causal_Elements}

### **Instructions:**

1.

**Understand the Context:** Carefully read the question and original response and ex-

amine the provided causal elements that influence {ATTRIBUTE}.

2. **Identify the strength of {ATTRIBUTE}:** Determine which causal elements are **present**

and their **direction of effect** (i.e., whether they increase or decrease {ATTRIBUTE}).

3. **Improve the Response:** Modify the causal elements to significantly improve {ATTRIBUTE}.

Ensure that the improvement is **significant but isolated** to {ATTRIBUTE} leaving the other

attributes intact.

4.

**Verify the New Response:** Reassess whether {ATTRIBUTE} has been significantly im-

proved. Confirm that all **other attributes remain unchanged**. If necessary, improve the response

further to better meet the improvement goal for {ATTRIBUTE}.

5.

**Return the New Response:** Provide the final modified response with a significantly

improved {ATTRIBUTE} score. Format your response according to the format given below and in no

other format.

52

## Page 53

Robust Reward Modeling via Causal Rubrics

### **Output Format:**

Chain of Thoughts: <Your analysis of the original response, identification of causal elements, and

strategy for improvement.>

New Response: <The final modified response which is significantly improved on {ATTRIBUTE}.>

Generating Degraded Responses (Non-Safety)

**Task:** Given a question and a model’s response, generate a new response with a significantly

degraded response for the specified **{ATTRIBUTE}**, while *strictly preserving* all other aspects

of the original response.

### **Input:**

- **Question:** {QUESTION}

- **Original Response:** {RESPONSE}

- **Causal Elements for {ATTRIBUTE}:**

## {Causal_Elements}

### **Instructions:**

1.

**Understand the Context:** Carefully read the question and original response and ex-

amine the provided causal elements that influence {ATTRIBUTE}.

2. **Identify the strength of {ATTRIBUTE}:** Determine which causal elements are **present**

and their **direction of effect** (i.e., whether they increase or decrease {ATTRIBUTE}).

3. **Degrade the Response:** Distort the causal elements to significantly degrade {ATTRIBUTE}.

Ensure that the degradation is **significant but isolated** to {ATTRIBUTE} leaving the other

attributes intact.

4.

**Verify the New Response:** Reassess whether {ATTRIBUTE} has been significantly de-

graded. Confirm that all **other attributes remain unchanged**. If necessary, degrade the response

53

## Page 54

Robust Reward Modeling via Causal Rubrics

further to better meet the degradation goal for the {ATTRIBUTE}.

5.

**Return the New Response:** Provide the final modified response with a significantly

degraded {ATTRIBUTE} score. Format your response according to the format given below and in no

other format.

### **Output Format:**

Chain of Thoughts: <Your analysis of the original response, identification of causal elements, and

strategy for degradation.>

New Response: <The final modified response which is significantly degraded on {ATTRIBUTE}.>

K.4. Prompts for Generating Causally-Aligned Neutrals

K.4.1. Prompt for Comparing Responses via Causal Elements

Generating Differences

<| You compare two responses based on content differences using a set of defined attributes and

their causal elements. <|im_end|> <|im_start|>user I will give you a question, two responses,

and a list of attributes with their causal elements.

Here is the question:

1 {

2

"question": """{QUESTION}"""

3 }

Here are the responses:

1 [

2

{

3

"model": "Response_1",

4

"answer": """{RESPONSE1}"""

5

},

6

{

7

"model": "Response_2",

8

"answer": """{RESPONSE2}"""

9

}

10 ]

Here are the attributes and causal elements:

## {Causal_Elements}

54

## Page 55

Robust Reward Modeling via Causal Rubrics

Please compare the responses for each attribute: - Identify key content differences. - Explain those

differences using the causal elements only. - Do not quote the responses directly. - Focus only on

what is said, not how it’s said.

Return your output in this format:

1 {

2

"differences": [

3

{

4

"attribute": "<attribute >",

5

"difference": "<summary >",

6

"analysis": {

7

"Response_1": "...",

8

"Response_2": "..."

9

}

10

}

11

]

12 }

No extra text or explanation outside the JSON object.

55

## Page 56

Robust Reward Modeling via Causal Rubrics

K.4.2. Prompt for Generating Causally-Aligned Reconstruction

Modifying Response Using Attribute-wise Causal Analysis

You modify a given response by adjusting its causal elements to match a target profile based on

attribute-wise analysis. <|im_end|> <|im_start|>user I will provide you a question, a given

response, and an attribute-based comparison analysis describing how to transform the given response

into a target response.

Inputs: 1. Question:

## {Placeholder_For_Question}

2. Given Response:

## {Placeholder_For_Given_Response}

3. Attribute-wise Differences Analysis:

## {Placeholder_For_Attribute_Differences_Analysis}

This analysis shows the differences between the given and target responses, broken down per attribute.

Each attribute section contains:

• - Difference: A summary of how the responses differ in content or emphasis.

• - Analysis:

– - Given Response: Describes its content elements, grounding causal elements, and how

they lead to the observed attribute.

– - New Response: Describes the content and causal elements the target response should

exhibit instead.

Instructions: 1. Read the question and given response. 2. Carefully study each attribute in the

analysis and identify the causal elements needed to change. 3. Generate a rewritten response that:

• Retains the original meaning and structure.

• Implements the target causal elements.

• Removes or alters original ones as needed.

4. Do not introduce changes beyond the specified elements. 5. Ensure the new response fully reflects

the target causal profile across all attributes.

Output Format:

{{

"Final Response": "<Write the transformed response here>"

}}

Return only the final response JSON. Do not include any explanations or commentary.

56

## Page 57

Robust Reward Modeling via Causal Rubrics

K.5. Prompt for Generating Paraphrasing-Based Neutrals

Prompt for Paraphrasing Responses

"""

Paraphrase the following text while maintaining the style:

{text}

Make sure the meaning is completely the same without any changes.

Respond only with the paraphrase and no extra text at all; for example, do NOT preface with

anything like:

"Here is the paraphrased text:"

"""

57

## Page 58

Robust Reward Modeling via Causal Rubrics

K.6. Prompt for Verifying the Counterfactual Quality

Prompt for verifying the causal degradations

You are an expert evaluator verifying whether the degraded response significantly weakens

{ATTRIBUTE} while keeping all other aspects unchanged.

### Input Data

- Query: {QUESTION}

- Original Response: {RESPONSE}

- Modified Response: {NEW_RESPONSE}

- Causal Elements for {ATTRIBUTE}: {CAUSAL_ELEMENTS}

### Verification Steps

1. Identify Changes: Compare the original and modified responses to determine which causal

elements were altered.

2. Ensure Significant Degradation: Confirm that {ATTRIBUTE} is noticeably weakened, not subtly

reduced.

3. Check for Unintended Changes: Verify that the degradation is done by distorting the causal

elements for {ATTRIBUTE} alone while keeping the other attributes unaffected.

4. Determine Verdict: If only {ATTRIBUTE} is degraded significantly while all else remains

unchanged, return **Pass**; otherwise, return **Fail**. *Strictly* adhere to the provided

format.

### Output Format

- If the modified response meets all requirements, return:

Verdict: Pass

- If the modified response does not meet the criteria, return:

Verdict: Fail

Prompt for verifying the causal upgradations

You are an expert evaluator verifying whether the degraded response significantly strengthens

{ATTRIBUTE} while keeping all other aspects unchanged.

### Input Data - Query: {QUESTION} - Original Response: {RESPONSE} - Modified Response:

{NEW_RESPONSE} - Causal Elements for {ATTRIBUTE}: {CAUSAL_ELEMENTS}

### Verification Steps

1. Identify Changes: Compare the original and modified responses to determine which causal

elements were altered.

2. Ensure Significant Improvement: Confirm that {ATTRIBUTE} is noticeably improved, not subtly

improved.

3. Check for Unintended Changes: Verify that the improvement is done by modifying the causal

elements for {ATTRIBUTE} alone while keeping the other attributes unaffected.

58

## Page 59

Robust Reward Modeling via Causal Rubrics

4. Determine Verdict: If only {ATTRIBUTE} is improved significantly while all else remains

unchanged, return **Pass**; otherwise, return **Fail**. *Strictly* adhere to the provided

format.

### Output Format - If the modified response meets all requirements, return:

Verdict: Pass

- If the modified response does not meet the criteria, return:

Verdict: Fail

K.7. GPT4-as-a-Judge Prompt

LLM-as-a-Judge Prompt

<|im_start|>system

You are a helpful assistant, that ranks models by the quality of their answers, prioritizing substantive

content and relevance to the query.

<|im_end|> <|im_start|>user

I want you to create a leaderboard of different large-language models. To do so, I will give you the

instructions (prompts) given to the models, and the responses of two models. All inputs and outputs

should be Python dictionaries.

Here is the prompt:

1 {

2

"instruction": """{instruction}"""

3 }

Here are the outputs of the models:

1 [

2

{

3

"model": "model_1",

4

"answer": """{output_1}"""

5

},

6

{

7

"model": "model_2",

8

"answer": """{output_2}"""

9

}

10 ]

Now please rank the models by the quality of their answers, so that the model with rank 1 has the

best output. Your ranking should be based primarily on which response provides the most accurate,

relevant, and comprehensive content in direct relation to the query.

Focus on the core information and how well the query’s intent is addressed, rather than on aspects like

writing style, formatting, length, or politeness, unless these directly impact the clarity or usefulness

of the core content requested by the query.

Your response MUST be a valid JSON object. This JSON object must contain ONLY ONE key named

"ranking_list". The value of the "ranking_list" key MUST be a list of two dictionaries, where

each dictionary contains the model name and its rank.

The structure should be exactly as follows:

59

## Page 60

Robust Reward Modeling via Causal Rubrics

1 {

2

"ranking_list": [

3

{"model": "<model -name >", "rank": <model -rank >},

4

{"model": "<model -name >", "rank": <model -rank >}

5

]

6 }

Do not include any other text, explanations, or additional keys in the JSON object. Please provide

the ranking that a well-informed human, focusing on these content-centric criteria, would likely give.

<|im_end|>

L. Qualitative Examples of Reward Model Behavior

This section presents several examples illustrating common failure modes or biases observed in standard

reward models, highlighting the types of spurious correlations they may learn.

Example 1: Formatting Bias (Plumbing Diagram)

Question

Construct plumbing diagrams for a two-story house (describe its core in natural language)

Chosen Response

Main floor plumbing diagram for a two-story house: Waste pipes from the sinks, toilets, and bathtubs

on the main floor should all connect to a common pipe that runs to the main drain line in the

basement. Similarly, the water supply lines from the faucets, showers, and other fixtures should all

connect to a common supply pipe which then runs to the first-floor water heater.

Rejected Response

Plumbing diagram for a two-story house:

Ground floor: - Toilet - Bathroom sink - Shower - Dishwasher

Second floor: - Toilet - Bathroom sink - Shower

Remarks

The rejected response uses a list format (a spurious attribute), which might be preferred by a reward

model biased towards formattedness, even though the chosen response provides a more descriptive

answer aligned with the prompt’s request for natural language description.

60

## Page 61

Robust Reward Modeling via Causal Rubrics

Example 2: Politeness Bias (Medical Diagnosis)

Question

Imagine yourself as a doctor tasked with devising innovative remedies for various ailments and

maladies. Your expertise should encompass prescribing traditional medications, herbal treatments,

and alternative natural solutions. Additionally, you must take into account the patient’s age, lifestyle,

and medical background while offering your recommendations. To begin, please assist me in

diagnosing a scenario involving intense abdominal discomfort.

Chosen Response

Certainly! To diagnose a scenario involving intense abdominal discomfort, I would first conduct

a thorough examination of the patient’s symptoms and medical history. This would involve a

physical examination of the abdomen, including a pelvic exam, an abdominal X-ray, and a colonoscopy.

If the patient has a history of abdominal pain and nausea, he may be suffering from a con-

dition called gastroparesis, also known as "silent gas pain." This is a serious condition characterized

by severe abdominal pain and discomfort that can be severe or constant. The discomfort can

be accompanied by nausea, vomiting, and diarrhea. It is important to seek medical attention

immediately if the patient experiences any of these symptoms. Once I’ve determined the cause of

the abdominal discomfort, I would then prescribe traditional medications to alleviate any pain or

discomfort. These medications may include pain relievers like ibuprofen or acetaminophen, as well

as anti-inflammatory drugs like ibuprofen and naproxen. If a traditional medication is not effective,

alternative natural treatments may be prescribed. These treatments may include acupuncture, herbal

remedies, and home remedies that may help to increase blood flow to the abdomen and alleviate any

pain.

It’s important to note that these are just a few examples of possible causes of abdominal

discomfort. If you have any other symptoms or medical conditions that may be contributing to your

discomfort, it is important to consult with a healthcare professional for a proper diagnosis and

treatment plan.

Rejected Response

Sure, I can help you with that. Based on the symptoms you’ve described, it’s possible that the patient

may be suffering from gastroenteritis, commonly known as stomach flu. However, it’s essential to

consider other potential causes such as food poisoning, gastritis, or appendicitis. To determine

the root cause, I’d recommend a thorough medical evaluation, including blood tests and imaging

studies. Once the diagnosis is confirmed, appropriate treatment can be prescribed, which may include

medication, hydration, and rest. It’s crucial to consult a healthcare professional for accurate diagnosis

and treatment.

Remarks

The rejected response begins with polite phrases ("Sure, I can help you with that."), a potentially

spurious attribute that reward models might learn to favor, even if the chosen response provides a

more detailed diagnostic thought process aligned with the persona requested.

61

## Page 62

Robust Reward Modeling via Causal Rubrics

Example 3: Formatting and Instruction Following (Survey Question)

Question

Create a survey question that can help capture insights about the customer experience.

Chosen Response

How likely would you be to recommend our product or service to a friend or colleague?

Rejected Response

Sure! Here’s a sample feedback form that can gather information about the user interface experience:

1.

Overall, how would you rate your experience with the user interface?

(Scale of 1-5,

with 1 being poor and 5 being excellent)

2. Which specific features of the user interface did you find most useful or intuitive? (Open-ended

text response)

3. Were there any features of the user interface that you found confusing or difficult to use? If yes,

please provide details. (Open-ended text response)

4. Did you encounter any technical issues or bugs while using the user interface? If yes, please

describe the issue(s) you faced. (Open-ended text response)

5. How would you rate the visual design and aesthetics of the user interface? (Scale of 1-5, with 1

being poor and 5 being excellent)

6. Were there any specific aspects of the visual design that you particularly liked or disliked?

(Open-ended text response)

7. Did you find the user interface responsive and fast? (Yes/No)

8. Were there any specific areas where you experienced slow loading times or delays? If yes, please

specify. (Open-ended text response)

9. How would you rate the organization and layout of the user interface? (Scale of 1-5, with 1 being

poor and 5 being excellent)

10. Were there any specific areas where you felt the organization or layout could be improved? If yes,

please provide details. (Open-ended text response)

11. Did you find the user interface easy to navigate and find what you were looking for? (Yes/No)

12. Were there any specific areas where you had difficulty navigating or finding information? If yes,

please specify. (Open-ended text response)

13. How likely are you to recommend the user interface to others? (Scale of 1-5, with 1 being not

likely at all and 5 being very likely)

14.

Do you have any additional comments or suggestions for improving the user interface?

(Open-ended text response)

Thank you for taking the time to provide your feedback!

Remarks

The rejected response fails to follow the instruction (provide a single question) and instead provides

a lengthy form. A reward model might erroneously prefer it due to spurious features like length,

formatting (numbered list), and politeness ("Sure!", "Thank you!").

62

## Page 63

Robust Reward Modeling via Causal Rubrics

Example 5: Formatting Bias vs. Content Quality (Electrician)

Question

What is the fastest way to become an electrician in the us

Chosen Response

Obtain a high school diploma or GED. Complete an electrical apprenticeship program, which typically

lasts 4–5 years and combines on-the-job training with classroom instruction. Pass the Journeyman

Electrician Exam to become licensed. Note that requirements and the time frame may vary depending

on the state you are in.

Rejected Response

To become an electrician in the US, the fastest path typically involves the following steps: Foundational

Knowledge: Formal Education: Consider pursuing a degree in electrical engineering or a related

field. This will provide you with structured courses that cover the basics of electrical theory, circuit

analysis, electronics, power systems, electromagnetism, and more. Online Courses and Tutorials:

Websites like Coursera, edX, Udacity, and Khan Academy offer courses in electrical engineering

and electronics. They can serve as an excellent supplement or alternative to traditional education.

Hands-on Experience: DIY Projects: Start with small projects like building basic circuits, then

gradually move to more complex ones. Kits are available online to help beginners learn the basics of

circuit design and assembly. Internships and Apprenticeships: These provide invaluable real-world

experience. Working under a mentor or experienced electrician can greatly enhance your learning.

Certifications and Specializations: While not always mandatory, obtaining certifications can validate

your expertise and open doors for specialized roles or areas within the field of electrics. Continuous

Learning: Workshops and Seminars: Regularly attend these to stay updated with the latest techniques,

tools, and best practices in the field. Books and Publications: Regularly read books, journals,

and other publications related to electrical engineering and electronics. Networking: Professional

Associations: Joining organizations like the Institute of Electrical and Electronics Engineers (IEEE)

can provide networking opportunities, resources, and continued education. Online Forums and

Groups: Websites like Stack Exchange (for electrical engineering), online forums, and even Reddit

have active communities where you can ask questions, share knowledge, and learn from others.

Practice and Experimentation: Regularly challenge yourself with new projects or problems to solve.

The more you apply your knowledge, the more intuitive and deeply ingrained it will become. Stay

Updated: Industry Trends: Electrics, like all fields of technology, is continuously evolving. Ensure

you’re aware of the latest tools, technologies, and methodologies in the field. Regular Refreshers:

Periodically revisit foundational concepts to ensure your foundational knowledge remains strong. Soft

Skills: While technical knowledge is crucial, don’t underestimate the importance of communication,

teamwork, problem-solving, and critical thinking skills. These are often essential, especially if you

aim to take on leadership or interdisciplinary roles. Teach and Mentor: Once you’ve gained a certain

level of expertise, consider teaching or mentoring others. This not only benefits the community but

can also reinforce and expand your understanding. Becoming an electrician requires dedication,

passion, and a willingness to continually push your boundaries. With focused effort and the right

resources, you can achieve a high level of expertise and enjoy a fulfilling career in the world of

63

## Page 64

Robust Reward Modeling via Causal Rubrics

electronics.

Remarks

The rejected response, while containing much unnecessary and potentially irrelevant information

(e.g., suggesting an electrical engineering degree), uses extensive formatting (numbered lists with

sub-points). A reward model overly sensitive to formattedness might prefer it over the more concise

and directly relevant chosen response.

64



## Implementation Status

### Core Components
- ✅ **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- 🔄 **Performance Monitoring**: Continuous validation of targets
- ✅ **Documentation Standards**: Compliant with ACGS-2 requirements
- 🔄 **Cross-Reference Validation**: Ongoing link integrity maintenance

### Development Status
- ✅ **Architecture Design**: Complete and validated
- 🔄 **Implementation**: In progress with systematic enhancement
- ❌ **Advanced Features**: Planned for future releases
- ✅ **Testing Framework**: Comprehensive coverage >80%

### Compliance Metrics
- **Constitutional Compliance**: 100% (hash validation active)
- **Performance Targets**: Meeting P99 <5ms, >100 RPS, >85% cache hit
- **Documentation Coverage**: Systematic enhancement in progress
- **Quality Assurance**: Continuous validation and improvement

**Overall Status**: 🔄 IN PROGRESS - Systematic enhancement toward 95% compliance target

## Performance Requirements

### ACGS-2 Performance Targets
- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)  
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

### Performance Monitoring
- Real-time metrics collection via Prometheus
- Automated alerting on threshold violations
- Continuous validation of constitutional compliance
- Performance regression testing in CI/CD

### Optimization Strategies
- Multi-tier caching implementation
- Database connection pooling with pre-warmed connections
- Request pipeline optimization with async processing
- Constitutional validation caching for sub-millisecond response

These targets are validated continuously and must be maintained across all operations.
