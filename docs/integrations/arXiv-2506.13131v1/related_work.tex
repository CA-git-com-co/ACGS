\section{Related work}
\label{sec:related_work}

\paragraph{Evolutionary methods.}
\method extends a long tradition of research on \emph{evolutionary} or \emph{genetic programming} \citep{langdon2013foundations}, where one repeatedly uses a set of mutation and crossover operators to evolve a pool of programs \citep{koza1994genetic,banzhaf1998genetic}.
In particular, classical evolutionary techniques have succeeded in symbolic regression applications \citep{schmidt2009distilling,ma2022evolving}, automated scientific \citep{cranmer2023interpretable} or algorithmic \citep{chen2023symbolic} discovery, and scheduling \citep{zhang2021genetic} problems.
However, a challenge with these methods is the use of handwritten evolution operators, which can be hard to design and may fail to capture important properties of the domain.
In contrast, \method uses LLMs to automate the construction of these operators---it leverages the LLM's world knowledge to mutate programs without the need to pre-define a set of allowed mutation operations. 

\method was preceded by a body of recent efforts that combine LLMs and evolution; specifically, it extends the FunSearch system, introduced by \citet{paredes2023mathematical} as an approach to mathematical discovery.
FunSearch was subsequently used in downstream tasks such as learning acquisition functions for Bayesian optimization \citep{aglietti2025funbo}, discovering cognitive models \citep{castro2025discovering}, computing distances between graphs \citep{verma2025grail}, or combinatorial competitive programming \citep{velickovic2024amplifying}. 
\method goes beyond FunSearch and its recent reimplementation \citep{ellenberg2025generative} in three key ways.
First, while FunSearch only allowed the evolution of a single Python function, \method allows evolution over entire codebases written in a wide range of programming languages.
Second, FunSearch optimized a single objective function, while \method provides the ability to perform multiobjective optimization.
Third, the LLMs in FunSearch were relatively small and solely trained on code.
By contrast, \method uses frontier LLMs and rich forms of natural-language context and feedback.
As has been demonstrated in this paper, these extensions allow \method to address important challenging problems that were not amenable to FunSearch.

Other efforts in this category include the approach by \citet{lehman2023evolution}, which uses an LLM-guided evolution process to discover programmatic policies for a set of simulated robots; or the approach by \citet{hemberg2024evolving} for code synthesis.
Similar approaches have found use in several scientific and mathematical tasks, including symbolic regression \citep{shojaee2025llmsr,grayeli2024symbolic}, discovering heuristics for combinatorial optimization \citep{liu2024evolution,ye2024reevo,yao2025multiobjective}, and synthesizing molecular structures \citep{wang2025efficient}.
LLM-guided evolution has also been used to improve AI systems by enhancing LLM prompts \citep{fernando2023promptbreeder} and searching over neural architectures \citep{chen2023evoprompting,morris2024llm}.
\method differs from these approaches in its scale, flexibility, and general applicability to a broad range of domains.

Some recent efforts have augmented the basic paradigm of LLM-guided evolution with complementary ideas.
For example, \citet{surina2025algorithm} complement the evolution process by continuously finetuning the LLM through reinforcement learning.
\citet{grayeli2024symbolic} enhance the evolution process with an LLM-directed concept learning step that summarizes high-performing programs in the pool into natural language.
More investigation is required to understand the benefits of these ideas at the scale at which \method operates. 

Evolutionary methods have also found use in the recent AI Co-Scientist work~\citep{gottweis2025towards}, which seeks to automate scientific discovery using distinct agents for tasks like hypothesis discovery, ranking of hypotheses, and literature review.
While AI Co-Scientist represents scientific hypotheses and their evaluation criteria in \emph{natural language}, \method focuses on evolving \emph{code}, and directs evolution using programmatic evaluation functions.
This choice enables us to substantially sidestep LLM hallucinations, which allows \method to carry on the evolution process for a large number of time steps.
Nevertheless, it is possible in principle to combine the two approaches, leading to a method that allows a flexible combination of natural-language and programmatic idioms. 

\paragraph{Superoptimization and algorithm discovery.}
\method can be viewed as a method for \emph{code superoptimization} in that it iteratively improves an initial program using execution feedback. 
The idea of code superoptimization goes back to the 1980s \citep{Massalin87}; pre-LLM approaches to the problem included systematic enumeration \citep{Massalin87}, genetic search \citep{cooper2002adaptive}, Monte Carlo sampling \citep{schkufza2013stochastic}, and deep reinforcement learning \citep{mankowitz2023faster}.
Additionally, in limited settings that focus on a single problem such as matrix multiplication, there have been systems such as AlphaTensor that were also able to discover provably correct algorithms \citep{fawzi2022discovering}.

More recently, a body of LLM-based approaches to superoptimization and algorithm discovery have emerged.
This literature builds on the success of LLMs in coding tasks, perhaps best illustrated by their success in (simulated) programming competitions as in the case of AlphaCode \citep{li2022competitionlevel}.
For instance, LLM agents have been used to optimize certain operations in GPU kernels, such as the attention operation \citep{chen2025automating} or more general user-specified operations \citep{lange2025aicuda}.
There is also work on using LLMs to discover novel evolutionary algorithms \citep{lange2024large}, train language models \citep{lehman2024evolution}, and optimize warehouse-scale computers \citep{lin2025eco}.
Other recent work \citep{wu2023autogen} has also proposed the use of multiple LLM agents that converse with each other to accomplish mathematical and coding tasks. 

While previous work on using LLMs for algorithm discovery provided promising results, \method's approach to leverage it for evolutionary algorithms allows us to address significantly more challenging problems, as demonstrated in \Cref{sec:results}.

\paragraph{AI for scientific and mathematical discovery.}
Over the last decade, AI systems have been applied to a wide range of scientific disciplines and tasks, from protein structure prediction \citep{jumper2021highly} to quantum physics \citep{bausch2024learning,ruiz2025quantum} to climate sciences \citep{lam2023learning}.
In particular, there are numerous recent LLM-based methods that target scientific problems in multiple disciplines, such as materials science \citep{miret2024llms,zhang2024honeycomb,jia2024llmatdesign,song2025llmfeynman}, chemistry \citep{caldasramos2025review,luo2025leveraging}, bioinformatics \citep{sarumi2024large,madani2023large}, geoscience \citep{pantiukhin2025accelerating}, and quantum physics \citep{frohnert2025discovering,pan2025quantum} (for surveys on the topic, see \citep{gridach2025agentic,luo2025llm4sr,ren2025towards}).

Many of these methods use LLMs to automate several distinct stages of the scientific discovery process \citep{wang2024scimon,xia2025nature,li2025large,gu2024interesting,yang2024large}, e.g., for generating and ranking hypotheses and ideas \citep{guo2024embracing,si2025canllms}.
Of these methods, especially related to \method are the methods that use LLM-guided tree search-based algorithms \citep{bran2025chemical} or LLM-guided evolutionary algorithms \cite{yang2025moosechem,zhou2024hypothesis,gottweis2025towards}.
Other works use LLMs to optimize experimental planning and design \citep{huang2024crisprgpt,bran2024augmenting,boiko2023autonomous,naumov2025dora} or experiment execution and workflow \citep{wang2025efficient,rives2021biological,lin2023evolutionary,ye2023drugassist,ferruz2022controllable}.
Finally, there are also works focusing on the data analysis stage \citep{rasheed2024canlarge}.
\method differs from most of these methods in its use of programmatic hypothesis representations and evaluation metrics.

AI systems have also contributed to advances in pure mathematics \citep{davies2021advancing}.
In this context, the FunSearch approach \citep{paredes2023mathematical,ellenberg2025generative} established LLM-guided evolution as a powerful tool for discovering witnesses for, and counterexamples to, mathematical statements---a problem that is complementary to that of finding formal and informal proofs of mathematical statements~\citep{trinh2024solving,hubert2024ai,yang2024formal,yang2023leandojo,thakur2024context,collins2024evaluating}.
