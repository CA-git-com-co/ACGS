\section{Discussion}
\label{sec:discussion}

\method demonstrates the surprising power of combining state-of-the-art LLMs with automated evaluation metrics within an evolutionary framework, which can lead to new discoveries on decades-old mathematical problems as well as practical improvements to highly optimized compute stacks.

Interestingly, \method often allows approaching the same problem in different ways: searching for the solution directly, finding a function that constructs it from scratch, or evolving a search algorithm to find it.
Applying \method in different ways  comes with different biases (for example, finding constructive functions may favor discovering highly symmetric objects~\cite{paredes2023mathematical}) and thus can suit different problems.

\method can also be seen as a test-time compute agent that, through its evolutionary procedure, significantly enhances the capability of the base LLM (compared to, e.g., repeated sampling).
On one hand, this can be seen as a compelling demonstration of how machine feedback is able to sustain test-time compute scaling up to regimes where new scientific discoveries and highly valuable practical optimizations are made.
On the other hand, a natural next step will be to consider distilling the \method-augmented performance of the base LLMs into the next generation of the base models.
This can have intrinsic value and also, likely, uplift the next version of \method.

Beyond distillation, it is also intriguing that \method can make practical discoveries that increase the efficiency of its own infrastructure and of (future versions of) its base LLMs.
Currently, the gains are moderate and the feedback loops for improving the next version of \method are on the order of months.
However, with these improvements we envision that the value of setting up more environments (problems) with robust evaluation functions will become more widely recognized, which in turn will result in more high-value practical discoveries going forward.

The main limitation of \method is that it handles problems for which it is possible to devise an automated evaluator.
While this is true of many problems in the mathematical and computational sciences, there are domains such as the natural sciences where only some experiments can be simulated or automated.
While \method does allow for LLM-provided evaluation of ideas, this is not a setting we have optimized for.
However, concurrent work shows this is possible~\cite{gottweis2025towards}, and a natural step would be to link the two settings, with LLMs providing feedback on high-level ideas before transitioning to an implementation stage, for which machine feedback is available through code execution.
