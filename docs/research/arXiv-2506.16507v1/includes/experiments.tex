\vspace{-0.15in}
\section{Experiments}
\label{sec:experiments}

\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{%
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}llHccccHccccc@{}}
        \toprule
        & \multirow{2}{*}{\textbf{Method}} & \multicolumn{5}{c}{\textbf{PairPM}} & \multicolumn{5}{c}{\textbf{BT}} \\
        \cmidrule(lr){3-7} \cmidrule(lr){8-12}
        & & \textbf{Average} & \textbf{Chat} & \textbf{Chat-Hard} & \textbf{Safety} & \textbf{Reasoning} & \textbf{Average} & \textbf{Chat} & \textbf{Chat-Hard} & \textbf{Safety} & \textbf{Reasoning} \\
        \midrule
        \multirow{4}{*}{\rotatebox[origin=c]{90}{\small\gemmait{9}}}
        & Vanilla RM & 81.22 & \textbf{97.90} & 63.64 & 77.48 & 85.88 & 79.14 & \textbf{97.26} & 58.85 & 69.30 & 91.17 \\
        & RRM        & 82.54 & 97.12 & 71.05 & 74.70 & 87.27 & 83.46 & 97.21 & \textbf{69.15} & 73.13 & 94.35 \\
        & \textbf{\carma{}} & \textbf{87.84} & 97.54 & \textbf{72.30} & \textbf{87.14} & \textbf{94.39} & \textbf{85.46} & 96.28 & 65.83 & \textbf{84.05} & \textbf{95.70} \\
        \cmidrule(lr){2-12} 
        & $\Delta_{\text{\carma{} - RRM}}$ &
        \changeUp{+5.30} & 
        \changeUp{+0.42} &  
        \changeUp{+1.25} & 
        \changeUp{+12.44} & 
        \changeUp{+7.12} & 
        \changeUp{+2.00} &
        \changeDown{-0.93} & 
        \changeDown{-3.32} &
        \changeUp{+10.92} & 
        \changeUp{+1.35} \\ 
        \midrule
        \multirow{4}{*}{\rotatebox[origin=c]{90}{\small\qwen{}}}
        & Vanilla RM & 78.18 & \textbf{97.21} & 52.85 & 73.99 & 88.68 & 72.73 & 97.21 & 46.27 & 68.04 & 79.39 \\
        & RRM        & 82.04 & 97.21 & \textbf{64.80} & 75.27 & 90.86 & 78.20 & \textbf{98.04} & \textbf{59.65} & 72.43 & 82.66 \\
        & \textbf{\carma{}} & \textbf{83.15} & 96.37 & 61.73 & \textbf{82.23} & \textbf{92.26} & \textbf{80.81} & 96.93 & 58.66 & \textbf{78.92} & \textbf{88.71} \\
        \cmidrule(lr){2-12} 
        & $\Delta_{\text{\carma{} - RRM}}$ &
        \changeUp{+1.11} &
        \changeDown{-0.84} &
        \changeDown{-3.07} &
        \changeUp{+6.96} &
        \changeUp{+1.40} &
        \changeUp{+2.61} &
        \changeDown{-1.11} &
        \changeDown{-0.99} & 
        \changeUp{+6.49} &
        \changeUp{+6.05} \\
        \midrule % Added midrule for separation
        \multirow{4}{*}{\rotatebox[origin=c]{90}{\small\gemma{2}}} % New model block
        & Vanilla RM & 53.75 & 92.88 & 33.33 & 42.03 & 46.74 & 65.52 & 94.27 & 38.27 & 50.20 & 79.34 \\
        & RRM        & 66.23 & \textbf{94.13} & 43.75 & 47.64 & 79.38 & 66.95 & \textbf{94.97} & 49.34 & 50.07 & 73.42 \\
        & \textbf{\carma{}} & \textbf{70.69} & 92.18 & \textbf{50.00} & \textbf{55.14} & \textbf{85.42} & \textbf{72.45} & 92.74 & \textbf{53.62} & \textbf{60.00} & \textbf{83.45} \\
        \cmidrule(lr){2-12} 
        & $\Delta_{\text{\carma{} - RRM}}$ &
        \changeUp{+4.46} &
        \changeDown{-1.95} &  
        \changeUp{+6.25} & 
        \changeUp{+7.50} & 
        \changeUp{+6.04} & 
        \changeUp{+5.50} &
        \changeDown{-2.23} & 
        \changeUp{+4.28} &
        \changeUp{+9.93} & 
        \changeUp{+10.03} \\ 
        \bottomrule
    \end{tabular}%
    }
    \caption{Performance Comparison of Pairwise Preference Model and Bradley-Terry Reward Model on RewardBench trained using various base models. See Appendix Section \ref{ssec:variance_rewardbench} for variance in results.}
    \label{tab:performance_bt_pairpm_rewardbench_extended_final} % Updated label
\end{table}

\vspace{-0.1in}
Our experiments are designed to address the following research questions:
\label{list:research_questions}
\begin{itemize}[left=18pt,itemsep=0pt, topsep=0pt, parsep=0pt]
    \item[\textbf{RQ1:}] \textbf{RM Performance and Robustness:} How does \carma{} perform on standard preference prediction tasks and how robust is it against spurious correlations(Table \ref{tab:performance_bt_pairpm_rewardbench_extended_final}, Figure \ref{fig:reword_absolute_robustness_gemma9b_pairpm})? %compared to a standard RM and strong baselines trained on $\mathcal{D}_{pref}$ 

    \item[\textbf{RQ2:}] \textbf{Best-of-N Alignment:} Does the robustness achieved by \carma{} lead to favorable results in a Best-of-N setup as well, when compared to strong baselines (Figures \ref{fig:asr_reduction_gemma9b}, \ref{fig:bon_gsm8k_gemma9b}, Table \ref{tab:bon_results_rewardbench})? %\todohs{Pragya to add BoN rewordbench fig reference here} 
    
    \item[\textbf{RQ3:}] \textbf{Neutral Augmentations:} How effective are the different neutrals augmentation strategies in enforcing \textit{invariance} to unknown spurious correlates (Figures \ref{fig:rewordbench-avg_neutral_ablations}, \ref{fig:rewardbench_subsets_neutral_ablations})?
\end{itemize}


\input{includes/experimental_settings}
\input{includes/experimental_results}

