\vspace{-0.15in}

\section{Related Works}
\label{sec:related_works_main}

Our work on causally robust reward modeling, \carma{}, addresses the challenge of reward hacking in the context of aligning Large Language Models (LLMs) via Reinforcement Learning from Human Feedback (RLHF) \citep{ouyang2022training, bai2022training}. Standard RLHF relies on a reward model (RM), typically trained on pairwise preferences using Bradley-Terry \citep{bradley1952rank} or pairwise ranking approaches \citep{liu2025pairwise, qin2023large}. 
A critical limitation of learned RMs is \textit{reward hacking} \citep{gao2023scaling, skalse2022defining}, where the RM assigns high scores based on \textit{spurious} attributes (e.g., verbosity \citep{singhal2023long}, formatting \citep{zhang2024lists}, sycophancy \citep{denison2024sycophancy}) that are correlated with, but do not cause, true response quality. This leads to misaligned policies that exploit these spurious cues \citep{shen2023trickle}. Various mitigation strategies exist, including architectural modifications like \odin{} \citep{chen2024odin}, policy-level adjustments \citep{park2024disentangling}, and data-centric methods involving ensembles \citep{rame2024warm} or consistency checks \citep{shen2023trickle}. Recent causal-inspired approaches include using MMD regularization against pre-specified spurious factors \citep{wang2025beyond} or estimating the causal effects of a given attribute of a response using corrected rewrites \citep{reber2024rate}.

Our approach falls into the data-centric category, using synthetic data augmentation guided by principles of causal inference \citep{pearl2009causality, peters2017elements}. While prior work has used LLMs for causal reasoning \citep{kiciman2023causal} or counterfactual data augmentation in NLP \citep{kaushik2019learning}, and related methods like \rrm{} \citep{liu2024rrm}, \rewordbench{} \citep{wu2025rewordbench}  target RM robustness, \carma{} is distinct in its explicit use of a causal graph framework (Section \ref{subsec:causal_graph}) which guides the answer generation and the reward labeling process. We leverage LLMs to generate targeted \textit{causal} (attribute-specific upgrade/degradation) and \textit{neutral} (spurious-varying, causally-equivalent) counterfactual examples. By training on this augmented data, \carma{} aims to systematically disentangle causal attributes ($C$) from spurious ones ($SP$), learning a reward function that is inherently more robust and aligned with the true drivers of quality, as detailed in Section \ref{sec:methodology}. 
We provide a longer version of related work in Appendix \ref{sec:extended_related_works}.
