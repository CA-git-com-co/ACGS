\vspace{-0.5cm}
\begin{abstract}
Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from \textit{reward hacking}. They tend to latch on to superficial or \textit{spurious} attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true \textit{causal} drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce \carma{} (Causally Robust Reward Modeling), a novel framework grounded in an explicit causal model designed to mitigate reward hacking. \carma{} employs the following synthetic \textit{targeted augmentations} during training: (1) \textit{Causal Augmentations}, which are pairs that differ along specific causal attributes, to enforce \textit{sensitivity} along each causal attribute individually, and (2) \textit{Neutral Augmentations}, which are tie-label pairs varying primarily in spurious attributes, to enforce \textit{invariance} along spurious attributes. Notably, our augmentations are produced without \textit{any} knowledge of spurious factors, via answer interventions only along causal rubrics, that are identified by querying an oracle LLM.
Empirically, \carma{} significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.4\% and achieving gains of up to \textbf{13.2\%} and \textbf{7.2\%} in specific categories. The robustness of \carma{} is further testified by the consistent gains obtained in a Best-of-N inference setting across increasing N, across various benchmarks, including the popular RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and the reasoning-specific GSM8k.
\end{abstract}