\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and Lopez-Paz]{arjovsky2019invariant}
M.~Arjovsky, L.~Bottou, I.~Gulrajani, and D.~Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones, Joseph, Mann, DasSarma, et~al.]{askell2021general}
A.~Askell, Y.~Bai, A.~Chen, D.~Drain, D.~Ganguli, T.~Henighan, A.~Jones, N.~Joseph, B.~Mann, N.~DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021.

\bibitem[Azar et~al.(2024)Azar, Guo, Piot, Munos, Rowland, Valko, and Calandriello]{azar2024general}
M.~G. Azar, Z.~D. Guo, B.~Piot, R.~Munos, M.~Rowland, M.~Valko, and D.~Calandriello.
\newblock A general theoretical paradigm to understand learning from human preferences.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 4447--4455. PMLR, 2024.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Y.~Bai, A.~Jones, K.~Ndousse, A.~Askell, A.~Chen, N.~DasSarma, D.~Drain, S.~Fort, D.~Ganguli, T.~Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Y.~Bai, S.~Kadavath, S.~Kundu, A.~Askell, J.~Kernion, A.~Jones, A.~Chen, A.~Goldie, A.~Mirhoseini, C.~McKinnon, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Bradley and Terry(1952)]{bradley1952rank}
R.~A. Bradley and M.~E. Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando, Freedman, Korbak, Lindner, Freire, et~al.]{casper2023open}
S.~Casper, X.~Davies, C.~Shi, T.~K. Gilbert, J.~Scheurer, J.~Rando, R.~Freedman, T.~Korbak, D.~Lindner, P.~Freire, et~al.
\newblock Open problems and fundamental limitations of reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2307.15217}, 2023.

\bibitem[Chen et~al.(2024)Chen, Zhu, Soselia, Chen, Zhou, Goldstein, Huang, Shoeybi, and Catanzaro]{chen2024odin}
L.~Chen, C.~Zhu, D.~Soselia, J.~Chen, T.~Zhou, T.~Goldstein, H.~Huang, M.~Shoeybi, and B.~Catanzaro.
\newblock Odin: Disentangled reward mitigates hacking in rlhf.
\newblock \emph{arXiv preprint arXiv:2402.07319}, 2024.

\bibitem[Chi et~al.(2024)Chi, Li, Yang, Liu, Lan, Ren, Liu, and Han]{chi2024unveiling}
H.~Chi, H.~Li, W.~Yang, F.~Liu, L.~Lan, X.~Ren, T.~Liu, and B.~Han.
\newblock Unveiling causal reasoning in large language models: Reality or mirage?
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 96640--96670, 2024.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
P.~F. Christiano, J.~Leike, T.~Brown, M.~Martic, S.~Legg, and D.~Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert, J.~Tworek, J.~Hilton, R.~Nakano, C.~Hesse, and J.~Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Coste et~al.(2023)Coste, Anwar, Kirk, and Krueger]{coste2023reward}
T.~Coste, U.~Anwar, R.~Kirk, and D.~Krueger.
\newblock Reward model ensembles help mitigate overoptimization.
\newblock \emph{arXiv preprint arXiv:2310.02743}, 2023.

\bibitem[Cui et~al.(2023)Cui, Yuan, Ding, Yao, Zhu, Ni, Xie, Liu, and Sun]{cui2023ultrafeedback}
G.~Cui, L.~Yuan, N.~Ding, G.~Yao, W.~Zhu, Y.~Ni, G.~Xie, Z.~Liu, and M.~Sun.
\newblock Ultrafeedback: Boosting language models with high-quality feedback.
\newblock \emph{arXiv preprint arXiv:2310.01377}, 2023.

\bibitem[Denison et~al.(2024)Denison, MacDiarmid, Barez, Duvenaud, Kravec, Marks, Schiefer, Soklaski, Tamkin, Kaplan, et~al.]{denison2024sycophancy}
C.~Denison, M.~MacDiarmid, F.~Barez, D.~Duvenaud, S.~Kravec, S.~Marks, N.~Schiefer, R.~Soklaski, A.~Tamkin, J.~Kaplan, et~al.
\newblock Sycophancy to subterfuge: Investigating reward-tampering in large language models.
\newblock \emph{arXiv preprint arXiv:2406.10162}, 2024.

\bibitem[Eisenstein et~al.(2023)Eisenstein, Nagpal, Agarwal, Beirami, D'Amour, Dvijotham, Fisch, Heller, Pfohl, Ramachandran, et~al.]{eisenstein2023helping}
J.~Eisenstein, C.~Nagpal, A.~Agarwal, A.~Beirami, A.~D'Amour, D.~Dvijotham, A.~Fisch, K.~Heller, S.~Pfohl, D.~Ramachandran, et~al.
\newblock Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking.
\newblock \emph{arXiv preprint arXiv:2312.09244}, 2023.

\bibitem[Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky, and Kiela]{ethayarajh2024kto}
K.~Ethayarajh, W.~Xu, N.~Muennighoff, D.~Jurafsky, and D.~Kiela.
\newblock Kto: Model alignment as prospect theoretic optimization.
\newblock \emph{arXiv preprint arXiv:2402.01306}, 2024.

\bibitem[Feder et~al.(2021)Feder, Oved, Shalit, and Reichart]{feder2021causalm}
A.~Feder, N.~Oved, U.~Shalit, and R.~Reichart.
\newblock Causalm: Causal model explanation through counterfactual language models.
\newblock \emph{Computational Linguistics}, 47\penalty0 (2):\penalty0 333--386, 2021.

\bibitem[Feder et~al.(2022)Feder, Keith, Manzoor, Pryzant, Sridhar, Wood-Doughty, Eisenstein, Grimmer, Reichart, Roberts, et~al.]{feder2022causal}
A.~Feder, K.~A. Keith, E.~Manzoor, R.~Pryzant, D.~Sridhar, Z.~Wood-Doughty, J.~Eisenstein, J.~Grimmer, R.~Reichart, M.~E. Roberts, et~al.
\newblock Causal inference in natural language processing: Estimation, prediction, interpretation and beyond.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 1138--1158, 2022.

\bibitem[Gao et~al.(2023)Gao, Schulman, and Hilton]{gao2023scaling}
L.~Gao, J.~Schulman, and J.~Hilton.
\newblock Scaling laws for reward model overoptimization.
\newblock In \emph{International Conference on Machine Learning}, pages 10835--10866. PMLR, 2023.

\bibitem[Gupta et~al.(2025)Gupta, Shandilya, Zhang, Madhavan, Ghosh, Bansal, Yao, and Rajmohan]{gupta2025carmodynamiccriteriageneration}
T.~Gupta, S.~Shandilya, X.~Zhang, R.~Madhavan, S.~Ghosh, C.~Bansal, H.~Yao, and S.~Rajmohan.
\newblock Carmo: Dynamic criteria generation for context-aware reward modelling, 2025.
\newblock URL \url{https://arxiv.org/abs/2410.21545}.

\bibitem[Han et~al.(2024)Han, Rao, Ettinger, Jiang, Lin, Lambert, Choi, and Dziri]{wildguard2024}
S.~Han, K.~Rao, A.~Ettinger, L.~Jiang, B.~Y. Lin, N.~Lambert, Y.~Choi, and N.~Dziri.
\newblock Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.18495}.

\bibitem[Hong et~al.(2024)Hong, Lee, and Thorne]{hong2024orpo}
J.~Hong, N.~Lee, and J.~Thorne.
\newblock Orpo: Monolithic preference optimization without reference model.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 11170--11189, 2024.

\bibitem[Kaushik et~al.(2019)Kaushik, Hovy, and Lipton]{kaushik2019learning}
D.~Kaushik, E.~Hovy, and Z.~C. Lipton.
\newblock Learning the difference that makes a difference with counterfactually-augmented data.
\newblock \emph{arXiv preprint arXiv:1909.12434}, 2019.

\bibitem[Khalifa et~al.(2025)Khalifa, Agarwal, Logeswaran, Kim, Peng, Lee, Lee, and Wang]{khalifa2025process}
M.~Khalifa, R.~Agarwal, L.~Logeswaran, J.~Kim, H.~Peng, M.~Lee, H.~Lee, and L.~Wang.
\newblock Process reward models that think.
\newblock \emph{arXiv preprint arXiv:2504.16828}, 2025.

\bibitem[Kiciman et~al.(2023)Kiciman, Ness, Sharma, and Tan]{kiciman2023causal}
E.~Kiciman, R.~Ness, A.~Sharma, and C.~Tan.
\newblock Causal reasoning and large language models: Opening a new frontier for causality.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu, Dziri, Kumar, Zick, Choi, et~al.]{lambert2024rewardbench}
N.~Lambert, V.~Pyatkin, J.~Morrison, L.~Miranda, B.~Y. Lin, K.~Chandu, N.~Dziri, S.~Kumar, T.~Zick, Y.~Choi, et~al.
\newblock Rewardbench: Evaluating reward models for language modeling.
\newblock \emph{arXiv preprint arXiv:2403.13787}, 2024.

\bibitem[Li and Li(2024)]{li2024process}
W.~Li and Y.~Li.
\newblock Process reward model with q-value rankings.
\newblock \emph{arXiv preprint arXiv:2410.11287}, 2024.

\bibitem[Liu et~al.(2024)Liu, Xiong, Ren, Chen, Wu, Joshi, Gao, Shen, Qin, Yu, et~al.]{liu2024rrm}
T.~Liu, W.~Xiong, J.~Ren, L.~Chen, J.~Wu, R.~Joshi, Y.~Gao, J.~Shen, Z.~Qin, T.~Yu, et~al.
\newblock Rrm: Robust reward model training mitigates reward hacking.
\newblock \emph{arXiv preprint arXiv:2409.13156}, 2024.

\bibitem[Liu et~al.(2025)Liu, Yao, Min, Cao, Hou, and Li]{liu2025pairwise}
Y.~Liu, Z.~Yao, R.~Min, Y.~Cao, L.~Hou, and J.~Li.
\newblock Pairwise rm: Perform best-of-n sampling with knockout tournament.
\newblock \emph{arXiv preprint arXiv:2501.13007}, 2025.

\bibitem[Long et~al.(2023)Long, Pich{\'e}, Zantedeschi, Schuster, and Drouin]{long2023causal}
S.~Long, A.~Pich{\'e}, V.~Zantedeschi, T.~Schuster, and A.~Drouin.
\newblock Causal discovery with language models as imperfect experts.
\newblock \emph{arXiv preprint arXiv:2307.02390}, 2023.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lou et~al.(2024)Lou, Yan, Shen, Yan, Xie, and Zhang]{lou2024uncertainty}
X.~Lou, D.~Yan, W.~Shen, Y.~Yan, J.~Xie, and J.~Zhang.
\newblock Uncertainty-aware reward model: Teaching reward models to know what is unknown.
\newblock \emph{arXiv preprint arXiv:2410.00847}, 2024.

\bibitem[Meng et~al.(2024)Meng, Xia, and Chen]{meng2024simpo}
Y.~Meng, M.~Xia, and D.~Chen.
\newblock Simpo: Simple preference optimization with a reference-free reward.
\newblock \emph{arXiv preprint arXiv:2405.14734}, 2024.

\bibitem[Mishra et~al.(2024)Mishra, Nayak, Bhattacharya, Kumar, Shah, and Foltin]{mishra2024llm}
A.~Mishra, G.~Nayak, S.~Bhattacharya, T.~Kumar, A.~Shah, and M.~Foltin.
\newblock Llm-guided counterfactual data generation for fairer ai.
\newblock In \emph{Companion Proceedings of the ACM Web Conference 2024}, pages 1538--1545, 2024.

\bibitem[Negahban et~al.(2009)Negahban, Yu, Wainwright, and Ravikumar]{negahban2009unified}
S.~Negahban, B.~Yu, M.~J. Wainwright, and P.~Ravikumar.
\newblock A unified framework for high-dimensional analysis of $ m $-estimators with decomposable regularizers.
\newblock \emph{Advances in neural information processing systems}, 22, 2009.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang, S.~Agarwal, K.~Slama, A.~Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pace et~al.(2024)Pace, Mallinson, Malmi, Krause, and Severyn]{pace2024west}
A.~Pace, J.~Mallinson, E.~Malmi, S.~Krause, and A.~Severyn.
\newblock West-of-n: Synthetic preference generation for improved reward modeling.
\newblock \emph{arXiv preprint arXiv:2401.12086}, 2024.

\bibitem[Pan et~al.(2022)Pan, Bhatia, and Steinhardt]{pan2022effectsrewardmisspecificationmapping}
A.~Pan, K.~Bhatia, and J.~Steinhardt.
\newblock The effects of reward misspecification: Mapping and mitigating misaligned models, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.03544}.

\bibitem[Park et~al.(2024)Park, Rafailov, Ermon, and Finn]{park2024disentangling}
R.~Park, R.~Rafailov, S.~Ermon, and C.~Finn.
\newblock Disentangling length from quality in direct preference optimization.
\newblock \emph{arXiv preprint arXiv:2403.19159}, 2024.

\bibitem[Pearl(2009)]{pearl2009causality}
J.~Pearl.
\newblock \emph{Causality}.
\newblock Cambridge university press, 2009.

\bibitem[Peters et~al.(2017)Peters, Janzing, and Sch{\"o}lkopf]{peters2017elements}
J.~Peters, D.~Janzing, and B.~Sch{\"o}lkopf.
\newblock \emph{Elements of causal inference: foundations and learning algorithms}.
\newblock The MIT Press, 2017.

\bibitem[Qiang et~al.(2024)Qiang, Nandi, Mehrabi, Steeg, Kumar, Rumshisky, and Galstyan]{qiang2024prompt}
Y.~Qiang, S.~Nandi, N.~Mehrabi, G.~V. Steeg, A.~Kumar, A.~Rumshisky, and A.~Galstyan.
\newblock Prompt perturbation consistency learning for robust language models.
\newblock \emph{arXiv preprint arXiv:2402.15833}, 2024.

\bibitem[Qin et~al.(2023)Qin, Jagerman, Hui, Zhuang, Wu, Yan, Shen, Liu, Liu, Metzler, et~al.]{qin2023large}
Z.~Qin, R.~Jagerman, K.~Hui, H.~Zhuang, J.~Wu, L.~Yan, J.~Shen, T.~Liu, J.~Liu, D.~Metzler, et~al.
\newblock Large language models are effective text rankers with pairwise ranking prompting.
\newblock \emph{arXiv preprint arXiv:2306.17563}, 2023.

\bibitem[Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
R.~Rafailov, A.~Sharma, E.~Mitchell, C.~D. Manning, S.~Ermon, and C.~Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Ram{\'e} et~al.(2024)Ram{\'e}, Vieillard, Hussenot, Dadashi, Cideron, Bachem, and Ferret]{rame2024warm}
A.~Ram{\'e}, N.~Vieillard, L.~Hussenot, R.~Dadashi, G.~Cideron, O.~Bachem, and J.~Ferret.
\newblock Warm: On the benefits of weight averaged reward models.
\newblock \emph{arXiv preprint arXiv:2401.12187}, 2024.

\bibitem[Ravfogel et~al.(2025)Ravfogel, Svete, Snæbjarnarson, and Cotterell]{ravfogel2025gumbelcounterfactualgenerationlanguage}
S.~Ravfogel, A.~Svete, V.~Snæbjarnarson, and R.~Cotterell.
\newblock Gumbel counterfactual generation from language models, 2025.
\newblock URL \url{https://arxiv.org/abs/2411.07180}.

\bibitem[Reber et~al.(2024)Reber, Richardson, Nief, Garbacea, and Veitch]{reber2024rate}
D.~Reber, S.~Richardson, T.~Nief, C.~Garbacea, and V.~Veitch.
\newblock Rate: Score reward models with imperfect rewrites of rewrites.
\newblock \emph{arXiv preprint arXiv:2410.11348}, 2024.

\bibitem[Sch{\"o}lkopf et~al.(2021)Sch{\"o}lkopf, Locatello, Bauer, Ke, Kalchbrenner, Goyal, and Bengio]{scholkopf2021toward}
B.~Sch{\"o}lkopf, F.~Locatello, S.~Bauer, N.~R. Ke, N.~Kalchbrenner, A.~Goyal, and Y.~Bengio.
\newblock Toward causal representation learning.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (5):\penalty0 612--634, 2021.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Bi, Zhang, Zhang, Li, Wu, et~al.]{shao2024deepseekmath}
Z.~Shao, P.~Wang, Q.~Zhu, R.~Xu, J.~Song, X.~Bi, H.~Zhang, M.~Zhang, Y.~Li, Y.~Wu, et~al.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Shen et~al.(2024)Shen, Xu, Jun, Qin, Liu, Yang, Liang, Baumgartner, and Bendersky]{shen2024boosting}
J.~Shen, R.~Xu, Y.~Jun, Z.~Qin, T.~Liu, C.~Yang, Y.~Liang, S.~Baumgartner, and M.~Bendersky.
\newblock Boosting reward model with preference-conditional multi-aspect synthetic data generation.
\newblock \emph{arXiv preprint arXiv:2407.16008}, 2024.

\bibitem[Shen et~al.(2023)Shen, Chen, Song, Jin, Peng, Mi, Khashabi, and Yu]{shen2023trickle}
L.~Shen, S.~Chen, L.~Song, L.~Jin, B.~Peng, H.~Mi, D.~Khashabi, and D.~Yu.
\newblock The trickle-down impact of reward (in-) consistency on rlhf.
\newblock \emph{arXiv preprint arXiv:2309.16155}, 2023.

\bibitem[Singhal et~al.(2023)Singhal, Goyal, Xu, and Durrett]{singhal2023long}
P.~Singhal, T.~Goyal, J.~Xu, and G.~Durrett.
\newblock A long way to go: Investigating length correlations in rlhf.
\newblock \emph{arXiv preprint arXiv:2310.03716}, 2023.

\bibitem[Skalse et~al.(2022)Skalse, Howe, Krasheninnikov, and Krueger]{skalse2022defining}
J.~Skalse, N.~Howe, D.~Krasheninnikov, and D.~Krueger.
\newblock Defining and characterizing reward gaming.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 9460--9471, 2022.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
N.~Stiennon, L.~Ouyang, J.~Wu, D.~Ziegler, R.~Lowe, C.~Voss, A.~Radford, D.~Amodei, and P.~F. Christiano.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3008--3021, 2020.

\bibitem[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love, et~al.]{team2024gemma}
G.~Team, T.~Mesnard, C.~Hardin, R.~Dadashi, S.~Bhupatiraju, S.~Pathak, L.~Sifre, M.~Rivi{\`e}re, M.~S. Kale, J.~Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}, 2024.

\bibitem[Tu et~al.(2023)Tu, Ma, and Zhang]{tu2023causal}
R.~Tu, C.~Ma, and C.~Zhang.
\newblock Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis.
\newblock \emph{arXiv preprint arXiv:2301.13819}, 2023.

\bibitem[Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib, et~al.]{tunstall2023zephyr}
L.~Tunstall, E.~Beeching, N.~Lambert, N.~Rajani, K.~Rasul, Y.~Belkada, S.~Huang, L.~von Werra, C.~Fourrier, N.~Habib, et~al.
\newblock Zephyr: Direct distillation of lm alignment.
\newblock \emph{arXiv preprint arXiv:2310.16944}, 2023.

\bibitem[Wang et~al.(2025)Wang, Zhao, Jiang, Chen, Zhu, Chen, Liu, Zhang, Fan, Ma, et~al.]{wang2025beyond}
C.~Wang, Z.~Zhao, Y.~Jiang, Z.~Chen, C.~Zhu, Y.~Chen, J.~Liu, L.~Zhang, X.~Fan, H.~Ma, et~al.
\newblock Beyond reward hacking: Causal rewards for large language model alignment.
\newblock \emph{arXiv preprint arXiv:2501.09620}, 2025.

\bibitem[Wang et~al.(2024)Wang, Xiong, Xie, Zhao, and Zhang]{wang2024interpretable}
H.~Wang, W.~Xiong, T.~Xie, H.~Zhao, and T.~Zhang.
\newblock Interpretable preferences via multi-objective reward modeling and mixture-of-experts.
\newblock \emph{arXiv preprint arXiv:2406.12845}, 2024.

\bibitem[Wu et~al.(2025)Wu, Yasunaga, Cohen, Kim, Celikyilmaz, and Ghazvininejad]{wu2025rewordbench}
Z.~Wu, M.~Yasunaga, A.~Cohen, Y.~Kim, A.~Celikyilmaz, and M.~Ghazvininejad.
\newblock rewordbench: Benchmarking and improving the robustness of reward models with transformed inputs.
\newblock \emph{arXiv preprint arXiv:2503.11751}, 2025.

\bibitem[Yang et~al.(2024{\natexlab{a}})Yang, Yang, Zhang, Hui, Zheng, Yu, Li, Liu, Huang, Wei, et~al.]{yang2024qwen2}
A.~Yang, B.~Yang, B.~Zhang, B.~Hui, B.~Zheng, B.~Yu, C.~Li, D.~Liu, F.~Huang, H.~Wei, et~al.
\newblock Qwen2. 5 technical report.
\newblock \emph{arXiv preprint arXiv:2412.15115}, 2024{\natexlab{a}}.

\bibitem[Yang et~al.(2024{\natexlab{b}})Yang, Pan, Luo, Qiu, Zhong, Yu, and Chen]{yang2024rewards}
R.~Yang, X.~Pan, F.~Luo, S.~Qiu, H.~Zhong, D.~Yu, and J.~Chen.
\newblock Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment.
\newblock \emph{arXiv preprint arXiv:2402.10207}, 2024{\natexlab{b}}.

\bibitem[Zhang et~al.(2024)Zhang, Xiong, Chen, Zhou, Huang, and Zhang]{zhang2024lists}
X.~Zhang, W.~Xiong, L.~Chen, T.~Zhou, H.~Huang, and T.~Zhang.
\newblock From lists to emojis: How format bias affects model alignment.
\newblock \emph{arXiv preprint arXiv:2409.11704}, 2024.

\bibitem[Zhao et~al.(2025)Zhao, Liu, Zhang, Zhou, Gao, Li, Lyu, Qian, Qi, Li, et~al.]{zhao2025genprm}
J.~Zhao, R.~Liu, K.~Zhang, Z.~Zhou, J.~Gao, D.~Li, J.~Lyu, Z.~Qian, B.~Qi, X.~Li, et~al.
\newblock Genprm: Scaling test-time compute of process reward models via generative reasoning.
\newblock \emph{arXiv preprint arXiv:2504.00891}, 2025.

\bibitem[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{zhao2023slic}
Y.~Zhao, R.~Joshi, T.~Liu, M.~Khalman, M.~Saleh, and P.~J. Liu.
\newblock Slic-hf: Sequence likelihood calibration with human feedback.
\newblock \emph{arXiv preprint arXiv:2305.10425}, 2023.

\bibitem[Zhu et~al.(2025)Zhu, Tan, Chen, Sennrich, Zhang, and Hu]{zhu2025charm}
X.~Zhu, C.~Tan, P.~Chen, R.~Sennrich, Y.~Zhang, and H.~Hu.
\newblock Charm: Calibrating reward models with chatbot arena scores.
\newblock \emph{arXiv preprint arXiv:2504.10045}, 2025.

\end{thebibliography}
