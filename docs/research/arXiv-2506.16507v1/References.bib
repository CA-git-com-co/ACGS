@article{liu2023trustworthy,
  title={Trustworthy llms: a survey and guideline for evaluating large language models' alignment},
  author={Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Guo, Ruocheng and Cheng, Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
  journal={arXiv preprint arXiv:2308.05374},
  year={2023}
}

@article{negahban2009unified,
  title={A unified framework for high-dimensional analysis of $ m $-estimators with decomposable regularizers},
  author={Negahban, Sahand and Yu, Bin and Wainwright, Martin J and Ravikumar, Pradeep},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}

@article{liu2024rrm,
  title={Rrm: Robust reward model training mitigates reward hacking},
  author={Liu, Tianqi and Xiong, Wei and Ren, Jie and Chen, Lichang and Wu, Junru and Joshi, Rishabh and Gao, Yang and Shen, Jiaming and Qin, Zhen and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2409.13156},
  year={2024}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kiciman2023causal,
  title={Causal reasoning and large language models: Opening a new frontier for causality},
  author={Kiciman, Emre and Ness, Robert and Sharma, Amit and Tan, Chenhao},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@article{cui2023ultrafeedback,
  title={Ultrafeedback: Boosting language models with high-quality feedback},
  author={Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and Zhu, Wei and Ni, Yuan and Xie, Guotong and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2310.01377},
  year={2023}
}

@article{liu2024datasets,
  title={Datasets for large language models: A comprehensive survey},
  author={Liu, Yang and Cao, Jiahuan and Liu, Chongyu and Ding, Kai and Jin, Lianwen},
  journal={arXiv preprint arXiv:2402.18041},
  year={2024}
}

@article{yang2024llm,
  title={Llm voting: Human choices and ai collective decision making},
  author={Yang, Joshua C and Korecki, Marcin and Dailisan, Damian and Hausladen, Carina I and Helbing, Dirk},
  journal={arXiv preprint arXiv:2402.01766},
  year={2024}
}

@article{yuan2024advancing,
  title={Advancing llm reasoning generalists with preference trees},
  author={Yuan, Lifan and Cui, Ganqu and Wang, Hanbin and Ding, Ning and Wang, Xingyao and Deng, Jia and Shan, Boji and Chen, Huimin and Xie, Ruobing and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2404.02078},
  year={2024}
}

@article{rame2024warm,
  title={Warm: On the benefits of weight averaged reward models},
  author={Ram{\'e}, Alexandre and Vieillard, Nino and Hussenot, L{\'e}onard and Dadashi, Robert and Cideron, Geoffrey and Bachem, Olivier and Ferret, Johan},
  journal={arXiv preprint arXiv:2401.12187},
  year={2024}
}

@article{singhal2023long,
  title={A long way to go: Investigating length correlations in rlhf},
  author={Singhal, Prasann and Goyal, Tanya and Xu, Jiacheng and Durrett, Greg},
  journal={arXiv preprint arXiv:2310.03716},
  year={2023}
}

@article{pace2024west,
  title={West-of-n: Synthetic preference generation for improved reward modeling},
  author={Pace, Aliz{\'e}e and Mallinson, Jonathan and Malmi, Eric and Krause, Sebastian and Severyn, Aliaksei},
  journal={arXiv preprint arXiv:2401.12086},
  year={2024}
}

@article{shen2023trickle,
  title={The trickle-down impact of reward (in-) consistency on rlhf},
  author={Shen, Lingfeng and Chen, Sihao and Song, Linfeng and Jin, Lifeng and Peng, Baolin and Mi, Haitao and Khashabi, Daniel and Yu, Dong},
  journal={arXiv preprint arXiv:2309.16155},
  year={2023}
}

@article{coste2023reward,
  title={Reward model ensembles help mitigate overoptimization},
  author={Coste, Thomas and Anwar, Usman and Kirk, Robert and Krueger, David},
  journal={arXiv preprint arXiv:2310.02743},
  year={2023}
}

@article{long2023causal,
  title={Causal discovery with language models as imperfect experts},
  author={Long, Stephanie and Pich{\'e}, Alexandre and Zantedeschi, Valentina and Schuster, Tibor and Drouin, Alexandre},
  journal={arXiv preprint arXiv:2307.02390},
  year={2023}
}

@article{chi2024unveiling,
  title={Unveiling causal reasoning in large language models: Reality or mirage?},
  author={Chi, Haoang and Li, He and Yang, Wenjing and Liu, Feng and Lan, Long and Ren, Xiaoguang and Liu, Tongliang and Han, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={96640--96670},
  year={2024}
}

@article{skalse2022defining,
  title={Defining and characterizing reward gaming},
  author={Skalse, Joar and Howe, Nikolaus and Krasheninnikov, Dmitrii and Krueger, David},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9460--9471},
  year={2022}
}

@misc{pan2022effectsrewardmisspecificationmapping,
      title={The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models}, 
      author={Alexander Pan and Kush Bhatia and Jacob Steinhardt},
      year={2022},
      eprint={2201.03544},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.03544}, 
}

@article{denison2024sycophancy,
  title={Sycophancy to subterfuge: Investigating reward-tampering in large language models},
  author={Denison, Carson and MacDiarmid, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and others},
  journal={arXiv preprint arXiv:2406.10162},
  year={2024}
}

@article{khalifa2025process,
  title={Process Reward Models That Think},
  author={Khalifa, Muhammad and Agarwal, Rishabh and Logeswaran, Lajanugen and Kim, Jaekyeom and Peng, Hao and Lee, Moontae and Lee, Honglak and Wang, Lu},
  journal={arXiv preprint arXiv:2504.16828},
  year={2025}
}

@article{zhao2025genprm,
  title={Genprm: Scaling test-time compute of process reward models via generative reasoning},
  author={Zhao, Jian and Liu, Runze and Zhang, Kaiyan and Zhou, Zhimu and Gao, Junqi and Li, Dong and Lyu, Jiafei and Qian, Zhouyi and Qi, Biqing and Li, Xiu and others},
  journal={arXiv preprint arXiv:2504.00891},
  year={2025}
}

@article{li2024process,
  title={Process reward model with q-value rankings},
  author={Li, Wendi and Li, Yixuan},
  journal={arXiv preprint arXiv:2410.11287},
  year={2024}
}

@article{lou2024uncertainty,
  title={Uncertainty-aware reward model: Teaching reward models to know what is unknown},
  author={Lou, Xingzhou and Yan, Dong and Shen, Wei and Yan, Yuzi and Xie, Jian and Zhang, Junge},
  journal={arXiv preprint arXiv:2410.00847},
  year={2024}
}

@article{liu2025pairwise,
  title={Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament},
  author={Liu, Yantao and Yao, Zijun and Min, Rui and Cao, Yixin and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2501.13007},
  year={2025}
}

@article{zhu2025charm,
  title={CHARM: Calibrating Reward Models With Chatbot Arena Scores},
  author={Zhu, Xiao and Tan, Chenmien and Chen, Pinzhen and Sennrich, Rico and Zhang, Yanlin and Hu, Hanxu},
  journal={arXiv preprint arXiv:2504.10045},
  year={2025}
}

@book{pearl2009causality,
  title={Causality},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge university press}
}

@book{peters2017elements,
  title={Elements of causal inference: foundations and learning algorithms},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={The MIT Press}
}

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@article{chen2024odin,
  title={Odin: Disentangled reward mitigates hacking in rlhf},
  author={Chen, Lichang and Zhu, Chen and Soselia, Davit and Chen, Jiuhai and Zhou, Tianyi and Goldstein, Tom and Huang, Heng and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2402.07319},
  year={2024}
}

@article{eisenstein2023helping,
  title={Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking},
  author={Eisenstein, Jacob and Nagpal, Chirag and Agarwal, Alekh and Beirami, Ahmad and D'Amour, Alex and Dvijotham, DJ and Fisch, Adam and Heller, Katherine and Pfohl, Stephen and Ramachandran, Deepak and others},
  journal={arXiv preprint arXiv:2312.09244},
  year={2023}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@article{tu2023causal,
  title={Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis},
  author={Tu, Ruibo and Ma, Chao and Zhang, Cheng},
  journal={arXiv preprint arXiv:2301.13819},
  year={2023}
}

@article{feder2022causal,
  title={Causal inference in natural language processing: Estimation, prediction, interpretation and beyond},
  author={Feder, Amir and Keith, Katherine A and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={1138--1158},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}

@article{jiang2023llm,
  title={Llm-blender: Ensembling large language models with pairwise ranking and generative fusion},
  author={Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2306.02561},
  year={2023}
}

@article{qin2023large,
  title={Large language models are effective text rankers with pairwise ranking prompting},
  author={Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Yan, Le and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2306.17563},
  year={2023}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{yang2024rewards,
  title={Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment},
  author={Yang, Rui and Pan, Xiaoman and Luo, Feng and Qiu, Shuang and Zhong, Han and Yu, Dong and Chen, Jianshu},
  journal={arXiv preprint arXiv:2402.10207},
  year={2024}
}

@article{wang2024interpretable,
  title={Interpretable preferences via multi-objective reward modeling and mixture-of-experts},
  author={Wang, Haoxiang and Xiong, Wei and Xie, Tengyang and Zhao, Han and Zhang, Tong},
  journal={arXiv preprint arXiv:2406.12845},
  year={2024}
}

@article{dong2024rlhf,
  title={Rlhf workflow: From reward modeling to online rlhf},
  author={Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong},
  journal={arXiv preprint arXiv:2405.07863},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{gupta2024swepo,
  title={SWEPO: Simultaneous Weighted Preference Optimization for Group Contrastive Alignment},
  author={Gupta, Taneesh and Madhavan, Rahul and Zhang, Xuchao and Bansal, Chetan and Rajmohan, Saravan},
  journal={arXiv preprint arXiv:2412.04628},
  year={2024}
}

@article{wu2025rewordbench,
  title={rewordbench: Benchmarking and improving the robustness of reward models with transformed inputs},
  author={Wu, Zhaofeng and Yasunaga, Michihiro and Cohen, Andrew and Kim, Yoon and Celikyilmaz, Asli and Ghazvininejad, Marjan},
  journal={arXiv preprint arXiv:2503.11751},
  year={2025}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  month = {5},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@article{wang2023rlhf,
  title={Is rlhf more difficult than standard rl? a theoretical perspective},
  author={Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={76006--76032},
  year={2023}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@misc{gupta2025carmodynamiccriteriageneration,
      title={CARMO: Dynamic Criteria Generation for Context-Aware Reward Modelling}, 
      author={Taneesh Gupta and Shivam Shandilya and Xuchao Zhang and Rahul Madhavan and Supriyo Ghosh and Chetan Bansal and Huaxiu Yao and Saravan Rajmohan},
      year={2025},
      eprint={2410.21545},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21545}, 
}

@article{xu2024dpo,
  title={Is dpo superior to ppo for llm alignment? a comprehensive study},
  author={Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
  journal={arXiv preprint arXiv:2404.10719},
  year={2024}
}

@article{lambert2024rewardbench,
  title={Rewardbench: Evaluating reward models for language modeling},
  author={Lambert, Nathan and Pyatkin, Valentina and Morrison, Jacob and Miranda, LJ and Lin, Bill Yuchen and Chandu, Khyathi and Dziri, Nouha and Kumar, Sachin and Zick, Tom and Choi, Yejin and others},
  journal={arXiv preprint arXiv:2403.13787},
  year={2024}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{gao2021simcse,
  title={Simcse: Simple contrastive learning of sentence embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  journal={arXiv preprint arXiv:2104.08821},
  year={2021}
}

@article{chen2024noise,
  title={Noise contrastive alignment of language models with explicit rewards},
  author={Chen, Huayu and He, Guande and Yuan, Lifan and Cui, Ganqu and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2402.05369},
  year={2024}
}

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@inproceedings{song2024preference,
  title={Preference ranking optimization for human alignment},
  author={Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={18990--18998},
  year={2024}
}

@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004}
}

@article{klein2024contrastive,
  title={Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models},
  author={Klein, Tassilo and Nabi, Moin},
  journal={arXiv preprint arXiv:2401.08491},
  year={2024}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@article{cheng2023adversarial,
  title={Adversarial preference optimization},
  author={Cheng, Pengyu and Yang, Yifan and Li, Jian and Dai, Yong and Du, Nan},
  journal={arXiv preprint arXiv:2311.08045},
  year={2023}
}

@article{liu2023statistical,
  title={Statistical rejection sampling improves preference optimization},
  author={Liu, Tianqi and Zhao, Yao and Joshi, Rishabh and Khalman, Misha and Saleh, Mohammad and Liu, Peter J and Liu, Jialu},
  journal={arXiv preprint arXiv:2309.06657},
  year={2023}
}

@article{tang2024generalized,
  title={Generalized preference optimization: A unified approach to offline alignment},
  author={Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, R{\'e}mi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo {\'A}vila and Piot, Bilal},
  journal={arXiv preprint arXiv:2402.05749},
  year={2024}
}

@inproceedings{joachims2002optimizing,
  title={Optimizing search engines using clickthrough data},
  author={Joachims, Thorsten},
  booktitle={Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={133--142},
  year={2002}
}

@article{schulman2015trust,
  title={Trust Region Policy Optimization},
  author={Schulman, John},
  journal={arXiv preprint arXiv:1502.05477},
  year={2015}
}

@article{ranzato2015sequence,
  title={Sequence level training with recurrent neural networks},
  author={Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1511.06732},
  year={2015}
}

@inproceedings{azar2024general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}

@article{wu2024self,
  title={Self-play preference optimization for language model alignment},
  author={Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
  journal={arXiv preprint arXiv:2405.00675},
  year={2024}
}

@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={international conference on machine learning},
  pages={1311--1320},
  year={2017},
  organization={Pmlr}
}

@article{cui2024or,
  title={OR-Bench: An Over-Refusal Benchmark for Large Language Models},
  author={Cui, Justin and Chiang, Wei-Lin and Stoica, Ion and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2405.20947},
  year={2024}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{arditi2024refusal,
  title={Refusal in language models is mediated by a single direction},
  author={Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11717},
  year={2024}
}

@article{long2024llms,
  title={LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs},
  author={Long, Do Xuan and Ngoc, Hai Nguyen and Sim, Tiviatis and Dao, Hieu and Joty, Shafiq and Kawaguchi, Kenji and Chen, Nancy F and Kan, Min-Yen},
  journal={arXiv preprint arXiv:2408.08656},
  year={2024}
}

@inproceedings{zhou2024beyond,
  title={Beyond one-preference-fits-all alignment: Multi-objective direct preference optimization},
  author={Zhou, Zhanhui and Liu, Jie and Shao, Jing and Yue, Xiangyu and Yang, Chao and Ouyang, Wanli and Qiao, Yu},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={10586--10613},
  year={2024}
}

@article{zhang2024lists,
  title={From lists to emojis: How format bias affects model alignment},
  author={Zhang, Xuanchang and Xiong, Wei and Chen, Lichang and Zhou, Tianyi and Huang, Heng and Zhang, Tong},
  journal={arXiv preprint arXiv:2409.11704},
  year={2024}
}


@article{wolf2023fundamental,
  title={Fundamental limitations of alignment in large language models},
  author={Wolf, Yotam and Wies, Noam and Avnery, Oshri and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2304.11082},
  year={2023}
}

@article{tao2024cultural,
  title={Cultural bias and cultural alignment of large language models},
  author={Tao, Yan and Viberg, Olga and Baker, Ryan S and Kizilcec, Ren{\'e} F},
  journal={PNAS nexus},
  volume={3},
  number={9},
  pages={pgae346},
  year={2024},
  publisher={Oxford University Press US}
}

@inproceedings{hu2017toward,
  title={Toward controlled generation of text},
  author={Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2017},
  organization={PMLR}
}

@article{zhou2023beyond,
  title={Beyond one-preference-for-all: Multi-objective direct preference optimization},
  author={Zhou, Zhanhui and Liu, Jie and Yang, Chao and Shao, Jing and Liu, Yu and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu},
  journal={arXiv preprint arXiv:2310.03708},
  year={2023}
}

@article{wang2024preference,
  title={Preference Optimization with Multi-Sample Comparisons},
  author={Wang, Chaoqi and Zhao, Zhuokai and Zhu, Chen and Sankararaman, Karthik Abinav and Valko, Michal and Cao, Xuefei and Chen, Zhaorun and Khabsa, Madian and Chen, Yuxin and Ma, Hao and others},
  journal={arXiv preprint arXiv:2410.12138},
  year={2024}
}

@article{dathathri2019plug,
  title={Plug and play language models: A simple approach to controlled text generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  journal={arXiv preprint arXiv:1912.02164},
  year={2019}
}

@article{xu2021detoxifying,
  title={Detoxifying language models risks marginalizing minority voices},
  author={Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
  journal={arXiv preprint arXiv:2104.06390},
  year={2021}
}

@article{krause2020gedi,
  title={Gedi: Generative discriminator guided sequence generation},
  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  journal={arXiv preprint arXiv:2009.06367},
  year={2020}
}



@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{reber2024rate,
  title={RATE: Score Reward Models with Imperfect Rewrites of Rewrites},
  author={Reber, David and Richardson, Sean and Nief, Todd and Garbacea, Cristina and Veitch, Victor},
  journal={arXiv preprint arXiv:2410.11348},
  year={2024}
}

@article{wang2025beyond,
  title={Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment},
  author={Wang, Chaoqi and Zhao, Zhuokai and Jiang, Yibo and Chen, Zhaorun and Zhu, Chen and Chen, Yuxin and Liu, Jiayi and Zhang, Lizhu and Fan, Xiangjun and Ma, Hao and others},
  journal={arXiv preprint arXiv:2501.09620},
  year={2025}
}

@article{shen2024boosting,
  title={Boosting reward model with preference-conditional multi-aspect synthetic data generation},
  author={Shen, Jiaming and Xu, Ran and Jun, Yennie and Qin, Zhen and Liu, Tianqi and Yang, Carl and Liang, Yi and Baumgartner, Simon and Bendersky, Michael},
  journal={arXiv preprint arXiv:2407.16008},
  year={2024}
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{zhao2023slic,
  title={Slic-hf: Sequence likelihood calibration with human feedback},
  author={Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J},
  journal={arXiv preprint arXiv:2305.10425},
  year={2023}
}

@misc{ravfogel2025gumbelcounterfactualgenerationlanguage,
      title={Gumbel Counterfactual Generation From Language Models}, 
      author={Shauli Ravfogel and Anej Svete and Vésteinn Snæbjarnarson and Ryan Cotterell},
      year={2025},
      eprint={2411.07180},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.07180}, 
}

@article{qiang2024prompt,
  title={Prompt perturbation consistency learning for robust language models},
  author={Qiang, Yao and Nandi, Subhrangshu and Mehrabi, Ninareh and Steeg, Greg Ver and Kumar, Anoop and Rumshisky, Anna and Galstyan, Aram},
  journal={arXiv preprint arXiv:2402.15833},
  year={2024}
}

@article{feder2021causalm,
  title={CausaLM: Causal model explanation through counterfactual language models},
  author={Feder, Amir and Oved, Nadav and Shalit, Uri and Reichart, Roi},
  journal={Computational Linguistics},
  volume={47},
  number={2},
  pages={333--386},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{mishra2024llm,
  title={Llm-guided counterfactual data generation for fairer ai},
  author={Mishra, Ashish and Nayak, Gyanaranjan and Bhattacharya, Suparna and Kumar, Tarun and Shah, Arpit and Foltin, Martin},
  booktitle={Companion Proceedings of the ACM Web Conference 2024},
  pages={1538--1545},
  year={2024}
}

@article{keskar2019ctrl,
  title={Ctrl: A conditional transformer language model for controllable generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@article{kaushik2019learning,
  title={Learning the difference that makes a difference with counterfactually-augmented data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C},
  journal={arXiv preprint arXiv:1909.12434},
  year={2019}
}

@article{liu2021dexperts,
  title={DExperts: Decoding-time controlled text generation with experts and anti-experts},
  author={Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A and Choi, Yejin},
  journal={arXiv preprint arXiv:2105.03023},
  year={2021}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint ArXiv:2005.14165},
  year={2020}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@article{dai2022can,
  title={Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{wang2020generalizing,
  title={Generalizing from a few examples: A survey on few-shot learning},
  author={Wang, Yaqing and Yao, Quanming and Kwok, James T and Ni, Lionel M},
  journal={ACM computing surveys (csur)},
  volume={53},
  number={3},
  pages={1--34},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{ravi2016optimization,
  title={Optimization as a model for few-shot learning},
  author={Ravi, Sachin and Larochelle, Hugo},
  booktitle={International conference on learning representations},
  year={2016}
}

@article{parnami2022learning,
  title={Learning from few examples: A summary of approaches to few-shot learning},
  author={Parnami, Archit and Lee, Minwoo},
  journal={arXiv preprint arXiv:2203.04291},
  year={2022}
}

@article{cahyawijaya2024llms,
  title={LLMs Are Few-Shot In-Context Low-Resource Language Learners},
  author={Cahyawijaya, Samuel and Lovenia, Holy and Fung, Pascale},
  journal={arXiv preprint arXiv:2403.16512},
  year={2024}
}

@article{tunstall2022efficient,
  title={Efficient few-shot learning without prompts},
  author={Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  journal={arXiv preprint arXiv:2209.11055},
  year={2022}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}



@article{peters2019tune,
  title={To tune or not to tune? adapting pretrained representations to diverse tasks},
  author={Peters, Matthew E and Ruder, Sebastian and Smith, Noah A},
  journal={arXiv preprint arXiv:1903.05987},
  year={2019}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@article{li2017learning,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}


@article{guo2024controllable,
  title={Controllable preference optimization: Toward controllable multi-objective alignment},
  author={Guo, Yiju and Cui, Ganqu and Yuan, Lifan and Ding, Ning and Sun, Zexu and Sun, Bowen and Chen, Huimin and Xie, Ruobing and Zhou, Jie and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2402.19085},
  year={2024}
}

@inproceedings{hong2024orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={11170--11189},
  year={2024}
}

@article{yuan2024following,
  title={Following length constraints in instructions},
  author={Yuan, Weizhe and Kulikov, Ilia and Yu, Ping and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Weston, Jason and Xu, Jing},
  journal={arXiv preprint arXiv:2406.17744},
  year={2024}
}

@article{pang2024iterative,
  title={Iterative reasoning preference optimization},
  author={Pang, Richard Yuanzhe and Yuan, Weizhe and Cho, Kyunghyun and He, He and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2404.19733},
  year={2024}
}

@article{qi2024online,
  title={Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing},
  author={Qi, Biqing and Li, Pengfei and Li, Fangyuan and Gao, Junqi and Zhang, Kaiyan and Zhou, Bowen},
  journal={arXiv preprint arXiv:2406.05534},
  year={2024}
}

@article{liu2024iterative,
  title={Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level},
  author={Liu, Jie and Zhou, Zhanhui and Liu, Jiaheng and Bu, Xingyuan and Yang, Chao and Zhong, Han-Sen and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2406.11817},
  year={2024}
}

@article{kim2024sdpo,
  title={sDPO: Don't Use Your Data All at Once},
  author={Kim, Dahyun and Kim, Yungi and Song, Wonho and Kim, Hyeonwoo and Kim, Yunsu and Kim, Sanghoon and Park, Chanjun},
  journal={arXiv preprint arXiv:2403.19270},
  year={2024}
}

@article{dong2023raft,
  title={Raft: Reward ranked finetuning for generative foundation model alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  journal={arXiv preprint arXiv:2304.06767},
  year={2023}
}

@article{yuan2023rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}

@article{chen2024self,
  title={Self-play fine-tuning converts weak language models to strong language models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:2401.01335},
  year={2024}
}

@article{meng2024simpo,
  title={Simpo: Simple preference optimization with a reference-free reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734},
  year={2024}
}

@article{xu2024contrastive,
  title   = {Contrastive Preference Optimization: Pushing the Boundaries of {LLM} Performance in Machine Translation},
  author  = {Haoran Xu and Amr Sharaf and Yunmo Chen and Weiting Tan and Lingfeng Shen and Benjamin Van Durme and Kenton Murray and Young Jin Kim},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2401.08417}
}



@article{zeng2024token,
  title={Token-level Direct Preference Optimization},
  author={Zeng, Yongcheng and Liu, Guoqing and Ma, Weiyu and Yang, Ning and Zhang, Haifeng and Wang, Jun},
  journal={arXiv preprint arXiv:2404.11999},
  year={2024}
}

@article{park2024disentangling,
  title={Disentangling length from quality in direct preference optimization},
  author={Park, Ryan and Rafailov, Rafael and Ermon, Stefano and Finn, Chelsea},
  journal={arXiv preprint arXiv:2403.19159},
  year={2024}
}


@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@techreport{tran2023iterative,
  title={Iterative dpo alignment},
  author={Tran, Hoang and Glaze, Chris and Hancock, Braden},
  year={2023},
  institution={Technical report, Snorkel AI}
}


@article{li2022domain,
  title={Domain generalization using pretrained models without fine-tuning},
  author={Li, Ziyue and Ren, Kan and Jiang, Xinyang and Li, Bo and Zhang, Haipeng and Li, Dongsheng},
  journal={arXiv preprint arXiv:2203.04600},
  year={2022}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{he2021towards,
  title={Towards a unified view of parameter-efficient transfer learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2021}
}

@article{pfeiffer2020adapterhub,
  title={Adapterhub: A framework for adapting transformers},
  author={Pfeiffer, Jonas and R{\"u}ckl{\'e}, Andreas and Poth, Clifton and Kamath, Aishwarya and Vuli{\'c}, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2007.07779},
  year={2020}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{pfeiffer2020adapterfusion,
  title={Adapterfusion: Non-destructive task composition for transfer learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}

@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{dubois2024length,
  title={Length-controlled alpacaeval: A simple way to debias automatic evaluators},
  author={Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2404.04475},
  year={2024}
}

@article{ding2023enhancing,
  title={Enhancing chat language models by scaling high-quality instructional conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}
@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{mirzadeh2024gsm,
  title={Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}
@misc{wildguard2024,
      title={WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs}, 
      author={Seungju Han and Kavel Rao and Allyson Ettinger and Liwei Jiang and Bill Yuchen Lin and Nathan Lambert and Yejin Choi and Nouha Dziri},
      year={2024},
      eprint={2406.18495},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18495}, 
}
@article{taori2020measuring,
  title={Measuring robustness to natural distribution shifts in image classification},
  author={Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18583--18599},
  year={2020}
}