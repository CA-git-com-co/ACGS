\subsection{Our Contributions}
\label{subsec:our_contributions}

In this work, we address the persistent challenge of \emph{reward hacking} in large language model alignment by proposing \textbf{\carma{} (Causal Augmentation to disentangle Reward Modeling Attributes)}. Our main contributions are as follows:

\begin{itemize}[left=2pt,itemsep=0pt]
    \item \textbf{A Principled Causal Framework for Reward Modeling.}
    We propose a principled causal framework (Section~\ref{sec:preliminaries}, Figure~\ref{fig:causal_graph}) that explicitly models answer generation, distinguishing fundamental \emph{causal} drivers of quality (e.g., factual correctness, logical coherence) from \emph{spurious} correlates (e.g., stylistic choices, formatting). This framework underpins \carma{}'s design to train reward models that are sensitive to genuine quality and robust to superficial cues.

    \item \textbf{Targeted Data Augmentation via Counterfactuals.}
    Leveraging our causal model, we develop targeted data augmentation strategies (Section~\ref{sec:methodology}) using LLM-generated counterfactuals. These comprise: (1) \emph{Causal Augmentations}, creating minimal pairs that isolate changes in specific causal attributes to instill sensitivity to true quality differences, and (2) \emph{Neutral Augmentations}, varying spurious attributes while preserving causal content and using tie-labels to enforce invariance. We detail a systematic generation and filtering process for these informative examples.

    \item \textbf{Theoretical Analysis of Reward Hacking Mitigation.}
    We present a formal theoretical analysis (Section~\ref{sec:theory}) demonstrating that training with \carma{}'s augmentation scheme guides the reward model towards causal faithfulness. Under defined data coverage and optimization assumptions, our analysis shows this process discourages reliance on spurious attributes, and proves that an RM optimally fitting the augmented data would be robust against reward hacking via superficial manipulations.

    \item \textbf{State-of-the-Art Empirical Performance on Reward Benchmarks.}
    We conduct comprehensive experiments on \textbf{RewardBench} (Section~\ref{sec:experiments}), demonstrating that \carma{} significantly outperforms both standard reward models and the strong RRM baseline \citep{liu2024rrm}. Our approach yields substantial accuracy improvements across diverse categories, notably increasing Safety accuracy by up to \textbf{13.4\%} and Reasoning accuracy by up to \textbf{5.4\%} points (Tables~\ref{tab:results_pairpm_rewardbench}, \ref{tab:results_bt_rewardbench}).

    \item \textbf{Downstream Alignment Benefits.}
    Beyond static preference accuracy, we show that selecting Best of N (BoN)  responses from the policy model based on our reward model gives performance improvement of up to 10\% on AlpacaEval. Furthermore, policies trained via direct preference optimization (DPO) using \carma{}-based reward models exhibit less reward hacking tendencies and more robust performance, indicating the practical alignment advantages of our approach.

\end{itemize}
