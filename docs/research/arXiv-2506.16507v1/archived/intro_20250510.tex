\section{Introduction}
\label{sec:introduction}

Aligning Large Language Models (LLMs) with human preferences is paramount for their safe and effective deployment. Reinforcement Learning from Human Feedback (RLHF) methods relying on reward models (RMs) have emerged as the dominant paradigm for this task \citep{christiano2017deep, ouyang2022training, bai2022training,schulman2017proximal,shao2024deepseekmath}. 
This learned RM serves as a proxy for human judgment, guiding policy optimization via algorithms like PPO \citep{schulman2017proximal} or through direct preference optimization methods like DPO \citep{rafailov2024direct}. The fidelity and robustness of the RM are critical, as flaws directly propagate to the aligned policy \citep{casper2023open}.

However, standard RM training faces a significant challenge --- \textit{reward hacking} \citep{gao2023scaling, skalse2022defining}. RMs often learn to assign high scores based on superficial or spurious attributesâ€”such as response length \citep{singhal2023long}, specific formatting patterns \citep{zhang2024lists}, or stylistic quirks, mainly because these features are statistically correlated with preferred responses in the training dataset. This occurs because standard training objectives, like minimizing pairwise preference prediction error, do not explicitly require the RM to disentangle the true \textit{causal} drivers of response quality (e.g., factuality, relevance, coherence) from these spurious correlates. Consequently, RMs may become brittle, exhibiting poor generalization and enabling policies to maximize predicted reward via superficial manipulations rather than genuine quality improvements \citep{shen2023trickle, eisenstein2023helping}.

To address spurious correlations in reward modeling, recent works have explored several methods for robustness, sometimes drawing on causal concepts. The \rewordbench{} benchmark \citep{wu2025rewordbench} highlighted the brittleness of RMs to meaning-preserving input transformations and proposed regularizing RMs for consistency on paraphrased inputs as a mitigation strategy. \citet{liu2024rrm} introduced \rrm, employing data augmentation with non-contextual and query-independent response pairs to disrupt certain spurious associations learned from the context. Concurrently, attribute-based evaluation, such as dynamically generating criteria via LLMs as in \carmo{} \citep{gupta2025carmodynamiccriteriageneration}, aims to provide more grounded reward signals by focusing on specific quality dimensions.

However, these approaches exhibit certain limitations. Paraphrase regularization \citep{wu2025rewordbench} and MMD regularization for length bias and sycophantic bias \citep{wang2025beyond}, address only specific types of spurious variation; moreover, the complete set of spurious correlates may not be specifiable a priori. Furthermore, the method does not explicitly amplify the signal from underlying causal quality attributes.
Conversely, methods like RATE \citep{reber2024rate} emphasize causal estimation via ``rewrite of rewrites'', rather than directly training robust RMs through augmentation with causal and neutrality constraints.
\rrm's augmentation strategy, while improving general robustness, operates at a coarse level (swapping entire responses) and lacks fine-grained control to isolate the effects of specific causal or spurious attributes within a response. \carmo, focusing on criteria generation for evaluation, relies heavily on large LLMs at inference time, potentially incurring significant costs, and does not directly intervene on the RM's training data distribution using counterfactuals to enforce desired robustness properties like invariance. There remains a need for a method that systematically leverages a causal understanding of preference formation to train RMs that are directly sensitive to causal factors and robustly invariant to a wider range of spurious ones.


To address this gap, we propose \textbf{\carma{}} (Causal Attribute-aware Reward Modeling), a novel framework grounded in the explicit causal model of answer generation and evaluation introduced earlier (Section \ref{sec:preliminaries}, Figure \ref{fig:causal_graph}). The core idea of \carma{} is to directly teach the reward model to differentiate between genuine quality drivers and superficial cues by augmenting the standard preference dataset with targeted, LLM-generated counterfactual examples. This involves creating two key types of synthetic training pairs: (1) \textit{Causal Augmentation}, which generates minimally different responses that isolate changes along specific attributes identified as \textit{causal} (like factuality or relevance), thereby explicitly teaching the model sensitivity to genuine quality improvements or degradations. (2) \textit{Neutral Augmentation}, which produces pairs that vary primarily in \textit{spurious} ways (such as style or formatting) while preserving the core causal content, explicitly training the model for invariance to these superficial changes via tie-labels. By training on this enriched dataset using a modified preference loss (detailed in Section \ref{sec:methodology}), \carma{} learns a reward function that aligns more closely with the underlying causal factors of quality. 
Our empirical evaluations demonstrate that \carma{} leads to significantly improved robustness, boosting average accuracy on RewardBench by up to 4.5 percentage points over standard baselines, with significant gains in safety and reasoning capabilities.

The remainder of this paper details our causal framework (Section \ref{sec:preliminaries}), the \carma{} methodology (Section \ref{sec:methodology}), theoretical justifications (Section \ref{sec:theory}), and comprehensive experimental results (Section \ref{sec:experiments}) which address key \hyperref[list:research_questions]{research questions} towards robust reward modeling.
