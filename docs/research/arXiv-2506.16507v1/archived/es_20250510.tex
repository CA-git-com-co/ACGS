\subsection{Experimental Settings}
\label{subsec:experimental_settings}

\paragraph{Datasets.}
Our primary source of human preference data ($\mathcal{D}_{pref}$) is the Ultrafeedback dataset \cite{cui2023ultrafeedback}, containing approximately 60k preference pairs across diverse domains. For checking robustness to training data, we provide ablations with training data generated from the UltraInteract dataset \cite{yuan2024advancing} as well. The data augmentation process, following Section \ref{sec:methodology}, employs Gemini 2.0 Flash to identify $\ell=5$ causal attributes and subsequently generate causal upgrade/degradation pairs targeting these attributes, alongside neutral pairs using our reconstruction method. This augmented data $\mathcal{D}_{aug}$ is then filtered using a model-based confidence filter with threshold $\tau=0.2$, applied to a baseline RM trained solely on $\mathcal{D}_{pref}$. The final training set $\mathcal{D} = \mathcal{D}_{pref} \cup \mathcal{D}_{aug\_filtered}$ typically contains 3-3.5 times the number of examples in $\mathcal{D}_{pref}$. This is obtained as follows. First, we vary the positives and negatives with 5 counterfactuals each (corresponding to 5 causal attributes). This leads us to 10 times the original dataset size. Then we augment the neutrals to include positive and negative neutrals corresponding to 2 times the original size. Finally we verify and filter the data leaving us with 3-3.5 times the original dataset size.

\paragraph{Models.}
We instantiate our reward models using state-of-the-art transformer architectures, specifically Qwen2.5 7b \cite{yang2024qwen2} and Gemma2 9b \cite{team2024gemma}. Our primary architecture is the pairwise preference model, where input follows the format `Q, A, B`, and the model predicts a preference token ('A' or 'B'), which is trained using a cross entropy loss. We call this variant as $\carma\text{-PairPM}$. We propose another variant called $\carma \text{-BT}$

For downstream alignment, we fine-tune the corresponding base LLMs (Qwen2.5 7b, Gemma2 9b) using Direct Preference Optimization (DPO) \cite{rafailov2024direct} and Multi Preference Optimization (MPO) \cite{gupta2024swepo}.

\paragraph{Baselines and Ablations.}
We compare our full \carma{} approach against several alternatives. The primary baseline is a \textbf{Base RM} trained solely on $\mathcal{D}_{pref}$. We also compare against the \textbf{RRM Baseline} \cite{liu2024rrm}, which employs a different augmentation strategy involving non-contextual examples. 
Concretely, the \textsc{Rrm} Baseline augments using non-contextual examples, as well as examples from responses from different queries --- which are not specifically aligned with causal and spurious attributes. 
Ablations include training \carma{} with only causal augmentations (\textbf{\carma-Causal}, equivalent to $\lambda=0$), only neutral augmentations (\textbf{\carma-Neutral}), and exploring various weights for the neutral loss term (\textbf{\carma{} with varying $\lambda \in \{0.1, 0.5, 1.0, 2.0, 5.0\}$}). Additionally, we may compare our neutral generation method against alternatives like paraphrasing or temperature sampling.

\paragraph{Training Details.}
All models are trained in PyTorch using Hugging Face Transformers library. Reward models are trained for 1 epoch with the AdamW optimizer \cite{loshchilov2017decoupled}, a learning rate of $1e-6$, and a global batch size of $256$, employing a cosine learning rate schedule. DPO policy training proceeds for 1 epoch using AdamW, with a learning rate of $1e-6$, batch size $256$, and DPO $\beta = 0.01$.

\paragraph{Evaluation.}
Reward model quality is assessed using accuracy on the \textbf{RewardBench} benchmark \cite{lambert2024rewardbench}, with performance broken down by category (Chat, Chat-Hard, Safety, Reasoning). Robustness may be further evaluated using \textbf{Re-word Bench} \cite{wu2025rewordbench}. Policy performance is measured using standard alignment benchmarks: \textbf{MT-Bench} \cite{zheng2023judging} for multi-turn conversational ability (judged by GPT-4), and \textbf{AlpacaEval 2} \cite{alpaca_eval} for single-turn instruction following, reporting both standard and length-controlled (LC) win rates against reference models. We also analyze the \textbf{average response length} of the policies to evaluate verbosity bias and conduct \textbf{qualitative analysis} of generated responses to identify variations and failure models.