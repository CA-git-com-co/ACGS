\section{Methodology: Training a Robust Reward Model}
\label{sec:methodology}

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{images/DataAugmentationPipeline_new.pdf} % Assuming this path is correct
\caption{The \carma{} data augmentation pipeline. Original preference data ($\mathcal{D}_{\mathrm{pref}}$) is used as a basis to generate: (1) \textit{Causal Augmentations} ($\mathcal{D}_{\mathrm{causal}}$) by performing \textbf{Attribute Upgradation and Degradation} on specific attributes to enforce sensitivity to genuine quality drivers, and (2) \textit{Neutral Augmentations} ($\mathcal{D}_{\mathrm{neutral}}$) via Causally-Aligned Reconstructions and Irrelevant Pair Comparisons (with tie-labels) to teach spurious feature invariance. After optional filtering, the reward model is trained on the combined original and augmented dataset.}
\label{fig:data_augmentation_pipeline}
\end{figure}

The \carma{} framework employs a causally-motivated data augmentation strategy to train robust reward models. The process involves two main phases: generating attribute-aware counterfactual data based on the causal model (Section \ref{sec:preliminaries}), and then training the reward model using a specialized loss function on the combined original and augmented data, as illustrated in Figure \ref{fig:data_augmentation_pipeline}.

\subsection{Attribute-Aware Counterfactual Data Generation}
\label{subsec:data_generation_phase}

This phase prepares the augmented dataset $\mathcal{D}_{\mathrm{aug}} = \mathcal{D}_{\mathrm{causal}} \cup \mathcal{D}_{\mathrm{neutral}}$ required for robust training, involving three conceptual steps:

\paragraph{Step 1: Attribute Identification.}
As a prerequisite, we identify the Principal Causal Components $\mathrm{C} = (\mathrm{C}_1, \dots, \mathrm{C}_\ell)$ relevant to the task, leveraging the causal framework from Section \ref{subsec:causal_graph}. This typically involves LLM prompting and refinement (Details in Appendix~\ref{subsec:attribute_identification_appendix}).

\paragraph{Step 2: Counterfactual Generation.}
Using the identified attributes $\mathrm{C}$, we generate synthetic data pairs via LLM-approximated counterfactuals, as defined in Section \ref{subsec:approximating_counterfactuals}. Following the strategies summarized in Table \ref{tab:augmentation_summary} and detailed conceptually in Section \ref{subsec:data_augmentation}, we create:
\begin{itemize}[itemsep=0pt, left=10pt]
    \item \textit{Causal Augmentation Pairs} ($\mathcal{D}_{\mathrm{causal}}$): Examples enforcing sensitivity to individual causal attributes $\mathrm{C}_j$ via \textbf{Attribute Upgradation} and \textbf{Degradation}, with  standard preference labels ($\succ$).
    \item \textit{Neutral Augmentation Pairs} ($\mathcal{D}_{\mathrm{neutral}}$): Examples enforcing invariance to spurious attributes $\mathrm{SP}$ while holding causal content $\mathrm{C}$ constant or ensuring $\mathrm{C}$ is irrelevant. These are generated via \textbf{Causally-Aligned Reconstruction} and \textbf{Irrelevant Pair Comparison} (using query modification). These receive tie labels ($\approx$).
\end{itemize}
The specific LLM prompts driving this generation are provided in Appendix~\ref{sec:prompt_templates}. This step yields the raw augmented dataset $\mathcal{D}_{\mathrm{aug}}$.

\paragraph{Step 3: Data Filtering.}
To improve training efficiency, the raw $\mathcal{D}_{\mathrm{aug}}$ can be filtered into $\mathcal{D}_{\mathrm{aug\_filtered}}$. This often involves using a baseline model $\hat{\mathrm{R}}_{\mathrm{base}}$ (trained only on $\mathcal{D}_{\mathrm{pref}}$) to select only those augmented pairs where the baseline is uncertain or incorrect, thus focusing training on the most informative examples. Details on the filtering strategy and thresholds are provided in Section \ref{sec:experiments} and Appendix \ref{subsec:filtering_appendix}.

This data generation phase outputs the datasets $\mathcal{D}_{\mathrm{pref}}$ (original preferences) and $\mathcal{D}_{\mathrm{aug\_filtered}}$, which serve as input to the model training phase.

\subsection{Robust Reward Model Training}
\label{subsec:training_phase}

Given the original data $\mathcal{D}_{\mathrm{pref}}$ and the filtered augmented data $\mathcal{D}_{\mathrm{aug\_filtered}}$, the final \carma{} reward model $\hat{\mathrm{R}}_\theta$ is trained by minimizing a composite loss function $\mathcal{L}(\theta)$ over the combined dataset $\mathcal{D} = \mathcal{D}_{\mathrm{pref}} \cup \mathcal{D}_{\mathrm{aug\_filtered}}$:
\begin{align}
\mathcal{L}(\theta) = &- \underbrace{\sum_{(\mathrm{Q}, \mathrm{y}_w, \mathrm{y}_l) \in \mathcal{D}_{\mathrm{pref}} \cup \mathcal{D}_{\mathrm{causal}}} \log \sigmoid(\hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{y}_w) - \hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{y}_l))}_{\text{Preference Loss (Causal Sensitivity)}} \nonumber \\
&- \lambda \underbrace{\sum_{(\mathrm{Q}, \mathrm{A}_1, \mathrm{A}_2, y=\text{tie})\in \mathcal{D}_{\mathrm{neutral}}} \left( -\frac{1}{2} \left[ \log \sigmoid(\Delta_{12}) + \log \sigmoid(-\Delta_{12}) \right] \right)}_{\text{Neutral Tie Loss (Spurious Invariance)}}
\label{eq:combined_loss_methodology}
\end{align}
where $\Delta_{12} = \hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{A}_1) - \hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{A}_2)$.
The first component is the standard Bradley-Terry negative log-likelihood loss, applied to $\mathcal{D}_{\mathrm{pref}}$ and the Causal Augmentation pairs $\mathcal{D}_{\mathrm{causal}}$. It ensures the model learns to prefer responses with higher causal quality, aligning with the ground truth $\mathrm{R}^*$ and the specific attribute manipulations.

The second component specifically targets the neutral pairs $\mathcal{D}_{\mathrm{neutral}}$ to enforce invariance. The tie loss term encourages the model score difference $\Delta_{12}$ to approach zero for pairs that differ only spuriously (or are both equally irrelevant), effectively penalizing reliance on $\mathrm{SP}(\mathrm{A})$ when $\mathrm{C}(\mathrm{A})$ is constant or irrelevant. The hyperparameter $\lambda \ge 0$ balances these two objectives, controlling the strength of the enforced spurious invariance relative to fitting preferences; its value is determined experimentally (Section \ref{sec:experiments}).

Optimizing this composite loss $\mathcal{L}(\theta)$ trains $\hat{\mathrm{R}}_\theta$ to be simultaneously sensitive to the identified causal attributes $\mathrm{C}$ and robustly invariant to the variations in spurious attributes $\mathrm{SP}$ captured by the neutral augmentations. We will show experimental verification of such mitigation of reward hacking through downstream experiments on RewardBench and Re-word Bench \citep{lambert2024rewardbench,wu2025rewordbench}. Furthermore, we also show improved generation performance when we use this reward model for best-of-n sampling, as well as policy optimization in the DPO/MPO training loop.