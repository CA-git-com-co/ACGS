
We provide a high-level theoretical justification for \carma{}'s ability to mitigate spurious reward hacking, with full details and formal proofs deferred to Appendix \ref{sec:theoretical_analysis_detailed}. Our core argument is that by training with specifically constructed causal and neutral augmentation pairs, the learned reward model $\hat{\mathrm{R}}_\theta$ is guided away from relying on superficial correlations.

\vspace{-0.05in}
\subsection{Spurious Reward Hacking}
Consider three answers $\mathrm{A}_{high}$, $\mathrm{A}_{low}$, and $\tilde{\mathrm{A}}_{low}$ for a query $\mathrm{Q}$.
A reward model $\hat{\mathrm{R}}_\theta$ exhibits \textit{spurious reward hacking} if:
\begin{enumerate}[label=(\roman*), itemsep=0pt, left=15pt, topsep=3pt]
    \item $\mathrm{R}^*(\mathrm{Q}, \mathrm{A}_{high}) > \mathrm{R}^*(\mathrm{Q}, \mathrm{A}_{low})$ (causal superiority of $\mathrm{A}_{high}$ over $\mathrm{A}_{low}$),
    \item $\mathrm{C}(\tilde{\mathrm{A}}_{low}) = \mathrm{C}(\mathrm{A}_{low})$ and $\mathrm{SP}(\tilde{\mathrm{A}}_{low}) \neq \mathrm{SP}(\mathrm{A}_{low})$ (making $\tilde{\mathrm{A}}_{low}$ a spuriously modified version of $\mathrm{A}_{low}$, thus $\mathrm{R}^*(\mathrm{Q}, \tilde{\mathrm{A}}_{low}) = \mathrm{R}^*(\mathrm{Q}, \mathrm{A}_{low})$),
\end{enumerate}
\vspace{-0.05in}
yet the model $\hat{\mathrm{R}}_\theta$ prefers the spuriously modified inferior answer over the causally superior one:

\vspace{-0.15in}
\begin{equation}
\hat{\mathrm{R}}_\theta(\mathrm{Q}, \tilde{\mathrm{A}}_{low}) > \hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{A}_{high}).
% \text{Optionally, can also state: } \dots > \hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{A}_{low}) % if space and desired
\label{eq:hacking_condition_main}
\end{equation}
This captures exploitation via spurious manipulations. A formal definition is provided in Appendix \ref{subsec:theory_definition}. Using this definition, and assumptions on causal coverage, and model expressivity, we provide guarantees on mitigation of reward hacking. A formal statement and proof of Theorem \ref{thm:mitigation_main_informal} are provided in Appendix \ref{subsec:theory_mitigation}.

\vspace{-0.2in}
\subsection{Mitigation through Augmented Data Constraints}
\carma{}'s training objective (Eq. \ref{eq:combined_loss_methodology}) combines a standard preference loss with a tie-loss for neutral pairs. This structure, when applied to our augmented data, directly counteracts reward hacking.

\begin{theorem}[Mitigation of Spurious Reward Hacking (Informal)]
\label{thm:mitigation_main_informal}
Under specific assumptions about data coverage and optimization success (detailed in Appendix \ref{subsec:theory_assumptions}), training a reward model $\hat{\mathrm{R}}_\theta$ with \carma{}'s causally-informed data augmentation and composite loss discourages the model parameters $\theta$ from satisfying the spurious reward hacking condition (Eq. \ref{eq:hacking_condition_main}).
\end{theorem}

\vspace{-0.1in}
\paragraph{Intuition:}

\vspace{-0.05in}
\begin{enumerate}[itemsep=0pt, left=10pt, topsep=3pt]
    \item \textbf{Causal Sensitivity} from $\mathcal{D}_{\mathrm{causal}}$ encourages $\hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{A}_{high}) > \hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{A}_{low})$.
    \item \textbf{Spurious Invariance} from $\mathcal{D}_{\mathrm{neutral}}$ encourages $\hat{\mathrm{R}}_\theta(\mathrm{Q}, \tilde{\mathrm{A}}_{low}) \approx \hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{A}_{low})$.
\end{enumerate}
These together guide $\hat{\mathrm{R}}_\theta$ towards $\hat{\mathrm{R}}_\theta(\mathrm{A}_{high}) > \hat{\mathrm{R}}_\theta(\mathrm{A}_{low}) \approx \hat{\mathrm{R}}_\theta(\tilde{\mathrm{A}}_{low})$, opposing the reward hacking condition. A formal statement and proof of Theorem \ref{thm:mitigation_main_informal} are provided in Appendix \ref{subsec:theory_mitigation}.
% Effectiveness depends on the quality of LLM-approximated counterfactuals.

