% \subsection{Our Contributions}
% \label{subsec:our_contributions}

The principal question we wish to address in the paper is the following:
\begin{remarkbox}{Mitigating Reward Hacking from Unknown Spurious Correlates via Causal Attributes}
How do we train reward models to be robust against reward hacking, particularly when the specific spurious attributes that an RM may exploit are not known a priori, by focussing only on stable causal drivers of answer quality found in ground truth/human preferences.
\end{remarkbox}


We provide the following main contributions in this work:
\textbf{1. Spurious-unaware Robustness} We introduce a causal framework (Section~\ref{sec:preliminaries}, Figure~\ref{fig:causal_graph}) that models answer generation by distinguishing fundamental \emph{causal} drivers of reward labels from \emph{spurious} correlates.


% We address the persistent challenge of \emph{reward hacking} in LLM alignment by proposing \textbf{\carma{} (Causal Augmentation to disentangle Reward Modeling Attributes)}. Our main contributions are:

% \vspace{-0.2in}
\begin{itemize}[left=2pt,itemsep=0pt]
    \item \textbf{A Principled Causal Framework for Spurious-Unaware Robust Reward Modeling.}
    We introduce a causal framework (Section~\ref{sec:preliminaries}, Figure~\ref{fig:causal_graph}) that models answer generation by distinguishing fundamental \emph{causal} drivers of reward labels from \emph{spurious} correlates. Crucially, this framework does not require a priori specification of spurious attributes, enabling \carma{} to train RMs that are robust to a broader range of unforeseen superficial cues while focusing on genuine quality.

    \item \textbf{Novel Targeted Augmentations for Causal and Neutral Attributes.}
    Building on our framework, we introduce \textit{targeted data augmentation for specific causal attributes} (Section~\ref{sec:methodology}). Unlike prior work focusing on broader neutral or coarse-grained augmentations, our \emph{Causal Augmentations} create minimal pairs isolating changes in individual causal dimensions (e.g., factuality, safety) to instill precise sensitivity. These are complemented by \emph{Neutral Augmentations} that enforce invariance to spurious features by varying them while preserving causal content, using tie-labels. We present a systematic generation and filtering process for these highly informative counterfactual examples. Our framework is also general enough to incorporate augmentation strategies from related works (Appendix \ref{sec:causal_model_details}). 
    {\color{blue} We query an oracle LLM to produce rubrics on a given answer that are only causally relevant to reward labeling and perform the above augmentations on those rubrics. We do not use any explicit knowledge of spurious factors/attributes.}
    \item \textbf{Formal Definition and Theoretical Guarantees for Reward Hacking Mitigation.}
    We provide a \textit{formal definition of spurious reward hacking} (Section~\ref{sec:theory}, Definition~\ref{def:reward_hacking_formal_theory} in Appendix~\ref{subsec:theory_definition}) and present a rigorous theoretical analysis. We prove that, under specified data coverage and optimization assumptions, an RM trained with \carma{}'s augmentation scheme is guided towards causal faithfulness and is robust against reward hacking via superficial manipulations (Theorem~\ref{thm:mitigation_formal_theory} in Appendix~\ref{subsec:theory_mitigation}).{\color{brown}: Can we move \emph{formal definition of reward hacking} to point no 1? Also do we really want to claim that we provide formal guarantees ? The empirical results are stronger compared to the theory so it is better to highlight that }

    \item \textbf{State-of-the-Art Empirical Performance and Downstream Alignment Benefits.}
    Comprehensive experiments (Section~\ref{sec:experiments}) demonstrate \carma{}'s superiority. On \textbf{RewardBench} \citep{lambert2024rewardbench}, \carma{} significantly outperforms standard RMs and strong baselines like \rrm{} \citep{liu2024rrm}, improving average accuracy by up to 5.4\%, with notable gains in Safety (up to \textbf{13.18\%}) and Reasoning (up to \textbf{7.19\%}) (Table~\ref{tab:performance_bt_pairpm_rewardbench}). We also beat the existing baselines on absolute robustness metric on \textbf{reWordBench} \citep{wu2025rewordbench}, which systematically transforms reward model inputs in meaning or ranking preserving ways (Figure \ref{fig:reword_absolute_robustness}). This robustness translates to downstream benefits: Best-of-N response selection using \carma{}-RMs improves AlpacaEval scores by up to 10\%, which is attributed to the \textit{diminished} reward hacking and enhanced performance.
\end{itemize}