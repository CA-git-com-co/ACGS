\section{Detailed \carma{} Methodology}
\label{sec:detailed_methodology}

This appendix provides the detailed implementation steps for the \carma{} framework introduced in Section \ref{sec:methodology}, covering attribute identification, counterfactual data generation, filtering, and the specific training objective.

\subsection{Step 1: Attribute Identification}
\label{subsec:attribute_identification_appendix}

The foundation involves identifying the attributes that genuinely determine answer quality versus those merely correlated with it, as defined in Section \ref{subsec:causal_graph}. For a query $\mathrm{Q}$ and example answers $(\mathrm{y}_w, \mathrm{y}_l)$ from $\mathcal{D}_{\mathrm{pref}}$, we define: \textit{Causal attributes} $\mathrm{C} = (\mathrm{C}_1, \dots, \mathrm{C}_\ell)$ (e.g., factuality) and \textit{Spurious attributes} $\mathrm{SP} = (\mathrm{SP}_1, \dots, \mathrm{SP}_k)$ (e.g., verbosity).

\paragraph{Automated Attribute Extraction.} We employ an LLM prompted with $\mathrm{Q}$ and example responses (see Appendix~\ref{sec:prompt_templates} for prompt). The primary output is the set of attributes $\mathrm{C}$.

\paragraph{Refinement and Verification.} The LLM-generated list $\mathrm{C}$ is reviewed for coherence and consistency in this verification phase. The verification prompts are provided in Appendix \ref{sec:prompt_templates}.

\subsection{Step 2: Generating Counterfactual Augmented Data}
\label{subsec:counterfactual_generation_appendix}

Using identified attributes $\mathrm{C}$, we generate $\mathcal{D}_{\mathrm{aug}}$ via LLM-approximated counterfactuals (Section \ref{subsec:approximating_counterfactuals}).

\paragraph{Causal Augmentation ($\mathcal{D}_{\mathrm{causal}}$).} Pairs $(\mathrm{A}, \mathrm{A}')$ are generated to differ primarily along a single causal attribute $\mathrm{C}_j$. We use LLM prompts (Appendix~\ref{sec:prompt_templates}) for \textit{upgradation} (generating an improved $\mathrm{A}'$ from a ground-truth rejected answer $\mathrm{A}$) and \textit{degradation} (generating a degraded $\mathrm{A}'$ from a ground-truth selected answer $\mathrm{A}$), aiming to keep other attributes constant. Pairs are labeled $\succ$ accordingly.

\paragraph{Neutral Augmentation ($\mathcal{D}_{\mathrm{neutral}}$).} 
Notice that when we causally augment an answer in $\mathcal{D}_{\mathrm{causal}}$, we might in-advertantly move spurious correlates (as illustrated in Figure \ref{fig:carma_augmentation_visual_overview}). Furthermore, even in our dataset, there could be a systematic effect where spurious attributes highly correlate with the better (or worse) answer. In such cases, we need to create a dataset of equivalent pairs, with a tie label to teach the model invariance to spurious correlates.

Our primary technique is \textit{irrelevant query neutrals} (IQN). Here, the idea is that given a new query, the causal attribute $\mathrm{C}$ becomes irrelevant. Essentially, for the new query, the causal attributes are spurious. Hence, by taking any two answers for a given query, and labeling them a tie, given an irrelevant query, the reward model learns invariance to these features. For example, if the reward model has spuriously learnt that bullet points in an answer should be rewarded, our tie labels teach them that bullet points should be rewarded only if the content of the answer is relevant to the query.
Similarly, by creating such pairs with our own causally augmented data in $\mathcal{D}_{\mathrm{causal}}$, we teach the model invariance to the spurious pairs that move when the causal attributes (CA) are perturbed.

\subsection{Step 3: Filtering Augmented Data}
\label{subsec:filtering_appendix} 

The raw $\mathcal{D}_{\mathrm{aug}}$ is then filtered to $\mathcal{D}_{\mathrm{aug\_filtered}}$.

\paragraph{Model-based Confidence Filtering.} Using a baseline $\hat{\mathrm{R}}_{\mathrm{base}}$, we calculate $p = \mathrm{P}_{\mathrm{base}}(\mathrm{B} \succ \mathrm{A})$ for each augmented pair $(\mathrm{A}, \mathrm{B})$ with target label $y$. We retain the pair only if $|p - \mathbb{I}(y = \text{B} \succ \mathrm{A}) - 0.5 \cdot \mathbb{I}(y = \text{tie})| > \tau$. We use threshold $\tau=0.2$, focusing training on examples where the baseline is uncertain or incorrect \citep{liu2024rrm}.

\paragraph{Quality Verification.} Further checks (e.g., automated fluency scoring) verify pair validity. The result is $\mathcal{D}_{\mathrm{aug\_filtered}}$.

\subsection{Step 4: Training the Robust Reward Model}
\label{subsec:training_appendix} 

The final model $\hat{\mathrm{R}}_\theta$ is trained on $\mathcal{D} = \mathcal{D}_{\mathrm{pref}} \cup \mathcal{D}_{\mathrm{aug\_filtered}}$ by minimizing the composite loss:
\begin{align}
\mathcal{L}(\theta) = &- \sum_{(\mathrm{Q}, \mathrm{y}_w, \mathrm{y}_l) \in \mathcal{D}_{\mathrm{pref}} \cup \mathcal{D}_{\mathrm{causal}}} \log \sigmoid(\hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{y}_w) - \hat{\mathrm{R}}_\theta(\mathrm{Q}, \mathrm{y}_l)) \nonumber \\
&- \lambda \sum_{(\mathrm{Q}, \mathrm{A}_1, \mathrm{A}_2, y=\text{tie})\in \mathcal{D}_{\mathrm{neutral}}} \mathcal{L}_{\mathrm{tie}}(\theta; \mathrm{Q}, \mathrm{A}_1, \mathrm{A}_2)
\label{eq:combined_loss_appendix}
\end{align}
where $\mathcal{L}_{\mathrm{tie}}$ is defined as in Eq. \ref{eq:combined_loss_methodology}. The hyperparameter $\lambda \ge 0$ weights the neutral tie loss and is tuned on a validation set (Section \ref{sec:experiments}).