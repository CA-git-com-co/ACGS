\clearpage
\section{Experimental Setup Details}
\label{sec:experimental_details}

This appendix provides supplementary details to the experimental settings outlined in Section \ref{subsec:experimental_settings} of the main paper.

\subsection{Best-of-N Experimental Methodology}
\label{ssec:bon_pairpm_expt_methodology}

\begin{algorithm}[H]
  \caption{Best-of-$N$ Selection with Pairwise Preference Model}
  \label{alg:best_of_n_selection}
  \begin{algorithmic}[1]          % [1] = display line numbers
    \STATE \textbf{Input:}  Query $Q$; responses $\mathcal{A} = (A_1,\dots,A_N)$ with $N \ge 1$
    \STATE \textbf{Input:}  Pairwise model $\hat{\mathrm{R}}_\theta : (Q,A_i,A_j) \to \{1,2\}$\\
    $\triangleright$ The output $\{1,2\}$ from the Pairwise preference model indicates if the first answer is better or the second, given the query.
    \STATE \textbf{Output:} Selected best response $A_{\text{best}}$
    \STATE $A_{\text{best}} \leftarrow A_1$
    \FOR{$i \gets 2$ \TO $N$}
        \STATE $A_{\text{cand}} \leftarrow A_i$
        \IF{$\hat{\mathrm{R}}_\theta(Q, A_{\text{best}}, A_{\text{cand}}) = 2$}
            \STATE $A_{\text{best}} \leftarrow A_{\text{cand}}$
        \ENDIF
    \ENDFOR
    \RETURN $A_{\text{best}}$
  \end{algorithmic}
\end{algorithm}



For all our Best-of-N results using PairPM models, we follow a simple procedure to find the best response out of $N$ responses generated by a base LLM. In particular, PairPM models take responses 2 at a time, and provide the better response for the given query.
Given $N$ response $\mathcal{A} = (A_1,\dots,A_N)$ with $N \ge 1$, in a randomly shuffled order, we sequentially compare responses 2 at a time (starting from $A_1$ and $A_2$) using the PairPM reward model and keep track of the best response. At each iteration, the best response is compared to the next response in the list and the best response is updated. The best response after $N-1$ iterations is taken as the selected response.
The algorithm for this procedure is given in Algorithm \ref{alg:best_of_n_selection}.



\subsection{Experimental setting for Calculating Win Rates on RewardBench Prompts}
To show the performance of \carma{} on general purpose datasets, we follow reWordBench \citep{wu2025rewordbench} and use all 2985 prompts from RewardBench \citep{lambert2024rewardbench}. We use \gemmait{9} as the base model and sample N responses for each prompt in this set. Following this, we use the PairPM reward models (RM, RRM and \carma{}) to select the best response among the N responses, as described in supplementary Section \ref{ssec:bon_pairpm_expt_methodology}. We use \texttt{GPT-4} as a judge to compare \carma{}'s responses with baselines RM and RRM.

\subsection{WildGuardTest and GSM8K experimental settings}
For both WildGuardTest results (main paper Figure \ref{fig:asr_reduction_gemma9b} as well as supplementary Table \ref{tab:results_comparison_lowest_is_best}), as well as GSM8K results (main paper Figure \ref{fig:bon_gsm8k_gemma9b}), we use \gemmait{9} as the base model and sample N responses from it. Following this, we use the PairPM reward models (RM, RRM and \carma{}) to select the best response among the N responses, as described in supplementary Section \ref{ssec:bon_pairpm_expt_methodology}. For WildGuarTest, for obtaining results given the final responses, we use the WildGuard model \cite{wildguard2024} to obtain annotations for \texttt{prompt-harmfulness}, \texttt{response-harmfulness}, \texttt{response-refusal}, \texttt{is-parsing-error}, as described in the WildGuard repository\footnote{https://github.com/allenai/wildguard}. Using these annotations, we obtain ASR and RTA for \carma{} and baselines.

\subsection{Datasets and Augmentation}
\label{app:datasets_aug_details}

For human preference data ($\mathcal{D}_{\text{pref}}$) we use \textbf{Ultrafeedback} \citep{cui2023ultrafeedback}, which furnishes approximately 60,000 preference pairs across diverse domains. 

The data augmentation process, central to \carma{} (Section \ref{sec:methodology}), employs Gemini 2.0 Flash. This LLM is first used to identify $\ell=5$ principal causal attributes relevant to response quality. Subsequently, Gemini 2.0 Flash generates (a) causal upgrade/degradation pairs targeting these attributes ($\mathcal{D}_{\text{causal}}$), and (b) neutral pairs ($\mathcal{D}_{\text{neutral}}$).

The raw augmented data, $\mathcal{D}_{\text{aug}}$, undergoes a filtering step. This involves applying a model-based confidence filter, using a baseline RM (trained solely on $\mathcal{D}_{\text{pref}}$) with a threshold of $\tau=0.2$. This filtering focuses the training on more informative examples. The amplification process involves initially generating approximately 10x data from causal augmentations (5 attributes, 2 versions per original response) and 1x data from neutral augmentations, followed by verification and the confidence-based filtering. The final training dataset $\mathcal{D} = \mathcal{D}_{\text{pref}} \cup \mathcal{D}_{\text{aug\_filtered}}$ typically contains about 3.5 times the number of examples in the original $\mathcal{D}_{\text{pref}}$, similar to RRM \citep{liu2024rrm}.

\subsection{Models and Training}
\label{app:models_training_details}

\paragraph{Reward Models (RMs):} We instantiate RMs using {\qwen{}} \citep{yang2024qwen2} and {\gemmait{9}}, {\gemma{2}} \citep{team2024gemma} as base transformer architectures. Our RM variant, $\carma\text{-PairPM}$, processes inputs formatted as `Q, A, B` and predicts a preference token ('A' or 'B') via a cross-entropy loss. An alternative variant, $\carma\text{-BT}$, implements the Bradley-Terry model by deriving scalar scores for each answer.

\paragraph{Policy Models:} For downstream alignment tasks, we use the Best-of-N setup
where we generate N responses using \gemmait{9} and use \carma{} as well as baseline reward models to select the best candidate response.

\paragraph{Training Hyperparameters:} All models are trained in PyTorch with the Hugging Face Transformers library. For RM training, following \citet{liu2024rrm}, we use the AdamW optimizer \citep{loshchilov2017decoupled} for 1 epoch, with a learning rate of $1 e^{-6}$, a global batch size of 256, and a cosine learning rate schedule. We use a warmup ratio of 0.03. For training all models, we use 8 NVIDIA A100 80GB GPUs. RM training runs require time between 10-16 hours for 2B to 9B mdoels we consider.

\subsection{Baselines and Evaluation}
\label{app:baselines_ablations_eval_details}

\paragraph{Baselines:} Our full \carma{} approach is compared against two primary baselines:
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt, leftmargin=*]
    \item A \textbf{Base RM}, trained solely on the original $\mathcal{D}_{\text{pref}}$.
    \item The \textbf{RRM Baseline} \citep{liu2024rrm}, which employs a distinct augmentation strategy using non-contextual examples and responses from different queries, not specifically aligned with identified causal or spurious attributes.
\end{enumerate}

\paragraph{Evaluation Benchmarks:}
RM quality is assessed by accuracy on \textbf{RewardBench} \citep{lambert2024rewardbench} (overall and per category: Chat, Chat-Hard, Safety, Reasoning) and robustness on \textbf{Re-word Bench} \citep{wu2025rewordbench}. BoN Policy performance is evaluated using RewardBench, WildGuardTest \citep{wildguard2024}, GSM8K \citep{cobbe2021gsm8k}.

