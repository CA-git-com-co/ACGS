\section{Extended Related Works} 
\label{sec:extended_related_works}

Our work on \carma{}, a framework for causally robust reward modeling, intersects with and builds upon several key areas of research: the alignment of Large Language Models (LLMs) via human feedback, techniques for reward model training, the persistent challenge of reward hacking, the application of causal inference principles to machine learning, and data augmentation strategies for enhancing model robustness.

\paragraph{LLM Alignment and RLHF.}
The dominant paradigm for steering LLM behavior towards desired attributes like helpfulness, honesty, and harmlessness is Reinforcement Learning from Human Feedback (RLHF) \citep{christiano2017deep, stiennon2020learning, ouyang2022training, bai2022training, askell2021general}. The standard RLHF process involves training a reward model (RM) on human preferences (typically pairwise comparisons) and subsequently using this RM as a reward signal to fine-tune the LLM policy via RL algorithms such as PPO \citep{schulman2017proximal}. The quality, calibration, and robustness of the RM are paramount, as flaws in the RM directly impact the alignment outcome \citep{casper2023open}. While alternative alignment algorithms like Direct Preference Optimization (DPO) \citep{rafailov2024direct} and its extensions (e.g., IPO \citep{azar2024general}, KTO \citep{ethayarajh2024kto}, ORPO \citep{hong2024orpo}, SimPO \citep{meng2024simpo}) bypass explicit RM training by directly optimizing the policy on preference data, they still implicitly rely on the preference information learnable from the data, making the problem of distinguishing true quality from spurious correlates equally relevant.

\paragraph{Reward Modeling Techniques.}
Learning accurate reward models from preference data remains a central challenge. Methodologies include Bradley-Terry style pointwise models that learn a scalar score $r(x, y)$ \citep{bradley1952rank, ouyang2022training, bai2022training}, and pairwise ranking models that directly predict preference probabilities, often implemented within the LLM architecture itself (PairPM) \citep{liu2025pairwise, qin2023large}. Other approaches explore Q-function based rewards \citep{li2024process} or process supervision \citep{khalifa2025process}. Significant effort focuses on improving specific RM properties like calibration \citep{zhu2025charm, zhao2023slic}, training efficiency \citep{tunstall2023zephyr}, uncertainty quantification \citep{lou2024uncertainty}, interpretability through multi-aspect rewards \citep{wang2024interpretable, yang2024rewards}, and scalability via reasoning or chain-of-thought mechanisms \citep{zhao2025genprm}. Our work complements these efforts by focusing specifically on enhancing the causal \textbf{robustness} of the learned reward function $\hat{R}$ against spurious attributes.

\paragraph{Reward Hacking and Spurious Correlations.}
Learned reward models are notoriously susceptible to \textit{reward hacking} or \textit{over-optimization} \citep{gao2023scaling, skalse2022defining, pan2022effectsrewardmisspecificationmapping}. Because RMs are trained on finite, potentially biased data, they often learn to associate high rewards with superficial or \textit{spurious} features that are merely correlated with desirable responses in the training set. Common examples include excessive length or verbosity \citep{singhal2023long}, specific formatting patterns like lists or markdown \citep{zhang2024lists}, adherence to stylistic conventions like politeness, or even sycophantic agreement with user views \citep{denison2024sycophancy}. Policies optimized against such RMs learn to exploit these spurious cues, leading to outputs that maximize the predicted reward but fail to align with genuine human preferences or task goals \citep{shen2023trickle}.

\paragraph{Approaches to Mitigating Reward Hacking.}
Various strategies have been proposed to address reward hacking. Model-centric approaches include using ensembles of RMs to average out idiosyncratic biases \citep{coste2023reward, eisenstein2023helping, rame2024warm}, incorporating explicit calibration methods \citep{zhao2023slic}, or designing architectures that factorize reward components, such as ODIN's disentanglement of quality and length \citep{chen2024odin}. Policy-optimization techniques might involve adding explicit penalties for spurious features (e.g., length penalties \citep{park2024disentangling}) or using specific regularization methods during fine-tuning. Data-centric approaches aim to improve the training data or process itself. Examples include iterative re-labeling or refinement \citep{bai2022constitutional}, performing consistency checks across related prompts \citep{shen2023trickle}, or augmenting the dataset with synthetic examples designed to improve robustness \citep{pace2024west, shen2024boosting}. Our work, \carma{}, falls firmly in this data-centric category. It is closely related to RRM \citep{liu2024rrm}, which also uses data augmentation (non-contextual and query-independent pairs) for robustness. However, \carma{} is distinct in its use of an explicit causal framework and its generation of targeted, attribute-specific counterfactuals to disentangle causal from spurious factors.

\paragraph{Causal Inference in Machine Learning.}
Causal inference provides formal tools, such as Structural Causal Models (SCMs) and DAGs \citep{pearl2009causality, peters2017elements}, for reasoning about cause-effect relationships, confounding, and counterfactuals. Applying causal principles in machine learning aims to build models that are more robust, fair, and interpretable by focusing on underlying causal mechanisms rather than potentially brittle statistical correlations \citep{scholkopf2021toward}. Techniques like Invariant Risk Minimization (IRM) seek models that perform well across different environments by relying on invariant (presumably causal) predictors \citep{arjovsky2019invariant}. Our work adopts this causal perspective, framing spurious attributes as non-causal factors whose influence on the learned reward model should be minimized.

\paragraph{Causality in LLMs and NLP.}
The intersection of causality and LLMs is rapidly evolving. Research includes probing the innate causal reasoning abilities of LLMs \citep{kiciman2023causal, chi2024unveiling}, leveraging LLMs as tools for automating parts of the causal discovery or analysis pipeline \citep{long2023causal, tu2023causal}, and applying causal methods to enhance NLP tasks. For instance, counterfactual reasoning and data augmentation have been used to improve robustness against biases in text classification \citep{kaushik2019learning, feder2021causalm} and assess fairness \citep{feder2022causal}. \carma{} uniquely employs a predefined causal graph to structure the generation of counterfactual data specifically for training a robust RM, using LLMs as the generation engine.

\paragraph{Data Augmentation for Robustness.}
Data augmentation is a cornerstone technique for improving model generalization. Beyond traditional NLP methods like synonym replacement or back-translation \citep{wu2025rewordbench}, more recent approaches leverage LLMs for sophisticated augmentations, including paraphrasing, style transfer, generating adversarial examples \citep{qiang2024prompt}, or creating counterfactuals \citep{mishra2024llm, feder2021causalm}. Counterfactual generation, often using LLMs as rewriters, is also central to evaluation methods like RATE \citep{reber2024rate}, which uses ``rewrites of rewrites'' to estimate causal effects robustly. Methods based on sampling, like Gumbel temperature sampling, have also been explored for counterfactual generation \citep{ravfogel2025gumbelcounterfactualgenerationlanguage}. In the specific context of reward modeling, data augmentation aims to enhance robustness against spurious correlations; examples include the non-contextual and query-independent pairs used by RRM \citep{liu2024rrm} or consistency checks via paraphrased inputs as explored in \rewordbench{} \citep{wu2025rewordbench}. Furthermore, generating entirely synthetic preference pairs \citep{pace2024west, shen2024boosting} represents another data-centric approach to improving reward models. Counterfactual data augmentation, particularly generating minimally different pairs to isolate specific features \citep{kaushik2019learning}, is highly relevant to disentangling causal factors. Our work, \carma{}, operationalizes this concept within an explicit causal framework, generating targeted "causal" (attribute-isolating) and ``neutral'' (spurious-varying) pairs via LLM rewriting to enforce specific invariance and sensitivity properties in the trained RM.

\paragraph{Positioning of \carma.}
\carma{} integrates insights from causal inference and data augmentation to address the critical problem of reward hacking in LLM alignment. While related works like RRM \citep{liu2024rrm} use data augmentation for robustness and  \carma{} is distinguished by its explicit grounding in a causal graph model of answer attributes. It systematically generates attribute-specific counterfactual and neutral examples via guided LLM prompting to directly train the RM to distinguish causal quality drivers ($C$) from spurious correlates ($SP$). This allows \carma{} to potentially handle a wider range of spurious attributes beyond commonly studied ones like length, aiming for a more principled and generalizable form of robustness. We provide the methodology and empirical validation (Section \ref{sec:experiments}) demonstrating that this causally-informed data augmentation leads to more robust reward models and better downstream policy alignment compared to standard baselines.