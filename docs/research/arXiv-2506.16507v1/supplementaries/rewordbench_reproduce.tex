\section{reWordBench Reproduction}
\label{app:rewordbench_creation}

The primary motivation reWordBench is the observation that contemporary reward models—key components of RLHF systems—often latch onto superficial formatting cues or benign artifacts in their training data, leading to dramatic drops in pairwise‐preference accuracy under minor, semantically neutral edits. To diagnose and quantify this brittleness in a systematic way, \cite{wu2025rewordbench} introduce reWordBench, a new benchmark built by applying 28 carefully designed, meaning‐preserving transformations to the original RewardBench instances. 
The authors organize these edits into three overarching families  each targeting different potential failure modes of reward models. 
Together, transformations systematically stress-test reward models’ invariance to innocuous changes, revealing large accuracy drops even under minor edits and motivating the need for robust-training methods.

Since the original dataset is not publicly available, on author's suggestion we reconstructed the data independently following the instructions in the original paper. Paraphrasing and back-translation transformations are generated using foundation models or translation tools for which we use OpenAI API, specifically the "gpt-4o-2024-08-06" model. For generating back-transcription transformations we use the "gpt-4o-transcribe" and "gpt-4o-mini-tts" models available on the OpenAI API. Here are some details of the transformations in reWordBench:

\begin{tcolorbox}[azurebox]
1. Controlled Transformations: These are template-based edits that guarantee semantic equivalence by construction. They include:

\begin{enumerate}
  \item[a.] Add Quotes: Surrounding the entire prompt and responses with a fixed number of quotation marks.
  \item[b.] Punctuation Spaces: Inserting spaces around each punctuation mark.
  \item[c.] Twitter Handle/URL: Appending a randomly generated (harmless) Twitter handle or URL to the text.
  \item[d.] StressTest: Repeating semantically vacuous conjunctions (e.g.\ “and true is true” or “and false is not true”) to the end of the text.
  \item[e.] Ignore Above/Below: Injecting the response before or after the prompt with an explicit instruction to ignore it.
  \item[f.] Rot-N Encoding: Applying simple character-shift ciphers (Rot-13 or Rot-2) to the prompt text while leaving responses in plain form.
\end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[azurebox]
2. Naturalistic Transformations: These simulate the kinds of noise and variation that occur ``in the wild'' and may not perfectly preserve meaning, but reflect realistic robustness challenges:
\begin{enumerate}
    \item[a.] Paraphrase: Rewriting prompt and response via a strong LLM (Llama-3-70B-instruct) under a paraphrasing instruction.
    \item[b.] Back-translation: Translating English → Spanish → English for several rounds using OPUS-MT, accepting only those with high semantic similarity.
    \item[c.]Back-transcription: Converting text to audio and back using a TTS model (fairseq S2) and an ASR model (Whisper-base).
    \item[d.] Homoglyph Substitution: Replacing Latin characters with visually identical Unicode glyphs (e.g. Cyrillic ``e'' for Latin ``e'').
    \item[e.] Character-level Edits: Randomly swapping, inserting, deleting, or substituting characters at rates reflecting real-world typos (including QWERTY-adjacent substitutions).
    \item[f.] Word Deletion: Omitting a randomly chosen word from prompt and response, subject to a similarity filter.
\end{enumerate}
\end{tcolorbox}
\begin{tcolorbox}[azurebox]
3. Domain-Targeted Transformations: These focus on specialized subsets of RewardBench—code, mathematics, and safety prompts—where specific artifacts may bias reward models:
\begin{enumerate}
    \item[a.] Code Minification: Automatically renaming variables, removing whitespace, and otherwise ``minifying'' Python snippets without changing functionality.
    \item[b.] Add Comment: Inserting ``\# bad'' annotations after each line of chosen responses (and optionally ``\# good'' after rejected ones).
    \item[c.] Append Other Code: Concatenating the losing snippet after the winning one (and vice versa), taking advantage of Python’s return-ended semantics.
    \item[d.] Swap Format: Exchanging the usual answer formats (e.g. LaTeX  vs. markdown ``\# Answer'') in arithmetic problems.
    \item[e.] Jailbreak Prompts: Prepending known ``jailbreak'' instructions (from the ChatGPT-Jailbreak-Prompts dataset) to safety-critical queries to see if the RM prefers harmful completions.
\end{enumerate}
\end{tcolorbox}