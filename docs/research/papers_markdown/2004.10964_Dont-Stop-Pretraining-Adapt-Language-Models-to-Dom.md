# 2004.10964_Dont-Stop-Pretraining-Adapt-Language-Models-to-Dom

**Original PDF**: 2004.10964_Dont-Stop-Pretraining-Adapt-Language-Models-to-Dom.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2

---

## Page 1

Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks

Suchin Gururangan†

Ana Marasovi´c†♦

Swabha Swayamdipta†

Kyle Lo†

Iz Beltagy†

Doug Downey†

Noah A. Smith†♦

†Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA

♦Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA

{suching,anam,swabhas,kylel,beltagy,dougd,noah}@allenai.org

Abstract

Language models pretrained on text from a

wide variety of sources form the foundation

of today’s NLP. In light of the success of

these broad-coverage models, we investigate

whether it is still helpful to tailor a pretrained

model to the domain of a target task.

We

present a study across four domains (biomedi-

cal and computer science publications, news,

and reviews) and eight classiﬁcation tasks,

showing that a second phase of pretraining in-

domain (domain-adaptive pretraining) leads

to performance gains, under both high- and

low-resource settings.

Moreover, adapting

to the task’s unlabeled data (task-adaptive

pretraining) improves performance even after

domain-adaptive pretraining. Finally, we show

that adapting to a task corpus augmented us-

ing simple data selection strategies is an effec-

tive alternative, especially when resources for

domain-adaptive pretraining might be unavail-

able. Overall, we consistently ﬁnd that multi-

phase adaptive pretraining offers large gains in

task performance.

1

Introduction

Today’s pretrained language models are trained on

massive, heterogeneous corpora (Raffel et al., 2019;

Yang et al., 2019). For instance, ROBERTA (Liu

et al., 2019) was trained on over 160GB of uncom-

pressed text, with sources ranging from English-

language encyclopedic and news articles, to literary

works and web content. Representations learned

by such models achieve strong performance across

many tasks with datasets of varying sizes drawn

from a variety of sources (e.g., Wang et al., 2018,

2019). This leads us to ask whether a tasks textual

domain—a term typically used to denote a distribu-

tion over language characterizing a given topic or

genre (such as “science” or “mystery novels”)—is

still relevant. Do the latest large pretrained mod-

els work universally or is it still helpful to build

Figure 1: An illustration of data distributions. Task

data is comprised of an observable task distribution,

usually non-randomly sampled from a wider distribu-

tion (light grey ellipsis) within an even larger target do-

main, which is not necessarily one of the domains in-

cluded in the original LM pretraining domain – though

overlap is possible. We explore the beneﬁts of contin-

ued pretraining on data from the task distribution and

the domain distribution.

separate pretrained models for speciﬁc domains?

While some studies have shown the beneﬁt of

continued pretraining on domain-speciﬁc unlabeled

data (e.g., Lee et al., 2019), these studies only con-

sider a single domain at a time and use a language

model that is pretrained on a smaller and less di-

verse corpus than the most recent language mod-

els. Moreover, it is not known how the beneﬁt of

continued pretraining may vary with factors like

the amount of available labeled task data, or the

proximity of the target domain to the original pre-

training corpus (see Figure 1).

We address this question for one such high-

performing model, ROBERTA (Liu et al., 2019)

(§2). We consider four domains (biomedical and

computer science publications, news, and reviews;

§3) and eight classiﬁcation tasks (two in each do-

main). For targets that are not already in-domain

for ROBERTA, our experiments show that contin-

arXiv:2004.10964v3  [cs.CL]  5 May 2020

## Page 2

ued pretraining on the domain (which we refer to as

domain-adaptive pretraining or DAPT) consistently

improves performance on tasks from the target do-

main, in both high- and low-resource settings.

Above, we consider domains deﬁned around gen-

res and forums, but it is also possible to induce a

domain from a given corpus used for a task, such

as the one used in supervised training of a model.

This raises the question of whether pretraining on

a corpus more directly tied to the task can fur-

ther improve performance. We study how domain-

adaptive pretraining compares to task-adaptive pre-

training, or TAPT, on a smaller but directly task-

relevant corpus: the unlabeled task dataset (§4),

drawn from the task distribution. Task-adaptive

pretraining has been shown effective (Howard and

Ruder, 2018), but is not typically used with the

most recent models. We ﬁnd that TAPT provides

a large performance boost for ROBERTA, with or

without domain-adaptive pretraining.

Finally, we show that the beneﬁts from task-

adaptive pretraining increase when we have addi-

tional unlabeled data from the task distribution that

has been manually curated by task designers or an-

notators. Inspired by this success, we propose ways

to automatically select additional task-relevant un-

labeled text, and show how this improves perfor-

mance in certain low-resource cases (§5). On all

tasks, our results using adaptive pretraining tech-

niques are competitive with the state of the art.

In summary, our contributions include:

• a thorough analysis of domain- and task-

adaptive pretraining across four domains and

eight tasks, spanning low- and high-resource

settings;

• an investigation into the transferability of

adapted LMs across domains and tasks; and

• a study highlighting the importance of pre-

training on human-curated datasets, and a sim-

ple data selection strategy to automatically

approach this performance.

Our code as well as pretrained models for multiple

domains and tasks are publicly available.1

2

Background: Pretraining

Learning for most NLP research systems since

2018 consists of training in two stages. First, a

neural language model (LM), often with millions

of parameters, is trained on large unlabeled cor-

1https://github.com/allenai/

dont-stop-pretraining

pora. The word (or wordpiece; Wu et al. 2016)

representations learned in the pretrained model are

then reused in supervised training for a downstream

task, with optional updates (ﬁne-tuning) of the rep-

resentations and network from the ﬁrst stage.

One such pretrained LM is ROBERTA (Liu

et al., 2019), which uses the same transformer-

based architecture (Vaswani et al., 2017) as its

predecessor, BERT (Devlin et al., 2019).

It is

trained with a masked language modeling objec-

tive (i.e., cross-entropy loss on predicting randomly

masked tokens). The unlabeled pretraining corpus

for ROBERTA contains over 160 GB of uncom-

pressed raw text from different English-language

corpora (see Appendix §A.1). ROBERTA attains

better performance on an assortment of tasks than

its predecessors, making it our baseline of choice.

Although ROBERTA’s pretraining corpus is de-

rived from multiple sources, it has not yet been

established if these sources are diverse enough to

generalize to most of the variation in the English

language. In other words, we would like to un-

derstand what is out of ROBERTA’s domain. To-

wards this end, we explore further adaptation by

continued pretraining of this large LM into two

categories of unlabeled data: (i) large corpora of

domain-speciﬁc text (§3), and (ii) available unla-

beled data associated with a given task (§4).

3

Domain-Adaptive Pretraining

Our approach to domain-adaptive pretraining

(DAPT) is straightforward—we continue pretrain-

ing ROBERTA on a large corpus of unlabeled

domain-speciﬁc text. The four domains we focus

on are biomedical (BIOMED) papers, computer sci-

ence (CS) papers, newstext from REALNEWS, and

AMAZON reviews. We choose these domains be-

cause they have been popular in previous work, and

datasets for text classiﬁcation are available in each.

Table 1 lists the speciﬁcs of the unlabeled datasets

in all four domains, as well as ROBERTA’s training

corpus.1

3.1

Analyzing Domain Similarity

Before performing DAPT, we attempt to quantify

the similarity of the target domain to ROBERTA’s

pretraining domain. We consider domain vocab-

ularies containing the top 10K most frequent uni-

grams (excluding stopwords) in comparably sized

1For BIOMED and CS, we used an internal version of

S2ORC that contains papers that cannot be released due to

copyright restrictions.

## Page 3

Domain

Pretraining Corpus

# Tokens

Size

## Lrob.

## Ldapt

## Biomed

2.68M full-text papers from S2ORC (Lo et al., 2020)

## 7.55B

## 47Gb

1.32

0.99

## Cs

2.22M full-text papers from S2ORC (Lo et al., 2020)

## 8.10B

## 48Gb

1.63

1.34

## News

11.90M articles from REALNEWS (Zellers et al., 2019)

## 6.66B

## 39Gb

1.08

1.16

## Reviews

24.75M AMAZON reviews (He and McAuley, 2016)

## 2.11B

## 11Gb

2.10

1.93

ROBERTA (baseline)

see Appendix §A.1

## N/A

## 160Gb

‡1.19

-

Table 1: List of the domain-speciﬁc unlabeled datasets. In columns 5 and 6, we report ROBERTA’s masked LM

loss on 50K randomly sampled held-out documents from each domain before (LROB.) and after (LDAPT) DAPT

(lower implies a better ﬁt on the sample). ‡ indicates that the masked LM loss is estimated on data sampled from

sources similar to ROBERTA’s pretraining corpus.

## Pt

News

Reviews

BioMed

## Cs

## Pt

News

Reviews

BioMed

## Cs

100.0

54.1

34.5

27.3

19.2

54.1

100.0

40.0

24.9

17.3

34.5

40.0

100.0

18.3

12.7

27.3

24.9

18.3

100.0

21.4

19.2

17.3

12.7

21.4

100.0

Figure 2:

Vocabulary overlap (%) between do-

mains. PT denotes a sample from sources similar to

ROBERTA’s pretraining corpus. Vocabularies for each

domain are created by considering the top 10K most

frequent words (excluding stopwords) in documents

sampled from each domain.

random samples of held-out documents in each do-

main’s corpus. We use 50K held-out documents

for each domain other than REVIEWS, and 150K

held-out documents in REVIEWS, since they are

much shorter. We also sample 50K documents from

sources similar to ROBERTA’s pretraining corpus

(i.e., BOOKCORPUS, STORIES, WIKIPEDIA, and

REALNEWS) to construct the pretraining domain

vocabulary, since the original pretraining corpus

is not released. Figure 2 shows the vocabulary

overlap across these samples. We observe that

ROBERTA’s pretraining domain has strong vocab-

ulary overlap with NEWS and REVIEWS, while

CS and BIOMED are far more dissimilar to the

other domains. This simple analysis suggests the

degree of beneﬁt to be expected by adaptation of

ROBERTA to different domains—the more dissim-

ilar the domain, the higher the potential for DAPT.

3.2

Experiments

Our LM adaptation follows the settings prescribed

for training ROBERTA. We train ROBERTA on

each domain for 12.5K steps, which amounts to

single pass on each domain dataset, on a v3-8 TPU;

see other details in Appendix B. This second phase

of pretraining results in four domain-adapted LMs,

one for each domain. We present the masked LM

loss of ROBERTA on each domain before and after

DAPT in Table 1. We observe that masked LM loss

decreases in all domains except NEWS after DAPT,

where we observe a marginal increase. We discuss

cross-domain masked LM loss in Appendix §E.

Under each domain, we consider two text clas-

siﬁcation tasks, as shown in Table 2. Our tasks

represent both high- and low-resource (≤5K la-

beled training examples, and no additional unla-

beled data) settings. For HYPERPARTISAN, we use

the data splits from Beltagy et al. (2020). For RCT,

we represent all sentences in one long sequence for

simultaneous prediction.

Baseline

As our baseline, we use an off-the-shelf

ROBERTA-base model and perform supervised

ﬁne-tuning of its parameters for each classiﬁcation

task. On average, ROBERTA is not drastically be-

hind the state of the art (details in Appendix §A.2),

and serves as a good baseline since it provides a

single LM to adapt to different domains.

Classiﬁcation Architecture

Following standard

practice (Devlin et al., 2019) we pass the ﬁnal layer

[CLS] token representation to a task-speciﬁc feed-

forward layer for prediction (see Table 14 in Ap-

pendix for more hyperparameter details).

Results

Test results are shown under the DAPT

column of Table 3 (see Appendix §C for valida-

tion results).

We observe that DAPT improves

over ROBERTA in all domains. For BIOMED,

CS, and REVIEWS, we see consistent improve-

## Page 4

Domain

Task

Label Type

Train (Lab.)

Train (Unl.)

Dev.

Test

Classes

## Biomed

## Chemprot

relation classiﬁcation

4169

-

2427

3469

13

## †Rct

abstract sent. roles

18040

-

30212

30135

5

## Cs

## Acl-Arc

citation intent

1688

-

114

139

6

## Scierc

relation classiﬁcation

3219

-

455

974

7

## News

## Hyperpartisan

partisanship

515

5000

65

65

2

## †Agnews

topic

115000

-

5000

7600

4

## Reviews

## †Helpfulness

review helpfulness

115251

-

5000

25000

2

## †Imdb

review sentiment

20000

50000

5000

25000

2

Table 2: Speciﬁcations of the various target task datasets. † indicates high-resource settings. Sources: CHEMPROT

(Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), SCIERC (Luan

et al., 2018), HYPERPARTISAN (Kiesel et al., 2019), AGNEWS (Zhang et al., 2015), HELPFULNESS (McAuley

et al., 2015), IMDB (Maas et al., 2011).

Dom. Task

## Roba.

## Dapt

## ¬Dapt

## Bm

## Chemprot 81.91.0

84.20.2

79.41.3

## †Rct

87.20.1

87.60.1

86.90.1

## Cs

## Acl-Arc

63.05.8

75.42.5

66.44.1

## Scierc

77.31.9

80.81.5

79.20.9

## News

## Hyp.

86.60.9

88.25.9

76.44.9

## †Agnews

93.90.2

93.90.2

93.50.2

## Rev.

## †Helpful. 65.13.4

66.51.4

65.12.8

## †Imdb

95.00.2

95.40.2

94.10.4

Table 3: Comparison of ROBERTA (ROBA.)

and

DAPT to adaptation to an irrelevant domain (¬

DAPT). Reported results are test macro-F1, except for

CHEMPROT and RCT, for which we report micro-F1,

following Beltagy et al. (2019). We report averages

across ﬁve random seeds, with standard deviations as

subscripts. † indicates high-resource settings. Best task

performance is boldfaced. See §3.3 for our choice of

irrelevant domains.

ments over ROBERTA, demonstrating the beneﬁt

of DAPT when the target domain is more distant

from ROBERTA’s source domain. The pattern is

consistent across high- and low- resource settings.

Although DAPT does not increase performance on

AGNEWS, the beneﬁt we observe in HYPERPAR-

TISAN suggests that DAPT may be useful even for

tasks that align more closely with ROBERTA’s

source domain.

3.3

Domain Relevance for DAPT

Additionally, we compare DAPT against a setting

where for each task, we adapt the LM to a domain

outside the domain of interest. This controls for the

case in which the improvements over ROBERTA

might be attributed simply to exposure to more data,

regardless of the domain. In this setting, for NEWS,

we use a CS LM; for REVIEWS, a BIOMED LM;

for CS, a NEWS LM; for BIOMED, a REVIEWS

LM. We use the vocabulary overlap statistics in

Figure 2 to guide these choices.

Our results are shown in Table 3, where the last

column (¬DAPT) corresponds to this setting. For

each task, DAPT signiﬁcantly outperforms adapting

to an irrelevant domain, suggesting the importance

of pretraining on domain-relevant data. Further-

more, we generally observe that ¬DAPT results

in worse performance than even ROBERTA on

end-tasks. Taken together, these results indicate

that in most settings, exposure to more data with-

out considering domain relevance is detrimental

to end-task performance. However, there are two

tasks (SCIERC and ACL-ARC) in which ¬DAPT

marginally improves performance over ROBERTA.

This may suggest that in some cases, continued pre-

training on any additional data is useful, as noted

in Baevski et al. (2019).

3.4

Domain Overlap

Our analysis of DAPT is based on prior intuitions

about how task data is assigned to speciﬁc domains.

For instance, to perform DAPT for HELPFULNESS,

we only adapt to AMAZON reviews, but not to any

REALNEWS articles. However, the gradations in

Figure 2 suggest that the boundaries between do-

mains are in some sense fuzzy; for example, 40%

of unigrams are shared between REVIEWS and

NEWS. As further indication of this overlap, we

also qualitatively identify documents that overlap

cross-domain: in Table 4, we showcase reviews

and REALNEWS articles that are similar to these

reviews (other examples can be found in Appendix

§D). In fact, we ﬁnd that adapting ROBERTA to

## Page 5

IMDB review

REALNEWS article

The Shop Around the Corner is one of the great ﬁlms from director

Ernst Lubitsch . In addition to the talents of James Stewart and Margaret Sullavan ,

it’s ﬁlled with a terriﬁc cast of top character actors such as Frank Morgan and Felix

Bressart. [...] The makers of You’ve Got Mail claim their ﬁlm to be a remake , but

that’s just nothing but a lot of inﬂated self praise. Anyway, if you have an affection for

romantic comedies of the 1940 ’s, you’ll ﬁnd The Shop Around the Corner to be

nothing short of wonderful. Just as good with repeat viewings.

[...] Three great festive ﬁlms... The Shop Around

the Corner (1940) Delightful Comedy by Ernst

Lubitsch stars James Stewart and Margaret Sulla-

van falling in love at Christmas. Remade as

Youve Got Mail. [...]

HELPFULNESS review

REALNEWS article

Simply the Best! I’ve owned countless Droids and iPhones, but this one destroys them

all. Samsung really nailed it with this one, extremely fast , very pocketable, gorgeous

display , exceptional battery life , good audio quality, perfect GPS & WiFi

performance, transparent status bar, battery percentage, ability to turn off soft key

lights, superb camera for a smartphone and more! [...]

Were living in a world with a new Samsung.

[...] more on battery life later [...] Exposure is

usually spot on and focusing is very fast. [...]

The design, display, camera and performance

are all best in class, and the phone feels smaller

than it looks. [...]

Table 4: Examples that illustrate how some domains might have overlaps with others, leading to unexpected

positive transfer. We highlight expressions in the reviews that are also found in the REALNEWS articles.

NEWS not as harmful to its performance on RE-

VIEWS tasks (DAPT on NEWS achieves 65.52.3 on

HELPFULNESS and 95.00.1 on IMDB).

Although this analysis is by no means compre-

hensive, it indicates that the factors that give rise to

observable domain differences are likely not mu-

tually exclusive. It is possible that pretraining be-

yond conventional domain boundaries could result

in more effective DAPT; we leave this investiga-

tion to future work. In general, the provenance of

data, including the processes by which corpora are

curated, must be kept in mind when designing pre-

training procedures and creating new benchmarks

that test out-of-domain generalization abilities.

4

Task-Adaptive Pretraining

Datasets curated to capture speciﬁc tasks of inter-

est tend to cover only a subset of the text avail-

able within the broader domain.

For example,

the CHEMPROT dataset for extracting relations be-

tween chemicals and proteins focuses on abstracts

of recently-published, high-impact articles from

hand-selected PubMed categories (Krallinger et al.,

2017, 2015). We hypothesize that such cases where

the task data is a narrowly-deﬁned subset of the

broader domain, pretraining on the task dataset

itself or data relevant to the task may be helpful.

Task-adaptive pretraining (TAPT) refers to pre-

training on the unlabeled training set for a given

task; prior work has shown its effectiveness (e.g.

Howard and Ruder, 2018). Compared to domain-

adaptive pretraining (DAPT; §3), the task-adaptive

approach strikes a different trade-off: it uses a far

smaller pretraining corpus, but one that is much

more task-relevant (under the assumption that the

training set represents aspects of the task well).

This makes TAPT much less expensive to run than

DAPT, and as we show in our experiments, the per-

formance of TAPT is often competitive with that of

## Dapt.

4.1

Experiments

Similar to DAPT, task-adaptive pretraining consists

of a second phase of pretraining ROBERTA, but

only on the available task-speciﬁc training data. In

contrast to DAPT, which we train for 12.5K steps,

we perform TAPT for 100 epochs. We artiﬁcially

augment each dataset by randomly masking differ-

ent words (using the masking probability of 0.15)

across epochs. As in our DAPT experiments, we

pass the ﬁnal layer [CLS] token representation to

a task-speciﬁc feedforward layer for classiﬁcation

(see Table 14 in Appendix for more hyperparameter

details).

Our results are shown in the TAPT column of Ta-

ble 5. TAPT consistently improves the ROBERTA

baseline for all tasks across domains. Even on the

news domain, which was part of ROBERTA pre-

training corpus, TAPT improves over ROBERTA,

showcasing the advantage of task adaptation. Par-

ticularly remarkable are the relative differences be-

tween TAPT and DAPT. DAPT is more resource in-

tensive (see Table 9 in §5.3), but TAPT manages to

match its performance in some of the tasks, such as

SCIERC. In RCT, HYPERPARTISAN, AGNEWS,

HELPFULNESS, and IMDB, the results even ex-

ceed those of DAPT, highlighting the efﬁcacy of

this cheaper adaptation technique.

## Page 6

Additional Pretraining Phases

Domain

Task

## Roberta

## Dapt

## Tapt

## Dapt + Tapt

## Biomed

## Chemprot

81.91.0

84.20.2

82.60.4

84.40.4

## †Rct

87.20.1

87.60.1

87.70.1

87.80.1

## Cs

## Acl-Arc

63.05.8

75.42.5

67.41.8

75.63.8

## Scierc

77.31.9

80.81.5

79.31.5

81.31.8

## News

## Hyperpartisan

86.60.9

88.25.9

90.45.2

90.06.6

## †Agnews

93.90.2

93.90.2

94.50.1

94.60.1

## Reviews

## †Helpfulness

65.13.4

66.51.4

68.51.9

68.71.8

## †Imdb

95.00.2

95.40.1

95.50.1

95.60.1

Table 5: Results on different phases of adaptive pretraining compared to the baseline ROBERTA (col. 1). Our

approaches are DAPT (col. 2, §3), TAPT (col. 3, §4), and a combination of both (col. 4). Reported results follow the

same format as Table 3. State-of-the-art results we can compare to: CHEMPROT (84.6), RCT (92.9), ACL-ARC

(71.0), SCIERC (81.8), HYPERPARTISAN (94.8), AGNEWS (95.5), IMDB (96.2); references in §A.2.

## Biomed

## Rct

## Chemprot

## Tapt

87.70.1

82.60.5

Transfer-TAPT

87.10.4 (↓0.6)

80.40.6 (↓2.2)

## News

## Hyperpartisan

## Agnews

## Tapt

89.99.5

94.50.1

Transfer-TAPT

82.27.7 (↓7.7)

93.90.2 (↓0.6)

## Cs

## Acl-Arc

## Scierc

## Tapt

67.41.8

79.31.5

Transfer-TAPT

64.12.7 (↓3.3)

79.12.5 (↓0.2)

## Reviews

## Helpfulness

## Imdb

## Tapt

68.51.9

95.70.1

Transfer-TAPT

65.02.6 (↓3.5)

95.00.1 (↓0.7)

Table 6: Though TAPT is effective (Table 5), it is harmful when applied across tasks. These ﬁndings illustrate

differences in task distributions within a domain.

Combined DAPT and TAPT

We investigate the

effect of using both adaptation techniques together.

We begin with ROBERTA and apply DAPT then

TAPT under this setting. The three phases of pre-

training add up to make this the most computation-

ally expensive of all our settings (see Table 9). As

expected, combined domain- and task-adaptive pre-

training achieves the best performance on all tasks

(Table 5).2

Overall, our results show that DAPT followed by

TAPT achieves the best of both worlds of domain

and task awareness, yielding the best performance.

While we speculate that TAPT followed by DAPT

would be susceptible to catastrophic forgetting of

the task-relevant corpus (Yogatama et al., 2019), al-

ternate methods of combining the procedures may

result in better downstream performance. Future

work may explore pretraining with a more sophisti-

cated curriculum of domain and task distributions.

2Results on HYPERPARTISAN match those of TAPT, within

a standard deviation arising from the ﬁve seeds.

Cross-Task Transfer

We complete the compari-

son between DAPT and TAPT by exploring whether

adapting to one task transfers to other tasks in the

same domain. For instance, we further pretrain

the LM using the RCT unlabeled data, ﬁne-tune it

with the CHEMPROT labeled data, and observe the

effect. We refer to this setting as Transfer-TAPT.

Our results for tasks in all four domains are shown

in Table 6. We see that TAPT optimizes for single

task performance, to the detriment of cross-task

transfer. These results demonstrate that data distri-

butions of tasks within a given domain might differ.

Further, this could also explain why adapting only

to a broad domain is not sufﬁcient, and why TAPT

after DAPT is effective.

5

Augmenting Training Data for

Task-Adaptive Pretraining

In §4, we continued pretraining the LM for task

adaptation using only the training data for a super-

vised task. Inspired by the success of TAPT, we

next investigate another setting where a larger pool

of unlabeled data from the task distribution exists,

## Page 7

Pretraining

## Biomed

## News

## Reviews

## Rct-500

## Hyp.

## Imdb †

## Tapt

79.81.4

90.45.2

95.50.1

## Dapt + Tapt

83.00.3

90.06.6

95.60.1

Curated-TAPT

83.40.3

89.99.5

95.70.1

DAPT + Curated-TAPT

83.80.5

92.13.6

95.80.1

Table 7:

Mean test set macro-F1 (for HYP. and

IMDB) and micro-F1 (for RCT-500), with Curated-

TAPT across ﬁve random seeds, with standard devia-

tions as subscripts. † indicates high-resource settings.

typically curated by humans.

We explore two scenarios. First, for three tasks

(RCT, HYPERPARTISAN, and IMDB) we use this

larger pool of unlabeled data from an available

human-curated corpus (§5.1). Next, we explore

retrieving related unlabeled data for TAPT, from a

large unlabeled in-domain corpus, for tasks where

extra human-curated data is unavailable (§5.2).

5.1

Human Curated-TAPT

Dataset creation often involves collection of a large

unlabeled corpus from known sources. This corpus

is then downsampled to collect annotations, based

on the annotation budget. The larger unlabeled cor-

pus is thus expected to have a similar distribution

to the task’s training data. Moreover, it is usually

available. We explore the role of such corpora in

task-adaptive pretraining.

Data

We simulate a low-resource setting RCT-

500, by downsampling the training data of the RCT

dataset to 500 examples (out of 180K available),

and treat the rest of the training data as unlabeled.

The HYPERPARTISAN shared task (Kiesel et al.,

2019) has two tracks: low- and high-resource. We

use 5K documents from the high-resource setting as

Curated-TAPT unlabeled data and the original low-

resource training documents for task ﬁne-tuning.

For IMDB, we use the extra unlabeled data man-

ually curated by task annotators, drawn from the

same distribution as the labeled data (Maas et al.,

2011).

Results

We compare Curated-TAPT to TAPT and

DAPT + TAPT in Table 7. Curated-TAPT further

improves our prior results from §4 across all three

datasets. Applying Curated-TAPT after adapting to

the domain results in the largest boost in perfor-

mance on all tasks; in HYPERPARTISAN, DAPT

+ Curated-TAPT is within standard deviation of

Curated-TAPT. Moreover, curated-TAPT achieves

Figure 3:

An illustration of automated data selec-

tion (§5.2). We map unlabeled CHEMPROT and 1M

BIOMED sentences to a shared vector space using the

VAMPIRE model trained on these sentences. Then,

for each CHEMPROT sentence, we identify k nearest

neighbors, from the BIOMED domain.

Pretraining

## Biomed

## Cs

## Chemprot

## Rct-500

## Acl-Arc

## Roberta

81.91.0

79.30.6

63.05.8

## Tapt

82.60.4

79.81.4

67.41.8

## Rand-Tapt

81.90.6

80.60.4

69.73.4

## 50Nn-Tapt

83.30.7

80.80.6

70.72.8

## 150Nn-Tapt

83.20.6

81.20.8

73.32.7

## 500Nn-Tapt

83.30.7

81.70.4

75.51.9

## Dapt

84.20.2

82.50.5

75.42.5

Table 8:

Mean test set micro-F1 (for CHEMPROT

and RCT) and macro-F1 (for ACL-ARC), across ﬁve

random seeds, with standard deviations as subscripts,

comparing RAND-TAPT (with 50 candidates) and kNN-

TAPT selection. Neighbors of the task data are selected

from the domain data.

95% of the performance of DAPT + TAPT with the

fully labeled RCT corpus (Table 5) with only 0.3%

of the labeled data. These results suggest that curat-

ing large amounts of data from the task distribution

is extremely beneﬁcial to end-task performance.

We recommend that task designers release a large

pool of unlabeled task data for their tasks to aid

model adaptation through pretraining.

5.2

Automated Data Selection for TAPT

Consider a low-resource scenario without access to

large amounts of unlabeled data to adequately bene-

ﬁt from TAPT, as well as absence of computational

resources necessary for DAPT (see Table 9 for de-

tails of computational requirements for different

pretraining phases). We propose simple unsuper-

## Page 8

vised methods to retrieve unlabeled text that aligns

with the task distribution, from a large in-domain

corpus. Our approach ﬁnds task-relevant data from

the domain by embedding text from both the task

and domain in a shared space, then selects candi-

dates from the domain based on queries using the

task data. Importantly, the embedding method must

be lightweight enough to embed possibly millions

of sentences in a reasonable time.

Given these constraints, we employ VAMPIRE

(Gururangan et al., 2019; Figure 3), a lightweight

bag-of-words language model. We pretrain VAM-

PIRE on a large deduplicated3 sample of the do-

main (1M sentences) to obtain embeddings of the

text from both the task and domain sample. We

then select k candidates of each task sentence from

the domain sample, in embeddings space. Candi-

dates are selected (i) via nearest neighbors selection

(kNN-TAPT)4, or (ii) randomly (RAND-TAPT). We

continue pretraining ROBERTA on this augmented

corpus with both the task data (as in TAPT) as well

as the selected candidate pool.

Results

Results in Table 8 show that kNN-TAPT

outperforms TAPT for all cases. RAND-TAPT is gen-

erally worse than kNN-TAPT, but within a standard

deviation arising from 5 seeds for RCT and ACL-

ARC. As we increase k, kNN-TAPT performance

steadily increases, and approaches that of DAPT.

Appendix F shows examples of nearest neighbors

of task data. Future work might consider a closer

study of kNN-TAPT, more sophisticated data selec-

tion methods, and the tradeoff between the diversity

and task relevance of selected examples.

5.3

Computational Requirements

The computational requirements for all our adap-

tation techniques on RCT-500 in the BIOMED do-

main in Table 9. TAPT is nearly 60 times faster

to train than DAPT on a single v3-8 TPU and stor-

age requirements for DAPT on this task are 5.8M

times that of TAPT. Our best setting of DAPT +

TAPT amounts to three phases of pretraining, and at

ﬁrst glance appears to be very expensive. However,

once the LM has been adapted to a broad domain, it

can be reused for multiple tasks within that domain,

with only a single additional TAPT phase per task.

While Curated-TAPT tends to achieve the best cost-

3We deduplicated this set to limit computation, since dif-

ferent sentences can share neighbors.

4We use a ﬂat search index with cosine similarity between

embeddings with the FAISS (Johnson et al., 2019) library.

Pretraining

Steps

Docs.

Storage

## F1

## Roberta

-

-

-

79.30.6

## Tapt

## 0.2K

500

## 80Kb

79.81.4

## 50Nn-Tapt

## 1.1K

## 24K

## 3Mb

80.80.6

## 150Nn-Tapt

## 3.2K

## 66K

## 8Mb

81.20.8

## 500Nn-Tapt

## 9.0K

## 185K

## 24Mb

81.70.4

Curated-TAPT

## 8.8K

## 180K

## 27Mb

83.40.3

## Dapt

## 12.5K

## 25M

## 47Gb

82.50.5

## Dapt + Tapt

## 12.6K

## 25M

## 47Gb

83.00.3

Table 9: Computational requirements for adapting to

the RCT-500 task, comparing DAPT (§3) and the vari-

ous TAPT modiﬁcations described in §4 and §5.

beneﬁt ratio in this comparison, one must also take

into account the cost of curating large in-domain

data. Automatic methods such as kNN-TAPT are

much cheaper than DAPT.

6

Related Work

Transfer

learning

for

domain

adaptation

Prior work has shown the beneﬁt of continued

pretraining in domain (Alsentzer et al., 2019;

Chakrabarty et al., 2019; Lee et al., 2019).5 We

have contributed further investigation of the effects

of a shift between a large, diverse pretraining

corpus and target domain on task performance.

Other studies (e.g., Huang et al., 2019) have

trained language models (LMs) in their domain

of interest, from scratch. In contrast, our work

explores multiple domains, and is arguably more

cost effective, since we continue pretraining an

already powerful LM.

Task-adaptive pretraining

Continued pretrain-

ing of a LM on the unlabeled data of a given task

(TAPT) has been show to be beneﬁcial for end-

task performance (e.g. in Howard and Ruder, 2018;

Phang et al., 2018; Sun et al., 2019). In the pres-

ence of domain shift between train and test data

distributions of the same task, domain-adaptive pre-

training (DAPT) is sometimes used to describe what

we term TAPT (Logeswaran et al., 2019; Han and

Eisenstein, 2019). Related approaches include lan-

guage modeling as an auxiliary objective to task

classiﬁer ﬁne-tuning (Chronopoulou et al., 2019;

Radford et al., 2018) or consider simple syntactic

structure of the input while adapting to task-speciﬁc

5In contrast, Peters et al. (2019) ﬁnd that the Jensen-

Shannon divergence on term distributions between BERT’s

pretraining corpora and each MULTINLI domain (Williams

et al., 2018) does not predict its performance, though this

might be an isolated ﬁnding speciﬁc to the MultiNLI dataset.

## Page 9

Training Data

Domain

(Unlabeled)

Task

(Unlabeled)

Task

(Labeled)

## Roberta

✓

## Dapt

✓

✓

## Tapt

✓

✓

## Dapt + Tapt

✓

✓

✓

kNN-TAPT

(Subset)

✓

✓

Curated-TAPT

(Extra)

✓

Table 10: Summary of strategies for multi-phase pre-

training explored in this paper.

data (Swayamdipta et al., 2019). We compare DAPT

and TAPT as well as their interplay with respect to

dataset size for continued pretraining (hence, ex-

pense of more rounds of pretraining), relevance to

a data sample of a given task, and transferability to

other tasks and datasets. See Table 11 in Appendix

§A for a summary of multi-phase pretraining strate-

gies from related work.

Data selection for transfer learning

Selecting

data for transfer learning has been explored in NLP

(Moore and Lewis, 2010; Ruder and Plank, 2017;

Zhang et al., 2019, among others). Dai et al. (2019)

focus on identifying the most suitable corpus to

pretrain a LM from scratch, for a single task: NER,

whereas we select relevant examples for various

tasks in §5.2. Concurrent to our work, Aharoni and

Goldberg (2020) propose data selection methods

for NMT based on cosine similarity in embedding

space, using DISTILBERT (Sanh et al., 2019) for

efﬁciency. In contrast, we use VAMPIRE, and

focus on augmenting TAPT data for text classiﬁ-

cation tasks. Khandelwal et al. (2020) introduced

kNN-LMs that allows easy domain adaptation of

pretrained LMs by simply adding a datastore per

domain and no further training; an alternative to

integrate domain information in an LM. Our study

of human-curated data §5.1 is related to focused

crawling (Chakrabarti et al., 1999) for collection of

suitable data, especially with LM reliance (Remus

and Biemann, 2016).

What is a domain?

Despite the popularity of

domain adaptation techniques, most research and

practice seems to use an intuitive understanding of

domains. A small body of work has attempted to

address this question (Lee, 2001; Eisenstein et al.,

2014; van der Wees et al., 2015; Plank, 2016; Ruder

et al., 2016, among others). For instance, Aharoni

and Goldberg (2020) deﬁne domains by implicit

clusters of sentence representations in pretrained

LMs. Our results show that DAPT and TAPT com-

plement each other, which suggests a spectra of

domains deﬁned around tasks at various levels of

granularity (e.g., Amazon reviews for a speciﬁc

product, all Amazon reviews, all reviews on the

web, the web).

7

Conclusion

We investigate several variations for adapting pre-

trained LMs to domains and tasks within those do-

mains, summarized in Table 10. Our experiments

reveal that even a model of hundreds of millions of

parameters struggles to encode the complexity of

a single textual domain, let alone all of language.

We show that pretraining the model towards a spe-

ciﬁc task or small corpus can provide signiﬁcant

beneﬁts. Our ﬁndings suggest it may be valuable

to complement work on ever-larger LMs with par-

allel efforts to identify and use domain- and task-

relevant corpora to specialize models. While our

results demonstrate how these approaches can im-

prove ROBERTA, a powerful LM, the approaches

we studied are general enough to be applied to

any pretrained LM. Our work points to numerous

future directions, such as better data selection for

TAPT, efﬁcient adaptation large pretrained language

models to distant domains, and building reusable

language models after adaptation.

Acknowledgments

The authors thank Dallas Card, Mark Neumann,

Nelson Liu, Eric Wallace, members of the Al-

lenNLP team, and anonymous reviewers for help-

ful feedback, and Arman Cohan for providing data.

This research was supported in part by the Ofﬁce of

Naval Research under the MURI grant N00014-18-

1-2670. TPU machines for conducting experiments

were provided by Google.

References

Roee Aharoni and Yoav Goldberg. 2020. Unsupervised

domain clusters in pretrained language models. In

ACL. To appear.

Emily Alsentzer, John Murphy, William Boag, Wei-

Hung Weng, Di Jindi, Tristan Naumann, and

Matthew McDermott. 2019. Publicly available clin-

ical BERT embeddings. In Proceedings of the 2nd

Clinical Natural Language Processing Workshop.

## Page 10

Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke

Zettlemoyer, and Michael Auli. 2019. Cloze-driven

pretraining of self-attention networks. In EMNLP.

Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-

ERT: A pretrained language model for scientiﬁc text.

In EMNLP.

Iz Beltagy, Matthew E. Peters, and Arman Cohan.

2020. Longformer: The long-document transformer.

arXiv:2004.05150.

Soumen Chakrabarti, Martin van den Berg, and Byron

Dom. 1999. Focused Crawling: A New Approach to

Topic-Speciﬁc Web Resource Discovery. Comput.

Networks, 31:1623–1640.

Tuhin Chakrabarty, Christopher Hidey, and Kathy

McKeown. 2019. IMHO ﬁne-tuning improves claim

detection. In NAACL.

Ciprian Chelba, Tomas Mikolov, Michael Schuster,

Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony

Robinson. 2014. One billion word benchmark for

measuring progress in statistical language modeling.

In INTERSPEECH.

Alexandra Chronopoulou,

Christos Baziotis,

and

Alexandros Potamianos. 2019. An embarrassingly

simple approach for transfer learning from pre-

trained language models. In NAACL.

Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi,

and Dan Weld. 2019. Pretrained language models

for sequential sentence classiﬁcation. In EMNLP.

Xiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile

Paris. 2019. Using similarity measures to select pre-

training data for NER. In NAACL.

Franck Dernoncourt and Ji Young Lee. 2017. Pubmed

200k RCT: a dataset for sequential sentence classiﬁ-

cation in medical abstracts. In IJCNLP.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2019.

BERT: Pre-training of

deep bidirectional transformers for language under-

standing. In NAACL.

Jesse Dodge, Suchin Gururangan, Dallas Card, Roy

Schwartz, and Noah A Smith. 2019.

Show your

work: Improved reporting of experimental results.

In EMNLP.

Jacob Eisenstein, Brendan O’connor, Noah A. Smith,

and Eric P. Xing. 2014. Diffusion of lexical change

in social media. PloS ONE.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind

Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-

ters, Michael Schmitz, and Luke Zettlemoyer. 2018.

AllenNLP: A deep semantic natural language pro-

cessing platform. In NLP-OSS.

Aaron Gokaslan and Vanya Cohen. 2019. OpenWeb-

Text Corpus.

Suchin Gururangan, Tam Dang, Dallas Card, and

Noah A. Smith. 2019.

Variational pretraining for

semi-supervised text classiﬁcation. In ACL.

Xiaochuang Han and Jacob Eisenstein. 2019. Unsuper-

vised domain adaptation of contextualized embed-

dings for sequence labeling. In EMNLP.

Ruining He and Julian McAuley. 2016. Ups and downs:

Modeling the visual evolution of fashion trends with

one-class collaborative ﬁltering. In WWW.

Matthew Honnibal and Ines Montani. 2017. spaCy 2:

Natural language understanding with Bloom embed-

dings, convolutional neural networks and incremen-

tal parsing.

Jeremy Howard and Sebastian Ruder. 2018. Universal

language model ﬁne-tuning for text classiﬁcation. In

## Acl.

Kexin Huang, Jaan Altosaar, and Rajesh Ranganath.

2019. ClinicalBERT: Modeling clinical notes and

predicting hospital readmission. arXiv:1904.05342.

Jeff Johnson, Matthijs Douze, and Herv´e J´egou. 2019.

Billion-scale similarity search with gpus.

## Ieee

Transactions on Big Data.

David Jurgens, Srijan Kumar, Raine Hoover, Daniel A.

McFarland, and Dan Jurafsky. 2018. Measuring the

evolution of a scientiﬁc ﬁeld through citation frames.

## Tacl.

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke

Zettlemoyer, and Mike Lewis. 2020. Generalization

through memorization: Nearest neighbor language

models. In ICLR. To appear.

Johannes Kiesel, Maria Mestre, Rishabh Shukla, Em-

manuel Vincent, Payam Adineh, David Corney,

Benno Stein, and Martin Potthast. 2019. SemEval-

2019 Task 4: Hyperpartisan news detection. In Se-

mEval.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A

method for stochastic optimization. In ICLR.

Martin Krallinger,

Obdulia Rabal,

Saber Ahmad

Akhondi, Mart´ın P´erez P´erez, J´es´us L´opez Santa-

mar´ıa, Gael P´erez Rodr´ıguez, Georgios Tsatsaro-

nis, Ander Intxaurrondo, Jos´e Antonio Baso L´opez,

Umesh Nandal, E. M. van Buel, A. Poorna Chan-

drasekhar, Marleen Rodenburg, Astrid Lægreid,

Marius A. Doornenbal, Julen Oyarz´abal, An´alia

Loureno, and Alfonso Valencia. 2017. Overview of

the biocreative vi chemical-protein interaction track.

In Proceedings of the BioCreative VI Workshop.

Martin Krallinger, Obdulia Rabal, Florian Leitner,

Miguel Vazquez,

David Salgado,

Zhiyong Lu,

Robert Leaman, Yanan Lu, Donghong Ji, Daniel M

Lowe, et al. 2015. The chemdner corpus of chemi-

cals and drugs and its annotation principles. Journal

of cheminformatics, 7(1):S2.

## Page 11

Jens Kringelum, Sonny Kim Kjærulff, Søren Brunak,

Ole Lund, Tudor I. Oprea, and Olivier Taboureau.

2016. ChemProt-3.0: a global chemical biology dis-

eases mapping. In Database.

David YW Lee. 2001. Genres, registers, text types, do-

mains and styles: Clarifying the concepts and nav-

igating a path through the BNC jungle. Language

Learning & Technology.

Jinhyuk

Lee,

Wonjin

Yoon,

Sungdong

Kim,

Donghyeon Kim,

Sunkyu Kim,

Chan Ho So,

and Jaewoo Kang. 2019. BioBERT: A pre-trained

biomedical

language

representation

model

for

biomedical text mining. Bioinformatics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-

dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,

Luke Zettlemoyer, and Veselin Stoyanov. 2019.

RoBERTa: A robustly optimized BERT pretraining

approach. arXiv:1907.11692.

Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-

ney, and Daniel S. Weld. 2020.

S2ORC: The Se-

mantic Scholar Open Research Corpus. In ACL. To

appear.

Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee,

Kristina Toutanova, Jacob Devlin, and Honglak Lee.

2019. Zero-shot entity linking by reading entity de-

scriptions. In ACL.

Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh

Hajishirzi. 2018.

Multi-task identiﬁcation of enti-

ties, relations, and coreference for scientiﬁc knowl-

edge graph construction. In EMNLP.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,

Dan Huang, Andrew Y. Ng, and Christopher Potts.

2011. Learning word vectors for sentiment analysis.

In ACL.

Julian McAuley, Christopher Targett, Qinfeng Shi, and

Anton Van Den Hengel. 2015. Image-based recom-

mendations on styles and substitutes. In ACM SI-

## Gir.

Arindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal,

Swaroop Ranjan Mishra, and Chitta Baral. 2020.

Exploring ways to incorporate additional knowledge

to improve natural language commonsense question

answering. arXiv:1909.08855v3.

Robert C. Moore and William Lewis. 2010. Intelligent

selection of language model training data. In ACL.

Sebastian Nagel. 2016. CC-NEWS.

Mark Neumann, Daniel King, Iz Beltagy, and Waleed

Ammar. 2019. Scispacy: Fast and robust models for

biomedical natural language processing.

Proceed-

ings of the 18th BioNLP Workshop and Shared Task.

Matthew E. Peters, Sebastian Ruder, and Noah A.

Smith. 2019.

To tune or not to tune?

Adapt-

ing pretrained representations to diverse tasks. In

RepL4NLP.

Jason Phang, Thibault F´evry, and Samuel R. Bow-

man. 2018. Sentence encoders on STILTs: Supple-

mentary training on intermediate labeled-data tasks.

arXiv:1811.01088.

Barbara Plank. 2016. What to do about non-standard

(or non-canonical) language in NLP. In KONVENS.

Alec Radford, Karthik Narasimhan, Tim Salimans, and

Ilya Sutskever. 2018.

Improving language under-

standing by generative pre-training.

Colin Raffel, Noam Shazeer, Adam Kaleo Roberts,

Katherine Lee, Sharan Narang, Michael Matena,

Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.

Ex-

ploring the limits of transfer learning with a uniﬁed

text-to-text transformer. arXiv:1910.10683.

Steffen Remus and Chris Biemann. 2016.

Domain-

Speciﬁc Corpus Expansion with Focused Webcrawl-

ing. In LREC.

Sebastian Ruder, Parsa Ghaffari, and John G. Breslin.

2016. Towards a continuous modeling of natural lan-

guage domains. In Workshop on Uphill Battles in

Language Processing: Scaling Early Achievements

to Robust Methods.

Sebastian Ruder and Barbara Plank. 2017. Learning to

select data for transfer learning with Bayesian opti-

mization. In EMNLP.

Victor Sanh, Lysandre Debut, Julien Chaumond, and

Thomas Wolf. 2019. DistilBERT, a distilled version

of BERT: smaller, faster, cheaper and lighter.

In

EMC2 @ NeurIPS.

Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.

2019. How to ﬁne-tune BERT for text classiﬁcation?

In CCL.

Swabha Swayamdipta, Matthew Peters, Brendan Roof,

Chris Dyer, and Noah A Smith. 2019. Shallow syn-

tax in deep water. arXiv:1908.11047.

Tan Thongtan and Tanasanee Phienthrakul. 2019. Sen-

timent classiﬁcation using document embeddings

trained with cosine similarity. In ACL SRW.

Trieu H. Trinh and Quoc V. Le. 2018. A simple method

for commonsense reasoning. arXiv:1806.02847.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob

Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz

Kaiser, and Illia Polosukhin. 2017. Attention is all

you need. In NeurIPS.

Alex Wang,

Yada Pruksachatkun,

Nikita Nangia,

Amanpreet Singh, Julian Michael, Felix Hill, Omer

Levy, and Samuel R. Bowman. 2019. SuperGLUE:

A stickier benchmark for general-purpose language

understanding systems. In NeurIPS.

Alex Wang, Amanpreet Singh, Julian Michael, Fe-

lix Hill, Omer Levy, and Samuel Bowman. 2018.

GLUE: A multi-task benchmark and analysis plat-

form for natural language understanding. In Black-

boxNLP @ EMNLP.

## Page 12

Marlies van der Wees, Arianna Bisazza, Wouter

Weerkamp, and Christof Monz. 2015. What’s in a

domain? Analyzing genre and topic differences in

statistical machine translation. In ACL.

Adina Williams, Nikita Nangia, and Samuel Bowman.

2018. A broad-coverage challenge corpus for sen-

tence understanding through inference. In NAACL.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien

Chaumond, Clement Delangue, Anthony Moi, Pier-

ric Cistac, Tim Rault, Rmi Louf, Morgan Funtow-

icz, and Jamie Brew. 2019. HuggingFace’s Trans-

formers: State-of-the-art natural language process-

ing. arXiv:1910.03771.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V

Le,

Mohammad Norouzi,

Wolfgang Macherey,

Maxim Krikun,

Yuan Cao,

Qin Gao,

Klaus

Macherey, et al. 2016.

Google’s neural machine

translation system: Bridging the gap between human

and machine translation.

Hu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019a. BERT

post-training for review reading comprehension and

aspect-based sentiment analysis. In NAACL.

Hu Xu,

Bing Liu,

Lei Shu,

and Philip S. Yu.

2019b. Review conversational reading comprehen-

sion. arXiv:1902.00821v2.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-

bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.

XLNet: Generalized autoregressive pretraining for

language understanding. In NeurIPS.

Dani Yogatama, Cyprien de Masson d’Autume, Jerome

Connor, Tom´as Kocisk´y, Mike Chrzanowski, Ling-

peng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu,

Chris Dyer, and Phil Blunsom. 2019. Learning and

evaluating general linguistic intelligence.

Rowan Zellers,

Ari Holtzman,

Hannah Rashkin,

Yonatan Bisk, Ali Farhadi, Franziska Roesner, and

Yejin Choi. 2019.

Defending against neural fake

news. In NeurIPS.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.

Character-level convolutional networks for text clas-

siﬁcation. In NeurIPS.

Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul Mc-

Namee, Marine Carpuat, and Kevin Duh. 2019. Cur-

riculum learning for domain adaptation in neural ma-

chine translation. In NAACL.

Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan

Salakhutdinov, Raquel Urtasun, Antonio Torralba,

and Sanja Fidler. 2015. Aligning books and movies:

Towards story-like visual explanations by watching

movies and reading books. In ICCV.

## Page 13

Appendix Overview

In this supplementary material, we provide: (i)

additional information for producing the results in

the paper, and (ii) results that we could not ﬁt into

the main body of the paper.

Appendix A. A tabular overview of related work

described in Section §6, a description of the corpus

used to train ROBERTA in Liu et al. (2019), and

references to the state of the art on our tasks.

Appendix B. Details about the data preprocessing,

training, and implementation of domain- and task-

adaptive pretraining.

Appendix C. Development set results.

Appendix D. Examples of domain overlap.

Appendix E. The cross-domain masked LM loss

and reproducibility challenges.

Appendix F. Illustration of our data selection

method and examples of nearest neighbours.

## A

Related Work

Table 11 shows which of the strategies for contin-

ued pretraining have already been explored in the

prior work from the Related Work (§6). As evident

from the table, our work compares various strate-

gies as well as their interplay using a pretrained

language model trained on a much more heteroge-

neous pretraining corpus.

## A.1

ROBERTA’s Pretraining Corpus

ROBERTA was trained on data from BOOKCOR-

PUS (Zhu et al., 2015),6 WIKIPEDIA,7 a portion of

the CCNEWS dataset (Nagel, 2016),8 OPENWEB-

TEXT corpus of Web content extracted from URLs

shared on Reddit (Gokaslan and Cohen, 2019),9

and a subset of CommonCrawl that it is said to

resemble the “story-like” style of WINOGRAD

schemas (STORIES; Trinh and Le, 2018).10

## A.2

State of the Art

In this section, we specify the models achieving

state of the art on our tasks. See the caption of

6https://github.com/soskek/bookcorpus

7https://github.com/google-research/

bert

8https://github.com/fhamborg/

news-please

9https://github.com/jcpeterson/

openwebtext

10https://github.com/tensorflow/models/

tree/master/research/lm_commonsense

Table 5 for the reported performance of these mod-

els. For ACL-ARC, that is SCIBERT (Beltagy

et al., 2019), a BERT-base model for trained from

scratch on scientiﬁc text. For CHEMPROT and SCI-

ERC, that is S2ORC-BERT (Lo et al., 2020), a

similar model to SCIBERT. For AGNEWS and

IMDB, XLNet-large, a much larger model. For

RCT, Cohan et al. (2019). For HYPERPARTISAN,

LONGFORMER, a modiﬁed Transformer language

model for long documents (Beltagy et al., 2020).

Thongtan and Phienthrakul (2019) report a higher

number (97.42) on IMDB, but they train their word

vectors on the test set. Our baseline establishes the

ﬁrst benchmark for the HELPFULNESS dataset.

## B

Experimental Setup

Preprocessing for DAPT

The unlabeled corpus

in each domain was pre-processed prior to lan-

guage model training. Abstracts and body para-

graphs from biomedical and computer science

articles were used after sentence splitting using

scispaCy (Neumann et al., 2019). We used sum-

maries and full text of each news article, and the

entire body of review from Amazon reviews. For

both news and reviews, we perform sentence split-

ting using spaCy (Honnibal and Montani, 2017).

Training details for DAPT

We train ROBERTA

on each domain for 12.5K steps. We focused on

matching all the domain dataset sizes (see Table

1) such that each domain is exposed to the same

amount of data as for 12.5K steps it is trained for.

AMAZON reviews contain more documents, but

each is shorter. We used an effective batch size

of 2048 through gradient accumulation, as recom-

mended in Liu et al. (2019). See Table 13 for more

hyperparameter details.

Training details for TAPT

We use the same pre-

training hyperparameters as DAPT, but we artiﬁ-

cially augmented each dataset for TAPT by ran-

domly masking different tokens across epochs, us-

ing the masking probability of 0.15. Each dataset

was trained for 100 epochs. For tasks with less

than 5K examples, we used a batch size of 256

through gradient accumulation. See Table 13 for

more hyperparameter details.

Optimization

We used the Adam optimizer

(Kingma and Ba, 2015), a linear learning rate sched-

uler with 6% warm-up, a maximum learning rate

of 0.0005. When we used a batch size of 256, we

## Page 14

DAPT Domains

(if applicable)

Tasks

Model

## Dapt

## Tapt

## Dapt

## + Tapt

kNN-

## Tapt

Curated-

## Tapt

This Paper

biomedical & computer

science papers, news,

reviews

8 classiﬁcation

tasks

## Roberta

✓

✓

✓

✓

✓

Aharoni and Goldberg (2020)

-

## Nmt

## Distilbert +

Transformer NMT

-

-

-

similar

-

Alsentzer et al. (2019)

clinical text

## Ner, Nli,

de-identiﬁcation

## (Bio)Bert

✓

-

-

-

-

Chakrabarty et al. (2019)

opinionated claims from

Reddit

claim detection

## Ulmfit

✓

✓

-

-

-

Chronopoulou et al. (2019)

-

5 classiﬁcation

tasks

## Ulmfit†

-

similar

-

-

-

Han and Eisenstein (2019)

-

NER in historical

texts

## Elmo, Bert

-

✓

-

-

-

Howard and Ruder (2018)

-

6 classiﬁcation

tasks

## Ulmfit

-

✓

-

-

-

Khandelwal et al. (2020)

-

language modeling

Transformer LM

-

-

-

similar

-

Lee et al. (2019)

biomedical papers

NER, QA, relation

extraction

## Bert

✓

-

-

-

-

Logeswaran et al. (2019)

-

zero-shot entity

linking in Wikia

## Bert

-

✓

-

-

-

Mitra et al. (2020)

-

commonsense QA

## Bert

-

✓

-

-

-

Phang et al. (2018)

-

GLUE tasks

## Elmo, Bert,

## Gpt

-

✓

-

-

-

Radford et al. (2018)

-

## Nli, Qa,

similarity,

classiﬁcation

## Gpt

-

similar

-

-

-

Sun et al. (2019)

sentiment, question,

topic

7 classiﬁcation

tasks

## Bert

✓

✓

-

-

-

Swayamdipta et al. (2019)

-

NER, parsing,

classiﬁcation

## Elmo

-

similar

-

-

-

Xu et al. (2019a)

reviews

RC, aspect extract.,

sentiment

classiﬁcation

## Bert

✓

✓

✓

-

-

Xu et al. (2019b)

restaurant reviews,

laptop reviews

conversational RC

## Bert

✓

✓

-

-

-

Table 11: Overview of prior work across strategies for continued pre-training summarized in Table 10. ULMFIT is

pretrained on English Wikipedia; ULMFIT† on English tweets; ELMO on the 1BWORDBENCHMARK (newswire;

Chelba et al., 2014); GPT on BOOKCORPUS; BERT on English Wikipedia and BOOKCORPUS. In comparison to

these pretraining corpora, ROBERTA’s pretraining corpus is substantially more diverse (see Appendix §A.1).

used a maximum learning rate of 0.0001, as rec-

ommended in Liu et al. (2019). We observe a high

variance in performance between random seeds

when ﬁne-tuning ROBERTA to HYPERPARTISAN,

because the dataset is extremely small. To produce

ﬁnal results on this task, we discard and resample

degenerate seeds. We display the full hyperparam-

eter settings in Table 13.

Implementation

Our LM implementation uses

the HuggingFace transformers library

(Wolf et al., 2019)11 and PyTorch XLA for TPU

compatibility.12 Each adaptive pretraining exper-

11https://github.com/huggingface/

transformers

12https://github.com/pytorch/xla

iment was performed on a single v3-8 TPU from

Google Cloud.13 For the text classiﬁcation tasks,

we used AllenNLP (Gardner et al., 2018). Fol-

lowing standard practice (Devlin et al., 2019) we

pass the ﬁnal layer [CLS] token representation to

a task-speciﬁc feedforward layer for prediction.

## C

Development Set Results

Adhering to the standards suggested by Dodge et al.

(2019) for replication, we report our development

set results in Tables 15, 17, and 18.

13http://github.com/allenai/

tpu-pretrain

## Page 15

## D

Analysis of Domain Overlap

In Table 20 we display additional examples that

highlight the overlap between IMDB reviews and

REALNEWS articles, relevant for analysis in §3.1.

## E

Analysis of Cross-Domain Masked LM

Loss

In Section §3.2, we provide ROBERTA’s masked

LM loss before and after DAPT. We display cross-

domain masked-LM loss in Table 12, where we

evaluate masked LM loss on text samples in other

domains after performing DAPT.

We observe that the cross-domain masked-LM

loss mostly follows our intuition and insights from

the paper, i.e. ROBERTA’s pretraining corpus and

NEWS are closer, and BIOMED to CS (relative to

other domains). However, our analysis in §3.1 il-

lustrates that REVIEWS and NEWS also have some

similarities. This is supported with the loss of

ROBERTA that is adapted to NEWS, calculated

on a sample of REVIEWS. However, ROBERTA

that is adapted to REVIEWS results in the highest

loss for a NEWS sample. This is the case for all

domains. One of the properties that distinguishes

REVIEWS from all other domains is that its doc-

uments are signiﬁcantly shorter. In general, we

ﬁnd that cross-DAPT masked-LM loss can in some

cases be a noisy predictor of domain similarity.

## F

k-Nearest Neighbors Data Selection

In Table 21, we display nearest neighbor docu-

ments in the BIOMED domain identiﬁed by our

selection method, on the RCT dataset.

## Page 16

Data Sample Unseen During DAPT

## Pt

## Biomed

## Cs

## News

## Reviews

## Roberta

1.19

1.32

1.63

1.08

2.10

## Dapt











## Biomed

1.63

0.99

1.63

1.69

2.59

## Cs

1.82

1.43

1.34

1.92

2.78

## News

1.33

1.50

1.82

1.16

2.16

## Reviews

2.07

2.23

2.44

2.27

1.93

Table 12: ROBERTA’s (row 1) and domain-adapted ROBERTA’s (rows 2–5) masked LM loss on randomly sam-

pled held-out documents from each domain (lower implies a better ﬁt). PT denotes a sample from sources similar

to ROBERTA’s pretraining corpus. The lowest masked LM for each domain sample is boldfaced.

Computing Infrastructure

Google Cloud v3-8 TPU

Model implementations

https://github.com/allenai/tpu_pretrain

Hyperparameter

Assignment

number of steps

100 epochs (TAPT) or 12.5K steps (DAPT)

batch size

256 or 2058

maximum learning rate

0.0001 or 0.0005

learning rate optimizer

Adam

Adam epsilon

1e-6

Adam beta weights

0.9, 0.98

learning rate scheduler

None or warmup linear

Weight decay

0.01

Warmup proportion

0.06

learning rate decay

linear

Table 13: Hyperparameters for domain- and task- adaptive pretraining.

Computing Infrastructure

Quadro RTX 8000 GPU

Model implementation

https://github.com/allenai/dont-stop-pretraining

Hyperparameter

Assignment

number of epochs

3 or 10

patience

3

batch size

16

learning rate

2e-5

dropout

0.1

feedforward layer

1

feedforward nonlinearity

tanh

classiﬁcation layer

1

Table 14: Hyperparameters for ROBERTA text classiﬁer.

## Page 17

Additional Pretraining Phases

Domain

Task

## Roberta

## Dapt

## Tapt

## Dapt + Tapt

## Biomed

## Chemprot

83.21.4

84.10.5

83.00.6

84.10.5

## †Rct

88.10.05

88.50.1

88.30.1

88.50.1

## Cs

## Acl-Arc

71.32.8

73.21.5

73.23.6

78.62.9

## Scierc

83.81.1

88.41.7

85.90.8

88.01.3

## News

## Hyperpartisan

84.01.5

79.13.5

82.73.3

80.82.3

## †Agnews

94.30.1

94.30.1

94.70.1

94.90.1

## Reviews

## †Helpfulness

65.53.4

66.51.4

69.22.4

69.42.1

## †Imdb

94.80.1

95.30.1

95.40.1

95.70.2

Table 15: Results on different phases of adaptive pretraining compared to the baseline ROBERTA (col. 1). Our

approaches are DAPT (col. 2, §3), TAPT (col. 3, §4), and a combination of both (col. 4). Reported results are devel-

opment macro-F1, except for CHEMPROT and RCT, for which we report micro-F1, following Beltagy et al. (2019).

We report averages across ﬁve random seeds, with standard deviations as subscripts. † indicates high-resource set-

tings. Best task performance is boldfaced. State-of-the-art results we can compare to: CHEMPROT (84.6), RCT

(92.9), ACL-ARC (71.0), SCIERC (81.8), HYPERPARTISAN (94.8), AGNEWS (95.5), IMDB (96.2); references

in §A.2.

Dom.

Task

## Rob.

## Dapt

## ¬Dapt

## Bm

## Chemprot

83.21.4

84.10.5

80.90.5

## †Rct

88.10.0

88.50.1

87.90.1

## Cs

## Acl-Arc

71.32.8

73.21.5

68.15.4

## Scierc

83.81.1

88.41.7

83.90.9

## News

## Hyp.

84.01.5

79.13.5

71.64.6

## †Agnews

94.30.1

94.30.1

94.00.1

## Rev.

## †Helpful.

65.53.4

66.51.4

65.53.0

## †Imdb

94.80.1

95.30.1

93.80.2

Table 16: Development comparison of ROBERTA (ROBA.) and DAPT to adaptation to an irrelevant domain (¬

DAPT). See §3.3 for our choice of irrelevant domains. Reported results follow the same format as Table 5.

## Biomed

## Rct

## Chemprot

## Tapt

88.30.1

83.00.6

Transfer-TAPT

88.00.1 (↓0.3)

81.10.5 (↓1.9)

## News

## Hyperpartisan

## Agnews

## Tapt

82.73.3

94.70.1

Transfer-TAPT

77.63.6 (↓5.1)

94.40.1 (↓0.4)

## Cs

## Acl-Arc

## Scierc

## Tapt

73.23.6

85.90.8

Transfer-TAPT

74.04.5 (↑1.2)

85.51.1 (↓0.4)

AMAZON reviews

## Helpfulness

## Imdb

## Tapt

69.22.4

95.40.1

Transfer-TAPT

65.42.7 (↓3.8)

94.90.1 (↓0.5)

Table 17: Development results for TAPT transferability.

Pretraining

## Biomed

## News

## Reviews

## Rct-500

## Hyperpartisan

## †Imdb

## Tapt

80.51.3

82.73.3

95.40.1

## Dapt + Tapt

83.90.3

80.82.3

95.70.2

Curated-TAPT

84.40.3

84.91.9

95.80.1

DAPT + Curated-TAPT

84.50.3

83.13.7

96.00.1

Table 18: Mean development set macro-F1 (for HYPERPARTISAN and IMDB) and micro-F1 (for RCT-500), with

Curated-TAPT across ﬁve random seeds, with standard deviations as subscripts. † indicates high-resource settings.

## Page 18

Pretraining

## Biomed

## Cs

## Chemprot

## Rct-500

## Acl-Arc

## Roberta

83.21.4

80.30.5

71.32.8

## Tapt

83.00.6

80.51.3

73.23.6

## Rand-Tapt

83.30.5

81.60.6

78.74.0

## 50Nn-Tapt

83.30.8

81.70.5

70.13.5

## 150Nn-Tapt

83.30.9

81.90.8

78.52.2

## 500Nn-Tapt

84.50.4

82.60.4

77.42.3

## Dapt

84.10.5

83.50.8

73.21.5

Table 19: Mean development set macro-F1 (for HYP. and IMDB) and micro-F1 (for RCT), across ﬁve random

seeds, with standard deviations as subscripts, comparing RAND-TAPT (with 50 candidates) and kNN-TAPT selec-

tion. Neighbors of the task data are selected from the domain data.

IMDB review

REALNEWS article

Spooks is enjoyable trash, featuring some well directed sequences,

ridiculous plots and dialogue, and some third rate acting. Many have

described this is a UK version of 24, and one can see the similarities.

The American version shares the weak silly plots, but the execution is so

much slicker, sexier and I suspect, expensive. Some people describe

weak comedy as gentle comedy. This is gentle spy story hour, the exact

opposite of anything created by John Le Carre. Give me Smiley any day.

[...] Remember poor Helen Flynn from Spooks? In 2002, the headlong

BBC spy caper was in such a hurry to establish the high-wire stakes of its

morally compromised world that Lisa Faulkners keen-as-mustard MI5

rookie turned out to be a lot more expendable than her prominent billing

suggested. [...] Functioning as both a shocking twist and rather callous

statement that No-One Is Safe, it gave the slick drama an instant patina

of edginess while generating a record-breaking number of complaints.

[...]

The Sopranos is perhaps the most mind-opening series you could

possibly ever want to watch. It’s smart, it’s quirky, it’s funny - and it

carries the maﬁa genre so well that most people can’t resist watching.

The best aspect of this show is the overwhelming realism of the

characters, set in the subterranean world of the New York crime families.

For most of the time, you really don’t know whether the wise guys will

stab someone in the back, or buy them lunch. Further adding to the

realistic approach of the characters in this show is the depth of their

personalities - These are dangerous men, most of them murderers, but

by God if you don’t love them too. I’ve laughed at their wisecracks,

been torn when they’ve made err in judgement, and felt scared at the

sheer ruthlessness of a serious criminal. [...]

The drumbeat regarding the Breaking Bad ﬁnale has led to the inevitable

speculation on whether the ﬁnal chapter in this serialized gem will live

up to the hype or disappoint (thank you, Dexter, for setting that bar

pretty low), with debate, second-guessing and graduate-thesis-length

analysis sure to follow. The Most Memorable TV Series Finales of

All-Time [...] No ending in recent years has been more divisive than

The Sopranos for some, a brilliant ﬂash (literally, in a way) of genius;

for others (including yours truly), a too-cute copout, cryptically leaving

its characters in perpetual limbo. The precedent to that would be St.

Elsewhere, which irked many with its provocative, surreal notion that

the whole series was, in fact, conjured in the mind of an autistic child.

[...]

The Wicker Man, starring Nicolas Cage, is by no means a good movie,

but I can’t really say it’s one I regret watching. I could go on and on

about the negative aspects of the movie, like the terrible acting and the

lengthy scenes where Cage is looking for the girl, has a hallucination,

followed by another hallucination, followed by a dream sequence- with

a hallucination, etc., but it’s just not worth dwelling on when it comes to

a movie like this. Instead, here’s ﬁve reasons why you SHOULD watch

The Wicker Man, even though it’s bad: 5. It’s hard to deny that it has

some genuinely creepy ideas to it, the only problem is in its cheesy,

unintentionally funny execution. If nothing else, this is a movie that may

inspire you to see the original 1973 ﬁlm, or even read the short story on

which it is based. 4. For a cheesy horror/thriller, it is really aesthetically

pleasing. [...] NOTE: The Unrated version of the movie is the best to

watch, and it’s better to watch the Theatrical version just for its little

added on epilogue, which features a cameo from James Franco.

[...] What did you ultimately feel about ”The Wicker Man” movie

when all was said and done? [...] Im a fan of the original and Im glad

that I made the movie because they dont make movies like that anymore

and probably the result of what ”Wicker Man” did is the reason why

they dont make movies like that anymore. Again, its kind of that 70s

sensibility, but Im trying to do things that are outside the box. Sometimes

that means itll work and other times it wont. Again though Im going to

try and learn from anything that I do. I think that it was a great cast, and

Neil La Bute is one of the easiest directors that Ive ever worked with.

He really loves actors and he really gives you a relaxed feeling on the

set, that you can achieve whatever it is that youre trying to put together,

but at the end of the day the frustration that I had with The Wicker Man,

which I think has been remedied on the DVD because I believe the DVD

has the directors original cut, is that they cut the horror out of the horror

ﬁlm to try and get a PG-13 rating. I mean, I dont know how to stop

something like that. So Im not happy with the way that the picture ended,

but Im happy with the spirit with which it was made. [...]

Dr. Seuss would sure be mad right now if he was alive. Cat in the Hat

proves to show how movie productions can take a classic story and turn

it into a mindless pile of goop. We have Mike Myers as the infamous

Cat in the Hat, big mistake! Myers proves he can’t act in this ﬁlm. He

acts like a prissy show girl with a thousand tricks up his sleeve. The kids

in this movie are all right, somewhere in between the lines of dull and

annoying. The story is just like the original with a couple of tweaks and

like most movies based on other stories, never tweak with the original

story! Bringing in the evil neighbor Quin was a bad idea. He is a stupid

villain that would never get anywhere in life. [...]

The Cat in the Hat, [...] Based on the book by Dr. Seuss [...] From the

moment his tall, red-and-white-striped hat appears at their door, Sally

and her brother know that the Cat in the Hat is the most mischievous

cat they will ever meet. Suddenly the rainy afternoon is transformed

by the Cat and his antics. Will their house ever be the same? Can

the kids clean up before mom comes home? With some tricks (and a

ﬁsh) and Thing Two and Thing One, with the Cat in The Hat, the fun’s

never done!Dr. Seuss is known worldwide as the imaginative master of

children’s literature. His books include a wonderful blend of invented

and actual words, and his rhymes have helped many children and adults

learn and better their understanding of the English language. [...]

Table 20: Additional examples that highlight the overlap between IMDB reviews and REALNEWS articles.

## Page 19

Source

During median follow-up of 905 days ( IQR 773-1050 ) , 49 people died and 987 unplanned admissions were recorded (

totalling 5530 days in hospital ) .

Neighbor 0

Of this group, 26% died after discharge from hospital, and the median time to death was 11 days (interquartile range,

4.0-15.0 days) after discharge.

Neighbor 1

The median hospital stay was 17 days (range 8-26 days), and all the patients were discharged within 1 month.

Neighbor 2

The median hospital stay was 17 days (range 8-26 days).

Neighbor 3

The median time between discharge and death was 25 days (mean, 59.1 days) and no patient was alive after 193 days.

Neighbor 4

The length of hospital stay after colostomy formation ranged from 3 days to 14 days with a median duration of 6 days

(+IQR of 4 to 8 days).

Source

Randomized , controlled , parallel clinical trial .

Neighbor 0

Design: Unblinded, randomised clinical controlled trial.

Neighbor 1

These studies and others led to the phase III randomized trial RTOG 0617/NCCTG 0628/ CALGB 30609.

Neighbor 2

-Deﬁnitive randomized controlled clinical trial (RCT):

Neighbor 3

## Rct 1

4 randomized controlled trial.

Neighbor 4

randomized controlled trial [ Fig. 3(A)].

Source

Forty primary molar teeth in 40 healthy children aged 5-9 years were treated by direct pulp capping .

Neighbor 0

In our study, we speciﬁcally determined the usefulness of the Er:YAG laser in caries removal and cavity preparation of

primary and young permanent teeth in children ages 4 to 1 8 years.

Neighbor 1

Males watched more TV than females, although it was only in primary school-aged children and on weekdays.

Neighbor 2

Assent was obtained from children and adolescents aged 7-17 years.

Neighbor 3

Cardiopulmonary resuscitation was not applied to children aged ¡5 years (Table 2).

Neighbor 4

It measures HRQoL in children and adolescents aged 2 to 25 years.

Table 21: 5 nearest neighbors of sentences from the RCT dataset (Source) in the BIOMED domain (Neighbors

0–4).
