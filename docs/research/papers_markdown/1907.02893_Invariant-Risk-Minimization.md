# 1907.02893_Invariant-Risk-Minimization
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 1907.02893_Invariant-Risk-Minimization.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2


## Implementation Status

- âœ… **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- ğŸ”„ **Performance Monitoring**: Continuous validation of targets
- âœ… **Documentation Standards**: Compliant with ACGS-2 requirements
- ğŸ”„ **Cross-Reference Validation**: Ongoing link integrity maintenance

**Overall Status**: ğŸ”„ IN PROGRESS - Systematic enhancement implementation

---

## Page 1

Invariant Risk Minimization

Martin Arjovsky, LÂ´eon Bottou, Ishaan Gulrajani, David Lopez-Paz

1

Introduction

Machine learning suï¬€ers from a fundamental problem. While machines are able to

learn complex prediction rules by minimizing their training error, data are often

marred by selection biases, confounding factors, and other peculiarities [49, 48, 23].

As such, machines justiï¬ably inherit these data biases. This limitation plays an

essential role in the situations where machine learning fails to fulï¬ll the promises of

artiï¬cial intelligence. More speciï¬cally, minimizing training error leads machines into

recklessly absorbing all the correlations found in training data. Understanding which

patterns are useful has been previously studied as a correlation-versus-causation

dilemma, since spurious correlations stemming from data biases are unrelated to the

causal explanation of interest [31, 27, 35, 52]. Following this line, we leverage tools

from causation to develop the mathematics of spurious and invariant correlations, in

order to alleviate the excessive reliance of machine learning systems on data biases,

allowing them to generalize to new test distributions.

As a thought experiment, consider the problem of classifying images of cows and

camels [4]. To address this task, we label images of both types of animals. Due to a

selection bias, most pictures of cows are taken in green pastures, while most pictures

of camels happen to be in deserts. After training a convolutional neural network

on this dataset, we observe that the model fails to classify easy examples of images

of cows when they are taken on sandy beaches. Bewildered, we later realize that

our neural network successfully minimized its training error using a simple cheat:

classify green landscapes as cows, and beige landscapes as camels.

To solve the problem described above, we need to identify which properties of the

training data describe spurious correlations (landscapes and contexts), and which

properties represent the phenomenon of interest (animal shapes). Intuitively, a

correlation is spurious when we do not expect it to hold in the future in the same

manner as it held in the past. In other words, spurious correlations do not appear

to be stable properties [54]. Unfortunately, most datasets are not provided in a form

amenable to discover stable properties. Because most machine learning algorithms

depend on the assumption that training and testing data are sampled independently

from the same distribution [51], it is common practice to shuï¬„e at random the

training and testing examples. For instance, whereas the original NIST handwritten

data was collected from diï¬€erent writers under diï¬€erent conditions [19], the popular

MNIST training and testing sets [8] were carefully shuï¬„ed to represent similar mixes

of writers. Shuï¬„ing brings the training and testing distributions closer together, but

1

arXiv:1907.02893v3  [stat.ML]  27 Mar 2020

## Page 2

discards what information is stable across writers. However, shuï¬„ing the data is

something that we do, not something that Nature does for us. When shuï¬„ing, we

destroy information about how the data distribution changes when one varies the

data sources or collection speciï¬cs. Yet, this information is precisely what tells us

whether a property of the data is spurious or stable.

Here we take a step back, and assume that the training data is collected into

distinct, separate environments. These could represent diï¬€erent measuring circum-

stances, locations, times, experimental conditions, external interventions, contexts,

and so forth. Then, we promote learning correlations that are stable across training

environments, as these should (under conditions that we will study) also hold in

novel testing environments.

Returning to our motivational example, we would like to label pictures of cows

and camels under diï¬€erent environments. For instance, the pictures of cows taken

in the ï¬rst environment may be located in green pastures 80% of the time. In

the second environment, this proportion could be slightly diï¬€erent, say 90% of the

time (since pictures were taken in a diï¬€erent country). These two datasets reveal

that â€œcowâ€ and â€œgreen backgroundâ€ are linked by a strong, but varying (spurious)

correlation, which should be discarded in order to generalize to new environments.

Learning machines which pool the data from the two environments together may still

rely on the background bias when addressing the prediction task. But, we believe

that all cows exhibit features that allow us to recognize them as so, regardless of

their context.

This suggests that invariant descriptions of objects relate to the causal explanation

of the object itself (â€œWhy is it a cow?â€) [32]. As shown by [40, 22], there exists an

intimate link between invariance and causation useful for generalization. However,

[40] assumes a meaningful causal graph relating the observed variables, an awkward

assumption when dealing with perceptual inputs such as pixels. Furthermore, [40]

only applies to linear models, and scales exponentially with respect to the number

of variables in the learning problem. As such, the seamless integration of causation

tools [41] into machine learning pipelines remains cumbersome, disallowing what we

believe to be a powerful synergy. Here, we work to address these concerns.

Contributions

We propose Invariant Risk Minimization (IRM), a novel learning

paradigm that estimates nonlinear, invariant, causal predictors from multiple training

environments, to enable out-of-distribution (OOD) generalization. To this end, we

ï¬rst analyze in Section 2 how diï¬€erent learning techniques fail to generalize OOD.

From this analysis, we derive our IRM principle in Section 3:

To learn invariances across environments, ï¬nd a data representation such that the

optimal classiï¬er on top of that representation matches for all environments.

Section 4 examines the fundamental links between causation, invariance, and OOD

generalization. Section 5 contains basic numerical simulations to validate our claims

empirically. Section 6 concludes with a Socratic dialogue discussing directions for

future research.

2

## Page 3

2

The many faces of generalization

Following [40], we consider datasets De := {(xe

i, ye

i )}ne

i=1 collected under multiple

training environments e âˆˆEtr. These environments describe the same pair of random

variables measured under diï¬€erent conditions. The dataset De, from environment

e, contains examples identically and independently distributed according to some

probability distribution P(Xe, Y e).1 Then, our goal is to use these multiple datasets

to learn a predictor Y â‰ˆf(X), which performs well across a large set of unseen but

related environments Eall âŠƒEtr. Namely, we wish to minimize

ROOD(f) = max

eâˆˆEall Re(f)

where Re(f) := EXe,Y e[â„“(f(Xe), Y e)] is the risk under environment e. Here, the set

of all environments Eall contains all possible experimental conditions concerning our

system of variables, both observable and hypothetical. This is in the spirit of modal

realism and possible worlds [29], where we could consider, for instance, environments

where we switch oï¬€the Sun. An example clariï¬es our intentions.

Example 1. Consider the structural equation model [55]:

X1 â†Gaussian(0, Ïƒ2),

Y â†X1 + Gaussian(0, Ïƒ2),

X2 â†Y + Gaussian(0, 1).

As we formalize in Section 4, the set of all environments Eall contains all modiï¬-

cations of the structural equations for X1 and X2, and those varying the noise of Y

within a ï¬nite range [0, Ïƒ2

MAX]. For instance, e âˆˆEall may replace the equation of

X2 by Xe

2 â†106, or vary Ïƒ2 within this ï¬nite range . To ease exposition consider:

Etr = {replace Ïƒ2 by 10, replace Ïƒ2 by 20}.

Then, to predict Y from (X1, X2) using a least-squares predictor Ë†Y e = Xe

1 Ë†Î±1 +Xe

2 Ë†Î±2

for environment e, we can:

â€¢ regress from Xe

1, to obtain Ë†Î±1 = 1 and Ë†Î±2 = 0,

â€¢ regress from Xe

2, to obtain Ë†Î±1 = 0 and Ë†Î±2 = Ïƒ(e)2/(Ïƒ(e)2 + 1

2),

â€¢ regress from (Xe

1, Xe

2), to obtain Ë†Î±1 = 1/(Ïƒ(e)2+1) and Ë†Î±2 = Ïƒ(e)2/(Ïƒ(e)2+1).

The regression using X1 is our ï¬rst example of an invariant correlation: this is the

only regression whose coeï¬ƒcients do not depend on the environment e. Conversely,

the second and third regressions exhibit coeï¬ƒcients that vary from environment to

environment. These varying (spurious) correlations would not generalize well to

novel test environments. Also, not all invariances are interesting: the regression

from the empty set of features into Y is invariant, but of weak predictive power.

1We omit the superscripts â€œeâ€ when referring to a random variable regardless of the environment.

3

## Page 4

The invariant rule Ë†Y = 1 Â· X1 + 0 Â· X2 is the only predictor with ï¬nite ROOD

across Eall (to see this, let X2 â†’âˆ). Furthermore, this predictor is the causal

explanation about how the target variable takes values across environments. In

other words, it provides the correct description about how the target variable reacts

in response to interventions on each of the inputs. This is compelling, as invariance

is a statistically testable quantity that we can measure to discover causation. We

elaborate on the relationship between invariance and causation in Section 4. But

ï¬rst, how can we learn the invariant, causal regression? Let us review four techniques

commonly discussed in prior work, as well as their limitations.

First, we could merge the data from all the training environments and learn a

predictor that minimizes the training error across the pooled data, using all features.

This is the ubiquitous Empirical Risk Minimization (ERM) principle [50]. In this

example, ERM would grant a large positive coeï¬ƒcient to X2 if the pooled training

environments lead to large Ïƒ2(e) (as in our example), departing from invariance.

Second, we could minimize Rrob(f) = maxeâˆˆEtr Re(f) âˆ’re, a robust learning

objective where the constants re serve as environment baselines [2, 6, 15, 46]. Setting

these baselines to zero leads to minimizing the maximum error across environments.

Selecting these baselines adequately prevents noisy environments from dominating

optimization.

For example, [37] selects re = V[Y e] to maximize the minimal

explained variance across environments. While promising, robust learning turns out

to be equivalent to minimizing a weighted average of environment training errors:

Proposition 2. Given KKT diï¬€erentiability and qualiï¬cation conditions, âˆƒÎ»e â‰¥0

such that the minimizer of Rrob is a ï¬rst-order stationary point of P

eâˆˆEtr Î»eRe(f).

This proposition shows that robust learning and ERM (a special case of robust

learning with Î»e =

1

|Etr|) would never discover the desired invariance, obtaining

inï¬nite ROOD. This is because minimizing the risk of any mixture of environments

associated to large Ïƒ2(e) yields a predictor with a large weight on X2. Unfortunately,

this correlation will vanish for testing environments associated to small Ïƒ2(e).

Third, we could adopt a domain adaptation strategy, and estimate a data repre-

sentation Î¦(X1, X2) that follows the same distribution for all environments [16, 33].

This would fail to ï¬nd the true invariance in Example 1, since the distribution of the

true causal feature X1 (and the one of the target Y ) can change across environments.

This illustrates why techniques matching feature distributions sometimes attempt

to enforce the wrong type of invariance, as discussed in Appendix C.

Fourth, we could follow invariant causal prediction techniques [40]. These search

for the subset of variables that, when used to estimate individual regressions for

each environment, produce regression residuals with equal distribution across all

environments. Matching residual distributions is unsuited for our example, since the

noise variance in Y may change across environments.

In sum, ï¬nding invariant predictors even on simple problems such as Example 1 is

surprisingly diï¬ƒcult. To address this issue, we propose Invariant Risk Minimization

(IRM), a learning paradigm to extract nonlinear invariant predictors across multiple

environments, enabling OOD generalization.

4

## Page 5

3

Algorithms for invariant risk minimization

In statistical parlance, our goal is to learn correlations invariant across training

environments. For prediction problems, this means ï¬nding a data representation

such that the optimal classiï¬er,2 on top of that data representation, is the same for

all environments. More formally:

Deï¬nition 3. We say that a data representation Î¦ : X â†’H elicits an invariant

predictor wâ—¦Î¦ across environments E if there is a classiï¬er w : H â†’Y simultaneously

optimal for all environments, that is, w âˆˆarg min Â¯

w:Hâ†’Y Re( Â¯w â—¦Î¦) for all e âˆˆE.

Why is Deï¬nition 3 equivalent to learning features whose correlations with the

target variable are stable? For loss functions such as the mean squared error and

the cross-entropy, optimal classiï¬ers can be written as conditional expectations. In

these cases, a data representation function Î¦ elicits an invariant predictor across

environments E if and only if for all h in the intersection of the supports of Î¦(Xe)

we have E[Y e|Î¦(Xe) = h] = E[Y eâ€²|Î¦(Xeâ€²) = h], for all e, eâ€² âˆˆE.

We believe that this concept of invariance clariï¬es common induction methods

in science. Indeed, some scientiï¬c discoveries can be traced to the realization that

distinct but potentially related phenomena, once described with the correct variables,

appear to obey the same exact physical laws. The precise conservation of these

laws suggests that they remain valid on a far broader range of conditions. If both

Newtonâ€™s apple and the planets obey the same equations, chances are that gravitation

is a thing.

To discover these invariances from empirical data, we introduce Invariant Risk

Minimization (IRM), a learning paradigm to estimate data representations eliciting

invariant predictors w â—¦Î¦ across multiple environments. To this end, recall that we

have two goals in mind for the data representation Î¦: we want it to be useful to

predict well, and elicit an invariant predictor across Etr. Mathematically, we phrase

these goals as the constrained optimization problem:

min

## Î¦:Xâ†’H

w:Hâ†’Y

## X

eâˆˆEtr

Re(w â—¦Î¦)

subject to

w âˆˆarg min

Â¯

w:Hâ†’Y

Re( Â¯w â—¦Î¦), for all e âˆˆEtr.

## (Irm)

This is a challenging, bi-leveled optimization problem, since each constraint calls an

inner optimization routine. So, we instantiate (IRM) into the practical version:

min

## Î¦:Xâ†’Y

## X

eâˆˆEtr

Re(Î¦) + Î» Â· âˆ¥âˆ‡w|w=1.0 Re(w Â· Î¦)âˆ¥2,

(IRMv1)

where Î¦ becomes the entire invariant predictor, w = 1.0 is a scalar and ï¬xed â€œdummyâ€

classiï¬er, the gradient norm penalty is used to measure the optimality of the dummy

classiï¬er at each environment e, and Î» âˆˆ[0, âˆ) is a regularizer balancing between

predictive power (an ERM term), and the invariance of the predictor 1 Â· Î¦(x).

2We will also use the term â€œclassiï¬erâ€ to denote the last layer w for regression problems.

5

## Page 6

3.1

From (IRM) to (IRMv1)

This section is a voyage circumventing the subtle optimization issues lurking behind

the idealistic objective (IRM), to arrive to the eï¬ƒcient proposal (IRMv1).

3.1.1

Phrasing the constraints as a penalty

We translate the hard constraints in (IRM) into the penalized loss

LIRM(Î¦, w) =

## X

eâˆˆEtr

Re(w â—¦Î¦) + Î» Â· D(w, Î¦, e)

(1)

where Î¦ : X â†’H, the function D(w, Î¦, e) measures how close w is to minimizing

Re(w â—¦Î¦), and Î» âˆˆ[0, âˆ) is a hyper-parameter balancing predictive power and

invariance. In practice, we would like D(w, Î¦, e) to be diï¬€erentiable with respect to

Î¦ and w. Next, we consider linear classiï¬ers w to propose one alternative.

3.1.2

Choosing a penalty D for linear classiï¬ers w

Consider learning an invariant predictor w â—¦Î¦, where w is a linear-least squares

regression, and Î¦ is a nonlinear data representation. In the sequel, all vectors

v âˆˆRd are by default in column form, and we denote by vâŠ¤âˆˆR1Ã—d the row form.

By the normal equations, and given a ï¬xed data representation Î¦, we can write

we

Î¦ âˆˆarg min Â¯

w Re( Â¯w â—¦Î¦) as:

we

Î¦ = EXe 

Î¦(Xe)Î¦(Xe)âŠ¤âˆ’1 EXe,Y e [Î¦(Xe)Y e] ,

(2)

where we assumed invertibility. This analytic expression would suggest a simple

discrepancy between two linear least-squares classiï¬ers:

Ddist(w, Î¦, e) = âˆ¥w âˆ’we

## Î¦âˆ¥2.

(3)

Figure 1 uses Example 1 to show why Ddist is a poor discrepancy. The blue

curve shows (3) as we vary the coeï¬ƒcient c for a linear data representation Î¦(x) =

xÂ·Diag([1, c]), and w = (1, 0). The coeï¬ƒcient c controls how much the representation

depends on the variable X2, responsible for the spurious correlations in Example 1.

We observe that (3) is discontinuous at c = 0, the value eliciting the invariant

predictor. This happens because when c approaches zero without being exactly

zero, the least-squares rule (2) compensates this change by creating vectors we

## Î¦

whose second coeï¬ƒcient grows to inï¬nity.

This causes a second problem, the

penalty approaching zero as âˆ¥câˆ¥â†’âˆ. The orange curve shows that adding severe

regularization to the least-squares regression does not ï¬x these numerical problems.

To circumvent these issues, we can undo the matrix inversion in (2) to construct:

Dlin(w, Î¦, e) =

EXe 

Î¦(Xe)Î¦(Xe)âŠ¤

w âˆ’EXe,Y e [Î¦(Xe)Y e]

2 ,

(4)

which measures how much does the classiï¬er w violate the normal equations. The

green curve in Figure 1 shows Dlin as we vary c, when setting w = (1, 0). The

6

## Page 7

âˆ’1.00

âˆ’0.75

âˆ’0.50

âˆ’0.25

0.00

0.25

0.50

0.75

1.00

c, the weight of Î¦ on the input with varying correlation

0

2

4

6

8

10

12

invariance penalty

Ddist((1, 0), Î¦, e)

Ddist (heavy regularization)

Dlin((1, 0), Î¦, e)

Figure 1: Diï¬€erent measures of invariance lead to diï¬€erent optimization landscapes in our

Example 1. The naÂ¨Ä±ve approach of measuring the distance between optimal classiï¬ers Ddist

leads to a discontinuous penalty (solid blue unregularized, dashed orange regularized). In

contrast, the penalty Dlin does not exhibit these problems.

penalty Dlin is smooth (it is a polynomial on both Î¦ and w), and achieves an

easy-to-reach minimum at c = 0 â€”the data representation eliciting the invariant

predictor. Furthermore, Dlin(w, Î¦, e) = 0 if and only if w âˆˆarg min Â¯

w Re( Â¯w â—¦Î¦). As

a word of caution, we note that the penalty Dlin is non-convex for general Î¦.

3.1.3

Fixing the linear classiï¬er w

Even when minimizing (1) over (Î¦, w) using Dlin, we encounter one issue. When

considering a pair (Î³Î¦, 1

Î³ w), it is possible to let Dlin tend to zero without impacting

the ERM term, by letting Î³ tend to zero. This problem arises because (1) is severely

over-parametrized. In particular, for any invertible mapping Î¨, we can re-write our

invariant predictor as

w â—¦Î¦ =

 w â—¦Î¨âˆ’1

|

{z

}

Ëœ

w

## â—¦(Î¨ â—¦Î¦)

| {z }

## ËœÎ¦

.

This means that we can re-parametrize our invariant predictor as to give w any

non-zero value Ëœw of our choosing. Thus, we may restrict our search to the data

representations for which all the environment optimal classiï¬ers are equal to the same

ï¬xed vector Ëœw. In words, we are relaxing our recipe for invariance into ï¬nding a data

representation such that the optimal classiï¬er, on top of that data representation,

is â€œ Ëœwâ€ for all environments. This turns (1) into a relaxed version of IRM, where

optimization only happens over Î¦:

LIRM,w= Ëœ

w(Î¦) =

## X

eâˆˆEtr

Re( Ëœw â—¦Î¦) + Î» Â· Dlin( Ëœw, Î¦, e).

(5)

7

## Page 8

As Î» â†’âˆ, solutions (Î¦âˆ—

Î», Ëœw) of (5) tend to solutions (Î¦âˆ—, Ëœw) of (IRM) for linear Ëœw.

3.1.4

Scalar ï¬xed classiï¬ers Ëœw are suï¬ƒcient to monitor invariance

Perhaps surprisingly, the previous section suggests that Ëœw = (1, 0, . . . , 0) would be a

valid choice for our ï¬xed classiï¬er. In this case, only the ï¬rst component of the data

representation would matter! We illustrate this apparent paradox by providing a

complete characterization for the case of linear invariant predictors. In the following

theorem, matrix Î¦ âˆˆRpÃ—d parametrizes the data representation function, vector

w âˆˆRp the simultaneously optimal classiï¬er, and v = Î¦âŠ¤w the predictor w â—¦Î¦.

Theorem 4. For all e âˆˆE, let Re : Rd â†’R be convex diï¬€erentiable cost functions.

A vector v âˆˆRd can be written v = Î¦âŠ¤w, where Î¦ âˆˆRpÃ—d, and where w âˆˆRp

simultaneously minimize Re(w â—¦Î¦) for all e âˆˆE, if and only if vâŠ¤âˆ‡Re(v) = 0 for

all e âˆˆE. Furthermore, the matrices Î¦ for which such a decomposition exists are the

matrices whose nullspace Ker(Î¦) is orthogonal to v and contains all the âˆ‡Re(v).

So, any linear invariant predictor can be decomposed as linear data representations

of diï¬€erent ranks. In particular, we can restrict our search to matrices Î¦ âˆˆR1Ã—d

and let Ëœw âˆˆR1 be the ï¬xed scalar 1.0. This translates (5) into:

LIRM,w=1.0(Î¦âŠ¤) =

## X

eâˆˆEtr

Re(Î¦âŠ¤) + Î» Â· Dlin(1.0, Î¦âŠ¤, e).

(6)

Section 4 shows that the existence of decompositions with high-rank data representa-

tion matrices Î¦âŠ¤are key to out-of-distribution generalization, regardless of whether

we restrict IRM to search for rank-1 Î¦âŠ¤.

Geometrically, each orthogonality condition vâŠ¤âˆ‡Re(v) = 0 in Theorem 4 deï¬nes

a (dâˆ’1)-dimensional manifold in Rd.

Their intersection is itself a manifold of

dimension greater than dâˆ’m, where m is the number of environments. When using

the squared loss, each condition is a quadratic equation whose solutions form an

ellipsoid in Rd. Figure 2 shows how their intersection is composed of multiple

connected components, one of which contains the trivial solution v = 0. This shows

that (6) remains nonconvex, and therefore sensitive to initialization.

3.1.5

Extending to general losses and multivariate outputs

Continuing from (6), we obtain our ï¬nal algorithm (IRMv1) by realizing that the

invariance penalty (4), introduced for the least-squares case, can be written as a

general function of the risk, namely D(1.0, Î¦, e) = âˆ¥âˆ‡w|w=1.0Re(w Â· Î¦)âˆ¥2, where Î¦

is again a possibly nonlinear data representation. This expression measures the

optimality of the ï¬xed scalar classiï¬er w = 1.0 for any convex loss, such as the

cross-entropy. If the target space Y returned by Î¦ has multiple outputs, we multiply

all of them by the ï¬xed scalar classiï¬er w = 1.0.

8

## Page 9

â€¢ vâ‹†

1

2

â€¢

vâ‹†

2

2

Solutions

intersections of ellipsoids

:

zero is a solution

Figure 2:

The solutions of the invariant linear predictors v = Î¦âŠ¤w coincide with the

intersection of the ellipsoids representing the orthogonality condition vâŠ¤âˆ‡Re(v) = 0.

3.2

Implementation details

When estimating the objective (IRMv1) using mini-batches for stochastic gradient

descent, one can obtain an unbiased estimate of the squared gradient norm as

b

## X

k=1

h

âˆ‡w|w=1.0â„“(w Â· Î¦(Xe,i

k ), Y e,i

k ) Â· âˆ‡w|w=1.0â„“(w Â· Î¦(Xe,j

k ), Y e,j

k

)

i

,

where (Xe,i, Y e,i) and (Xe,j, Y e,j) are two random mini-batches of size b from

environment e, and â„“is a loss function. We oï¬€er a PyTorch example in Appendix D.

3.3

About nonlinear invariances w

How restrictive is it to assume that the invariant optimal classiï¬er w is linear? One

may argue that given a suï¬ƒciently ï¬‚exible data representation Î¦, it is possible to

write any invariant predictor as 1.0 Â· Î¦. However, enforcing a linear invariance may

grant non-invariant predictors a penalty Dlin equal to zero. For instance, the null

data representation Î¦0(Xe) = 0 admits any w as optimal amongst all the linear

classiï¬ers for all environments. But, the elicited predictor w â—¦Î¦0 is not invariant in

cases where E[Y e] Ì¸= 0. Such null predictor would be discarded by the ERM term in

the IRM objective. In general, minimizing the ERM term Re( Ëœw â—¦Î¦) will drive Î¦ so

that Ëœw is optimal amongst all predictors, even if Ëœw is linear.

We leave for future work several questions related to this issue. Are there non-

invariant predictors that would not be discarded by either the ERM or the invariance

term in IRM? What are the beneï¬ts of enforcing non-linear invariances w belonging

to larger hypothesis classes W? How can we construct invariance penalties D for

non-linear invariances?

9

## Page 10

4

Invariance, causality and generalization

The newly introduced IRM principle promotes low error and invariance across training

environments Etr. When do these conditions imply invariance across all environments

Eall? More importantly, when do these conditions lead to low error across Eall,

and consequently out-of-distribution generalization? And at a more fundamental

level, how does statistical invariance and out-of-distribution generalization relate to

concepts from the theory of causation?

So far, we have omitted how diï¬€erent environments should relate to enable

out-of-distribution generalization. The answer to this question is rooted in the

theory of causation. We begin by assuming that the data from all the environments

share the same underlying Structural Equation Model, or SEM [55, 39]:

Deï¬nition 5. A Structural Equation Model (SEM) C := (S, N) governing the

random vector X = (X1, . . . , Xd) is a set of structural equations:

Si : Xi â†fi(Pa(Xi), Ni),

where Pa(Xi) âŠ†{X1, . . . , Xd} \ {Xi} are called the parents of Xi, and the Ni are

independent noise random variables. We say that â€œXi causes Xjâ€ if Xi âˆˆPa(Xj).

We call causal graph of X to the graph obtained by drawing i) one node for each Xi,

and ii) one edge from Xi to Xj if Xi âˆˆPa(Xj). We assume acyclic causal graphs.

By running the structural equations of a SEM C according to the topological

ordering of its causal graph, we can draw samples from the observational distribution

P(X). In addition, we can manipulate (intervene) an unique SEM in diï¬€erent ways,

indexed by e, to obtain diï¬€erent but related SEMs Ce.

Deï¬nition 6. Consider a SEM C = (S, N). An intervention e on C consists of

replacing one or several of its structural equations to obtain an intervened SEM

Ce = (Se, N e), with structural equations:

Se

i : Xe

i â†f e

i (Pae(Xe

i ), N e

i ),

The variable Xe is intervened if Si Ì¸= Se

i or Ni Ì¸= N e

i .

Similarly, by running the structural equations of the intervened SEM Ce, we

can draw samples from the interventional distribution P(Xe). For instance, we

may consider Example 1 and intervene on X2, by holding it constant to zero, thus

replacing the structural equation of X2 by Xe

2 â†0. Admitting a slight abuse of

notation, each intervention e generates a new environment e with interventional

distribution P(Xe, Y e). Valid interventions e, those that do not destroy too much

information about the target variable Y , form the set of all environments Eall.

Prior work [40] considered valid interventions as those that do not change the

structural equation of Y , since arbitrary interventions on this equation render

prediction impossible. In this work, we also allow changes in the noise variance of

Y , since varying noise levels appear in real problems, and these do not aï¬€ect the

optimal prediction rule. We formalize this as follows.

10

## Page 11

Deï¬nition 7. Consider a SEM C governing the random vector (X1, . . . , Xd, Y ),

and the learning goal of predicting Y from X. Then, the set of all environments

Eall(C) indexes all the interventional distributions P(Xe, Y e) obtainable by valid

interventions e. An intervention e âˆˆEall(C) is valid as long as (i) the causal graph

remains acyclic, (ii) E[Y e|Pa(Y )] = E[Y |Pa(Y )], and (iii) V[Y e|Pa(Y )] remains

within a ï¬nite range.

Condition (iii) can be waived if one takes into account environment speciï¬c

baselines into the deï¬nition of ROOD, similar to those appearing in the robust

learning objective Rrob. We leave additional quantiï¬cations of out-of-distribution

generalization for future work.

The previous deï¬nitions establish fundamental links between causation and

invariance. Moreover, one can show that a predictor v : X â†’Y is invariant across

Eall(C) if and only if it attains optimal ROOD, and if and only if it uses only the

direct causal parents of Y to predict, that is, v(x) = ENY [fY (Pa(Y ), NY )]. The

rest of this section follows on these ideas to showcase how invariance across training

environments can enable out-of-distribution generalization across all environments.

4.1

Generalization theory for IRM

The goal of IRM is to build predictors that generalize out-of-distribution, that is,

achieving low error across Eall. To this end, IRM enforces low error and invariance

across Etr. The bridge from low error and invariance across Etr to low error across

Eall can be traversed in two steps.

First, one can show that low error across Etr and invariance across Eall leads

to low error across Eall. This is because, once the data representation Î¦ eliciting

an invariant predictor w â—¦Î¦ across Eall is estimated, the generalization error of

w â—¦Î¦ respects standard error bounds. Second, we examine the remaining condition

towards low error across Eall: namely, under which conditions does invariance across

training environments Etr imply invariance across all environments Eall?

For linear IRM, our starting point to answer this question is the theory of

Invariant Causal Prediction (ICP) [40, Theorem 2]. There, the authors prove that

ICP recovers the target invariance as long as the data (i) is Gaussian, (ii) satisï¬es a

linear SEM, and (iii) is obtained by certain types of interventions. Theorem 9 shows

that IRM learns such invariances even when these three assumptions fail to hold. In

particular, we allow for non-Gaussian data, dealing with observations produced as a

linear transformation of the variables with stable and spurious correlations, and do

not require speciï¬c types of interventions or the existence of a causal graph.

The setting of the theorem is as follows. Y e has an invariant correlation with

an unobserved latent variable Ze

1 by a linear relationship Y e = Ze

1 Â· Î³ + Ïµe, with Ïµe

independent of Ze

1. What we observe is Xe, which is a scrambled combination of Ze

1

and another variable Ze

2 that can be arbitrarily correlated with Ze

1 and Ïµe. Simply

regressing using all of Xe will then recklessly exploit Ze

2 (since it gives extra, but

spurious, information on Ïµe and thus Y e). A particular instance of this setting is

when Ze

1 is the cause of Y e, Ze

2 is an eï¬€ect, and Xe contains both causes and eï¬€ects.

To generalize out of distribution the representation has to discard Ze

2 and keep Ze

1.

11

## Page 12

Before showing Theorem 9, we need to make our assumptions precise.

To

learn useful invariances, one must require some degree of diversity across training

environments. On the one hand, extracting two random subsets of examples from a

large dataset does not lead to diverse environments, as both subsets would follow the

same distribution. On the other hand, splitting a large dataset by conditioning on

arbitrary variables can generate diverse environments, but may introduce spurious

correlations and destroy the invariance of interest [40, Section 3.3].

Therefore,

we will require sets of training environments containing suï¬ƒcient diversity and

satisfying an underlying invariance. We formalize the diversity requirement as

needing envirnments to lie in linear general position.

Assumption 8. A set of training environments Etr lie in linear general position of

degree r if |Etr| > d âˆ’r + d

r for some r âˆˆN, and for all non-zero x âˆˆRd:

dim



span

n

EXe

h

XeXeâŠ¤i

x âˆ’EXe,Ïµe [XeÏµe]

o

eâˆˆEtr



> d âˆ’r.

Intuitively, the assumption of linear general position limits the extent to which

the training environments are co-linear. Each new environment laying in linear

general position will remove one degree of freedom in the space of invariant solutions.

Fortunately, Theorem 10 shows that the set of cross-products EXe[XeXeâŠ¤] not

satisfying a linear general position has measure zero. Using the assumption of linear

general position, we can show that the invariances that IRM learns across training

environments transfer to all environments.

In words, the next theorem states the following. If one ï¬nds a representation Î¦

of rank r eliciting an invariant predictor w â—¦Î¦ across Etr, and Etr lie in linear general

position of degree r, then w â—¦Î¦ is invariant across Eall.

Theorem 9. Assume that

Y e = Ze

1 Â· Î³ + Ïµe,

Ze

1 âŠ¥Ïµe,

E[Ïµe] = 0,

Xe = S(Ze

1, Ze

2).

Here, Î³ âˆˆRc, Ze

1 takes values in Rc, Ze

2 takes values in Rq, and S âˆˆRdÃ—(c+q).

Assume that the Z1 component of S is invertible: that there exists ËœS âˆˆRcÃ—d such

that ËœS (S(z1, z2)) = z1, for all z1 âˆˆRc, z2 âˆˆRq. Let Î¦ âˆˆRdÃ—d have rank r > 0.

Then, if at least d âˆ’r + d

r training environments Etr âŠ†Eall lie in linear general

position of degree r, we have that

Î¦ EXe

h

XeXeâŠ¤i

Î¦âŠ¤w = Î¦ EXe,Y e [XeY e]

(7)

holds for all e âˆˆEtr iï¬€Î¦ elicits the invariant predictor Î¦âŠ¤w for all e âˆˆEall.

The assumptions about linearity, centered noise, and independence between

the noise Ïµe and the causal variables Z1 from Theorem 9 also appear in ICP [40,

Assumption 1], implying the invariance E[Y e|Ze

1 = z1] = z1 Â· Î³. As in ICP, we allow

12

## Page 13

correlations between Ïµe and the non-causal variables Ze

2, which leads ERM into

absorbing spurious correlations (as in our Example 1, where S = I and Ze

2 = Xe

2).

In addition, our result contains several novelties. First, we do not assume that the

data is Gaussian, the existence of a causal graph, or that the training environments

arise from speciï¬c types of interventions. Second, the result extends to â€œscrambled

setupsâ€ where S Ì¸= I. These are situations where the causal relations are not deï¬ned

on the observable features X, but on a latent variable (Z1, Z2) that IRM needs to

recover and ï¬lter. Third, we show that representations Î¦ with higher rank need

fewer training environments to generalize. This is encouraging, as representations

with higher rank destroy less information about the learning problem at hand.

We close this section with two important observations. First, while robust learning

generalizes across interpolations of training environments (recall Proposition 2),

learning invariances with IRM buys extrapolation powers. We can observe this in

Example 1 where, using two training environments, robust learning yields predictors

that work well for Ïƒ âˆˆ[10, 20], while IRM yields predictors that work well for all Ïƒ.

Finally, IRM is a diï¬€erentiable function with respect to the covariances of the training

environments. Therefore, in cases when the data follows an approximately invariant

model, IRM should return an approximately invariant solution, being robust to mild

model misspeciï¬cation. This is in contrast to common causal discovery methods

based on thresholding statistical hypothesis tests.

4.2

On the nonlinear case and the number of environments

In the same vein as the linear case, we could attempt to provide IRM with guar-

antees for the nonlinear regime. Namely, we could assume that each constraint

âˆ¥âˆ‡w|w=1.0Re(w Â· Î¦)âˆ¥= 0 removes one degree of freedom from the possible set of

solutions Î¦. Then, for a suï¬ƒciently large number of diverse training environments,

we would elicit the invariant predictor. Unfortunately, we were unable to phrase such

a â€œnonlinear general positionâ€ assumption and prove that it holds almost everywhere,

as we did in Theorem 10 for the linear case. We leave this eï¬€ort for future work.

While general, Theorem 9 is pessimistic, since it requires the number of training

environments to scale linearly with the number of parameters in the representation

matrix Î¦. Fortunately, as we will observe in our experiments from Section 5, it is often

the case that two environments are suï¬ƒcient to recover invariances. We believe that

these are problems where E[Y e|Î¦(Xe)] cannot match for two diï¬€erent environments

e Ì¸= eâ€² unless Î¦ extracts the causal invariance. The discussion from Section 3.3

gains relevance here, since enforcing W-invariance for larger families W should allow

discarding more non-invariant predictors with fewer training environments. All in all,

studying what problems allow the discovery of invariances from few environments is

a promising line of work towards a learning theory of invariance.

4.3

Causation as invariance

We promote invariance as the main feature of causation. Unsurprisingly, we are not

pioneers in doing so. To predict the outcome of an intervention, we rely on (i) the

13

## Page 14

properties of our intervention and (ii) the properties assumed invariant after the

intervention. Pearlâ€™s do-calculus [39] on causal graphs is a framework that tells which

conditionals remain invariant after an intervention. Rubinâ€™s ignorability [44] plays

the same role. Whatâ€™s often described as autonomy of causal mechanisms [20, 1] is

a speciï¬cation of invariance under intervention. A large body of philosophical work

[47, 42, 38, 12, 54, 13] studies the close link between invariance and causation. Some

works in machine learning [45, 18, 21, 26, 36, 43, 34, 7] pursue similar questions.

The invariance view of causation transcends some of the diï¬ƒculties of working

with causal graphs. For instance, the ideal gas law PV = nRT or Newtonâ€™s universal

gravitation F = G m1m2

r2

are diï¬ƒcult to describe using structural equation models

(What causes what?), but are prominent examples of laws that are invariant across

experimental conditions. When collecting data about gases or celestial bodies, the

universality of these laws will manifest as invariant correlations, which will sponsor

valid predictions across environments, as well as the conception of scientiï¬c theories.

Another motivation supporting the invariance view of causation are the problems

studied in machine learning. For instance, consider the task of image classiï¬cation.

Here, the observed variables are hundreds of thousands of correlated pixels. What is

the causal graph governing them? It is reasonable to assume that causation does not

happen between pixels, but between the real-world concepts captured by the camera.

In these cases, invariant correlations in images are a proxy into the causation at play

in the real world. To ï¬nd those invariant correlations, we need methods which can

disentangle the observed pixels into latent variables closer to the realm of causation,

such as IRM. In rare occasions we are truly interested in the entire causal graph

governing all the variables in our learning problem. Rather, our focus is often on the

causal invariances improving generalization across novel distributions of examples.

5

Experiments

We perform two experiments to assess the generalization abilities of IRM across

multiple environments. The source-code is available at

https://github.com/facebookresearch/InvariantRiskMinimization.

5.1

Synthetic data

As a ï¬rst experiment, we extend our motivating Example 1. First, we increase the

dimensionality of each of the two input features in X = (X1, X2) to 10 dimensions.

Second, as a form of model misspeciï¬cation, we allow the existence of a 10-dimensional

hidden confounder variable H. Third, in some cases the features Z will not be

directly observed, but only a scrambled version X = S(Z). Figure 3 summarizes

the SEM generating the data (Xe, Y e) for all environments e in these experiments.

More speciï¬cally, for environment e âˆˆR, we consider the following variations:

â€¢ Scrambled (S) observations, where S is an orthogonal matrix, or

unscrambled (U) observations, where S = I.

14

## Page 15

â€¢ Fully-observed (F) graphs, where Whâ†’1 = Whâ†’y = Whâ†’2 = 0, or

partially-observed (P) graphs, where (Whâ†’1, Whâ†’y, Whâ†’2) are Gaussian.

â€¢ Homoskedastic (O) Y -noise, where Ïƒ2

y = e2 and Ïƒ2

2 = 1, or

heteroskedastic (E) Y -noise, where Ïƒ2

y = 1 and Ïƒ2

2 = e2.

He

Ze

1

Y e

Ze

2

He â†N(0, e2)

Ze

1 â†N(0, e2) + Whâ†’1He

Y e â†Ze

1 Â· W1â†’y + N(0, Ïƒ2

y) + Whâ†’yHe

Ze

2 â†Wyâ†’2Y e + N(0, Ïƒ2

2) + Whâ†’2He

Figure 3: In our synthetic experiments, the task is to predict Y e from Xe = S(Ze

1, Ze

2).

These variations lead to eight setups referred to by their initials. For instance, the

setup â€œFOSâ€ considers fully-observed (F), homoskedastic Y -noise (O), and scrambled

observations (S). For all variants, (W1â†’y, Wyâ†’2) have Gaussian entries.

Each

experiment draws 1000 samples from the three training environments Etr = {0.2, 2, 5}.

IRM follows the variant (IRMv1), and uses the environment e = 5 to cross-validate

the invariance regularizer Î». We compare to ERM and ICP [40].

Figure 4 summarizes the results of our experiments. We show two metrics for each

estimated prediction rule Ë†Y = X1 Â· Ë†W1â†’y +X2 Â· Ë†Wyâ†’2. To this end, we consider a de-

scrambled version of the estimated coeï¬ƒcients ( Ë†

M1â†’y, Ë†

Myâ†’2) = ( Ë†W1â†’y, Ë†Wyâ†’2)âŠ¤SâŠ¤.

First, the plain barplots shows the average squared error between Ë†

M1â†’y and W1â†’y.

This measures how well does a predictor recover the weights associated to the causal

variables. Second, each striped barplot shows the norm of estimated weights Ë†

Myâ†’2

associated to the non-causal variable. We would like this norm to be zero, as the

desired invariant causal predictor is Ë†Y e = (W1â†’y, 0)âŠ¤SâŠ¤(Xe

1, Xe

2). In summary,

IRM is able to estimate the most accurate causal and non-causal weights across

all experimental conditions.

In most cases, IRM is orders of magnitude more

accurate than ERM (our y-axes are in log-scale). IRM also out-performs ICP, the

previous state-of-the-art method, by a large margin. Our experiments also show the

conservative behaviour of ICP (preferring to reject most covariates as direct causes),

leading to large errors on causal weights and small errors on non-causal weights.

5.2

Colored MNIST

We validate IRM at learning nonlinear invariant predictors with a synthetic binary

classiï¬cation task derived from MNIST. The goal is to predict a binary label assigned

to each image based on the digit. Whereas MNIST images are grayscale, we color

each image either red or green in a way that correlates strongly (but spuriously) with

the class label. By construction, the label is more strongly correlated with the color

than with the digit, so any algorithm purely minimizing training error will tend to

exploit the color. Such algorithms will fail at test time because the direction of the

15

## Page 16

## Fou

10âˆ’2

10âˆ’1

causal error

## Fos

10âˆ’2

100

## Feu

100

3 Ã— 10âˆ’1

4 Ã— 10âˆ’1

6 Ã— 10âˆ’1

## Fes

100

3 Ã— 10âˆ’1

4 Ã— 10âˆ’1

6 Ã— 10âˆ’1

10âˆ’4

10âˆ’2

non-causal error

10âˆ’3

10âˆ’2

100

3 Ã— 10âˆ’1

4 Ã— 10âˆ’1

6 Ã— 10âˆ’1

10âˆ’1

2 Ã— 10âˆ’1

3 Ã— 10âˆ’1

4 Ã— 10âˆ’1

6 Ã— 10âˆ’1

## Pou

10âˆ’1

causal error

## Pos

10âˆ’1

100

## Peu

100

2 Ã— 10âˆ’1

3 Ã— 10âˆ’1

4 Ã— 10âˆ’1

6 Ã— 10âˆ’1

## Pes

100

2 Ã— 10âˆ’1

3 Ã— 10âˆ’1

4 Ã— 10âˆ’1

6 Ã— 10âˆ’1

10âˆ’1

10âˆ’2

non-causal error

2 Ã— 10âˆ’2

3 Ã— 10âˆ’2

4 Ã— 10âˆ’1

6 Ã— 10âˆ’1

4 Ã— 10âˆ’1

6 Ã— 10âˆ’1

## Erm

## Icp

## Irm

Figure 4: Average errors on causal (plain bars) and non-causal (striped bars) weights for

our synthetic experiments. The y-axes are in log-scale. See main text for details.

correlation is reversed in the test environment. By observing that the strength of the

correlation between color and label varies between the two training environments, we

can hope to eliminate color as a predictive feature, resulting in better generalization.

We deï¬ne three environments (two training, one test) from MNIST transforming

each example as follows: ï¬rst, assign a preliminary binary label Ëœy to the image based

on the digit: Ëœy = 0 for digits 0-4 and Ëœy = 1 for 5-9. Second, obtain the ï¬nal label y

by ï¬‚ipping Ëœy with probability 0.25. Third, sample the color id z by ï¬‚ipping y with

probability pe, where pe is 0.2 in the ï¬rst environment, 0.1 in the second, and 0.9 in

the test one. Finally, color the image red if z = 1 or green if z = 0.

We train MLPs on the colored MNIST training environments using diï¬€erent

objectives and report results in Table 1. For each result we report the mean and

standard deviation across ten runs. Training with ERM returns a model with

high accuracy in the training environments but below-chance accuracy in the test

environment, since the ERM model classiï¬es mainly based on color. Training with

IRM results in a model that performs worse on the training environments, but relies

less on the color and hence generalizes better to the test environments. An oracle

that ignores color information by construction outperforms IRM only slightly.

To better understand the behavior of these models, we take advantage of the fact

that h = Î¦(x) (the logit) is one-dimensional and y is binary, and plot P(y = 1|h, e)

as a function of h for each environment and each model in Figure 5. We show each

algorithm in a separate plot, and each environment in a separate color. The ï¬gure

shows that, whether considering only the two training environments or all three

16

## Page 17

Algorithm

Acc. train envs.

Acc. test env.

## Erm

87.4 Â± 0.2

17.1 Â± 0.6

IRM (ours)

70.8 Â± 0.9

66.9 Â± 2.5

Random guessing (hypothetical)

50

50

Optimal invariant model (hypothetical)

75

75

ERM, grayscale model (oracle)

73.5 Â± 0.2

73.0 Â± 0.4

Table 1: Accuracy (%) of diï¬€erent algorithms on the Colored MNIST synthetic task. ERM

fails in the test environment because it relies on spurious color correlations to classify digits.

IRM detects that the color has a spurious correlation with the label and thus uses only the

digit to predict, obtaining better generalization to the new unseen test environment.

âˆ’5

0

5

0.0

0.5

1.0

P(y = 1|h)

## Erm

âˆ’5

0

5

h

## Irm

Train env. 1 (e=0.2)

Train env. 2 (e=0.1)

Test env. (e=0.9)

âˆ’5

0

5

Oracle

Figure 5: P(y = 1|h) as a function of h for diï¬€erent models trained on Colored MNIST: (left)

an ERM-trained model, (center) an IRM-trained model, and (right) an ERM-trained model

which only sees grayscale images and therefore is perfectly invariant by construction. IRM

learns approximate invariance from data alone and generalizes well to the test environment.

environments, the IRM model is closer to achieving invariance than the ERM model.

Notably, the IRM model does not achieve perfect invariance, particularly at the tails

of the P(h). We suspect this is due to ï¬nite sample issues: given the small sample

size at the tails, estimating (and hence minimizing) the small diï¬€erences in P(y|h, e)

between training environments can be quite diï¬ƒcult, regardless of the method.

We note that conditional domain adaptation techniques which match P(h|y, e)

across environments could in principle solve this task equally well to IRM, which

matches P(y|h, e). This is because the distribution of the causal features (the digit

shapes) and P(y|e) both happen to be identical across environments. However,

unlike IRM, conditional domain adaptation will fail if, for example, the distribution

of the digits changes across environments. We discuss this further in Appendix C.

Finally, Figure 5 shows that P(y = 1|h) cannot always be expressed with a linear

classiï¬er w. Enforcing nonlinear invariances (Section 3.3) could prove useful here.

17

## Page 18

6

Looking forward: a concluding dialogue

[ Eric and Irma are two graduate students studying the Invariant Risk Minimization (IRM)

manuscript. Over a cup of coï¬€ee at a cafÂ´e in Palais-Royal, they discuss the advantages and caveats

that invariance brings to Empirical Risk Minimization (ERM). ]

Irma: I have observed that predictors trained with ERM sometimes absorb

biases and spurious correlations from data. This leads to undesirable

behaviours when predicting about examples that do not follow the

distribution of the training data.

Eric: I have observed that too, and I wonder what are the reasons behind such

phenomena. After all, ERM is an optimal principle to learn predictors

from empirical data!

Irma: It is, indeed. But even when your hypothesis class allows you to ï¬nd the

empirical risk minimizer eï¬ƒciently, there are some assumptions at play.

First, ERM assumes that training and testing data are identically and

independently distributed according to the same distribution. Second,

generalization bounds require that the ratio between the capacity of our

hypothesis class and the number of training examples n tends to zero, as

n â†’âˆ. Third, ERM achieves zero test error only in the realizable case

â€”that is, when there exists a function in our hypothesis class able to

achieve zero error. I suspect that violating these assumptions leads ERM

into absorbing spurious correlations, and that this is where invariance

may prove useful.

Eric: Interesting. Should we study the three possibilities in turn?

Irma: Sure thing! But ï¬rst, letâ€™s grab another cup of coï¬€ee.

[We also encourage the reader to grab a cup of coï¬€ee.]

âˆ¼

Irma: First and foremost, we have the â€œidentically and independently dis-

tributedâ€ (iid) assumption. I once heard Professor Ghahramani refer to

this assumption as â€œthe big lie in machine learningâ€. This is to say that

all training and testing examples are drawn from the same distribution

## P(X, Y ) = P(Y |X)P(X).

Eric: I see.

This is obviously not the case when learning from multiple

environments, as in IRM. Given this factorization, I guess two things are

subject to change: either the marginal distribution P(X) of my inputs,

or the conditional distribution P(Y |X) mapping those inputs into my

targets.

Irma: Thatâ€™s correct. Letâ€™s focus ï¬rst on the case where P(Xe) changes across

environments e. Some researchers from the ï¬eld of domain adaptation

call this covariate shift. This situation is challenging when the supports

of P(Xe) are disjoint across environments. Actually, without a-priori

knowledge, there is no reason to believe that our predictor will generalize

outside the union of the supports of the training environments.

18

## Page 19

Eric: A daunting challenge, indeed. How could invariance help here?

Irma: Two things come to mind. On the one hand, we could try to transform

our inputs into some features Î¦(Xe), as to match the support of all

the training environments. Then, we could learn an invariant classiï¬er

w(Î¦(Xe)) on top of the transformed inputs.

[Appendix D studies the

shortcomings of this idea.] On the other hand, we could assume that the

invariant predictor w has a simple structure, that we can estimate given

limited supports. The authors of IRM follow this route, by assuming

linear classiï¬ers on top of representations.

Eric: I see! Even though the P(Xe) may be disjoint, if there is a simple

invariance satisï¬ed for all training environments separately, it may also

hold in unobserved regions of the space. I wonder if we could go further

by assuming some sort of compositional structure in w, the linear as-

sumption of IRM is just the simplest kind. I say this since compositional

assumptions often enable learning in one part of the input space, and

evaluating on another.

Irma: It sounds reasonable! What about the case where P(Y e|Xe) changes?

Does this happen in normal supervised learning? I remember attending a

lecture by Professor SchÂ¨olkopf [45, 25] where he mentioned that P(Y e|Xe)

is often invariant across environments when Xe is a cause of Y e, and

that it often varies when Xe is an eï¬€ect of Y e. For instance, he explains

that MNIST classiï¬cation is anticausal: as in, the observed pixels are an

eï¬€ect of the mental concept that led the writer to draw the digit in the

ï¬rst place. IRM insists on this relation between invariance and causation,

what do you think?

Eric: I saw that lecture too. Contrary to Professor SchÂ¨olkopf, I believe that

most supervised learning problems, such as image classiï¬cation, are causal.

In these problems we predict human annotations Y e from pixels Xe,

hoping that the machine imitates this cognitive process. Furthermore,

the annotation process often involves multiple humans in the interest of

making P(Y e|Xe) deterministic. If the annotation process is close to

deterministic and shared across environments, predicting annotations is

a causal problem, with an invariant conditional expectation.

Irma: Oh! This means that in supervised learning problems about predicting

annotations, P(Y e|Xe) is often stable across environments, so ERM has

great chances of succeeding. This is good news: it explains why ERM is

so good at supervised learning, and leaves less to worry about.

Eric: However, if any of the other problems appear (disjoint P(Xe), not

enough data, not enough capacity), ERM could get in trouble, right?

Irma: Indeed! Furthermore, in some supervised learning problems, the label

is not necessarily created from the input. For instance, the input could

be an X-ray image, and the target could be the result of a tumor biopsy

on the same patient. Also, there are problems where we predict parts of

the input from other parts of the input, like in self-supervised learning

[14]. In some other cases, we donâ€™t even have labels! This could include

19

## Page 20

the unsupervised learning of the causal factors of variation behind Xe,

which involves inverting the causal generative process of the data. In

all of these cases, we could be dealing with anticausal problems, where

the conditional distribution is subject to change across environments.

Then, I expect searching for invariance may help by focusing on invariant

predictors that generalize out-of-distribution.

Eric: That is an interesting divide between supervised and unsupervised

learning! [Figure 6 illustrates the main elements of this discussion.]

Nature variables

pixels

Nature causal

mechanisms

label

ï£®

ï£¯ï£¯ï£¯ï£°

0

1

...

0

ï£¹

ï£ºï£ºï£ºï£»

â€œcatâ€

human

cognition

supervised/causal learning

unsupervised/anticausal learning?

self-supervised

Figure 6: All learning problems use empirical observations, here referred to as â€œpixelsâ€. Fol-

lowing a causal and cognitive process, humans produce labels. Therefore, supervised learning

problems predicting annotations from observations are causal, and therefore P(label | pixel)

is often invariant. Conversely, types of unsupervised and self-supervised learning trying

to disentangle the underlying data causal factors of variation (Nature variables) should to

some extent reverse the process generating observations (Nature mechanisms). This leads to

anticausal learning problems, possibly with varying conditional distributions; an opportunity

to leverage invariance. Cat picture by www. flickr. com/ photos/ pustovit .

âˆ¼

Eric: Secondly, what about the ratio between the capacity of our classiï¬er

and the number of training examples n? Neural networks often have a

number of parameters on the same order of magnitude, or even greater,

than the number of training examples [56]. In these cases, such ratio

will not tend to zero as n â†’âˆ. So, ERM may be in trouble.

Irma: That is correct. Neural networks are often over-parametrized, and

over-parametrization carries subtle consequences. For instance, consider

that we are using the pseudo-inverse to solve an over-parametrized linear

least-squares problem, or using SGD to train an over-parametrized neural

network. Amongst all the zero training error solutions, these procedures

will prefer the solution with the smallest capacity [53, 3]. Unfortunately,

spurious correlations and biases are often simpler to detect than the true

phenomenon of interest [17, 9, 10, 11]. Therefore, low capacity solutions

prefer exploiting those simple but spurious correlations. For instance,

think about relying on large green textures to declare the presence of a

cow on an image.

20

## Page 21

Eric: The cows again!

Irma: Always. Although I can give you a more concrete example. Consider

predicting Y e from Xe = (Xe

1, Xe

2), where:

Y e â†106 Â· Xe

1Î±1,

Xe

2 â†106 Â· Y eÎ±âŠ¤

2 Â· e,

the coeï¬ƒcients satisfy âˆ¥Î±1âˆ¥= âˆ¥Î±2âˆ¥= 1, the training environments are

e = {1, 10}, and we have n samples for the 2n-dimensional input X. In

this over-parametrized problem, the invariant regression from the cause

X1 requires large capacity, while the spurious regression from the eï¬€ect

X2 requires low capacity.

Eric: Oh!

Then, the inductive bias of SGD would prefer to exploit the

spurious correlation for prediction. In a nutshell, a deï¬cit of training

examples forces us into regularization, and regularization comes with

the danger of absorbing easy spurious correlations. But, methods based

on invariance should realize that, after removing the nuisance variable

X2, the regression from X1 is invariant, and thus interesting for out-of-

distribution generalization. This means that invariance could sometimes

help ï¬ght the issues of small data and over-parametrization. Neat!

âˆ¼

Irma: As a ï¬nal obstacle to ERM, we have the case where the capacity of our

hypothesis class is insuï¬ƒcient to solve the learning problem at hand.

Eric: This sounds related to the previous point, in the sense that a model

with low capacity will stick to spurious correlations, if these are easier to

capture.

Irma: That is correct, although I can see an additional problem arising from

insuï¬ƒcient capacity. For instance, the only linear invariant prediction

rule to estimate the quadratic Y e = (Xe)2, where Xe âˆ¼Gaussian(0, e),

is the null predictor Y = 0 Â· X. Even though X is the only, causal, and

invariance-eliciting covariate!

Eric: Got it. Then, we should expect invariance to have a larger chance of

success when allowing high capacity. For low-capacity problems, I would

rely on cross-validation to lower the importance of the invariance penalty

in IRM, and fall back to good old ERM.

Irma: ERM is really withstanding the test of time, isnâ€™t it?

Eric: Deï¬nitely. From what we have discussed before, I think ERM is specially

useful in the realizable case, when there is a predictor in my hypothesis

class achieving zero error.

Irma: Why so?

Eric: In the realizable case, the optimal invariant predictor has zero error

across all environments.

Therefore it makes sense, as an empirical

principle, to look for zero training error across training environments.

This possibly moves towards an optimal prediction rule on the union of

21

## Page 22

the supports of the training environments. This means that achieving

invariance across all environments using ERM is possible in the realizable

case, although it would require data from lots of environments!

Irma: Wait a minute. Are you saying that achieving zero training error makes

sense from an invariance perspective?

Eric: In the realizable case, I would say so! Turns out all these people training

neural networks to zero training error were onto something!

âˆ¼

[ The barista approaches Eric and Irma to let them know that the cafÂ´e is closing. ]

Eric: Thank you for the interesting chat, Irma.

Irma: The pleasure is mine!

Eric: One of my takeaways is that discarding spurious correlations is some-

thing doable even when we have access only to two environments. The

remaining, invariant correlations sketch the core pieces of natural phe-

nomena, which in turn form a simpler model.

Irma: Simple models for a complex world. Why bother with the details, right?

Eric: Hah, right. It seems like regularization is more interesting than we

thought. IRM is a learning principle to discover unknown invariances

from data. This diï¬€ers from typical regularization techniques to enforce

known invariances, often done by architectural choices (using convolutions

to achieve translation invariance) and data augmentation.

I wonder what other applications we can ï¬nd for invariance. Perhaps

we could think of reinforcement learning episodes as diï¬€erent environ-

ments, so we can learn robust policies that leverage the invariant part of

behaviour leading to reward.

Irma: That is an interesting one. I was also thinking that invariance has

something to say about fairness. For instance, we could consider diï¬€erent

groups as environments. Then, learning an invariant predictor means

ï¬nding a representation such that the best way to treat individuals with

similar relevant features is shared across groups.

Eric: Interesting! I was also thinking that it may be possible to formalize

IRM in terms of invariance and equivariance concepts from group theory.

Do you want to take a stab at these things tomorrow at the lab?

Irma: Surely. See you tomorrow, Eric.

Eric: See you tomorrow!

[ The students pay their bill, leave the cafÂ´e, and stroll down the streets of Paris, quiet and warm

during the Summer evening. ]

22

## Page 23

Acknowledgements

We are thankful to Francis Bach, Marco Baroni, Ishmael Belghazi, Diane Boucha-

court, FranÂ¸cois Charton, Yoshua Bengio, Charles Blundell, Joan Bruna, Lars Buesing,

Soumith Chintala, Kyunghyun Cho, Jonathan Gordon, Christina Heinze-Deml, Fer-

enc HuszÂ´ar, Alyosha Efros, Luke Metz, Cijo Jose, Anna Klimovskaia, Yann Ollivier,

Maxime Oquab, Jonas Peters, Alec Radford, Cinjon Resnick, Uri Shalit, Pablo

Sprechmann, SÂ´onar festival, Rachel Ward, and Will Whitney for their help.

References

[1] John Aldrich. Autonomy. Oxford Economic Papers, 1989.

[2] James Andrew Bagnell. Robust supervised learning. In AAAI, 2005.

[3] Peter L. Bartlett, Philip M. Long, GÂ´abor Lugosi, and Alexander Tsigler. Benign

Overï¬tting in Linear Regression. arXiv, 2019.

[4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita.

In ECCV, 2018.

[5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis

of representations for domain adaptation. In NIPS. 2007.

[6] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimiza-

tion. Princeton University Press, 2009.

[7] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, SÂ´ebastien

Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-

transfer objective for learning to disentangle causal mechanisms. arXiv, 2019.

[8] LÂ´eon Bottou, Corinna Cortes, John S. Denker, Harris Drucker, Isabelle Guyon,

Lawrence D. Jackel, Yann Le Cun, Urs A. Muller, Eduard SÂ¨ackinger, Patrice

Simard, and Vladimir Vapnik. Comparison of classiï¬er methods: a case study

in handwritten digit recognition. In ICPR, 1994.

[9] Wieland Brendel and Matthias Bethge. Approximating CNNs with bag-of-local-

features models works surprisingly well on imagenet. In ICLR, 2019.

[10] Joan Bruna and Stephane Mallat. Invariant scattering convolution networks.

## Tpami, 2013.

[11] Joan Bruna, Stephane Mallat, Emmanuel Bacry, and Jean-Franois Muzy. In-

termittent process analysis with scattering moments. The Annals of Statistics,

2015.

[12] Nancy Cartwright. Two theorems on invariance and causality. Philosophy of

Science, 2003.

23

## Page 24

[13] Patricia W. Cheng and Hongjing Lu. Causal invariance as an essential constraint

for creating a causal representation of the world. The Oxford handbook of causal

reasoning, 2017.

[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:

Pre-training of deep bidirectional transformers for language understanding.

## Naacl, 2019.

[15] John Duchi, Peter Glynn, and Hongseok Namkoong.

Statistics of robust

optimization: A generalized empirical likelihood approach. arXiv, 2016.

[16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo

Larochelle, FranÂ¸cois Laviolette, Mario March, and Victor Lempitsky. Domain-

adversarial training of neural networks. JMLR, 2016.

[17] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A.

Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards

texture; increasing shape bias improves accuracy and robustness. ICLR, 2019.

[18] AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Kun Zhang.

Learning causal structures using regression invariance. In NIPS, 2017.

[19] Patrick J. Grother. NIST Special Database 19: Handprinted forms and char-

acters database. https://www.nist.gov/srd/nist-special-database-19,

1995. File doc/doc.ps in the 1995 NIST CD ROM NIST Special Database 19.

[20] Trygve Haavelmo. The probability approach in econometrics. Econometrica:

Journal of the Econometric Society, 1944.

[21] Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties

and domain shift robustness. arXiv, 2017.

[22] Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen. Invariant causal

prediction for nonlinear models. Journal of Causal Inference, 2018.

[23] Allan Jabri, Armand Joulin, and Laurens Van Der Maaten. Revisiting visual

question answering baselines. In ECCV, 2016.

[24] Fredrik D. Johansson, David A. Sontag, and Rajesh Ranganath. Support and

invertibility in domain-invariant representations. AISTATS, 2019.

[25] Niki Kilbertus, Giambattista Parascandolo, and Bernhard SchÂ¨olkopf. General-

ization in anti-causal learning. arXiv, 2018.

[26] Kun Kuang, Peng Cui, Susan Athey, Ruoxuan Xiong, and Bo Li.

Stable

prediction across unknown environments. In SIGKDD, 2018.

[27] Brenden M. Lake, Tomer D. Ullman, Joshua B Tenenbaum, and Samuel J.

Gershman. Building machines that learn and think like people. Behavioral and

brain sciences, 2017.

24

## Page 25

[28] James M. Lee. Introduction to Smooth Manifolds. Springer, 2003.

[29] David Lewis. Counterfactuals. John Wiley & Sons, 2013.

[30] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang,

and Dacheng Tao. Deep domain generalization via conditional invariant adver-

sarial networks. In ECCV, 2018.

[31] David Lopez-Paz. From dependence to causation. PhD thesis, University of

Cambridge, 2016.

[32] David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf,

and LÂ´eon Bottou. Discovering causal signals in images. In CVPR, 2017.

[33] Gilles Louppe, Michael Kagan, and Kyle Cranmer. Learning to pivot with

adversarial networks. In Advances in neural information processing systems,

pages 981â€“990, 2017.

[34] Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip

Versteeg, and Joris M Mooij. Domain adaptation by using causal inference to

predict invariant conditional distributions. In NIPS, 2018.

[35] Gary Marcus. Deep learning: A critical appraisal. arXiv, 2018.

[36] Nicolai Meinshausen. Causality from a distributional robustness point of view.

In Data Science Workshop (DSW), 2018.

[37] Nicolai Meinshausen and Peter BÂ¨uhlmann. Maximin eï¬€ects in inhomogeneous

large-scale data. The Annals of Statistics, 2015.

[38] Sandra D. Mitchell. Dimensions of scientiï¬c law. Philosophy of Science, 2000.

[39] Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University

Press, 2nd edition, 2009.

[40] Jonas Peters, Peter BÂ¨uhlmann, and Nicolai Meinshausen. Causal inference

using invariant prediction: identiï¬cation and conï¬dence intervals. JRSS B,

2016.

[41] Jonas Peters, Dominik Janzing, and Bernhard SchÂ¨olkopf. Elements of causal

inference: foundations and learning algorithms. MIT press, 2017.

[42] Michael Redhead. Incompleteness, non locality and realism. a prolegomenon to

the philosophy of quantum mechanics. 1987.

[43] Mateo Rojas-Carulla, Bernhard SchÂ¨olkopf, Richard Turner, and Jonas Peters.

Invariant models for causal transfer learning. JMLR, 2018.

[44] Donald B. Rubin. Estimating causal eï¬€ects of treatments in randomized and

nonrandomized studies. Journal of educational Psychology, 1974.

25

## Page 26

[45] Bernhard SchÂ¨olkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun

Zhang, and Joris Mooij. On causal and anticausal learning. In ICML, 2012.

[46] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distribu-

tional robustness with principled adversarial training. ICLR, 2018.

[47] Brian Skyrms. Causal necessity: a pragmatic investigation of the necessity of

laws. Yale University Press, 1980.

[48] Bob L. Sturm. A simple method to determine if a music information retrieval

system is a â€œhorseâ€. IEEE Transactions on Multimedia, 2014.

[49] Antonio Torralba and Alexei Efros. Unbiased look at dataset bias. In CVPR,

2011.

[50] Vladimir Vapnik. Principles of risk minimization for learning theory. In NIPS.

1992.

[51] Vladimir N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.

[52] Max Welling. Do we still need models or just more data and compute?, 2019.

[53] Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin

Recht. The marginal value of adaptive gradient methods in machine learning.

In NIPS. 2017.

[54] James Woodward.

Making things happen: A theory of causal explanation.

Oxford university press, 2005.

[55] Sewall Wright. Correlation and causation. Journal of agricultural research,

1921.

[56] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.

Understanding deep learning requires rethinking generalization. ICLR, 2016.

26

## Page 27

## A

Additional theorems

Theorem 10. Let Î£e

X,X := EXe[XeXeâŠ¤] âˆˆSdÃ—d

+

, with SdÃ—d

+

the space of symmetric

positive semi-deï¬nite matrices, and Î£e

X,Ïµ := EXe[XeÏµe] âˆˆRd.

Then, for any

arbitrary tuple

 Î£e

X,Ïµ



eâˆˆEtr âˆˆ

 Rd|Etr|, the set

{(Î£e

X,X)eâˆˆEtr such that Etr does not satisfy general position}

has measure zero in (SdÃ—d

+

)|Etr|.

## B

Proofs

## B.1

Proof of Proposition 2

Let

f â‹†âˆˆmin

f

max

eâˆˆEtr Re(f) âˆ’re,

M â‹†= max

eâˆˆEtr Re(f â‹†) âˆ’re.

Then, the pair (f â‹†, M â‹†) solves the constrained optimization problem

min

f,M

## M

s.t.

Re(f) âˆ’re â‰¤M

for all e âˆˆEtr,

with Lagrangian L(f, M, Î») = M + P

eâˆˆEtr Î»e(Re(f) âˆ’re âˆ’M). If the problem

above satisï¬es the KKT diï¬€erentiability and qualiï¬cation conditions, then there

exist Î»e â‰¥0 with âˆ‡fL(f â‹†, M â‹†, Î») = 0, such that

âˆ‡f|f=f â‹†

## X

eâˆˆEtr

Î»eRe(f) = 0.

## B.2

Proof of Theorem 4

Let Î¦ âˆˆRpÃ—d, w âˆˆRp, and v = Î¦âŠ¤w. The simultaneous optimization

âˆ€e

wâ‹†âˆˆarg min

wâˆˆRp

Re(w â—¦Î¦)

(8)

is equivalent to

âˆ€e

vâ‹†âˆˆarg min

vâˆˆGÎ¦

Re(v),

(9)

where GÎ¦ = {Î¦âŠ¤w : w âˆˆRp} âŠ‚Rd is the set of vectors v = Î¦âŠ¤w reachable

by picking any w âˆˆRp. It turns out that GÎ¦ = Ker(Î¦)âŠ¥, that is, the subspace

orthogonal to the nullspace of Î¦. Indeed, for all v = Î¦âŠ¤w âˆˆGÎ¦ and all x âˆˆKer(Î¦),

27

## Page 28

we have xâŠ¤v = xâŠ¤Î¦âŠ¤w = (Î¦x)âŠ¤w = 0. Therefore GÎ¦ âŠ‚Ker(Î¦)âŠ¥. Since both

subspaces have dimension rank(Î¦) = d âˆ’dim(Ker(Î¦)), they must be equal.

We now prove the theorem: let v = Î¦âŠ¤w where Î¦ âˆˆRpÃ—d and w âˆˆRp minimizes

all Re(w â—¦Î¦). Since v âˆˆGÎ¦, we have v âˆˆKer(Î¦)âŠ¥. Since w minimizes Re(Î¦âŠ¤w),

we can also write

âˆ‚

âˆ‚w Re(Î¦âŠ¤w) = Î¦ âˆ‡Re(Î¦âŠ¤w) = Î¦âˆ‡Re(v) = 0 .

(10)

Therefore âˆ‡Re(v) âˆˆKer(Î¦). Finally vâŠ¤âˆ‡Re(v) = wâŠ¤Î¦ âˆ‡Re(Î¦âŠ¤w) = 0.

Conversely, let v âˆˆRd satisfy vâŠ¤âˆ‡Re(v) = 0 for all e âˆˆE. Thanks to these

orthogonality conditions, we can construct a subspace that contains all the âˆ‡Re(v)

and is orthogonal to v. Let Î¦ be any matrix whose nullspace satisï¬es these conditions.

Since v âŠ¥Ker(Î¦), that is, v âˆˆKer(Î¦)âŠ¥= GÎ¦, there is a vector w âˆˆRp such that

v = Î¦âŠ¤w. Finally, since âˆ‡Re(v) âˆˆKer(Î¦), the derivative (10) is zero.

## B.3

Proof of Theorem 9

Observing that Î¦ EXe,Y e [XeY e] = Î¦ EXe,Ïµe[Xe(( ËœSXe)

âŠ¤Î³ + Ïµe)], we re-write (7) as

## Î¦

ï£«

ï£¬

ï£¬

ï£­EXe

h

XeXeâŠ¤i

(Î¦âŠ¤w âˆ’ËœSâŠ¤Î³) âˆ’EXe,Ïµe [XeÏµe]

|

{z

}

:=qe

ï£¶

ï£·

ï£·

ï£¸= 0.

(11)

To show that Î¦ leads to the desired invariant predictor Î¦âŠ¤w = ËœSâŠ¤Î³, we as-

sume Î¦âŠ¤w Ì¸= ËœSâŠ¤Î³ and reach a contradiction. First, by Assumption 8, we have

dim(span({qe}eâˆˆEtr)) > d âˆ’r. Second, by (11), each qe âˆˆKer(Î¦). Therefore, it

would follow that dim(Ker(Î¦)) > d âˆ’r, which contradicts the assumption that

rank(Î¦) = r.

## B.4

Proof of Theorem 10

Let m = |Etr|, and deï¬ne G : Rd \ {0} â†’RmÃ—d as (G(x))e,i =

 Î£e

X,Xx âˆ’Î£e

X,Ïµ



i .

Let W = G

 Rd \ {0}



âŠ†RmÃ—d, which is a linear manifold of dimension at most

d, missing a single point (since G is aï¬ƒne, and its input has dimension d).

For the rest of the proof, let (Î£e

X,Ïµ)eâˆˆEtr âˆˆRd|Etr| be arbitrary and ï¬xed. We

want to show that for generic (Î£e

X,X)eâˆˆEtr, if m > d

r + d âˆ’r, the matrices G(x)

have rank larger than d âˆ’r. Analogously, if LR(m, d, k) âŠ†RmÃ—d is the set of m Ã— d

matrices with rank k, we want to show that W âˆ©LR(m, d, k) = âˆ…for all k < d âˆ’r.

We need to prove two statements. First, that for generic(Î£e

X,X)eâˆˆEtr W and

LR(m, d, k) intersect transversally as manifolds, or donâ€™t intersect at all.

This

will be a standard argument using Thomâ€™s transversality theorem. Second, by

dimension counting, that if W and LR(m, d, k) intersect transversally, and k < dâˆ’r,

m >

d

r + d âˆ’r, then the dimension of the intersection is negative, which is a

contradiction and thus W and LR(m, d, k) cannot intersect.

28

## Page 29

We then claim that W and LR(m, d, k) are transversal for generic (Î£e

X,X)eâˆˆEtr.

To do so, deï¬ne

F : (Rd \ {0}) Ã—

 SdÃ—d

+

m â†’RmÃ—d,

## F



x,

 Î£e

## X,X



eâˆˆEtr

eâ€²

l =



Î£eâ€²

X,Xx âˆ’Î£eâ€²

X,Ïµ



l

If we show that âˆ‡x,Î£X,XF : Rd Ã— (SdÃ—d)m â†’RmÃ—d is a surjective linear trans-

formation, then F is transversal to any submanifold of RmÃ—d (and in particular

to LR(m, d, k)). By the Thom transversality theorem, this implies that the set of

 Î£e

## X,X



eâˆˆEtr such that W is not transversal to LR(m, d, k) has measure zero in SdÃ—d

+

,

proving our ï¬rst statement.

Next, we show that âˆ‡x,Î£X,XF is surjective. This follows by by showing that

âˆ‡Î£X,XF : (SdÃ—d)m â†’RmÃ—d is surjective, since adding more columns to this

matrix can only increase its rank. We then want to show that the linear map

âˆ‡Î£X,XF : (SdÃ—d)m â†’RmÃ—d is surjective. To this end, we can write: âˆ‚Î£e

i,jF eâ€²

l

=

Î´e,eâ€² (Î´l,ixj + Î´l,jxi) , and let C âˆˆRmÃ—d. We want to construct a D âˆˆ

 SdÃ—dm such

that

Ceâ€²

l =

## X

i,j,e

Î´e,eâ€² (Î´l,ixj + Î´l,jxi) De

i,j.

The right hand side equals

## X

i,j,e

Î´e,eâ€² (Î´l,ixj + Î´l,jxi) De

i,j =

## X

j

Deâ€²

l,jxj +

## X

i

Deâ€²

i,lxi = (Deâ€²x)l + (xDeâ€²)l

If Deâ€² is symmetric, this equals (2Dex)l. Therefore, we only need to show that for

any vector Ce âˆˆRd, there is a symmetric matrix De âˆˆSdÃ—d with Ce = Dex. To

see this, let O âˆˆRdÃ—d be an orthogonal transformation such that Ox has no zero

entries, and name v = Ox, we = OCe. Furthermore, let Ee âˆˆRdÃ—d be the diagonal

matrix with entries Ee

i,i = we

i

vi . Then, Ce = OT EeOx. By the spectral theorem,

OT EeO is symmetric, showing that âˆ‡Î£X,XF : (SdÃ—d)m â†’RmÃ—d is surjective, and

thus that W and LR(m, d, k) are transversal for almost any

 Î£e

## X,X



eâˆˆEtr.

By transversality, we know that W cannot intersect LR(m, d, k) if dim(W) +

dim (LR(m, d, k)) âˆ’dim

 RmÃ—d

< 0. By a dimensional argument (see [28], example

5.30), it follows that codim(LR(m, d, k)) = dim

 RmÃ—d

âˆ’dim (LR(m, d, k)) = (m âˆ’

k)(d âˆ’k). Therefore, if k < d âˆ’r and m > d

r + d âˆ’r, it follows that

dim(W) + dim (LR(m, d, k)) âˆ’dim

 RmÃ—d

= dim(W) âˆ’codim (LR(m, d, k))

â‰¤d âˆ’(m âˆ’k)(d âˆ’k)

â‰¤d âˆ’(m âˆ’(d âˆ’r))(d âˆ’(d âˆ’r))

= d âˆ’r(m âˆ’d + r)

< d âˆ’r

d

r + d âˆ’r



âˆ’d + r



= d âˆ’d = 0.

Therefore, W âˆ©LR(m, d, k) = âˆ…under these conditions, ï¬nishing the proof.

29

## Page 30

## C

Failure cases for Domain Adaptation

Domain adaptation [5] considers labeled data from a source environment es and

unlabeled data from a target environment et with the goal of training a classiï¬er that

works well on et. Many domain adaptation techniques, including the popular Adver-

sarial Domain Adaptation [16, ADA], proceed by learning a feature representation

Î¦ such that (i) the input marginals P(Î¦(Xes)) = P(Î¦(Xet)), and (ii) the classiï¬er

w on top of Î¦ predicts well the labeled data from es. Thus, are domain adaptation

techniques applicable to ï¬nding invariances across multiple environments?

One shall proceed cautiously, as there are important caveats. For instance, con-

sider a binary classiï¬cation problem, where the only diï¬€erence between environments

is that P(Y es = 1) = 1

2, but P(Y et = 1) =

9

10. Using these data and the domain

adaptation recipe outlined above, we build a classiï¬er wâ—¦Î¦. Since domain adaptation

enforces P(Î¦(Xes)) = P(Î¦(Xet)), it consequently enforces P( Ë†Y es) = P( Ë†Y et), where

Ë†Y e = w(Î¦(Xe)), for all e âˆˆ{es, et}. Then, the classiï¬cation accuracy will be at

most 20%. This is worse than random guessing, in a problem where simply training

on the source domain leads to a classiï¬er that generalizes to the target domain.

Following on this example, we could think of applying conditional domain

adaptation techniques [30, C-ADA]. These enforce one invariance P(Î¦(Xes)|Y es) =

P(Î¦(Xet)|Y et) per value of Y e. Using Bayes rule, it follows that C-ADA enforces

a stronger condition than invariant prediction when P(Y es) = P(Y et). However,

there are general problems where the invariant predictor cannot be identiï¬ed by

## C-Ada.

To see this, consider a discrete input feature Xe âˆ¼P(Xe), and a binary target

Y e = F(Xe) âŠ•Bernoulli(p). This model represents a generic binary classiï¬cation

problem with label noise. Since the distribution P(Xe) is the only moving part across

environments, the trivial representation Î¦(x) = x elicits an invariant prediction rule.

Assuming that the discrete variable Xe takes n values, we can summarize P(Xe) as

the probability n-vector px,e. Then, Î¦(Xe) is also discrete, and we can summarize

its distribution as the probability vector pÏ†,e = AÏ†px,e, where AÏ† is a matrix of

zeros and ones. By Bayes rule,

Ï€Ï†,e := P(Î¦(Xe)|Y e = 1) = P(Y e = 1|Î¦(Xe)) âŠ™pÏ†,e

âŸ¨P(Y e = 1|Î¦(Xe)), pÏ†,eâŸ©= (AÎ¦ (v âŠ™px,e)) âŠ™(AÎ¦px,e)

âŸ¨(AÎ¦ (v âŠ™px,e)) , AÎ¦px,eâŸ©,

where âŠ™is the entry-wise multiplication, âŸ¨, âŸ©is the dot product, and v := P(Y e =

1|Xe) does not depend on e. Unfortunately for C-ADA, it can be shown that the set

Î Ï† := {(px,e, px,eâ€²) : Ï€Ï†,e = Ï€Ï†,eâ€²} has measure zero. Since the union of sets with

zero measure has zero measure, and there exists only a ï¬nite amount of possible AÏ†,

the set Î Ï† has measure zero for any Î¦. In conclusion and almost surely, C-ADA

disregards any non-zero data representation eliciting an invariant prediction rule,

regardless of the fact that the trivial representation Î¦(x) = x achieves such goal.

As a general remark, domain adaptation is often justiï¬ed using the bound [5]:

Erroret(w â—¦Î¦) â‰¤Errores(w â—¦Î¦) + Distance(Î¦(Xes), Î¦(Xet)) + Î»â‹†.

30

## Page 31

Here, Î»â‹†is the error of the optimal classiï¬er in our hypothesis class, operating on

top of Î¦, summed over the two domains. Crucially, Î»â‹†is often disregarded as a

constant, justifying the DA goals (i, ii) outlined above. But, Î»â‹†depends on the data

representation Î¦, instantiating a third trade-oï¬€that it is often ignored. For a more

in depth analysis of this issue, we recommend [24].

## D

Minimal implementation of IRM in PyTorch

import

torch

from

torch.autograd

import

grad

def

compute_penalty (losses , dummy_w ):

g1 = grad(losses [0::2]. mean(), dummy_w , create_graph =True )[0]

g2 = grad(losses [1::2]. mean(), dummy_w , create_graph =True )[0]

return (g1 * g2).sum ()

def

example_1(n=10000 , d=2, env =1):

x = torch.randn(n, d) * env

y = x + torch.randn(n, d) * env

z = y + torch.randn(n, d)

return

torch.cat((x, z), 1), y.sum(1, keepdim=True)

phi = torch.nn.Parameter(torch.ones(4, 1))

dummy_w = torch.nn.Parameter(torch.Tensor ([1.0]))

opt = torch.optim.SGD([ phi], lr=1e -3)

mse = torch.nn.MSELoss(reduction="none")

environments = [example_1(env =0.1) ,

example_1(env =1.0)]

for

iteration

in range (50000):

error = 0

penalty = 0

for x_e , y_e in

environments :

p = torch.randperm(len(x_e ))

error_e = mse(x_e[p] @ phi * dummy_w , y_e[p])

penalty

+=

compute_penalty (error_e , dummy_w)

error +=

error_e.mean ()

opt.zero_grad ()

(1e-5 * error + penalty ). backward ()

opt.step ()

if

iteration % 1000 == 0:

print(phi)

31


## Performance Targets

This component maintains the following performance requirements:

- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

These targets are validated continuously and must be maintained across all operations.

---

**Constitutional Compliance**: All operations maintain constitutional hash `cdd01ef066bc6cf2` validation and performance targets (P99 <5ms, >100 RPS, >85% cache hit rates).

**Last Updated**: 2025-07-17 - Constitutional compliance enhancement
