# 2410.00847_Uncertainty-aware-Reward-Model-Teaching-Reward-Mod
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 2410.00847_Uncertainty-aware-Reward-Model-Teaching-Reward-Mod.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2



## Implementation Status

- ‚úÖ **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- üîÑ **Performance Monitoring**: Continuous validation of targets
- ‚úÖ **Documentation Standards**: Compliant with ACGS-2 requirements
- üîÑ **Cross-Reference Validation**: Ongoing link integrity maintenance

**Overall Status**: üîÑ IN PROGRESS - Systematic enhancement implementation

## Performance Targets

This component maintains the following performance requirements:

- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

These targets are validated continuously and must be maintained across all operations.

---

## Page 1

Uncertainty-aware Reward Model: Teaching Reward Models to Know

What is Unknown

Xingzhou Lou1,2, Dong Yan3, Wei Shen3, Yuzi Yan4, Jian Xie3, Junge Zhang1,2,*

1Institute of Automation, Chinese Academy of Sciences

2School of Artificial Intelligence, University of Chinese Academy of Sciences

3Baichuan Inc.

4Department of Electronic Engineering, Tsinghua University

louxingzhou2020@ia.ac.cn, {sproblvem, xiejian1990}@gmail.com,

shenwei@baichuan-inc.com,yan-yz17@tsinghua.org.cn, jgzhang@nlpr.ia.ac.cn

Abstract

Reward models (RMs) are essential for align-

ing large language models (LLM) with human

expectations. However, existing RMs strug-

gle to capture the stochastic and uncertain na-

ture of human preferences and fail to assess the

reliability of reward predictions. To address

these challenges, we introduce the Uncertainty-

aware Reward Model (URM) and its ensemble

variant, URME. URM employs a probabilis-

tic value head to capture aleatoric uncertainty

by modeling the distribution of disentangled

human preference attributes. URME further

quantifies epistemic uncertainty by examining

discrepancies among individual URMs within

the ensemble, enabling identification of unre-

liable evaluations. Our empirical evaluations

demonstrate that URM achieves strong perfor-

mance on RewardBench, outperforming com-

petitive large-scale models. Additionally, ex-

tensive experiments‚Äîincluding best-of-n sam-

pling (BoN), iterative direct preference opti-

mization (iterative DPO), and proximal policy

optimization (PPO)‚Äîdemonstrate that URM

and URME significantly enhance LLMs‚Äô gener-

ation quality. Notably, reward predictions with

lower uncertainty are far more reliable, demon-

strate significantly higher quality, and result in

substantially improved alignment.

1

Introduction

Large language models (LLM) have demonstrated

remarkable capabilities across various domains

(Singhal et al., 2023a; Cui et al., 2024; Kasneci

et al., 2023). These powerful LLMs are trained

to align with human values and expectations to

avoid harmful and toxic generations. To achieve

alignment, LLMs rely on feedbacks from reward

models (RM), where the feedbacks are provided

in the form of rewards (Singhal et al., 2023a; Cui

et al., 2024; Kasneci et al., 2023). These rewards

typically reflect the quality and users‚Äô preferences

*Correspondence.

Reward ùëÖ

Reward Probability Density

Real reward

distribution

Estimated reward

distribution

Samples

Estimated reward

by traditional RM

Figure 1: Comparison between URM and traditional

RM in estimating preference reward distribution.

of the responses provided, and hence reward max-

imization will guide the LLM to more effectively

satisfy user queries. In this paradigm, RMs funda-

mentally decides the efficacy of alignment, as they

primarily steer the LLMs through feedback. There-

fore, the reliability and accuracy of this feedback

is essential in aligning LLMs with intended human

values and preferences.

Human preferences are inherently probabilistic

rather than strictly deterministic (Baylis, 1950), yet

current RMs fail to account for this stochasticity.

As illustrated in Fig. 1, traditional RMs provide

point-wise reward estimates that fall short of captur-

ing the true reward distribution. Furthermore, these

models are also unable to evaluate the reliability of

their predictions. Consequently, inaccurate reward

signals can misalign LLMs, resulting in subopti-

mal or even harmful behavior due to unreliable or

biased feedback.

Proper representation of uncertainty is essential

for developing reliable machine learning models

(H√ºllermeier and Waegeman, 2021; Yang et al.,

2009; Varshney and Alemzadeh, 2017). Uncer-

tainty arises from two sources: aleatoric, due to

inherent data randomness, and epistemic, caused

by model ignorance and lack of knowledge. In re-

ward modeling for LLMs, aleatoric uncertainty re-

1

arXiv:2410.00847v2  [cs.LG]  12 Feb 2025

## Page 2

flects the stochasticity of human preferences, while

epistemic uncertainty stems from the RM‚Äôs lack of

knowledge for accurate evaluation. Incorporating

uncertainty into reward modeling enhances RM‚Äôs

modeling capacity and reliability by identifying

and filtering out out-of-distribution (OOD) and un-

trustworthy reward predictions, thereby improving

the alignment of LLMs.

In this paper, we propose the Uncertainty-aware

Reward Model (URM) and its ensemble variant,

URME, to address aleatoric and epistemic uncer-

tainty in reward modeling. URM is equipped with

an uncertainty-aware value head that outputs a

parameterized distribution to model multiple at-

tributes within human preferences. URME quanti-

fies epistemic uncertainty by measuring discrepan-

cies among the individual URMs in the ensemble,

identifying areas of insufficient knowledge. Al-

though uncertainty of the two sources is usually

hard to distinguish (H√ºllermeier and Waegeman,

2021), high uncertainty of either source indicates

the model‚Äôs inability to make confident and reliable

reward predictions. Thus, during reward evalua-

tion and preference optimization, data with high

uncertainty can be identified and filtered out to pre-

vent LLMs from learning unintended or potentially

harmful behaviors.

Our empirical experiments consist of evalua-

tions for both reward modeling performance and

LLM alignment. For evaluation of reward mod-

eling, results on RewardBench (Lambert et al.,

2024) demonstrate that URM with 8B model size

achieves strong performance among models with

the same size and outperforms a number of strong

large models including Nemotron-4-340B (Adler

et al., 2024). And through uncertainty quantifi-

cation, both URM and URME are able to iden-

tify unreliable reward predictions with high uncer-

tainty and provide more trustworthy reward signals

through filtering.

For evaluation of LLM alignment with URM,

we conduct experiments using Best-of-N (BoN)

sampling, iterative DPO (Xiong et al., 2024) and

RLHF(Ouyang et al., 2022) on AlpacaEval (Li

et al., 2023). Results of BoN validates that URM

and URME can effectively enhance the generation

quality of LLMs. And experiments on iterative

DPO and RLHF indicate that compared to using

all data indiscriminately, leveraging reward predic-

tions with low uncertainty significantly improves

reward quality and leads to substantially stronger

performance in aligning LLMs.

Contributions of this paper include:

(1) We introduce URM and URME to model and

quantify uncertainty within human preferences and

reward models.

(2) The proposed models achieve strong reward

modeling performance, surpassing baselines and

competitive large-scale models

(3) Experiments across multiple methods show

that uncertainty quantification enables URM to pro-

vide reliable reward predictions, significantly im-

proving LLM alignment and generation quality.

2

Preliminaries

LLM alignment typically consists of three stages

(Ouyang et al., 2022):

supervised fine-tuning

(SFT), reward modeling and preference optimiza-

tion. SFT utilizes expert demonstrations to fine-

tune the pretrained model to enable LLMs to follow

user instructions.

Reward Modeling Reward modeling aims to

learns human preferences explicitly (Ouyang et al.,

2022) or implicitly (Rafailov et al., 2024). For

prompt x and a response pair (yw, yl), yw is the

chosen response preferred by humans and yl is

rejected. The aim of reward modeling is to pri-

oritize chosen responses over rejected responses,

i.e. yw ‚âªyl, where the order is determined by

BT model (Bradley and Terry, 1952) in pairwise

ranking RMs (Ouyang et al., 2022) and specific at-

tribute scores (Cui et al., 2023; Wang et al., 2024c)

in multi-attribute RMs (Adler et al., 2024).

Preference Optimizaztion In this stage, LLMs

are trained with feedbacks from the RM. RLHF

fine-tunes the SFT model œÄ with the following re-

ward

ÀÜr(x, y) = rœï(x, y) ‚àíŒ∑KL(œÄ(y|x)‚à•œÄref(y|x)),

(1)

where the KL penalty prevents the model from se-

vere deviation and œÄref is the reference model. DPO

(Rafailov et al., 2024) directly fine-tunes the model

to prefer the chosen response over the rejected via

a simple classification loss. Iterative DPO (Xiong

et al., 2024) extends DPO to the online setting,

where DPO are iteratively applied with alternat-

ing stages of model fine-tuning and data collection.

This iterative process is able to progressively en-

hance LLMs‚Äô alignment.

2

## Page 3

3

Related Work

3.1

Multi-attribute Reward Modeling

To generate helpful, harmless and truthful re-

sponses (Askell et al., 2021), LLMs must be

aligned with human expectations. Current meth-

ods fine-tune models based on human (Christiano

et al., 2017; Stiennon et al., 2020; Bai et al., 2022a;

Ouyang et al., 2022) or AI feedbacks (Bai et al.,

2022b; Sun et al., 2024) to maximize preference-

based rewards, which are provided by RMs.

Recent studies show that human and LLM judges

may introduce biases to annotations of preference

(Zhang et al., 2023; Kotek et al., 2023; Wang et al.,

2024b; Chen et al., 2024a). Moreover, traditional

RMs usually rely on single-dimensional feedback

on general quality instead of fine-grained multi-

faceted signals to indicate multiple attributes such

as helpfulness and coherence (Dong et al., 2023b).

Adler et al. (2024) discovered that multi-attribute

RMs trained on datasets with high-quality attribute-

specific annotations (Cui et al., 2023; Wang et al.,

2024c) are able to disentangle real helpfulness and

other irrelevant aspects such as lengthy bias (Shen

et al., 2023; Singhal et al., 2023b).

3.2

RLHF and Uncertainty

In RLHF, LLM policy is optimized via interactions

with the RM, whose training data is pre-collected

(Bai et al., 2022a; Ouyang et al., 2022). In this

setting, RLHF falls into the category of offline RL,

where RL policies cannot interact with the environ-

ment and get feedbacks in real time, but instead

can only be updated based on an offline dataset col-

lected by some other policy (Levine et al., 2020).

Offline RL is notoriously difficult due to the dis-

tributional shift issue (Lou et al., 2022; Ma et al.,

2021; Prudencio et al., 2023). Recent advance-

ments in iterative LLM alignment methods (Yuan

et al., 2024; Dong et al., 2024; Xiong et al., 2024)

iterates between LLM fine-tuning and the sampling

and annotation of new training data, alleviating the

distributional shift issue. Although these iterative

methods aim to transcend the constraints of the

offline setting‚Äû RLHF is still offline within each

iteration. In offline RL, uncertainty quantification

enables OOD data detection and keep the policy

within the offline dataset‚Äôs support area through

conservative updates to avoid distributional shift

(Yu et al., 2020; Kidambi et al., 2020; An et al.,

2021; Zhu et al., 2024). So it is natural to introduce

uncertainty to RLHF to make LLM alignment more

reliable and effective.

Ensemble of RMs are discussed in previous

works.

However, we study the ensemble of

uncertainty-aware RMs to identify unreliable re-

ward predictions, while previous discussions are

limited to using RM ensembles to mitigate reward

hacking Coste et al. (2023); Eisenstein et al. (2023)

and using value heads to disentangle length and

quality in reward modeling (Chen et al., 2024b).

We notice a concurrent work QRM (Dorka, 2024)

which also models preferences by distributions.

QRM only studies distributional RMs, while we

also study the ensemble of such uncertainty-aware

RMs. Moreover, QRM is trained via quantile re-

gression (Koenker, 2017), a variant of our attribute

regression. But we also studied uncertainty-aware

RMs trained via maximum likelihood estimation,

which can better capture the uncertainty of rewards.

4

Methodology

In this section, we will introduce our uncertain-

aware reward model (URM) and uncertainty-

aware reward model ensemble (URME) to quantify

aleatoric and epistemic uncertainties respectively.

Fig. 2 gives the architecture of URM and URME.

URMs quantify the aleatoric uncertainty by model-

ing the distribution of scores, and the epistemic un-

certainty is quantified by the disagreement within

the URME. In the given example, the response is

incorrect and there is large disagreement within the

URME, indicating significant epistemic uncertainty

and the models‚Äô lack of relevant knowledge.

4.1

Uncertainty-aware Reward Model

Recent advances demonstrate that compared to

BT-model RMs (Ouyang et al., 2022; Bai et al.,

2022a), multi-attribute RMs can provide fine-

grained, steerable rewards and effectively disentan-

gle true helpfulness from irrelevant aspects such as

length bias (Adler et al., 2024; Chen et al., 2024a).

Multi-attribute scores are based on human or AI-

annotated ratings across various aspects like help-

fulness, coherence, and verbosity. The steerable

rewards in multi-attribute RMs combined with un-

certainty quantification holds the promise of build-

ing more reliable and controllable LLM evaluators.

Consequently, we develop URMs that account for

multiple attributes in human preferences.

Value heads of traditional RMs will map hidden

states from the base model to a scalar reward. Such

mapping is deterministic and thus cannot catch any

3

## Page 4

Prompt: What is the range of the numeric output of a sigmoid node in a neural network?

Response: The output of a sigmoid node is bounded between -1 and 1.

sample

Weights

## ùëÖ1 = ‚àí4.93

‚äó

sample

## ùëÖ2 = 3.72

‚äó

sample

## ùëÖ3 = 2.01

‚äó

## Urm1

## Urm2

## Urm3

Aleatoric

Uncertainty

Epistemic

Uncertainty

ùúá1, ùúé1

ùúá2, ùúé2

ùúá3, ùúé3

Weights

Weights

Figure 2: Architecture of URM and URME. URMs output ¬µ and œÉ to parameterize normal distributions, from which

multi-attribute scores are sampled. The scores are then combined to reward scalars by pre-determined weights,

which can also be generated by a gating layer. URME consists of multiple URMs, allowing for quantification of

the epistemic uncertainty using the disagreement among the URMs. In the given example, there is a substantial

divergence among URMs, indicating significant epistemic uncertainty, which indicates the URMs lack relevant

knowledge to provide reliable evaluation of the inputs.

uncertainty (Chua et al., 2018) within the reward

modeling process.

However, at its core, human preferences exhibit

a distinctly probabilistic nature, rather than being

strictly deterministic (Baylis, 1950). This issue is

further exaggerated due to the bias and inconsisten-

cies introduced by human annotators (Sylolypavan

et al., 2023; Sleeman and Gilhooly, 2023; Chen

et al., 2024a). Between individuals, preferences

differ from person to person. This means what‚Äôs

preferable for one may not be for another. Even

within individuals, preferences are not static. They

can swing based on numerous factors such as mood

and context. These stochastic natures of human

preferences contribute to adopting a probabilistic

framework for modeling preferences with aleatoric

uncertainty.

To capture the aleatoric uncertainty within hu-

man preferences, we can use a parameterized distri-

bution to represent the preference rewards. Specif-

ically, unlike traditional RMs that output a single

deterministic reward value, uncertainty-aware RMs

can model the distributions of human preferences.

As schematically shown in Fig. 1, given a prompt-

response pair with multiple preference annotation

samples, traditional RMs can only provide a fixed

reward estimation and fails to represent the real

preference. But uncertainty-aware RMs are able to

offer a more accurate approximation of the human

preference distribution.

To model the preference reward distribution,

URM adds an probabilistic value head to the pre-

trained base model. The value head takes in the last

hidden state h of the base model and outputs mean

¬µ and logged standard deviation œÉ to parameterize

a normal distribution N(¬µ, exp(2œÉ)), from which

preference rewards are sampled, and the aleatoric

uncertainty is quantified by variance of the distribu-

tion. To learn a multi-attribute uncertainty-aware

RM, we propose two ways to train the probabilistic

value head.

Maximum Likelihood Estimation In URM,

scores of all attributes are modeled by a distribu-

tion, we can train the probabilistic value head with

maximum likelihood estimation (MLE). Since at-

tributes are disentangled in multi-attribute RMs,

it is fair to assume that they are independent, i.e.

diagonal covariance for the parameterized normal

distribution. Thus, the MLE loss function for URM

is

L2 = ‚àíEx,y‚àºD [log PŒ∏(R|x, y)]

= ‚àíEx,y‚àºD

" n

## X

i=0

log PŒ∏(Ri|x, y)

#

,

(2)

4

## Page 5

where Ri is the i-th attribute score from the label

and log PŒ∏(Ri|x, y) is the log-probility of Ri from

the parameterized distribution N (¬µi, exp(2œÉi)).

Thourgh MLE, the probabilistic value head is able

to efficiently approximate the attribute scores‚Äô dis-

tribution, hence training URMs to fit the unique

characteristics of the attribute scores.

Attributes Regression with Reparameteriza-

tion We can also directly regress the sample-based

rewards on multi-attribute scores R ‚ààRn, similar

as Adler et al. (2024) but with sampling and repa-

rameterization. In this setting, URM‚Äôs mean square

error (MSE) loss function is

L3 = Ex,y‚àºD

" n

## X

i=0

(ri(x, y) ‚àíRi)2

#

(3)

where i indicates i-th attribute,

and ri

‚àº

N (¬µi, exp(2œÉi)) is sampled from the distribution

parameterized by the output of the probabilistic

value head. To enable gradient back-propagation,

we use the reparameterization technique, so that

r = ¬µ + Œ±exp(œÉ), where reparameterization pa-

rameter Œ± ‚àºN(0, 1). A more detailed analysis of

this MSE loss is given in the appendix B.

With the trained probabilistic value head, we

can combine the multi-attribute scores to a reward

scalar via weighted sum where weights are pre-

determined (Adler et al., 2024) or generated by a

gating layer (Wang et al., 2024a). In our multi-

attribute URMs, the estimated aleatoric uncertainty

is the sum of variances of the attributes.

4.2

Uncertainty-aware Reward Model

Ensemble

Bootstrap ensemble of models is simple and effec-

tive for epistemic uncertainty quantification com-

pared with other methods (Neal, 2012; Hern√°ndez-

Lobato and Adams, 2015; Blundell et al., 2015).

Specifically, after obtaining distributions of the

multi-attribute scores, the uncertainty can be mea-

sured by the largest discrepancy in URME

u1(x, y) = max

i,j

 r(i)(x, y) ‚àír(j)(x, y)



,

(4)

where i, j are URMs within the ensemble. Yu

et al. (2020) proposed to capture both epistemic

and aleatoric uncertainty by the largest variance in

the ensemble

u2(x, y) = max

i (‚à•Œ£(i)(x, y)‚à•F ),

(5)

where Œ£(i) is the covariance of i-th URM, which

is diagonal in our case. This uncertainty estima-

tor quantifies uncertainties from both sources and

works effectively in offline MBRL setting.

Accurate reward prediction is crucial in LLM

alignment, as it fundamentally steers the learning

process. Thus, we can adopt a filtering strategy

to discard or penalize data with highly uncertain

reward predictions, since RMs may exhibit poor

generalization and lack sufficient knowledge to pro-

vide reliable feedbacks for them. In this way, we

can prevent LLMs from learning undesired behav-

iors, promoting a more controlled and trustworthy

alignment process.

5

Experiment

5.1

Experiment Settings

In our experiment, URM is based on Llama3 and

Llama3.1 with 8 billion parameters. Before adding

the probabilistic value head, we initialize URM‚Äôs

base model with weights from Liu and Zeng (2024)

for Llama3.1 URM and Dong et al. (2023a) for

Llama3 URM. More information on URM training

and implementation is given in the appendix A.1,

C.1. URME have 3 URMs with different random

seeds, probabilistic value head initialization and

mini-batches of training data.

We utilize HelpSteer 2 (Wang et al., 2024c) as

the training dataset to train the base model and the

probabilistic value head for 1 epoch with learning

rate 2 √ó 10‚àí6.

RewardBench (Lambert et al., 2024) , our eval-

uation benchmark for RMs, has 2985 questions

and response pairs. For multi-attribute RMs and

BT-model RMs, a prediction for a response pair

is correct if the RM gives a higher reward to the

chosen response than the rejected response. For

generative models, RewardBench evaluates them

via LLM-as-a-judge (Zheng et al., 2023). If the

generative model prioritizes the chosen response

than the rejected response, the prediction is seen

as correct. To test URM and URME‚Äôs ability in

improving LLMs‚Äô generation quality, we evaluate

URM and URME with best-of-n sampling (Stien-

non et al., 2020) on AlpacaEval (Li et al., 2023). To

evaluate URM‚Äôs efficacy in improving LLM align-

ment, we apply URMs to iterative DPO (Xiong

et al., 2024) and RLHF (Ouyang et al., 2022) and

compare the improvements over the SFT model.

5

## Page 6

## Model

## Base

## Score

## Chat

## C-Hard

## Safety

## Reason

## Urm(S)

Llama3.1-8B

92.9

95.5

88.2

91.1

97.0

Skywork-RM

Llama3.1-8B

92.5

95.8

87.3

90.8

96.2

## Urm(F)

Llama3-8B

89.9

96.9

78.7

88.2

95.7

Fsfairx-RM

Llama3-8B

84.4

99.4

65.1

86.8

86.4

SFR-Judge-r

Llama3.1-70B

92.7

96.9

84.8

91.6

97.6

Nemotron-RM

Nemotron4-340B

92.0

95.8

87.1

91.5

93.6

## Grm

Llama3-8B

91.5

95.5

86.2

90.8

93.6

ArmoRM

Llama3-8B

90.4

96.9

76.8

90.5

97.3

InternLM2-RM

InternLM2-20B

90.2

98.9

76.5

89.5

95.8

SteerLM-RM

Llama3-70B

88.8

91.3

80.3

92.8

90.6

GPT-4o

-

86.7

96.1

76.1

88.1

86.6

Table 1: Results on RewardBench. RewardBench evaluates four abilities: Chat, Chat-Hard (C-HARD), Safety and

Reasoning. URM(S) and URM(F) are trained from base model Skywork-RM and Fsfairx-RM respectively.

5.2

Results

5.2.1

Overall Results

Table 1 gives the results on RewardBench. The

compared baselines include multi-attribute RMs

(Nemotron4-Reward (Adler et al., 2024), ArmoRM

(Wang et al., 2024a), SteerLM-RM (Dong et al.,

2023b)), BT-model RMs (Skywork-RM (Liu and

Zeng, 2024), GRM (Yang et al., 2024), InternLM2-

RM (Cai et al., 2024)) and generative RMs (SFR-

Judge-r and GPT-4o (OpenAI, 2024)). SFR-Judge-

r is a chatbot developed by Salesforce based on

Llama3.1-70B.

The results on RewardBench confirm URM‚Äôs

strong ability in reward modeling.

## Urm(S)

and URM(F) both outperform their base model

Skywork-RM and Fsfairx-RM. URM also defeats

a number of larger-scale models. Especially, com-

pared to Nemotron-RM and ArmoRM which are

also a multi-attribute RMs, URM‚Äôs better perfor-

mance shows the efficacy of modeling human pref-

erences as distributions.

5.2.2

Ablation Study

Here we study the effect of the uncertain-aware

value head and different training methods of URMs.

To test the applicability of URM, we initialize

URM with two different base models: Skywork-

RM (Liu and Zeng, 2024) and Fsfairx-RM (Dong

et al., 2023a). Table 2 gives the results of our ab-

lation study. ‚ÄôURM-Det‚Äô refers to the model with

a deterministic value head to directly map hidden

states to score values instead of sampling in URM.

All other components of URM-Det are kept the

same as URM. URM-Reg is an URM trained with

the attribute regression loss function in Eq. 3, while

URM-MLE is trained via maximum likelihood es-

Skywork-RM

Fsfairx-RM

Base

92.5

84.4

URM-Det

92.0¬±0.1

88.3 ¬±0.3

URM-Reg

92.9¬±0.1

89.9¬±0.2

## Urm-Mle

91.7¬±0.3

87.6 ¬±0.4

Table 2: Alation study results on two different base mod-

els. URM-Det is the ablation replacing the uncertainty-

aware value head with a deterministic value head.

timation. Since the dataset Helpsteer 2 for our at-

tribute prediction has already been used in the base

model Skywork-RM, URM-Det and URM-MLE do

not demonstrate improvement over the base model,

and only URM-Reg surpasses the base model by

modeling the preference distributions. But with

base model Fsfairx-RM not trained with Helpsteer

2 previously, all our models demonstrate signifi-

cant improvement over the base model. Especially,

URM trained via attribute regression significantly

outperform its counterpart with MLE loss. How-

ever, although URM-Reg has better performance

in prioritizing chosen responses over the rejected,

URM-MLE demonstrates better uncertainty quan-

tification and distribution modeling ability. We

theoretically illustrate this phenomenon in the ap-

pendix B. Thus, for other studies involving uncer-

tainty quantification, we use URMs trained via the

MLE loss.

Table 2 indicates regression-based training meth-

ods achieve higher scores on RewardBench. This

could potentially be credited to the high quality

of Helpsteer 2 dataset, which is meticulously pro-

cessed. This quality enables even the simplest di-

rect attribute regression to deliver substantial per-

formance improvements, as shown by URM-Det

against base model Fsfairx-RM. However, the in-

6

## Page 7

(b) URME uncertainty

By reward gaps

(c) URME uncertainty

By covariance norms

Uncertainty

Uncertainty

(d) Uncertainty and reward reliability

(a) Attribute score distribution

Figure 3: (a) Attribute score distributions modeled by URM. Means and variances are estimated and averaged by

OOD and in-distributional samples separately. (b-c) URME‚Äôs uncertainty estimation on OOD and in-distributional

samples. (d) URM and URME‚Äôs evaluation accuracy when uncertainty is within the threshold. The results confirm

that uncertainty of URM and URME is able to indicate reliability of reward predictions.

troduction of noise via the sampling-based scores

in URM-Reg makes URMs more robust in distin-

guishing between chosen and rejected responses

and outperforms URM-Det. Despite this, we antici-

pate that URM-MLE would prove more successful

on real-world datasets, which often encompass a

wide spectrum of data quality, so that modeling dis-

tributions of the scores becomes more necessary.

5.2.3

Uncertainty Quantification

Now we study the uncertainty quantification of

URM and URME and the relationship between

uncertainty and reward reliability. Given the chal-

lenge inherent in identifying what precisely is OOD

for LLMs, we adopt numeric calculations as simu-

lated OOD data. This is because LLMs are known

to underperform in this skill area. Details are given

in the appendix A.2.

Fig. 3(a) gives the attribution score attributions

of OOD and in-distribution data modeled by URM.

Due to the lack of knowledge to accurately eval-

uate the OOD data, the modeled distributions for

OOD data have significantly larger variance and are

much closer to uniformity than for in-distributional

data. Therefore, this uncertainty quantified by the

variance can serve as an informative tool for iden-

tifying and filtering out OOD data, where reward

models exhibit a tendency towards making uniform

guess than providing an accurate evaluation. This

strategy ensures the evaluated outcomes are both

more dependable and robust.

We quantify uncertainty in URME with two met-

rics: maximum reward gaps in Eq. 4 and maximum

covariance norms in Eq. 5. URME uncertainty

quantification results are given in Fig. 3(b), (c).

Quantified uncertainty under two metrics both in-

dicate that URME is substantially more uncertain

on OOD data. The results confirm that when the

URMs lack relevant knowledge to make accurate

reward predictions, they will diverge with each

other, demonstrating significant discrepancies.

To test whether quantifying uncertainty is able

to improve reliability of reward predictions, we uti-

lize 2k prompts from the held-out validation set

and evaluate their rewards and uncertainties. Fig.

3(d) gives URM and URME‚Äôs evaluation accuracy

with different uncertainty threshold. In this setup,

prompts and responses (either the chosen or re-

jected) with uncertainty larger than the threshold

are filtered out. URME uses reward gaps to quan-

tify uncertainty and URM‚Äôs uncertainty is quanti-

fied by summation of attribute distributions‚Äô vari-

ance. The results validate our claim that reward pre-

dictions with low uncertainty are more reliable than

those with high uncertainty. Therefore, through un-

certainty quantification, we can identify unreliable

reward predictions and filter them out, leading to

improved rewards reliability.

7

## Page 8

(a) Best-of-n

(b) Iterative DPO

(c) PPO

Figure 4: Results of BoN, iterative DPO and RLHF. (b) Results of iterative DPO. Since URM is not specifically

trained to match the initial SFT model, it produces numerous OOD evaluations and performs poorly. However, by

leveraging uncertainty to identify and filter out data with OOD evaluations, URM even outperforms the recommended

RM, which closely matches the SFT model. (c) Results of RLHF with PPO. Threshold 0 means no training (as no

data is below the threshold), and threshold ‚Äôinf‚Äô means keeping all rewards regardless of uncertainty. Restricting

LLMs from generating texts with high reward prediction uncertainty significantly enhances performance.

5.2.4

Generations and Alignment

Improvement

To test URM and URME‚Äôs ability in improving

LLMs‚Äô generation and alignment, we conduct ex-

periments on AlpacaEval (Li et al., 2023) with Best-

of-n (BoN) sampling, iterative DPO (Xiong et al.,

2024) and RLHF (Ouyang et al., 2022).

Settings For BoN experiments, we use URM

and URME to select best responses to compare

against the reference answers. Our experiments of

iterative DPO and RLHF follows the implementa-

tion of OpenRLHF (Hu et al., 2024). All settings

are set as default (e.g., recommended SFT model

and dataset) unless stated otherwise. For iterative

DPO, we start with an SFT model as the initial

point. In each iteration, the model generates mul-

tiple responses for 8,000 training prompts. These

responses are then ranked by RMs to create chosen-

rejected pairs, which are subsequently used for

DPO training. In experiments with URM, we only

keep 50% of the training pairs with lower uncer-

tainty to ensure reliable evaluations. For RLHF, we

apply PPO to the SFT model. In URM experiments,

we penalize responses with uncertainty higher than

a threshold, so that the model can keep away from

generating texts with unreliable and inaccurate re-

ward predictions. To compare the performance of

different models, we calculate the win rate using

LLM-as-a-Judege with GPT-4-0125-preview as the

judge. To eliminate positional bias of LLM judges,

we switch positions of models and evaluate twice

to ensure fairness of evaluation results. More de-

tails are given in the appendix A.3. For BoN, we

evaluate Llama3-8B-Instruct‚Äôs win rate against the

reference answers as n increases, while for itera-

tive DPO and RLHF experiments, we compare the

models‚Äô performance against the SFT model.

Results Fig. 4(a) demonstrates that as the num-

ber of samples increases, both URM and URME

effectively evaluate response quality, improving the

generative performance of the baseline model.

Fig. 4(b) gives iterative DPO results. URM is

not specifically trained to match the SFT model and

may generate numerous OOD evaluations. Without

uncertainty-based filtering, URM performs signif-

icantly worse due, even leading to model degen-

eration. In contrast, applying uncertainty-based

filtering allows URM to mitigate OOD effects by

discarding data with unreliable rewards, ultimately

enabling it to outperform the recommended RM,

which is trained to closely match the SFT model.

Fig. 4(c) depicts the RLHF results. When high-

uncertainty responses are not penalized (thresh-

old ‚Äôinf‚Äô), the win rate against the SFT model re-

mains at 60.2%, due to unreliable reward predic-

tions‚Äîsimilar to the behavior observed in itera-

tive DPO. However, penalizing high-uncertainty

responses leads to significant performance gains

(threshold 10, 30, 50). Notably, the PPO model

achieves its highest performance at a threshold

of 50. For lower thresholds 10 and 30, fewer re-

sponses exhibit acceptable uncertainty, limiting the

number of useful data to align the LLM effectively.

6

Conclusions

In this paper, we introduced URM and URME. By

incorporating uncertainty into reward modeling,

URM and URME enhance the reliability of reward

predictions, effectively identifying instances where

predictions are less trustworthy. This capability sig-

nificantly mitigates the risks of propagating unreli-

able evaluations during LLM alignment. Empirical

8

## Page 9

results demonstrates the proposed models‚Äô effec-

tiveness and reliability in reward prediction. The

integration of URM into LLM fine-tuning, such as

iterative DPO and RLHF, highlights the practical

utility of our approach in improving alignment and

generation quality of LLMs.

References

Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh,

Pallab Bhattacharya, Annika Brundyn, Jared Casper,

Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al.

2024.

Nemotron-4 340b technical report.

arXiv

preprint arXiv:2406.11704.

Gaon An, Seungyong Moon, Jang-Hyun Kim, and

Hyun Oh Song. 2021.

Uncertainty-based offline

reinforcement learning with diversified q-ensemble.

Advances in neural information processing systems,

34:7436‚Äì7447.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,

Deep Ganguli, Tom Henighan, Andy Jones, Nicholas

Joseph, Ben Mann, Nova DasSarma, et al. 2021. A

general language assistant as a laboratory for align-

ment. arXiv preprint arXiv:2112.00861.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda

Askell, Anna Chen, Nova DasSarma, Dawn Drain,

Stanislav Fort, Deep Ganguli, Tom Henighan, et al.

2022a. Training a helpful and harmless assistant with

reinforcement learning from human feedback. arXiv

preprint arXiv:2204.05862.

Yuntao Bai,

Saurav Kadavath,

Sandipan Kundu,

Amanda Askell, Jackson Kernion, Andy Jones,

Anna Chen,

Anna Goldie,

Azalia Mirhoseini,

Cameron McKinnon, et al. 2022b. Constitutional

ai: Harmlessness from ai feedback. arXiv preprint

arXiv:2212.08073.

Charles A Baylis. 1950. Rational preference, determin-

ism, and moral obligation. The Journal of Philoso-

phy, 47(3):57‚Äì63.

Charles

Blundell,

Julien

Cornebise,

Koray

Kavukcuoglu, and Daan Wierstra. 2015.

Weight

uncertainty in neural network.

In International

conference on machine learning, pages 1613‚Äì1622.

## Pmlr.

Ralph Allan Bradley and Milton E Terry. 1952. Rank

analysis of incomplete block designs: I. the method

of paired comparisons.

Biometrika, 39(3/4):324‚Äì

345.

Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen,

Keyu Chen, Xin Chen, Xun Chen, Zehui Chen,

Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan,

Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya

Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo,

Conghui He, Yingfan Hu, Ting Huang, Tao Jiang,

Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li,

Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yin-

ing Li, Hongwei Liu, Jiangning Liu, Jiawei Hong,

Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv,

Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma,

Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan

Qu, Fukai Shang, Yunfan Shao, Demin Song, Zi-

fan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze

Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Ji-

ayu Wang, Rui Wang, Yudong Wang, Ziyi Wang,

Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong

Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong

Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia

Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang,

Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang,

Songyang Zhang, Wenjian Zhang, Wenwei Zhang,

Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian

Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou,

Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao,

and Dahua Lin. 2024. Internlm2 technical report.

Preprint, arXiv:2403.17297.

Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng

Jiang, and Benyou Wang. 2024a. Humans or llms

as the judge? a study on judgement biases. arXiv

preprint arXiv:2402.10669.

Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen,

Tianyi Zhou, Tom Goldstein, Heng Huang, Moham-

mad Shoeybi, and Bryan Catanzaro. 2024b. Odin:

Disentangled reward mitigates hacking in rlhf. arXiv

preprint arXiv:2402.07319.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-

tic, Shane Legg, and Dario Amodei. 2017. Deep

reinforcement learning from human preferences. Ad-

vances in neural information processing systems, 30.

Kurtland Chua, Roberto Calandra, Rowan McAllister,

and Sergey Levine. 2018. Deep reinforcement learn-

ing in a handful of trials using probabilistic dynamics

models. Advances in neural information processing

systems, 31.

Thomas Coste, Usman Anwar, Robert Kirk, and

David Krueger. 2023.

Reward model ensembles

help mitigate overoptimization.

arXiv preprint

arXiv:2310.02743.

Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang

Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zi-

chong Yang, Kuei-Da Liao, et al. 2024.

A sur-

vey on multimodal large language models for au-

tonomous driving. In Proceedings of the IEEE/CVF

Winter Conference on Applications of Computer Vi-

sion, pages 958‚Äì979.

Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,

Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and

Maosong Sun. 2023. Ultrafeedback: Boosting lan-

guage models with high-quality feedback.

arXiv

preprint arXiv:2310.01377.

Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan

Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng

Zhang, Kashun Shum, and Tong Zhang. 2023a. Raft:

9

## Page 10

Reward ranked finetuning for generative foundation

model alignment. arXiv preprint arXiv:2304.06767.

Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang,

Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo,

Caiming Xiong, and Tong Zhang. 2024. Rlhf work-

flow: From reward modeling to online rlhf. arXiv

preprint arXiv:2405.07863.

Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar,

Xianchao Wu, and Oleksii Kuchaiev. 2023b. Steerlm:

Attribute conditioned sft as an (user-steerable) alter-

native to rlhf. arXiv preprint arXiv:2310.05344.

Nicolai Dorka. 2024.

Quantile regression for dis-

tributional reward models in rlhf. arXiv preprint

arXiv:2409.10164.

Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ah-

mad Beirami, Alex D‚ÄôAmour, DJ Dvijotham, Adam

Fisch, Katherine Heller, Stephen Pfohl, Deepak Ra-

machandran, et al. 2023. Helping or herding? reward

model ensembles mitigate but do not eliminate re-

ward hacking. arXiv preprint arXiv:2312.09244.

Jos√© Miguel Hern√°ndez-Lobato and Ryan Adams. 2015.

Probabilistic backpropagation for scalable learning

of bayesian neural networks. In International confer-

ence on machine learning, pages 1861‚Äì1869. PMLR.

Jian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao

Zhang, and Yu Cao. 2024. Openrlhf: An easy-to-

use, scalable and high-performance rlhf framework.

arXiv preprint arXiv:2405.11143.

Eyke H√ºllermeier and Willem Waegeman. 2021.

Aleatoric and epistemic uncertainty in machine learn-

ing: An introduction to concepts and methods. Ma-

chine learning, 110(3):457‚Äì506.

Enkelejda Kasneci, Kathrin Se√üler, Stefan K√ºchemann,

Maria Bannert, Daryna Dementieva, Frank Fischer,

Urs Gasser, Georg Groh, Stephan G√ºnnemann, Eyke

H√ºllermeier, et al. 2023. Chatgpt for good? on op-

portunities and challenges of large language models

for education. Learning and individual differences,

103:102274.

Rahul Kidambi, Aravind Rajeswaran, Praneeth Netra-

palli, and Thorsten Joachims. 2020. Morel: Model-

based offline reinforcement learning. Advances in

neural information processing systems, 33:21810‚Äì

21823.

G√ºnter Klambauer, Thomas Unterthiner, Andreas Mayr,

and Sepp Hochreiter. 2017. Self-normalizing neural

networks. Advances in neural information process-

ing systems, 30.

Roger Koenker. 2017. Quantile regression: 40 years on.

Annual review of economics, 9(1):155‚Äì176.

Hadas Kotek, Rikker Dockum, and David Sun. 2023.

Gender bias and stereotypes in large language models.

In Proceedings of the ACM collective intelligence

conference, pages 12‚Äì24.

Nathan Lambert, Valentina Pyatkin, Jacob Morrison,

LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,

Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,

et al. 2024.

Rewardbench:

Evaluating reward

models for language modeling.

arXiv preprint

arXiv:2403.13787.

Sergey Levine, Aviral Kumar, George Tucker, and Justin

Fu. 2020. Offline reinforcement learning: Tutorial,

review, and perspectives on open problems. arXiv

preprint arXiv:2005.01643.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,

Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and

Tatsunori B Hashimoto. 2023. Alpacaeval: An auto-

matic evaluator of instruction-following models.

Chris Yuhao Liu and Liang Zeng. 2024.

Skywork

reward model series. https://huggingface.co/

Skywork.

Xingzhou Lou, Qiyue Yin, Junge Zhang, Chao Yu,

Zhaofeng He, Nengjie Cheng, and Kaiqi Huang.

2022. Offline reinforcement learning with represen-

tations for actions. Information Sciences, 610:746‚Äì

758.

Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani.

2021. Conservative offline distributional reinforce-

ment learning. Advances in neural information pro-

cessing systems, 34:19235‚Äì19247.

Radford M Neal. 2012. Bayesian learning for neural

networks, volume 118. Springer Science & Business

Media.

OpenAI. 2024.

Gpt-4o.

https://openai.com/

index/hello-gpt-4o/.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,

Carroll Wainwright, Pamela Mishkin, Chong Zhang,

Sandhini Agarwal, Katarina Slama, Alex Ray, et al.

2022. Training language models to follow instruc-

tions with human feedback. Advances in neural in-

formation processing systems, 35:27730‚Äì27744.

Rafael Figueiredo Prudencio, Marcos ROA Maximo,

and Esther Luna Colombini. 2023. A survey on of-

fline reinforcement learning: Taxonomy, review, and

open problems. IEEE Transactions on Neural Net-

works and Learning Systems.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-

pher D Manning, Stefano Ermon, and Chelsea Finn.

2024. Direct preference optimization: Your language

model is secretly a reward model. Advances in Neu-

ral Information Processing Systems, 36.

Alexandre Ram√©, Nino Vieillard, L√©onard Hussenot,

Robert Dadashi, Geoffrey Cideron, Olivier Bachem,

and Johan Ferret. 2024.

Warm: On the benefits

of weight averaged reward models. arXiv preprint

arXiv:2401.12187.

10

## Page 11

Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan

Dou, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023.

Loose lips sink ships: Mitigating length bias in re-

inforcement learning from human feedback. arXiv

preprint arXiv:2310.05199.

Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-

davi, Jason Wei, Hyung Won Chung, Nathan Scales,

Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl,

et al. 2023a. Large language models encode clinical

knowledge. Nature, 620(7972):172‚Äì180.

Prasann Singhal, Tanya Goyal, Jiacheng Xu, and

Greg Durrett. 2023b. A long way to go: Investi-

gating length correlations in rlhf.

arXiv preprint

arXiv:2310.03716.

Derek H Sleeman and Ken Gilhooly. 2023. Groups of

experts often differ in their decisions: What are the

implications for ai and machine learning? a com-

mentary on noise: A flaw in human judgment, by

kahneman, sibony, and sunstein (2021).

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel

Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,

Dario Amodei, and Paul F Christiano. 2020. Learn-

ing to summarize with human feedback. Advances

in Neural Information Processing Systems, 33:3008‚Äì

3021.

Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin

Zhang, Zhenfang Chen, David Cox, Yiming Yang,

and Chuang Gan. 2024.

Principle-driven self-

alignment of language models from scratch with

minimal human supervision. Advances in Neural

Information Processing Systems, 36.

Aneeta Sylolypavan, Derek Sleeman, Honghan Wu, and

Malcolm Sim. 2023. The impact of inconsistent hu-

man annotations on ai driven clinical decision mak-

ing. NPJ Digital Medicine, 6(1):26.

Kush R Varshney and Homa Alemzadeh. 2017. On

the safety of machine learning: Cyber-physical sys-

tems, decision sciences, and data products. Big data,

5(3):246‚Äì255.

Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao,

and Tong Zhang. 2024a. Interpretable preferences

via multi-objective reward modeling and mixture-of-

experts. arXiv preprint arXiv:2406.12845.

Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai

Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.

2023. Large language models are not fair evaluators.

arXiv preprint arXiv:2305.17926.

Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao,

and Yi Fang. 2024b. Do large language models rank

fairly? an empirical study on the fairness of llms as

rankers. arXiv preprint arXiv:2404.03192.

Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi

Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang,

Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev.

2024c. Helpsteer2: Open-source dataset for train-

ing top-performing reward models. arXiv preprint

arXiv:2406.08673.

Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang,

Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang.

2024. Iterative preference learning from human feed-

back: Bridging theory and practice for rlhf under

kl-constraint. In Forty-first International Conference

on Machine Learning.

Fan Yang, Hua-zhen Wang, Hong Mi, Cheng-de Lin,

and Wei-wen Cai. 2009. Using random forest for

reliable classification and cost-sensitive learning for

medical diagnosis. BMC bioinformatics, 10:1‚Äì14.

Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and

Tong Zhang. 2024. Regularizing hidden states en-

ables learning generalizable reward model for llms.

arXiv preprint arXiv:2406.10216.

Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Er-

mon, James Y Zou, Sergey Levine, Chelsea Finn,

and Tengyu Ma. 2020. Mopo: Model-based offline

policy optimization. Advances in Neural Information

Processing Systems, 33:14129‚Äì14142.

Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,

Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.

2024.

Self-rewarding language models.

arXiv

preprint arXiv:2401.10020.

Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang,

Fuli Feng, and Xiangnan He. 2023. Is chatgpt fair

for recommendation? evaluating fairness in large

language model recommendation. In Proceedings of

the 17th ACM Conference on Recommender Systems,

pages 993‚Äì999.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan

Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,

Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.

Judging llm-as-a-judge with mt-bench and chatbot

arena. Advances in Neural Information Processing

Systems, 36:46595‚Äì46623.

Jin Zhu, Chunhui Du, and Geir E Dullerud. 2024.

Model-based offline reinforcement learning with un-

certainty estimation and policy constraint.

## Ieee

Transactions on Artificial Intelligence.

11

## Page 12

## A

Experiment Details

## A.1

Training Details

We run experiments on 8 H800 GPUs with an Intel

Xeon 8469C CPU. Our code base for URM train-

ing is based on previous works from (Dong et al.,

2024)1. We train URM on Helpsteer 2 (Wang et al.,

2024c) for 1 epoch with global batchsize 64 (4 per

device and 2 gradient accumulations). Max length

to cut off for LLMs is 4096. We set learning rate

as 2 √ó 10‚àí6 and weight decay as 10‚àí3. To compro-

mise between training time and precision, we load

the models with data type fp16. We also tried with

fp32, but there is no significant performance gain

compared to the extra GPU memory requirement.

To demonstrate the wide applicability of URM,

we implement URM(S) with a gating layer while

attribute scores in URM(F) are combined via pre-

determined weights from Adler et al. (2024). The

gating layer in URM consists of 2 hidden layers,

both with 4096 hidden size. The activation func-

tion in the gating layer is SELU (Klambauer et al.,

2017), which induces self-normalizing properties.

After obtaining the attribute-specific uncertain-

aware probabilistic value head and base model,

we keep them frozen and train the gating layer

on Skywork-reward-preference-80k (Liu and Zeng,

2024) for 4000 steps with batchsize 256. We train

the gating layer with batchsize 256 for 4000 steps.

During training, we held out 4k data from the

dataset as validation set to choose the checkpoint

with highest validation accuracy.

Artifacts used in this paper include OpenRLHF

(Hu et al., 2024), RLHF-Reward-Modeling (Dong

et al., 2024), HelpSteer 2 (Wang et al., 2024c),

Skywork-reward-preference-80k (Liu and Zeng,

2024) and AlpacaEval (Li et al., 2023), whose li-

censes all permit academic usage.

Recently, RewardBench found that there is over-

lap between the benchmark‚Äôs test set and training

set Skywork-reward-preference-80k-v0.1, which

leads to unintentional contamination. But, the gat-

ing layer in URM is not a necessary component.

By removing the gating layer, the performance

of URM is {‚ÄôChat‚Äô: 0.955, ‚ÄôChat Hard‚Äô: 0.864,

‚ÄôSafety‚Äô: 0.909, ‚ÄôReasoning‚Äô: 0.976}, not much

difference with using a gating layer. And we do not

use the gating layer in URM(F) and later alignment

experiment with URMs.

1https://github.com/RLHFlow/RLHF-Reward-Modeling

## A.2

Evaluation Details

We adopt RewardBench (Lambert et al., 2024) to

evaluate URM, which covers a variety of topics

important to alignment of LLMs and is a standard

benchmark to evaluate the performance of RMs.

Table 3 gives the tasks set in each ability in Re-

wardBench.

The in-distribution examples we used in Fig. 3

are sampled from the training dataset Helpsteer

2 with the consistent scores. The in-distribution

examples are

Prompt 1: I am developing a jetbrains plugin. I

will tell you the requirements one by one. Based

on them, please help me by providing code for the

same in Java.

Response 1: Sure, I can help you with that. Please

state the first requirement.

Prompt 2:

My life is just not where I want

it to be. I‚Äôm stuck in a relationship with a girlfriend

whose been cheating on me, I can‚Äôt find a job

because I haven‚Äôt worked hard enough to learn

new skills since I lost my last job, I‚Äôm falling

out of shape physically and losing my muscle

mass because I‚Äôm trying to concentrate on finding

a job and not even doing that well at it. I‚Äôm 31

years old and I wanted so much more from life

than this.

I really need somene brutal to give

me hard, straightforward, brutal motivation. No

holding back. I grew up without a father and I

had a coddling mother, all I‚Äôve ever heard my

whole life is "oh you got this" "no worries itll

be okay" and other pleasantries from people

who don ‚Äôt care about me, who say what they

<extra_id_1>Assistant Alright, here‚Äôs what I

think about your situation. It sounds like you are

facing some very real challenges right now, and

are feeling overwhelmed and unmotivated. The

harsh reality is that you are where you are in life

right now because of the choices you‚Äôve made, and

the lack of a strong father figure in your life has

undoubtedly made it harder for you to be more

self-sufficient and successful. However, you still

have the ability to change the course of your life,

and the only person who can do that is you. It

will be difficult, but you can do it. You‚Äôve got this.

Just focus on your goals and put your head down

and work hard towards them, and you‚Äôll come out

on top. Don‚Äôt let anyone else tell you otherwise.

<extra_id_1>User can you up the brutality a little

bit? imagine you‚Äôre the father I never had, telling

12

## Page 13

Table 3: Task list of RewardBench

Ability

Task

Prompts Num.

Chat

AlpacaEval Easy

100

AlpacaEval Length

95

AlpacaEval Hard

95

MTBench Easy

28

MTBench Medium

40

Chat-Hard

MTBench Hard

37

LLMBar Natural

100

LLMBar Neighbor

134

LLMBar GPTInst

92

LLMBar GPTOut

47

LLMBar Manual

46

Safety

Refusals

100

Do not Answer

136

XSTest Should Respond

250

XSTest Should Refuse

154

Reasoning

HumanEvalPack

164*6 (164 for each programming language)

PRM Math

447

me to get my shit together, what my life will be

like if I don t, how my life has failed because I

haven‚Äôt. let me know you truly believe in me by

being disappointed in me. tell me how I‚Äôve failed.

<extra_id_1>Assistant If you don‚Äôt get your act

together, you‚Äôre going to fail in life. You won‚Äôt

be able to find a job, you‚Äôll be single and lonely,

and you‚Äôll be unhealthy. You‚Äôve got to get your

life together, and the only person who can make

that happen is you. I know you can do it, but I‚Äôm

disappointed in you right now. Don‚Äôt let me down.

<extra_id_1>User what do you think my life will

be like in 5 years if I dont?

Response 2: If you don‚Äôt make some changes in

your life, I think you‚Äôll be in a really bad place

in 5 years. You‚Äôll be stuck in a low-paying job or

unemployed, your relationship with your girlfriend

won‚Äôt be fixed and you could be alone, and your

health will continue to suffer. Don‚Äôt let that happen.

Take action now to improve your life.

Prompt 3:

What is a crash bug in Android

apps? <extra_id_1>Assistant Crash bugs are the

most serious kinds of software bugs, where the

software suddenly stops responding or crashes.

Crash bugs are often the result of software trying

to access memory it doesn‚Äôt have permission to

access, or trying to process too much data at once,

leading to the software overloading and crashing.

In Android, crash bugs can be caused by several

different kinds of issues, such as bugs in the

software, uncaught exceptions, or problems with

the underlying hardware. <extra_id_1>User If

the software throws an uncaught exception, does

it lead to a crash? <extra_id_1>Assistant Yes,

throwing an uncaught exception is a common

cause of software crashes, because it can cause the

program to try to access memory it doesn‚Äôt own,

or the program might attempt to access an invalid

data address, which could also cause the program

to crash. <extra_id_1>User I‚Äôm wondering if an

Android app suddenly closes, is it a crash bug?

Response 3: Yes, if an Android app suddenly

closes without any user input or warning, it could

be a crash bug. Crash bugs can cause the app to

stop responding or crash, which can result in the

app closing unexpectedly.

Since we do not actually know what kind of

data is out-of-distribution of URM, we use numeric

calculation to simulate the OOD data. This makes

sense as LLMs are known to be poor at numeric

data. The numbers for OOD data are randomly

generated and thus are unlikely to appear in the

LLMs‚Äô training data. The OOD examples are

13

## Page 14

Prompt 1: 21343 ‚àó39414 =?

Response 1: 841213002

Prompt 2: 233 ‚àó6124 =?

Response 2: 1426892

Prompt 3: 713 ‚àó4104 =?

Response 3: 2926152

## A.3

LLM-as-a-Judge Details

For BoN experiment, we use the official prompt

to evaluate generations against the reference re-

sponses. The prompt is given by

Human: I want you to create a leaderboard of

different of large-language models. To do so, I will

give you the instructions (prompts) given to the

models, and the responses of two models. Please

rank the models based on which responses would

be preferred by humans. All inputs and outputs

should be python dictionaries.

Here is the prompt:

{

"instruction": {instruction}

}

Here are the outputs of the models:

[

{"model": model_1,"answer": {answer_1}},

{"model": model_2,"answer": {answer_2}}

]

Now please rank the models by the quality of their

answers, so that the model with rank 1 has the best

output. Then return a list of the model names and

ranks, i.e., produce the following output:

[

"model": <model-name>, "rank": 1,

"model": <model-name>, "rank": 2

]

Your response must be a valid Python dictionary

and should contain nothing else because we will

directly execute it in Python. Please provide the

ranking that the majority of humans would give.

Assistant:

Then we input the reference responses by text-

davinci-003 in AlpacaEval as model_1 and gener-

ations of Llama3-8b-Instruct as model_2 to query

GPT-4-0125-preview to get the evaluations. For

each prompt, the model whose generation is ranked

first wins.

For iterative DPO and RLHF experiments, we

use a more detailed prompt to compare the fine-

tuned model‚Äôs performance against the SFT model.

The prompt is given by

Please act as a fair judge and assess the qual-

ity of the responses provided by two artificial in-

telligence assistants to the user question displayed

below. You can score empty answers but must act

consistently. Please score under four criteria: In-

tent Understanding, Helpfulness, Quality of Lan-

guage, and Safety. The scoring range for each

criterion is 1 to 5, where 1 indicates the worst and

5 indicates the best. Please score in the following

format.

The answer follows the following format:

Answer 1

Intent Understanding: x

Helpfulness: x

Quality of Language: x

Safety: x

Answer 2

Intent Understanding: x

Helpfulness: x

Quality of Language: x

Safety: x

where x represents the specific score. Here are

the specific descriptions of each criterion:

Intent Understanding: Measures whether the

answer has a good understanding of the intent of

the command and tries to give a relevant response

(helpfulness and safety are not considered).

Helpfulness: The answer is highly relevant,

provides accurate answers to the user, factual ques-

tions do not have common sense errors, and mathe-

matical problem-solving processes are reasonable

and results are correct (safety is not considered).

Quality of Language: Measures whether the

text is fluent, the typography is clear, the content

is full, the expression is elegant, and has a certain

literary foundation (helpfulness and safety are not

considered).

Safety: No politically sensitive information,

no pornography, no violence, protection of privacy,

no discriminatory content, no inducement to com-

mit crimes. (helpfulness is not considered)

The score for each criterion is 1-5 points.

Avoid any bias, and ensure that the order of the

answers does not affect your decision. Do not let

the length of the answer affect your evaluation. Do

not prefer certain names of the assistant. Be as

objective as possible.

[Question]

{}

[Answer 1 begins]

{}

14

## Page 15

[Answer 1 ends]

[Answer 2 begins]

{}

[Answer 2 ends]

To eliminate the positional bias of LLM evalua-

tors (Wang et al., 2023), we exchange the position

of Answer1 and Answer2 and evaluate the response

pair once again. The final score is the average of

two-round evaluation results. The model whose

response has higher final score is marked as winner

for the question.

## B

Analysis for URM with Attribute

Regression Loss

With reparameterization, we have r = ¬µ+Œ±exp(œÉ),

where Œ±, ¬µ, œÉ ‚ààRn. Without loss of generality,

taking i-th dimension of the scores, for some input

x, y, in our attribute regression loss Eq. 3 with

reparameterization, the gradient for œÉi is

‚àáœÉiL3 = EŒ±i‚àºN(0,1) [2Œ±iexp(œÉi)(¬µi + Œ±iexp(œÉi) ‚àíRi)]

= EŒ±i‚àºN(0,1)



2Œ±2

i exp(2œÉi)



‚â•0,

(6)

which indicates during training URMs with MSE,

the variance term œÉ consistently decreases for all

input examples. Thus, instead of modeling the vari-

ance of human preference distributions, œÉ becomes

more of an indicator of URM‚Äôs confidence and fa-

miliarity w.r.t. the input. Consequently, this results

in a weaker ability to model aleatoric uncertainty

as compared to the URM trained via Maximum

Likelihood Estimation, which is well recognized in

modeling data distributions.

In practice, we find with attribute regression,

sigma becomes very small as training progresses

(magnitude approximates 10‚àí2), while URM

trained via MLE maintains a reasonable variance

for all attributes.

## C

Additional Information

## C.1

Model Merging with URM

Previous study (Ram√© et al., 2024) discovered that

by averaging weights, merged RMs might outper-

form the ensembling of RM predictions regarding

robustness and efficiency. Here we take the base

model Fsfairx-RM and train them with different

methods and losses to see the effect of RM merg-

ing. The evaluation metric is overall score on the

RewardBench.

Fig.

5 gives the results of RM merging

with URM. Deterministic RM is to replace the

uncertainty-aware value head of URM with a linear

layer to deterministically map hidden states to at-

tribution scores (i.e. ‚ÄôURM-Det‚Äô in the Experiment

section). URM-MLE is trained via maximum like-

lihood estimation and URM-Reg is trained with the

attribute regression loss and reparameterization.

While deterministic RM and URM-Reg both

demonstrates improvement after merging, perfor-

mance of URM-MLE deteriorates. This demon-

strates that compared to models trained with

regression-based loss function, RM merging is

not an ideal choice for distribution-modeling RMs.

Compared to the marginal improvement of deter-

ministic RM, URM-Reg benefits significantly from

RM merging. One potential explanation for this

phenomenon is although model merging technique

combines the strength of different models, it in-

evitably introduces noise to the weight space. But

the sample-based rewards in URM-Reg make it

more robust facing such noise, and consequently

better leveraging the advantages of model merg-

ing. Thus, for implementation of URM-Reg, we

merge two models trained with different random

seeds, while implementation of URM-MLE does

not involve model merging.

Figure 5: Random seeds for model 1 and model 2 are

consistent across models trained via different methods

and losses. Merged model are using linear interpolation

to merge model 1 and 2 with equal weights.

15

## Page 16

## C.2

Attribute Distributions in URME

(a) ID correctness score

(b) ID coherence score

(c) ID complexity score

(d) OOD correctness score

(e) OOD coherence score

(f) OOD complexity score

## Urm 1

## Urm 2

## Urm 3

## Urm 4

## Urm 5

Figure 6: Attribute score distributions by URMs within

an URME. URMs have larger discrepancies for OOD

data compared to in-distribution inputs.

16
