# 2203.04600_Domain-Generalization-using-Pretrained-Models-with
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 2203.04600_Domain-Generalization-using-Pretrained-Models-with.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2



## Implementation Status

- âœ… **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- ğŸ”„ **Performance Monitoring**: Continuous validation of targets
- âœ… **Documentation Standards**: Compliant with ACGS-2 requirements
- ğŸ”„ **Cross-Reference Validation**: Ongoing link integrity maintenance

**Overall Status**: ğŸ”„ IN PROGRESS - Systematic enhancement implementation

## Performance Targets

This component maintains the following performance requirements:

- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

These targets are validated continuously and must be maintained across all operations.

---

## Page 1

Domain Generalization using Pretrained Models

without Fine-tuning

Ziyue Li

ShanghaiTech University

lizy@shanghaitech.edu.cn

Kan Ren

Microsoft Research Asia

kan.ren@microsoft.com

Xinyang Jiang

Microsoft Research Asia

xinyangjiang@microsoft.com

Bo Li

Nanyang Technological University

libo0013@e.ntu.edu.sg

Haipeng Zhang

ShanghaiTech University

zhanghp@shanghaitech.edu.cn

Dongsheng Li

Microsoft Research Asia

dongsli@microsoft.com

Abstract

Fine-tuning pretrained models is a common practice in domain generalization

(DG) tasks. However, ï¬ne-tuning is usually computationally expensive due to the

ever-growing size of pretrained models. More importantly, it may cause over-ï¬tting

on source domain and compromise their generalization ability as shown in recent

works. Generally, pretrained models possess some level of generalization ability

and can achieve decent performance regarding speciï¬c domains and samples. How-

ever, the generalization performance of pretrained models could vary signiï¬cantly

over different test domains even samples, which raises challenges for us to best

leverage pretrained models in DG tasks. In this paper, we propose a novel do-

main generalization paradigm to better leverage various pretrained models, named

specialized ensemble learning for domain generalization (SEDGE). It ï¬rst trains

a linear label space adapter upon ï¬xed pretrained models, which transforms the

outputs of the pretrained model to the label space of the target domain. Then, an

ensemble network aware of model specialty is proposed to dynamically dispatch

proper pretrained models to predict each test sample. Experimental studies on

several benchmarks show that SEDGE achieves signiï¬cant performance improve-

ments comparing to strong baselines including state-of-the-art method in DG tasks

and reduces the trainable parameters by âˆ¼99% and the training time by âˆ¼99.5%.

1

Introduction

Distribution shift is a common problem caused by physical or psychological factors of the real-world

applications, which breaks the independent and identically distributional (i.i.d.) assumption of

machine learning algorithms. Thus, generalization becomes increasingly important when training

and applying machine learning models in practice.

The task of domain generalization and the corresponding benchmark [Gulrajani and Lopez-Paz,

2020] have been proposed for studying and improving model generalization by training on source

domains and test on target domains. These methods focus on generalizable model training following

a ï¬ne-tuning paradigm which often leverages pretrained models like ResNet [He et al., 2016] as

an initialization and ï¬ne-tunes that with some elaborate training algorithms on the source domains.

Then, the trained models would be evaluated on the unseen target domains. One common assumption

arXiv:2203.04600v1  [cs.CV]  9 Mar 2022

## Page 2

Pretrained

model(s)

Training on

source domains

Fine-tuned

## (C) Sedge

Pretrained

models

Output

Target

label space

Adapter

Back-Propogation

Dispatcher

Fine-tuned

Fixed

Newly added

(A) Pretraining and ï¬ne-tuning

Model 1

Model K

Â·Â·Â·

(B) Fixed model with

label adapter

Â·Â·Â·

Â·Â·Â·

â€¦

0.6

0.1

Figure 1: Different training paradigms in domain generalization.

behind this commonly used paradigm is that ï¬ne-tuning brings better performance. However, ï¬ne-

tuning is usually computationally expensive due to the ever-growing size of pretrained model, and

proven to possibly compromise the generalization ability of pretrained models and under-perform in

out-of-distribution scenarios [Kumar et al., 2021, Yu et al., 2021].

Therefore, instead of using pretrained model as an initialization like most existing methods on domain

generalization, this paper seeks a better way to leverage the vast amount of the existing pretrained

models [He et al., 2016, Krizhevsky et al., 2012, Iandola et al., 2014, Zoph et al., 2018, He et al.,

2021, Radford et al., 2021]. Generally, the existing pretrained models have already possessed certain

generalization ability over out-of-distribution scenarios. As shown in Figure 1 (B), one simple way to

exploit pretrained modelsâ€™ generalization ability is to train a linear label space adapter over a ï¬xed

weight pretrained model, which directly transforms the outputs of the pretrained model to the target

label space. Our experiments show this minor adjustment bring enhancement over the ï¬ne-tuned one

on certain target domains (detailed results in section 5.3).

However, ï¬xed pretrained models do not constantly generalize on all domains, and the generalization

performances of different pretrained models vary signiï¬cantly over different target domains, label

classes or even samples, as shown in Figure 3. This is caused by various aspects of the pretraining

procedure such as model hypothesis, training algorithms and pretraining datasets.

Due to the signiï¬cant variance of pretrained modelsâ€™ generalization ability, it is essential to identify

the samples a pretrained model generalize to (i.e. model specialty). Here we propose a novel learning

paradigm that dispatches proper pretrained models to each sample based on their generalization

ability, named specialized ensemble learning for domain generalization (SEDGE). As shown in

Figure 1 (C), speciï¬cally, in addition to the label adapter that projects the pretrained domain to the

target domain upon the model with the ï¬xed parameters, we further incorporate a model specialty

aware ensemble network that selects a set of proper pretrained models and aggregate together to

conduct predictions for each speciï¬c sample.

The advantages of our proposed learning paradigm lie in three aspects. First, it shows a signiï¬cant

improvement over the existing state-of-the-art (SOTA) result using the model pool pretrained only on

ImageNet [Krizhevsky et al., 2012] dataset and gains even larger using the relatively larger model

pool pretrained with additional datasets. Second, it exhibits signiï¬cantly higher training efï¬ciency.

The only parameters trained on the source domains contain (1) a linear adapter transforming model

outputs to the target label space and (2) a lightweight ensemble network that has largely reduced

the training cost on the source domains comparing with that ï¬ne-tuning the pretrained models. We

visualize the comparison of the performance w.r.t. to training parameter size and cost of training time

in Figure 2. Last, this method illustrates a ï¬‚exible way to utilize pretrained models, making it easier

to exploit the abundant resource of pretrained models.

2

## Page 3

700 (s)

900 (s)

62

64

66

68

70

72

74

300 (s)

500 (s)

10 (h)

60 (h)

## Sedge

(0.2M,74.1%,329s)

## Sedge

(0.2M,69.4%,864s)

+

## Swad

(25.6M,66.9%,15h)

## Coral

(25.6M,64.1%,15h)

Fish

(25.6M,63.9%,15h)

## C-Dann

(25.6M,62.6%,15h)

Pretrained on

ImageNet

Pretrained on

additional dataset

Overall training time on DomainBed

Average performance on DomainBed  (%)

## Erm

(25.6M,63.8%,15h)

Figure 2: The comparison of the average performance (x-axis, the higher the better) of different

algorithms, their training time (y-axis, the smaller the better), and the number of their training

parameters (the size of the marker). We also list the corresponding information (number of training

parameters, test accuracy, training time) of each algorithm.

2

Related work

2.1

Domain Generalization

Mainstream domain generalization research can be divided into following categories. (1) Domain

alignment. In order to ï¬nd the invariant representation across various domains, Ganin et al. [2016]

adversarially train a generator and discriminator to reach the equilibrium of optimal invariant features

across domains, hence the classiï¬er trained on multiple source domains would generalize well to

target unseen domains. Gong et al. [2019] consider reducing domain discrepancy in a manifold

space. Some works resort to explicit feature distribution alignment on maximum mean discrepancy

(MMD) [Pan et al., 2010, Tzeng et al., 2014, Wang et al., 2018], second order correlation [Sun

et al., 2016, Sun and Saenko, 2016, Peng and Saenko, 2018], moment matching [Peng et al., 2019]

and Wasserstein distance [Zhou et al., 2020, Lyu et al., 2021], etc. Besides learning invariant

representation, Arjovsky et al. [2019] consider to learn an optimal invariant classiï¬er on top of the

representation space, and enforce the learned classiï¬er predicts according with causal mechanism.

(2) Data manipulation. Tobin et al. [2017] ï¬rst introduce this idea, which aims to create diverse

training data to simulate unseen target domain. Besides, Peng et al. [2018] and Tremblay et al.

[2018] strengthen the generalization capability of the models via domain randomization, while other

works consider using self-supervised learning [Carlucci et al., 2019, Kim et al., 2021] to match

representation of an image with various augmentations. (3) Meta-learning. Inspired by Finn et al.

[2017] and with the expectation to capture the most transferable representations across domains,

MLDG [Li et al., 2018a] split the multiple source domains data into meta-train and meta-test set

to simulate domain shifts to learn more generalizable representations. Dou et al. [2019] introduce

additional losses to explicitly pertain to the semantic structure in representations.

Balaji et al.

[2018] consider learning a regularization function on classiï¬er to avoid biasing to domain-speciï¬c

information, while Du et al. [2020] resort to regularize Kullback-Leibler (KL) divergence between

distributions of latent representations within samples from different domains.

While above categories more focus on algorithmic improvements, our proposed method SEDGE

emphasizes the innovation of a learning paradigm based on a specialized pretrained model ensemble.

2.2

Ensemble Learning

Ensemble learning methods [Hansen and Salamon, 1990, Zhou et al., 2018] exploit multiple models

to produce prediction results and combine the results with various techniques, e.g., boosting [Schapire,

1990, Freund, 1995, Moghimi et al., 2016] or mean aggregation [Zhou et al., 2018, Zhang et al.,

2020], etc., to achieve better performance than individual model alone. These methods combine base

3

## Page 4

model learning and ensemble as a whole and focus more on training diverse base models [Zhou et al.,

2018].

In DG, speciï¬cally, ensemble methods are used to exploit the relationship between source domains

and the overall prediction results are composed of the superposition of the multiple networks on each

domain. Mancini et al. [2018] proposed to aggregate different predictions from speciï¬c trained source

models. Segu et al. [2020] proposed domain speciï¬c batch-norm statistics for each source domain.

Zhou et al. [2021] proposed one shared CNN feature extractor with domain speciï¬c classiï¬ers and

each classiï¬er is an expert to its own domain but non-expert to other domains. MulDEns [Thopalli

et al., 2021] relaxes the requirement for domain-speciï¬c models and uses a model-domain relevance

matrix to deï¬ne the relations between models and domains. Besides aggregating different domain

expert models, other works consider combining model weights in different runs. SWAD [Cha et al.,

2021] avoids overï¬tting models to local sharp minima by averaging model weights below a validation

loss threshold. EoA [Arpit et al., 2021] further lessens the frequent computations on validation set by

averaging model weights simply from start to the end.

These ensemble learning methods rely on training or ï¬ne-tuning from a pretrained model, share

the same limitation of training cost and initialization model selection. They did not consider the

model specialty in different domains, classes or even samples. We start from a novel perspective that

incorporates various pretrained models without ï¬ne-tuning and builds a lightweight specialty-aware

ensemble network, which illustrates better generalization performance and largely reduces training

costs.

3

Preliminaries

3.1

Problem Formulation

Domain generalization aims to tackle the shift of data distribution among different domains by zero-

shot transferring knowledge from seen to unseen domains. Speciï¬cally, unlike domain adaptation,

samples from unseen target domain(s) are inaccessible in domain generalization. For a domain,

its input and label space can be denoted as X âˆˆRd and Y âˆˆRC, and its samples are observed

constructing a dataset D = {(xi, yi)}N

i=1 with N sample points. Consider that we have S source

domains Ds = {D1, . . . , DS} and T target domains Dt = {D1, . . . , DT } with different distributions

on X Ã—Y and sharing the label space. Given instances drawn from source domains, the task is to learn

a predictor parameterized by Î¸ as fÎ¸ âˆˆM: Rd 7âˆ’â†’RC, where d is the dimension of input and C is the

number of classes in Y. We can deï¬ne a population loss as ED(Î¸) =

1

## |D|

## P|D|

j=1 Exiâˆ¼Dj [l(fÎ¸(xi), yi)]

over the given domain D. The objective is to minimize the task-speciï¬c loss l (e.g., cross-entropy

loss for classiï¬cation) over both source domains Ds and target domains Dt by only minimizing

the empirical risk Ë†EDs(Î¸) w.r.t. model parameter Î¸. The performance on the target domains, then,

measures the generalization ability of the learned model.

3.2

Preliminary Analysis

In this section, we want to investigate the generalization ability of various pretrained models, to gain

some insights to motivate our method. As suggested by preliminary work [Kumar et al., 2021], a

pretrained model with a linear probing layer (i.e., replacing the last layer of the pretrained model and

retraining that) may achieve better accuracy in out-of-distribution scenarios than ï¬ne-tuning the whole

model. However, linear probing is not feasible due to different pretrained models having different

penultimate layer output feature dimensions. Instead, as shown in Figure 1 (B), we only train a label

space adapter which learns the mapping function parameterized with Ï† as hÏ† âˆˆA: RCo 7âˆ’â†’RC,

where Co is the dimension of the label space of the original pretraining dataset (pretraining domain).

Thus, all the pretrained models on the same pretraining dataset (e.g., ImageNet) share the same label

adapter, through which it largely reduces the adaptation cost of the pretrained models on the new

domains.

Given the pretrained model pool {fk}K

k=1 with K pretrained models each of which is parameterized

as Î¸k, we further parameterize the adapted model hÏ†(fk(Â·)) as Î¸â€²

k = [Ï†; Î¸k]. Then we train this shared

adapter hÏ† with the empirical loss Ë†EDs(Ï†) without ï¬ne-tuning the pretrained model parameters {Î¸k}.

With the trained adapter, we use the likelihood of the ground truth label p(yi | xi; Î¸â€²

k) on the i-th

4

## Page 5

8.0

>

(b) Class-level performance and performance divergence

of pretrained models

classes:

8.0

>

8.0

>

domains:

(a) Domain-level performance and performance divergence

of pretrained models

Figure 3: Performance distribution of the pretrained models over the samples within (a) different

domains and (b) different classes. Each column of the left panel displays the relative performance

distribution of the pretrained models; the right panel shows the Kullback-Leibler divergence between

the performance distribution of different (a) domains and (b) classes. The comparison of the domain-

level and class-level specialty shows that the performance of the pretrained models differs more

signiï¬cantly at the ï¬ner level.

sample produced by each adapted model, which also indicates the conï¬dence of the ground truth

label yi of the model and P

yâˆˆY p(y | xi; Î¸â€²

k) = 1. We utilize this likelihood as the evaluation metric

of its sample-level model specialty.

First, we analyze the specialty distribution of each pretrained model from an aggregation view

(i. .e, domains and classes, respectively), and we verify if there exists a dominant pretrained

model that generalizes best across different unseen domains. We calculate domain-level model

specialty as summation of logarithms of the sample-level specialty over all domain samples as

## P

(xi,yi)âˆ¼D log p(yi | xi; Î¸â€²

k), on TerraIncognita [Beery et al., 2018] with four domains. To reï¬‚ect

the relative model performance, we perform min-max normalization for model specialty values in

the same domain. These results are shown in Figure 3 (a). As can be seen, pretrained models vary

greatly in performance on different domains, with no single model being dominant in all domains. It

suggests that ï¬nding a speciï¬c powerful pretrained model is non-trivial and not straight-forward for

domain generalization.

Then, based on the previous ï¬nding, we further examine whether performance divergence also exists

at a ï¬ner-grained level, such as class-level. Similar as that at domain-level, Figure 3 (b) presents the

relative model performance on 10 classes in TerraIncognita-L100. Model performance variances

between classes are also noticeable. To clearly compare model specialty differences at the two levels,

we present heatmaps of specialty differences (measured by Kullback-Leibler divergence) for domain

and class pairs, respectively in Figure 3. The heatmaps exhibit a more pronounced divergence in

model specialty at the ï¬ner class level. It supports the necessity to utilize pretrained models on top of

taking their ï¬ne-grained specialty, in ï¬ner-grained level even on each sample, into account.

4

SEDGE: A New Paradigm for Domain Generalization

In this section, we introduce our proposed learning paradigm, namely specialized ensemble learning

for domain generalization (SEDGE), with the motivation and speciï¬c details of the whole method.

We ï¬rst present the whole framework in Section 4.1 and then discuss the gathered pretrained models

in Section 4.2. After that, we introduce the model dispatcher with ensemble learning in Section 4.3

and the corresponding learning algorithm in Section 4.4.

5

## Page 6

adapter

adapter

Fixed pretrained

model pool

Specialized dispatcher

Â·Â·Â·

Input

shared

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

0.1

0.2

0.5

encoder

(fixed)

learnable

model embeddings

Matching

sample

embedding

Ensemble network

Back-propagation

Label space adapter

Â·Â·Â·

Â·Â·Â·

Prediction

Adapted model outputs

Ensemble

Figure 4: SEDGE framework. Based on a pool of several ï¬xed pretrained models, an ensemble

network learns the matching of models and samples for model dispatching with the help of a label

space adapter for prediction transformation.

4.1

Framework: Pretrained Model without Fine-tuning

Recall that the focus of the paper is on leveraging pretrained models without ï¬ne-tuning to cope

with domain generalization. As motivated in Section 3.2, each model has its own specialty and each

sample may require to choose a speciï¬c set of models to give a good prediction. As a result, we learn

the matching of pretrained models and samples from the source domainsâ€™ training data.

Based on this idea, as illustrated in Figure 4, we propose a novel specialty-aware domain generaliza-

tion framework to dispatch an ensemble of specialized models for each sample. Speciï¬cally, a label

space adapter described in Section 3.2 is trained to transform the prediction of the pretrained models.

And then, an ensemble network is learned to dispatch the models in a model pool to each sample

according to their estimated specialty at sample level, and aggregate their outputs as an ensemble to

output the ï¬nal prediction for each sample.

4.2

Pretrained Model Pool

This section presents the pretrained models used in SEDGE. With more and more pretrained models

being published, it is straightforward to build a pretrained model pool consisting of several public

pretrained models for direct adapting to novel domains.

On one hand, utilizing a ConvNet backbone [LeCun et al., 2015] pretrained on ImageNet is a common

practice of DG algorithms [Kim et al., 2021]. Based on that, we ï¬rst build Model Pool-A which

only contains 15 models pretrained on ImageNet for fair comparison with the existing algorithms.

In Model Pool-A, we incorporate the architectures including AlexNet (1) [Krizhevsky et al., 2012],

DenseNet-121/169/201 (3) [Iandola et al., 2014], Dual-Path-Network-68 (1) [Chen et al., 2017],

NASNetMobile (1) [Zoph et al., 2018], ResNet-18/34/50 (3) [He et al., 2016], SE-ResNet-50 (1) [Hu

et al., 2018], SqueezeNet-1.0/1.1 (2) [Iandola et al., 2016], and MAE-ViT-Base/Large/Huge (3) [He

et al., 2021] with pretrained weights1.

On the other hand, several DG algorithms also use models pretrained on other datasets, such as

1G-1B [Arpit et al., 2021] and ILSVRC12 [Thomas et al., 2021]. Therefore, we build Model Pool-B

which contains two more CLIP models [Radford et al., 2021], ViT-B/16 and ViT-B/32, which are

trained on a subset of the YFCC100M dataset of roughly the same size as ImageNet. We denote

SEDGE using Model Pool-B as SEDGE+ to distinguish it from the one using Model Pool-A.

1https://github.com/Cadene/pretrained-models.pytorch

6

## Page 7

Note that, the models pretrained on the same dataset, i.e., ImageNet, will share the same label adapter

transforming the vanilla model outputs to the target label space.

4.3

Ensemble Network

As demonstrated by the ï¬ndings in Section 3.2, for generalizing to unseen domains, we need to take

advantage of each of the pretrained models with consideration of their specialties. Additionally, rather

than using one model to predict each sample, we propose to use an ensemble of multiple pretrained

models, which is known to bring less generalization error [Ueda and Nakano, 1996]. Moreover,

our method incorporates sample-level model specialty into consideration and conducts ï¬ne-grained

specialty-aware ensemble learning, which is novel comparing to the existing ensemble learning

methods as discussed in Section 2. This section will elaborate on the process of obtaining the most

specialized pretrained models for a given sample and aggregating the outputs of selected models

based on their specialty. The process can be divided into three steps as shown in Figure 4.

First, we embed the input sample and the available models to a hidden space. For an image sample

xi, we use a ï¬xed pretrained model (i.e., ResNet-34 in our implementation) to embed xi to ei âˆˆRdq.

Meanwhile, we introduce a learnable latent variable Em âˆˆRKÃ—dm as model embedding dictionary

corresponding to the K models {fk}K

k=1, which is randomly initialized and optimized during training.

Furthermore, we map ei and Em to a joint latent space as

ci = Ïƒ(eiWi), Cm = Ïƒ(EmWm),

(1)

where Wi âˆˆRdqÃ—dv, Wm âˆˆRdmÃ—dv, and Ïƒ(Â·) = max{Â·, 0}. We then perform matrix multi-

plication of ci and Cm to calculate the matching score s = ciCT

m âˆˆRK between the sample

and each model. To dispatch each model output to the ï¬nal prediction on the sample, we use

one layer multi-layer perceptron and perform softmax operation to output the ensemble weights

w = [w1, . . . , wK] âˆˆRK with wk equals to

wk =

e(Î¶(W(s)))k

## Pk

j=1 e(Î¶(W(s)))j ,

(2)

where W âˆˆRKÃ—K and Î¶(Â·) = log(1 + exp(Â·)). Finally, the prediction for xi based on an ensemble

of K model outputs is written as

Ë†yi =

## K

## X

k=1

wkhÏ†(fk(xi)), s.t.

## K

## X

k=1

wk = 1.

(3)

4.4

Learning Algorithm

As discussed above, the ensemble network acts as a model dispatcher through generating ensemble

weights to aggregate multiple model predictions for each sample. Section 3.2 shows that model

performance varies signiï¬cantly over samples. Thus, we expect to assign more weights to the models

with higher sample-level specialty to achieve the best utilization of the pretrained models. That is, we

try to minimize the estimation risk of the estimated model specialty on the ground truth, i.e., wk and

p(yi | xi; Î¸â€²

k), as

Lc = âˆ’

## K

## X

k=1

[p (yi | xi; Î¸â€²

k) Â· ln(wk) + (1 âˆ’p (yi | xi; Î¸â€²

k)) Â· ln(1 âˆ’wk)] .

(4)

Lc is used to optimize the ensemble network to be a specialty-aware model dispatcher.

To train the general label space adapter hÏ† for all pretrained models, we incorporate the classiï¬cation

losses of adapted predictions of pretrained models

Lb =

## K

## X

k=1

wk Â· l (hÏ†(fk(xi)), yi) ,

(5)

to update the shared adapter. Additionally, we use the classiï¬cation loss

Le = l (Ë†yi, yi)

(6)

7

## Page 8

to optimize the likelihood of ï¬nal ensemble output. Le is used to update both ensemble network and

adapter.

It is worth noting that the only parameters to update is the label space adapter and ensemble network,

each of which is lightweight compared to the pretrained models which remain ï¬xed in our method

yet have been ï¬ne-tuned in the previous works.

Relation to weight ensemble. Previous methods, such as SWAD [Cha et al., 2021] and EoA [Arpit

et al., 2021], show that averaging model weights during training can avoid overï¬tting and achieve

better generalization performance. Their experimental results show superior performance compared

with methods without weight averaging. While in SEDGE, all pretrained model weights are not

involved in training. Accordingly, we perform weight averaging for adapter and ensemble network

starting from a certain iteration, which is served as a hyper-parameter.

Top-k model selection in inference. To save the inference time, we further select models with the

highest k ensemble weights and perform softmax on their ensemble weights for aggregation. In this

paper, we set k as 6.

5

Experiments

5.1

Evaluation Protocol

We conduct experiments on DomainBed suite [Gulrajani and Lopez-Paz, 2020], which provides

like-for-like comparisons between algorithms and has a standard evaluation protocol to follow.

Datasets. We experiment on 5 real-world benchmark datasets including PACS (4 domains, 9,991

samples, 7 classes) [Li et al., 2017], VLCS (4 domains, 10,729 samples, 5 classes) [Fang et al., 2013],

Ofï¬ceHome (4 domains, 15,588 samples, 65 classes) [Venkateswara et al., 2017], TerraIncognita

(4 domains, 24,778 samples, 10 classes) [Beery et al., 2018], and DomainNet (6 domains, 586,575

samples, 345 classes) [Peng et al., 2019].

For fair comparison, we follow the training and evaluation protocol of DomainBed [Gulrajani

and Lopez-Paz, 2020]. We use the training-domain validation set protocol for model selection.

Speciï¬cally, one domain in a dataset is selected as the target domain and the rest as source domains,

from which 20% of samples are used as the validation set. All runs are repeated 3 times using different

random seeds, thus, with different train-validation splits. The out-of-domain test performance

averaged over all domains will be reported for each dataset. In addition, we use the standard

number of iterations of 5,000 for all datasets, with early-stop based on validated accuracy to reduce

unnecessary computational costs.

Baselines. We compare SEDGE with some strong DG baselines including state-of-the-art. As

discussed in Section 2, some of the compared methods incorporate elaborate learning algorithms

including ERM [Vapnick, 1998], CORAL [Sun and Saenko, 2016], MLDG [Li et al., 2018a],

MMD [Li et al., 2018b], DANN [Ganin et al., 2016], C-DANN [Li et al., 2018c], and Fish [Shi et al.,

2021].

Some other works compared in our evaluation incorporate ensemble learning as listed as below.

â€¢ Stochastic Weight Averaging Densely (SWAD) [Cha et al., 2021]: SWAD performs weight

ensemble during model training.

â€¢ Ensemble of Average (EoA) [Arpit et al., 2021]: EoA combines both model ensemble and

weight ensemble by taking an ensemble of moving average models from 6 runs. They

experiment with two different pretrained models as initialization. One is pretrained on

ImageNet with ResNet-50 and the other is pretrained on both ImageNet and a much larger

additional dataset, IG-1B, with a more advanced backbone, ResNeXt-50 [Xie et al., 2017].

We denote the latter one as EoA+ to indicate it uses the additional dataset.

â€¢ Random ensemble: In contrast to SEDGE of learning to select models for ensemble, we

also compare it with average ensemble of k models chosen randomly for each sample.

In addition, LP-FT [Kumar et al., 2021] reveals the generalization of pretrained models and proposes

an elaborated ï¬ne-tuning strategy. However, it does not follow the protocol of DomainBed and does

not provide implementation details for replication. Our runs for LP-FT show it performs worse than

8

## Page 9

Table 1: All baseline results are taken from their papers. Our experiments are repeated 3 times using

different random seeds.

Algorithm

## Pacs

## Vlcs

Ofï¬ceHome

TerraIncognita

DomainNet

avg.

Model Pool-A

Non-ensemble algorithms

DANN (JMLRâ€™16) [Ganin et al., 2016]

84.6Â±1.1

78.7Â±0.3

65.4Â±0.6

48.4Â±0.5

38.4Â±0.0

63.1

CORAL (ECCVâ€™16) [Sun and Saenko, 2016]

86.0Â±0.2

77.7Â±0.5

68.6Â±0.4

46.4Â±0.8

41.8Â±0.2

64.1

MLDG (AAAIâ€™18) [Li et al., 2018a]

84.8Â±0.6

77.1Â±0.4

68.2Â±0.1

46.1Â±0.8

41.8Â±0.4

63.6

MMD (CVPRâ€™18) [Li et al., 2018b]

85.0Â±0.2

76.7Â±0.9

67.7Â±0.1

49.3Â±1.4

39.4Â±0.8

63.6

C-DANN (ECCVâ€™18) [Li et al., 2018c]

82.8Â±1.5

78.2Â±0.4

65.6Â±0.5

47.6Â±0.8

38.9Â±0.1

62.6

ERM (ICLRâ€™21) [Gulrajani and Lopez-Paz, 2020]

85.7Â±0.5

77.4Â±0.3

67.5Â±0.5

47.2Â±0.4

41.2Â±0.2

63.8

Fish (ICLRâ€™22) [Shi et al., 2021]

85.5Â±0.3

77.8Â±0.3

68.6Â±0.4

45.1Â±1.3

42.7Â±0.2

63.9

Ensemble algorithms

SWAD (NIPSâ€™21) [Cha et al., 2021]

88.1Â±0.1

79.1Â±0.1

70.6Â±0.2

50.0Â±0.3

46.5Â±0.1

66.9

EoA (arxiv) [Arpit et al., 2021]

88.6

79.1

72.5

52.3

47.4

68.0

random ensemble

58.1Â±0.13

58.5Â±1.26

59.6Â±0.38

31.5Â±0.40

15.8Â±1.40

44.5

## Sedge

84.1Â±0.45

79.8Â±0.12

79.9Â±0.12

56.8Â±0.21

46.3Â±0.39

69.4

Model Pool-B

EoA+ (arxiv) [Arpit et al., 2021]

93.2

80.4

80.2

55.2

54.6

72.7

random ensemble

59.5Â±0.95

61.1Â±0.12

59.5Â±0.07

30.8Â±0.37

18.7Â±0.62

46.0

## Sedge+

96.1Â±0.04

82.2Â±0.03

80.7Â±0.21

56.8Â±0.29

54.7Â±0.10

74.1

ERM, whose results are shown in Appendix. Note that, all the compared methods mentioned above

incorporate a ï¬ne-tuning paradigm upon a pretrained model, which is essentially different to our

method.

5.2

DomainBed Benchmarking

This section presents experimental results on the DomainBed suite, with performance comparison

shown in Table 1 and training/inference time comparison in Table 2.

Comparison with ï¬ne-tuning paradigm. The main difference between previous algorithms and

SEDGE lies in ï¬ne-tuning or no ï¬ne-tuning on top of pretrained models. To verify whether the

dispatcher of ï¬xed pretrained models can outperform ï¬ne-tuning paradigm, we conduct a comparison

of the algorithms that use models only pretrained on ImageNet, e.g., SEDGE using Model Pool-A.

As shown in Table 1, SEDGE achieves an average performance of 69.4%, exceeding SWAD by 2.5%.

Results show evidence that our novel paradigm is more effective than the traditional paradigm.

Performance beneï¬ts from adding more pretrained models. SEDGE provides a feasible way

to incorporate the ever-emerging publicly available pretrained models. Although Model Pool-A

pretrained on ImageNet is in common use, Kumar et al. [2021] ï¬nds that model pretrained on

ImageNet may not be good for datasets such as DomainNet. By using Model Pool-B that includes

models that have been pretrained on the CLIP dataset [Radford et al., 2021], SEDGE+ further

improves the average performance by 4.7% over SEDGE and ranks ï¬rst on all datasets. This

conï¬rms that SEDGE paradigm is expected to generalize better on unseen domains by including

models pretrained on more diverse datasets in the model pool.

Training cost comparison. SEDGE only utilizes ï¬xed pretrained models and learns to dispatch them

through a lightweight ensemble network with the help of a linear label space adapter. Therefore, the

number of learnable parameters of SEDGE (up to 0.6M) is minor compared with the normal image

backbone network (25.6M for ResNet-50). For fair training cost comparison, we run experiments of

ERM, SWAD, SEDGE on a single Nvidia Tesla V100 and compare their overall back-propagation

time from the start of training to the end (or early-stop). As shown in Table 2, training SEDGE

paradigm uses noticeably less time. SEDGE+ takes only 0.6% of the time of ERM on DomainNet.

The signiï¬cant training time advantage of the method and its surpassing performance suggest that

SEDGE is an effective and efï¬cient paradigm for domain generalization.

Inference cost comparison. As shown in Table 2, although ensemble methods like EoA and SEDGE

achieve better generalization performance at the cost of higher inference FLOPs, SEDGE still

manages to save a large amount of inference cost compared to the previous best ensemble model

(half of the inference FLOPs compared to EoA). This is because SEDGE only selects models with

the highest k(< K) ensemble weights. Therefore, only k of K models are activated for inference per

sample, which reduces the inference cost to a large extent.

9

## Page 10

Table 2: The comparison of training and inference cost. The run for SWAD on DomainNet failed due

to out-of-memory. Here, â€œ# parametersâ€ means the number of learnable parameters.

ERM (our runs)

SWAD (our runs)

EoA (estimated)

SEDGE (Pool-A)

SEDGE (Pool-B)

Training

## Pacs

3.4h

2.1h

20.4h

19.4s

6.6s

## Vlcs

3.6h

2.4h

21.6h

49.4s

8.3s

Ofï¬ceHome

3.3h

2.1h

19.8h

76.6s

15.9s

TerraIncognita

3.4h

2.1h

20.4h

46.4s

52.1s

DomainNet

9.8h

/

58.8h

11.2m

4.1m

# parameters

## 25.6M

## 25.6M

## 153.4M

## 0.2 âˆ¼0.6M

## 0.3 âˆ¼0.6M

Inference

GFLOPs

3.9

3.9

23.5

12.0

10.4

Table 3: Results of applying label space adapter only and random ensemble.

Algorithm

## Pacs

## Vlcs

Ofï¬ceHome

TerraIncognita

DomainNet

avg.

Model Pool-A

SWAD (NIPSâ€™21) [Cha et al., 2021]

88.1Â±0.1

79.1Â±0.1

70.6Â±0.2

50.0Â±0.3

46.5Â±0.1

66.9

best single model + adapter

79.7

73.6

78.3

49.2

32.5

62.7

random ensemble

58.1Â±0.13

58.5Â±1.26

59.6Â±0.38

31.5Â±0.40

15.8Â±1.40

44.5

## Sedge

84.1Â±0.45

79.8Â±0.0

79.9Â±0.12

56.8Â±0.21

46.3Â±0.39

69.4

Model Pool-B

best single model + adapter

95.4

82.0

78.3

49.2

52.6

71.5

random ensemble

59.5Â±0.95

61.1Â±0.12

59.5Â±0.07

30.8Â±0.37

18.7Â±0.62

46.0

## Sedge+

96.1Â±0.04

82.2Â±0.03

80.7Â±0.21

56.8Â±0.29

54.7Â±0.10

74.1

5.3

Ablation Study

We want to verify the effectiveness of SEDGE design by answering two research questions: (Q1) Is

grafting a label space adapter on top of model outputs sufï¬cient, for utilizing pretrained models to

generalize to novel domains? (Q2) Is specialty-aware ensemble necessary, compared to an average

ensemble method?

To verify whether a single model with an adapter can perform well, we train an individual adapter on

source domains for each pretrained model in the model pool and compare their performance on target

domain with state-of-the-art DG algorithm. To show the â€œcheatingâ€ upper bound of performance

under this ablation study, we report the best single model performance on test set as best single

model + adapter. As shown in Table 3, the best single model + adapter among Model Pool-A can

outperform SWAD only on Ofï¬ceHome. It ï¬rst indicates that the generalization ability of the ï¬xed

pretrained models may be more promising than model with ï¬ne-tuning on speciï¬c domains. However,

in other four datasets, it lags behind SWAD by a large margin. This demonstrates a single pretrained

model with a label space adapter is not sufï¬cient to generalize to unseen domains, which motivates

the main contribution of our method of introducing ensemble learning.

DG algorithms that combine ensemble learning, such as SWAD and EoA, demonstrate promising

performance. A natural question is whether using an ensemble of pretrained models rather than a

single model can improve performance. Following the ensemble approaches [Lakshminarayanan

et al., 2017] using mean average, we experiment a random ensemble over the ï¬xed pretrained models,

i.e., randomly sampling k models for each sample and averaging their outputs for ï¬nal prediction.

The results are shown in Table 3. As can be seen, random ensemble results in worse performance

than the single model, while SEDGE with specialty-aware ensemble boosts the ï¬nal performance

signiï¬cantly, albeit with strong or weak individual model performance, which veriï¬es the necessity

to select and ensemble the pretrained models based on their specialty over samples as mentioned in

Section 3.2 (Q2).

5.4

Further Analysis

As shown in Figure 3, model performance varies across domains, while SEDGE is designed to

dispatch specialized models for samples. To analyze whether SEDGE is handling as expected, we

present its domain-level model assignment on different domains of TerraIncognita. Speciï¬cally, we

calculate the sum of ensemble weights assigned to a model as an evaluation of its importance. In

Figure 5, we show the rankings of model importance on different domains. By comparing ranking

between different domains, it can be seen that SEDGE dispatches models quite differently over

unseen target domains. For example, while CLIP-ViT-B/32 model is used frequently on L38/43/46

10

## Page 11

Most

used

Least

used

## L100

## L38

## L43

## L46

Figure 5: Ranking models using the sum of ensemble weights on the sample in four domains of

TerraIncognita. Each color block corresponds to a model. The higher rank indicates that this model is

given a higher weight in predicting the samples in this domain.

datasets, it lags behind other models on L100. It suggests that SEDGE is making rational model

selection as Figure 3 shows CLIP-ViT-B/32 is not a powerful model on this domain. Since the target

domain is not known prior to making predictions, SEDGE learns to ï¬nd suitable models for each

sample by learning on source domains only.

6

Conclusions

Domain generalization algorithms use the pretrained model as initialization for ï¬ne-tuning, while

a few works have found that ï¬ne-tuning may lead to out-of-distribution performance degradation.

Different from the previous ï¬ne-tuning paradigm, this paper proposes a novel paradigm for domain

generalization, specialized ensemble learning which learns to dispatch an ensemble of ï¬xed pretrained

models for each sample based on the model specialty on it. Experiments on ï¬ve benchmark datasets

show that our proposed method has achieved state-of-the-art performance with signiï¬cant training

cost reduction.

References

Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.

arXiv preprint arXiv:1907.02893, 2019.

Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong.

Ensemble of averages: Im-

proving model selection and boosting performance in domain generalization. arXiv preprint

arXiv:2110.10832, 2021.

Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain gen-

eralization using meta-regularization. Advances in neural information processing systems, 31,

2018.

Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the

European conference on computer vision (ECCV), pages 456â€“473, 2018.

11

## Page 12

Fabio M Carlucci, Antonio Dâ€™Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi.

Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on

Computer Vision and Pattern Recognition, pages 2229â€“2238, 2019.

Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee,

and Sungrae Park. Swad: Domain generalization by seeking ï¬‚at minima. Advances in Neural

Information Processing Systems, 34, 2021.

Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual path

networks. Advances in neural information processing systems, 30, 2017.

Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization

via model-agnostic learning of semantic features. Advances in Neural Information Processing

Systems, 32, 2019.

Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees GM Snoek, and Ling Shao.

Learning to learn with variational information bottleneck for domain generalization. In European

Conference on Computer Vision, pages 200â€“216. Springer, 2020.

Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple

datasets and web images for softening bias. In Proceedings of the IEEE International Conference

on Computer Vision, pages 1657â€“1664, 2013.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of

deep networks. In International conference on machine learning, pages 1126â€“1135. PMLR, 2017.

Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121(2):

256â€“285, 1995.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois

Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.

The journal of machine learning research, 17(1):2096â€“2030, 2016.

Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. Dlow: Domain ï¬‚ow for adaptation and

generalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern

recognition, pages 2477â€“2486, 2019.

Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint

arXiv:2007.01434, 2020.

Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern

analysis and machine intelligence, 12(10):993â€“1001, 1990.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,

pages 770â€“778, 2016.

Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked

autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.

Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE

conference on computer vision and pattern recognition, pages 7132â€“7141, 2018.

Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, and Kurt Keutzer.

Densenet: Implementing efï¬cient convnet descriptor pyramids. arXiv preprint arXiv:1404.1869,

2014.

Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt

Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size.

arXiv preprint arXiv:1602.07360, 2016.

Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised

contrastive regularization for domain generalization. In Proceedings of the IEEE/CVF International

Conference on Computer Vision, pages 9619â€“9628, 2021.

12

## Page 13

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiï¬cation with deep convolu-

tional neural networks. Advances in neural information processing systems, 25, 2012.

Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning

distorts pretrained features and underperforms out-of-distribution. In International Conference on

Learning Representations, 2021.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive

uncertainty estimation using deep ensembles. Advances in neural information processing systems,

30, 2017.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436â€“444,

2015.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain

generalization. In Proceedings of the IEEE international conference on computer vision, pages

5542â€“5550, 2017.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-

learning for domain generalization. In Thirty-Second AAAI Conference on Artiï¬cial Intelligence,

2018a.

Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial

feature learning. In Proceedings of the IEEE conference on computer vision and pattern recognition,

pages 5400â€“5409, 2018b.

Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.

Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the

European Conference on Computer Vision (ECCV), pages 624â€“639, 2018c.

Boyang Lyu, Thuan Nguyen, Prakash Ishwar, Matthias Scheutz, and Shuchin Aeron. Barycentric-

alignment and invertibility for domain generalization. arXiv preprint arXiv:2109.01902, 2021.

Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Best sources forward:

domain generalization through source-speciï¬c nets. In 2018 25th IEEE international conference

on image processing (ICIP), pages 1353â€“1357. IEEE, 2018.

Mohammad Moghimi, Serge J Belongie, Mohammad J Saberian, Jian Yang, Nuno Vasconcelos, and

Li-Jia Li. Boosted convolutional neural networks. In BMVC, volume 5, page 6, 2016.

Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer

component analysis. IEEE transactions on neural networks, 22(2):199â€“210, 2010.

Xingchao Peng and Kate Saenko. Synthetic to real adaptation with generative correlation alignment

networks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages

## 1982â€“1991. Ieee, 2018.

Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching

for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on

computer vision, pages 1406â€“1415, 2019.

Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of

robotic control with dynamics randomization. In 2018 IEEE international conference on robotics

and automation (ICRA), pages 3803â€“3810. IEEE, 2018.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,

Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual

models from natural language supervision. In International Conference on Machine Learning,

pages 8748â€“8763. PMLR, 2021.

Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197â€“227, 1990.

Mattia Segu, Alessio Tonioni, and Federico Tombari. Batch normalization embeddings for deep

domain generalization. arXiv preprint arXiv:2011.12672, 2020.

13

## Page 14

Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel

Synnaeve. Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021.

Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In

European conference on computer vision, pages 443â€“450. Springer, 2016.

Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In

Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 30, 2016.

Xavier Thomas, Dhruv Mahajan, Alex Pentland, and Abhimanyu Dubey. Adaptive methods for

aggregated domain generalization. arXiv preprint arXiv:2112.04766, 2021.

Kowshik Thopalli, Sameeksha Katoch, Jayaraman J Thiagarajan, Pavan K Turaga, and Andreas

Spanias. Multi-domain ensembles for domain generalization. In NeurIPS 2021 Workshop on

Distribution Shifts: Connecting Methods and Applications, 2021.

Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain

randomization for transferring deep neural networks from simulation to the real world. In 2017

IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23â€“30. IEEE,

2017.

Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang

To, Eric Cameracci, Shaad Boochoon, and Stan Birchï¬eld. Training deep networks with synthetic

data: Bridging the reality gap by domain randomization. In Proceedings of the IEEE conference

on computer vision and pattern recognition workshops, pages 969â€“977, 2018.

Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:

Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.

Naonori Ueda and Ryohei Nakano. Generalization error of ensemble estimators. In Proceedings of

International Conference on Neural Networks (ICNNâ€™96), volume 1, pages 90â€“95. IEEE, 1996.

Vladimir N Vapnick. Statistical learning theory. Wiley, New York, 1998.

Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep

hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on

computer vision and pattern recognition, pages 5018â€“5027, 2017.

Jindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, and Philip S Yu. Visual domain

adaptation with manifold embedded distribution alignment. In Proceedings of the 26th ACM

international conference on Multimedia, pages 402â€“410, 2018.

Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He. Aggregated residual

transformations for deep neural networks. In Proceedings of the IEEE conference on computer

vision and pattern recognition, pages 1492â€“1500, 2017.

Yaodong Yu, Heinrich Jiang, Dara Bahri, Hossein Mobahi, Seungyeon Kim, Ankit Singh Rawat,

Andreas Veit, and Yi Ma. An empirical study of pre-trained models on out-of-distribution general-

ization. 2021.

Shaofeng Zhang, Meng Liu, and Junchi Yan. The diversiï¬ed ensemble neural network. Advances in

Neural Information Processing Systems, 33, 2020.

Fan Zhou, Zhuqing Jiang, Changjian Shui, Boyu Wang, and Brahim Chaib-draa. Domain generaliza-

tion with optimal transport and metric learning. arXiv preprint arXiv:2007.10573, 2020.

Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain adaptive ensemble learning. IEEE

Transactions on Image Processing, 30:8008â€“8018, 2021.

Tianyi Zhou, Shengjie Wang, and Jeff A Bilmes. Diverse ensemble evolution: Curriculum data-model

marriage. In Proceedings of the 32nd International Conference on Neural Information Processing

Systems, pages 5909â€“5920, 2018.

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures

for scalable image recognition. In Proceedings of the IEEE conference on computer vision and

pattern recognition, pages 8697â€“8710, 2018.

14
