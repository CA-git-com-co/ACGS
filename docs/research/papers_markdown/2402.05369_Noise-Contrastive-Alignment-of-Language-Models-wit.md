# 2402.05369_Noise-Contrastive-Alignment-of-Language-Models-wit

**Original PDF**: 2402.05369_Noise-Contrastive-Alignment-of-Language-Models-wit.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2

---

## Page 1

Noise Contrastive Alignment of Language Models with

Explicit Rewards

Huayu Chen1,2, Guande He1,2, Lifan Yuan1, Ganqu Cui1, Hang Su1,2,3, Jun Zhu1,2∗

1Department of Computer Science and Technology, Tsinghua University

2Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University

3Zhongguancun Laboratory, Beijing, China

Abstract

User intentions are typically formalized as evaluation rewards to be maximized

when fine-tuning language models (LMs). Existing alignment methods, such as

Direct Preference Optimization (DPO), are mainly tailored for pairwise preference

data where rewards are implicitly defined rather than explicitly given. In this paper,

we introduce a general framework for LM alignment, leveraging Noise Contrastive

Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated

with scalar evaluations. Our framework comprises two parallel algorithms, NCA

and InfoNCA, both enabling the direct extraction of an LM policy from reward data

as well as preference data. Notably, we show that the DPO loss is a special case

of our proposed InfoNCA objective under pairwise preference settings, thereby

integrating and extending current alignment theories. By comparing NCA and

InfoNCA, we demonstrate that the well-observed decreasing-likelihood trend of

DPO/InfoNCA is caused by their focus on adjusting relative likelihood across

different responses. In contrast, NCA optimizes the absolute likelihood for each

response, thereby effectively preventing the chosen likelihood from decreasing. We

evaluate our methods in both reward and preference settings with Mistral-8×7B and

7B models. Experiments suggest that InfoNCA/NCA surpasses various preference

baselines when reward datasets are available. We also find NCA significantly

outperforms DPO in complex reasoning tasks like math and coding. Code: https:

//github.com/thu-ml/Noise-Contrastive-Alignment.

1

Introduction

Aligning pretrained Language Models (LMs) with scalar rewards that reflect human intentions is

crucial for enhancing their ability to follow instructions [35, 25]. These rewards can be given either

explicitly or implicitly. Explicit rewards can be scalar ratings of human annotators or advanced models

like GPT-4, while implicit rewards are usually preference labels assigned to pairwise responses.

One effective approach for aligning LMs with preference data is Direct Preference Optimization

(DPO, [33]). DPO applies a reward training loss but parameterizes the reward model as the response

likelihood ratio between two LMs, allowing for training reward models and extracting LM policies

simultaneously. This approach is more streamlined and thus more favorable compared with traditional

Reinforcement Learning (RL) methods [26], which typically require a two-stage training process:

first training reward models, then extracting LM policies.

Despite its simplicity and effectiveness, DPO is only tailored for preference data (x →{yw > yl}).

When multiple responses are available, directly assigning a scalar reward to each response is usually

more convenient and efficient than comparing them in a pairwise manner. The resulting reward

∗The corresponding author

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

arXiv:2402.05369v3  [cs.LG]  30 Oct 2024

## Page 2

Preference Data

## > B

## A

Reward Model

## A

7.8

## B

5.4

Policy

Reward Data

## A

8.0

## B

5.0

## C

3.5

Preference Data

## > B

## A

learn

prune

## A

8.0

## B

5.0

## C

3.5

## A

>

## B

>

## C

Reward Data

Preference Data

Special

Case

## Dpo

Figure 1: InfoNCA/NCA allows direct LM opti-

mization for both reward and preference data.

100

Epoch (%)

0

-1

-2

Relative Data Likelihood

ywin (DPO)

ylose (DPO)

ywin (NCA)

ylose (NCA)

100

Epoch (%)

0

1

Likelihood Margin

## Dpo

## Nca

Figure 2: Pairwise NCA prevents chosen likeli-

hood from decreasing while DPO cannot.

datasets (x →{yi, ri}1:K), however, cannot be directly leveraged for DPO training. Previous work

[40] usually prunes reward datasets by selecting the best response and pairing it with a random

remaining one. This is suboptimal as all reward values and additional dispreferred responses are

thrown away in its data-preprocessing process.

To address this issue, we present InfoNCA, an alignment method that allows directly extracting LM

policies from both reward datasets and preference datasets with arbitrary response numbers (Figure 1).

Notably, InfoNCA subsumes DPO loss as a special case under pairwise preference settings and can

thus be seen as a natural extension of DPO (Sec. 3.2). With strong theoretical guarantees, we show

DPO is a binary classification loss while InfoNCA is its multi-category version (Figure 3). However,

unlike DPO which is built upon assumptions of Bradley-Terry models or Plackett-Luce models,

InfoNCA is strictly derived from Information Noise Contrastive Estimation (InfoNCE, [24]), an

established contrastive method that is widely applied in language and visual representation learning

[31]. This closes the theoretical gap between current preference alignment methods and classic

contrastive learning frameworks.

A well-observed problem with DPO is that the likelihood of the preferred response tends to decrease

throughout training [27, 32]. Similar trends are shared by InfoNCA. We find this issue arises

mainly from InfoNCA/DPO’s focus on adjusting the relative likelihood across different responses

per instruction. In Sec. 4, we propose NCA as an alternative alignment method to InfoNCA to

mitigate this problem. NCA differs from InfoNCA by only loss definition and is also suitable for both

preference and reward datasets. However, NCA is built on NCE [14], a parallel contrastive learning

algorithm to InfoNCE, which optimizes the absolute data likelihood during training. In practice,

NCA effectively prevents the chosen likelihood from decreasing (Figure 2).

We evaluate our methods on Mistral-7B and 8×7B models from two dimensions. When reward

datasets [9] are available, we show that directly applying our reward-based alignment offers clear

improvement compared with preference-based algorithms, achieving higher evaluation rewards in

GPT-4 [47, 19] evaluations. We further validate this improvement comes from InfoNCA/NCA’s

ability to fully leverage the additional suboptimal responses. When only preference data is given

[44], we compare pairwise NCA against the DPO loss. Our experimental results spanning various

benchmarks show that NCA outperforms DPO in complex reasoning tasks such as math and coding.

Our main contributions: 1. We bridge the theoretical gap between DPO and classic contrastive

learning theories. InfoNCA and NCA are uniquely suited for both reward and preference data,

offering a general framework that integrates preference-based algorithms. 2. We show that suboptimal

responses are also important for LM optimization. Our method outperforms various preference

methods by fully exploiting data information in reward datasets. 3. NCA effectively mitigates the

data likelihood decline issue of DPO and offers practical performance improvement.

2

Background: Direct Preference Optimization

LM alignment is essentially a constrained policy optimization problem:

max

πθ Ep(x)



Eπθ(y|x)r(x, y) −αDKL (πθ(·|x)||µ(·|x))



,

(1)

where µ represents the pretrained LM. x and y are respectively instructions and responses. r is a

reward function that reflects human intentions. α is some temperature coefficient. Prior work [30, 29]

2

## Page 3

has proved that the optimal solution for the optimization problem in Eq. 1 is:

π∗(y|x) = µ(y|x)er(x,y)/α

Z(x)

∝µ(y|x)er(x,y)/α.

(2)

Direct Preference Optimization (DPO) [33] assumes we only have access to some pairwise preference

data x →{yw > yl} for each instruction x. The preference probability of human annotators is

modeled by a learnable implicit reward model rθ under Bradley-Terry theories [2]:

πθ(yw > yl|x) := σ(rθ(yw, x) −rθ(yl, x)),

where σ is the sigmoid function. To learn rθ, DPO simply adopts a binary classification loss:

LDPO = −E{x,yw>yl} log σ(rθ(yw, x) −rθ(yl, x)).

(3)

In practice, the latent function rθ is parameterized by the log-likelihood ratio between πθ and µ:

rθ(x, y) := β log πθ(y|x)

µ(y|x) ,

where β a linear coefficient for scaling rθ. This parameterization is crucial because it ensures

πθ(y|x) ∝µ(y|x)erθ(x,y)/β constantly hold. It transforms generative policy optimization into a

simple discriminative classification task: When rθ = r and β = α are satisfied, we naturally have

πθ = π∗.

3

InfoNCA: Extending DPO from Preference to Explicit Rewards

Compared with constructing preference datasets, annotating each response with scalar rewards can

be more flexible and convenient. Preference methods are only suitable for pairwise data (x →{yw >

yl}) and would require C2

K evaluations for comparing K responses. In contrast, reward datasets

(x →{yi, ri}1:K) allow an arbitrary number of responses per prompt with K evaluations.

Despite its simplicity in handling preference data, DPO is not tailored for reward datasets. We

introduce a new alignment method termed InfoNCA to mitigate this gap. We first strictly derive

InfoNCA in Sec. 3.1. We show that reward alignment can be solved by constructing a classification

problem to identify the optimal response from multiple candidates. We then demonstrate that

InfoNCA subsumes DPO as a special case and thus is a natural extension of DPO (Sec. 3.2).

3.1

Reward Alignment through Multi-Class Classification

In essence, DPO represents response rewards as LM likelihoods and constructs a binary classification

task for learning the reward model. Given that there are more than two (K > 2) responses per prompt

in reward datasets, we seek to construct a multi-class classification task for learning reward models

from explicit rewards instead of preference labels. We begin by formally defining this task:

Consider a batch of K responses {yi}1:K for an instruction x. {yi}1:K consists of one optimal

response yν that is sampled from π∗(y|x) ∝µ(y|x)er(x,y)/α, and K −1 suboptimal noises indepen-

dently sampled from µ(y|x). ν ∈1 : K is the random index of that optimal response. Our goal is to

identify which of the K candidates is yν, given only reward labels r(yi) for each response.

Intuitively, the response with higher rewards should have a higher probability of being the target

response. This intuition can be more rigorously expressed:

Proposition 3.1 (proof in Appendix A.1). Given the above K response candidates and their respec-

tive rewards, the posterior probability for the ν-th response being drawn from π∗is

p(ν|x, {yi}1:K) =

er(x,yν)

## Pk

i=1 er(x,yi) .

(4)

This finding is highly appealing because it shows response optimality is solely related to response

rewards. This allows us to conveniently train reward models rθ via maximum likelihood (MLE).

3

## Page 4

𝑦𝐴

r=9.0

Is 𝒚𝑨sampled from 𝛑∗?

…

𝑦𝐴

r=9.0

r=8.0

r=3.0

𝜋∗

𝜇

…

Which one of these

responses comes from

𝛑∗?

𝑦𝐴

𝑦B

𝑦B

Which

response is

preferred?

𝑦𝐵

𝑦𝑋

(a) DPO

(b) InfoNCA

(c) NCA

𝑝𝜃𝑦𝑖=

ⅇ𝑟𝜃𝑦𝑖

𝛴𝑗𝑒𝑟𝜃𝑦𝑗

𝑝𝜃𝑦𝑖= 𝜎𝑟𝜃𝑦i

𝑝𝜃𝑦𝐴= 𝜎𝑟𝜃𝑦𝐴−𝑟𝜃𝑦𝐵

𝜋∗

𝜇

Figure 3: DPO, InfoNCA, and NCA all optimize LLM through classification tasks. DPO compares

two responses and tells which one is preferred. InfoNCA compares multiple responses and identifies

the one sampled from π∗(Sec. 3.1). NCA predicts the model source of a single response (Sec. 4.1).

Theorem 3.2 (InfoNCA, proof in Appendix A.1). We define π∗(y|x) ∝µ(y|x)er(x,y)/α and

πθ(y|x) ∝µ(y|x)erθ(x,y). For any K > 1, α > 0, we have:

(a) Equivalent objective. The MLE objective for training rθ has an equivalent form:

max

θ

Ep(x,{yi}) log pθ(ν|x, {yi}) ⇔min

θ

−Ep(x) Q µ(yi|x)

## K

## X

i=1

"

er(x,yi)/α

Z(x)

log

erθ(x,yi)

## Pk

j=1 erθ(x,yj)

#

, (5)

where Z(x) = Eµ(y|x)er(x,y)/α.

(b) Optimal solution. Assume unlimited model capacity. The optimal rθ∗and πθ∗for solving (5) are

rθ∗(x, y) = r(x, y)/α + C(x),

and

πθ∗(y|x) = π∗(y|x) ∝µ(y|x)er(x,y)/α,

where C(x) is an arbitrary function conditioning on x.

In practical implementation of Eq. 5, we estimate Z(x) ≈1

## K

P erj/α and parameterize rθ(x, y) :=

β log πθ(y|x)

µ(y|x) following DPO (Eq. 2). The loss function becomes

LInfoNCA

θ

(x, {yi, ri}1:K) = −

## K

## X

i=1

h

eri/α

## Pk

j=1 erj/α

|

{z

}

soft labels

log

e

model logits

z }| {

rθ(x,yi)

## Pk

j=1 erθ(x,yj)

|

{z

}

predicted probability

i

rθ(x,y):=β log

πθ(y|x)

µ(y|x)

.

(6)

The loss function in Eq. 6 is termed InfoNCA, where A stands for Alignment. This naming reflects its

functional similarity to the Infomation Noise Contrastive Estimation (InfoNCE, [24]). Both methods

transform generative modeling problems into classification tasks by contrasting multiple data points.

How does InfoNCA work? InfoNCA loss (Eq. 6) can be seen as a K-category cross-entropy loss.

The soft label is calculated by dataset rewards through a softmax operation. The model’s predictions

are represented by learned reward rθ. The loss reaches 0 when rθ∗(x, y) = r(x, y)/α + C(x).

3.2

InfoNCA Subsumes DPO as A Special Case

Below we show that DPO is a special case of InfoNCA asymptotically. Specifically, setting response

number K = 2 and reward temperature α →0, we can fully recover the DPO objective:

LInfoNCA

θ

(x, {yi, ri}1:K) = −

## K

## X

i=1



1(ri = rmax) log

erθ(x,yi)

## Pk

j=1 erθ(x,yj)



(α →0)

= −log

erθ(x,yw)

erθ(x,yw) + erθ(x,yl)

(K = 2, suppose rw > rl)

= −log σ(rθ(x, yw) −rθ(x, yl))

(DPO loss, Eq. 3)

4

## Page 5

Alignment Method

InfoNCA (Sec. 3)

NCA (Sec. 4)

Modeling Target

π∗(y|x) ∝µ(y|x)er(x,y)/α

Model Definition

πθ(y|x) ∝µ(y|x)erθ(x,y)

πθ(y|x) = µ(y|x)erθ(x,y)

Reward Dataset

x →{yi, ri}1:K

Loss (K>1,α>0)

## −Pk

i=1



eri/α

## P

j erj /α log

erθ(x,yi)

## P

j erθ(x,yj )



## −Pk

i=1



eri/α

## P

j erj /α log σ(rθ(x, yi)) + 1

K log σ(−rθ(x, yi))



Preference Dataset

x →{yw > yl}

Loss (K=2, α→0)

−log σ(rθ(x, yw) −rθ(x, yl))(DPO)

−log σ(rθ(x, yw)) −1

2

## P

y∈{yw,yl} log σ(−rθ(x, y))

Loss Type

InfoNCE loss [24]

NCE loss [14]

Optimizing Target

relative value of log likelihood ratio

absolute value of log likelihood ratio

Optimal rθ∗(x, y)

r(x, y)/α + C(x)

r(x, y)/α −log Eµ(y|x)er(x,y)/α

rθ∗(x, ybest) ≥0 ?

not guaranteed

"

Table 1: Comparison of NCA and InfoNCA algorithm for aligning language models. Both reward

loss and pairwise preference loss are given. We provide pseudocode in Appendix B.

Empirical effect for varying hyperparameter K and α. As indicated by the derivation above, the

root difference between preference-based and reward-based methods lies in the choices of K and α.

K affects how accurately we can estimate the partition function Z(x) ≈PK

j=1 erj/α in Eq. 5. In

practice, we find larger K can lead to better performance (Sec. 5.1). α indicates a trade-off between

diversity and optimality. At a →0, the InfoNCA loss increases the likelihood only for the optimal

response and decreases it for all other responses, turning the reward dataset x →{yi, ri} into a

preference dataset x →{yw > yl}. We provide ablation studies of α and β in Appendix D.

4

NCA: Fixing Decreased Response Likelihood Issue for InfoNCA

A well-observed issue with DPO is that the likelihood of all responses continually decrease throughout

training [27, 32]. We find InfoNCA shares this trend due to their inherent equivalence. Decreased

data likelihood is concerning because it directly contradicts the maximum likelihood objective for

supervised training and may eventually harm performance [44].

We hypothesize the main cause of this decreasing likelihood is that InfoNCA methods only adjust

relative rewards among responses, rather than optimizing their absolute value. To address this

problem, we take inspiration from NCE, another contrastive learning method parallel to InfoNCE,

and propose NCA(lignment) (Sec. 4.1). Similar to InfoNCA, NCA can also guarantee convergence to

the optimal LM policy under ideal conditions (Theorem 4.2). However, it directly learns the absolute

reward for each response, thereby counteracting the decreasing likelihood trend (Sec. 4.2).

4.1

Reward Alignment through Absolute Reward Prediction

To avoid optimizing relative rewards across multiple responses, we construct a binary classification

task that deals with a single response.

Specifically, imagine sampling a response y randomly from either the optimal LM π∗(y|x) =

µ(y|x) er(x,y)/α

Z(x)

, or the pretrained LM µ(y|x). The marginal probability of y is p(y|x) := 1

2µ(y|x) +

1

2π∗(y|x). Our goal is to guess its model source when given a response y and its reward r(y).

Proposition 4.1 (proof in Appendix A.2). Let a binary variable ν = 1 indicates the response y is

sampled from π∗. The posterior probability of the distribution source given the response y satisfies:

p(ν = 1|x, y) =

π∗(y|x)

µ(y|x) + π∗(y|x) =

er(x,y)/α

Z(x) + er(x,y)/α .

(7)

Note that p(ν|x, y) is related to the partition function Z(x).

In order to represent model

likelihood pθ(ν|x, y) by only employing rθ similarly to Proposition 3.1, we have to redefine

5

## Page 6

πθ(y|x)=µ(y|x)erθ(x,y) by absorbing Zθ into rθ. Then we have

pθ(ν = 1|x, y) =

πθ(y|x)

µ(y|x) + πθ(y|x) = σ(rθ(x, y)).

(8)

Similarly to Theorem 3.2, we can derive a MLE-based training objective for optimizing rθ.

Theorem 4.2 (NCA, proof in Appendix A.2). Let α > 0, we have the maximum likelihood objective:

(a) Equivalent objective.

max

θ

Ep(x,y) log pθ(ν|x, y) ⇔min

θ

−Ep(x)µ(y|x)

her(x,y)/α

Z(x)

log σ(rθ(x, y)) + log σ(−rθ(x, y))

i

, (9)

where Z(x) = Eµ(y|x)er(x,y)/α.

(b) Optimal solution. Assume unlimited model capacity. The optimal rθ∗and πθ∗for solving (9) are

rθ∗(x, y) = r(x, y)/α −log Eµ(y|x)er(x,y)/α,

(10)

and

πθ∗(y|x) ∝µ(y|x)er(x,y)/α.

For reward datasets (x →{yi, ri}1:K), we estimate Z(x) ≈PK

i=1 eri/α in Eq. 9 and construct rθ

similarly to InfoNCA:

## Lnca

θ

(x, {yi, ri}1:K) = −

## K

## X

i=1



eri/α

## Pk

j=1 erj/α

|

{z

}

softmax weight

log σ(rθ(x, yi))

|

{z

}

optimizer ↑

(increasing force)

+ 1

K log σ(−rθ(x, yi))

|

{z

}

regularizer ↓

(decreasing force)



rθ(x,y):=β log

πθ(y|x)

µ(y|x)

(11)

How does NCA work? The loss function for NCA involves two opposing forces that jointly

determine the trend of increasing or decreasing rθ(x, y). Since log σ(·) is a monotonically increasing

function, the first term in Eq. 11 tends to increase rθ(x, y) while the second term tends to decrease it.

At the start of training, when rθ = −rθ = 0, the direction of the combined force for rθ is decided by

the difference in their weights, expressed as

eri/α

## Pk

j=1 erj /α −1

K . Responses with higher rewards would,

in principle, attain higher likelihood after training.

4.2

Connection between NCA and InfoNCA/DPO

Although both NCA and InfoNCA originate from solving a noise contrastive classification problem,

their optimization targets are markedly different (Table 1).

InfoNCA and DPO both calibrate relative values of reward models across various responses {yi}1:K

for an instruction x. In other words, the absolute value of rθ(x, y) is not directly constrained. This can

lead to some counterintuitive behaviors. For instance, the learned reward for even the highest-reward

response could decrease over time without contradicting the loss definition, as long as the reward

margin keeps increasing. This could lead to poor performance or training instability (Sec. 5.2).

In contrast, NCA specifically focuses on optimizing absolute values of the reward model. This

characteristic is determined by its model definition: πθ(y|x)=µ(y|x)erθ(x,y), where rθ has to be

self-normalized: Eµ(y|x)erθ(x,y) = 1. In practice, NCA effectively prevents the likelihood of the

preferred responses from decreasing. We find this is particularly helpful for math and coding tasks.

5

Experiments

We mainly seek to answer two questions in our experiments:

1. If we have access to reward-annotated datasets with >2 responses per prompt, does InfoNCA

or NCA offer empirical improvement compared with preference-based approaches that

simply prune reward datasets into preference datasets? (Sec. 5.1)

2. If only pairwise preference data is available, when should one choose NCA over DPO?

What benefits does NCA offer? (Sec. 5.2) Note that InfoNCA is exactly DPO in this setting.

6

## Page 7

Name

Annotation Type

MT-bench

AlpacaEval

Win vs. DPO

Baseline

Mixtral-7B-sft

SFT Data

6.45

85.20

-

## +Kto [11]

Preference

7.12

91.93

-

## +Ipo [1]

Preference

7.45

90.62

-

+DPO (Zephyr-β)

Preference

7.34

90.60

50.0

## +Dpo×3

Preference

7.22

91.60

58.1

## +Dpo×C2

4

Preference

7.38

90.29

48.1

Ours

+InfoNCA

Reward

7.63

92.35

56.9

## +Nca

Reward

7.52

90.31

59.4

Reference

Mixtral-ORPO-β

Preference+SFT

7.32

91.41

-

Mistral-7B-instruct

SFT Data

6.84

92.78

-

LLaMA2-chat-70b

Reward Model

6.86

92.66

-

## Gpt-4

Reward Model

9.18

93.78

-

Table 2: Comparison between reward-based methods (InfoNCA, NCA) and preference-based methods

(DPO, IPO, etc.) in LLM alignment. We focus on the general instruction-following abilities of

each method measured by GPT-4 evaluations and human preference. The highest number in each

benchmark is bolded and the second highest is underlined.

5.1

Aligning Language Models with Explicit Rewards

Reward dataset and Evaluation metric. We consider UltraFeedback [9], an instruction-following

dataset annotated by GPT-4. This dataset comprises ∼64k instructions. Each instruction has 4

responses generated by various LMs. GPT-4 rates each response with a scalar reward on a scale

of 0-10. Prior research indicates that these GPT-4 rewards closely align with human annotations

[47], establishing them as an efficient, cost-effective alternative to human feedback. In order to align

exactly with the definition of dataset rewards, we similarly choose well-acknowledged GPT4-based

benchmarks like MT-bench [47] and AlpacaEval [19] for evaluation. Human preference studies are

also conducted on evaluation prompts from MT-bench. The rating system is in Appendix C.

InfoNCA and NCA outperform preference-based methods given reward dataset. To handle

reward datasets with K > 2 responses per instruction, one approach is to simply prune them into

pairwise data and apply preference learning like DPO. For instance, Zephyr [40] selects the highest-

reward response and a random remaining one from UltraFeedback for each instruction. This procedure

discards two additional suboptimal responses in the dataset as well as their reward information.

In Table 2, we fine-tune a Mistral-7B model on UltraFeedback and compare InfoNCA/NCA against

the DPO baseline. Results show that our methods outperform preference baselines. This improvement

can be attributed to InfoNCA/NCA’s ability to exploit all information in the reward dataset.

Method

## K=2

## K=3

## K=4

InfoNCA (MT-bench)

73.8

75.9

76.3

InfoNCA (Alpaca)

90.7

90.2

92.4

NCA (MT-bench)

73.2

73.3

75.2

NCA (Alpaca)

89.9

90.3

90.3

Average

81.9

82.4

83.5

0.0

0.5

1.0

1.5

2.0

## Kl(

|| )

6.5

7.0

7.5

MT-bench Score

## K=2

## K=4

Figure 4: More suboptimal responses can also increase LLM’s instruction-following ability. We

fix the highest-reward response in the UltraFeedback dataset and ablate the number of suboptimal

responses per prompt, resulting in different contrastive response numbers K during training. Left:

Evaluation results under the same set of hyperparameters. Right: Performance-KL trade-off under

various α and β. Each dot represents an independent experiment trained for 1 epoch.

Suboptimal responses are also important. Previous practices always ensure selecting the highest-

performing response when constructing preference data. The assumption behind this strategy is

that the dataset’s best-performing response determines the upper limit of alignment performance.

However, our experiments contradict this assumption. Results in Figure 4 indicate that extra subop-

timal responses can also be advantageous for policy training. Specifically, we observe consistent

7

## Page 8

Model

Reasoning

Coding

Math

Avg.

BBH (CoT)

LeetCode

HumanEval

## Gsmplus

## Math

TheoremQA

## Svamp

ASDiv

Mixtral-7B-SFT

60.9

3.3

28.1

28.5

5.8

7.0

26.9

35.8

24.5

## + Dpo

61.7

= 2.2 ↓

31.7

= 12.1 ↓

6.4

9.8

34.1

46.1

25.5

## + Nca

= 60.8 ↓

3.3

= 26.8 ↓

32.3

11.7

11.0

65.3

74.3

35.7

Mixtral-8×7B-SFT

75.6

16.7

61.0

57.6

40.1

25.9

85.9

87.5

56.3

## + Dpo

= 74.9 ↓

17.2

= 47.6 ↓

= 55.8 ↓

= 35.3 ↓

26.9

= 67.3 ↓

= 75.7 ↓

=50.1↓

## + Nca

75.6

21.1

62.8

61.5

41.6

26.9

86.8

86.9

57.9

Table 3: Alignment results for UltraInteract. We mark numbers that have decreased (↓) after training.

performance improvements when increasing the number of data responses from K = 2 to K = 4 for

both InfoNCA and NCA algorithms, across various hyperparameters.

Combinatorial DPO are suboptimal solutions. Regarding the performance improvement offered by

more suboptimal responses, one might predict that applying the DPO to a combinatorially constructed

preference dataset would yield results comparable to NCA/InfoNCA. To investigate this, we examined

two variants of DPO that utilize all available responses in UltraFeedback.

DPO×3: We pair the highest-performing response with each of the remaining three separately.

## Dpo×C2

4: We sum up all DPO loss possibilities for two out of the four responses.

Our experiments, detailed in Table 2, reveal that naively applying combinatorial DPO loss to leverage

all response information underperforms InfoNCA/NCA. The DPO×3 shows some benefit, while

## Dpo×C2

4 is harmful compared with simple data pruning. This is expected because InfoNCA and

NCA possess theoretical guarantees (Theorem 3.2 and Theorem 4.2) that ensure convergence to the

optimal LM policy whereas combinatorial preference methods do not.

5.2

NCA vs. DPO in Aligning Language Models with Pairwise Preference

In previous experiments, our focus is on the reward dataset with K > 2 responses per prompt

(x →{yi, ri}1:K). However, at present most alignment datasets are pairwise (x →{yw > yl}),

making it essential also to evaluate our proposed methods in pairwise preference settings.

Since InfoNCA is equivalent to DPO when only pairwise preference data is available (Sec. 3.2), we

will focus on comparing and clarifying the differences between the DPO and NCA algorithms.

Preferecne dataset and evaluation metrics. We consider fine-tuning Mistral-7B and Mistral-8×7B

models on UltraInteract [44], a pairwise alignment dataset specifically designed for complex reasoning

tasks. Before alignment, we perform SFT on UltraInteract’s preferred responses for the 8×7B model

and use the existing Mistral-SFT model in Sec. 5.1. We evaluate the model’s performance in various

challenging tasks. This includes BBH-Hard [39] for CoT reasoning, HumanEval [4] and LeetCode

[12] for coding, GSM-Plus [18], MATH, TheoremQA [6], SVAMP [28], and ASDiv [22] for math.

DPO may hurt reasoning performance while NCA helps. Results are presented in Table 3. Overall,

NCA consistently outperforms DPO in various benchmarks. Notably, we observe DPO hurts the

overall performance in most reasoning tasks regarding the Mixtral-8×7B-SFT model. This indicates

that DPO might not be suitable for improving reasoning abilities, which echoes findings in concurrent

work [44]. In contrast, NCA shows clear improvement on both the 7B and 8×7B models.

100

Step (%)

-7.5

-5.0

-2.5

0.0

Model Reward

DPO (UltraInteract)

Chosen

Rejected

100

-2.0

-1.0

0.0

Model Reward

NCA (UltraInteract)

Chosen

Rejected

100

Step (%)

-2.0

-1.0

0.0

Model Reward

InfoNCA (UltraFeedback)

y1

y2

y3

y4

100

Step (%)

-0.6

-0.4

-0.2

0.0

Model Reward

NCA (UltraFeedback)

y1

y2

y3

y4

Figure 5: Comparision of data likelihood between InfoNCA/DPO and NCA.

NCA prevents the chosen-likelihood from decreasing. What distinct optimization characteristics

could cause performance differences between pairwise NCA and DPO? To understand this, we

empirically inspect how the data likelihood changes during training. As shown in Figure 5. The

likelihood of preferred responses interestingly decreases after DPO training and increases for NCA

training. This pattern is consistent across both preference and reward learning. The decreasing chosen-

likelihood trend is concerning because it directly contradicts the maximum-likelihood objective used

8

## Page 9

0.0

1.0

2.0

3.0

4.0

## Kl(

|| )

5.0

6.0

7.0

MT-bench Score

Collapsed Experiments

## Dpo

## Nca

0.0

1.0

2.0

3.0

4.0

5.0

## Kl(

|| )

5.0

6.0

7.0

MT-bench Score

InfoNCA

## Nca

Figure 6: NCA is more robust to hyperparameter changes and KL derivations. Left: Ablation results

of α and β for UltraFeedback-binarized. Right: Results for UltraFeedback-reward (K = 4).

during the SFT stage. This drawback is exacerbated in reasoning tasks, where the preferred response

is often the ground truth answer. Consequently, we hypothesize that NCA’s superior performance in

reasoning tasks is due to its ability to avoid decreasing chosen likelihood.

Since DPO is essentially a specialization of InfoNCA, their contrasting likelihood trends can be

explained theoretically. As we have elaborated in Sec. 4.2, NCA adjusts the absolute likelihood

of data, while DPO/InfoNCA only considers relative likelihood across different responses. Thus, a

declining chosen likelihood directly contradicts NCA’s training objective but not DPO’s.

Empirical takeaway: When to choose NCA over DPO? DPO and pairwise NCA have similar

theoretical guarantees. Their different performance in alignment tasks is largely empirical, depending

on the specific characteristics of datasets and the nature of tasks. Our observations show that NCA is

more suitable for reasoning tasks such as math and coding (Table 3), where high-quality responses

are sparse, and adhering closely to the preferred responses in the dataset is critical. DPO may be more

suitable for general instruction-following tasks like summarization/role-playing (Table 2), where

datasets only reflect human relative preference but do not contain "golden" answers. In essence, NCA

benefits from better dataset regularization, while DPO relies more on LLMs’ generalization abilities.

We also observe that NCA has a greater tolerance for divergence from the initial SFT policy and

is more robust to hyperparameter changes. As evidenced in Figure 6, we conduct a grid search on

α and β. DPO can drastically fail to improve alignment performance if the learned policy strays

too far from the SFT policy, and may randomly collapse under certain hyperparameters. In contrast,

the NCA method does not exhibit similar issues. These observations suggest that NCA may be a

worthwhile alternative if DPO training is unstable.

6

Related Work

Language model alignment. Current approaches cater to either explicit reward data or preference

data, often lacking the versatility to address both concurrently. Reinforcement Learning [34] is

inherently suitable for explicit reward scenarios. However, its on-policy nature necessitates learning a

reward model from data first, leading to an indirect two-stage optimization process [8, 26, 36]. Recent

developments in preference-based alignment techniques [33, 1, 11, 42, 16, 13] have streamlined

this process. They enable direct alignment of LMs through a singular loss, but this comes at the

expense of being confined to pairwise preference data. Other alignment approaches [43, 37, 45, 38]

are also not tailored for aligning with reward datasets. Recent work [3] attempts to extend DPO’s

parameterization technique to explicit reward contexts. However, it only considers binary rewards. In

comparison, our methods can handle both continuous rewards and preference data.

Noise contrastive estimation. NCE [14] and its variant, InfoNCE [24], are established optimization

methods for training unnormalized generative models [21]. NCE primarily leverages a binary

classification loss and can be applied in self-supervised representation learning. Examples are

Word2Vec [23], MoCo [15], and SimCLR [5]. InfoNCE is related to maximizing mutual information

between two distributions through a multiclass cross-entropy loss. It has successful applications in

representation learning, such as CLIP [31]. It is also widely used in language modeling [7], diffusion

modeling [20], and reinforcement learning [17].

9

## Page 10

7

Conclusion

In this work, we formally consider the language model alignment problem in the context of explicit

reward settings. By adeptly harnessing the NCE and InfoNCE theories, we introduce two practical

algorithms: NCA and InfoNCA. Our proposed methods are uniquely suited for both reward data and

preference data, including DPO as a special case. Our experiments show that reward-based alignment

methods outperform preference baseline by fully leveraging suboptimal responses in reward datasets.

In preference settings, pairwise NCA outperforms DPO in complex reasoning tasks by effectively

preventing data likelihood from decreasing.

Acknowledgments and Disclosure of Funding

We especially thank Cheng Lu, who greatly inspires us in linking the NCA algorithm with NCE theo-

ries. We also thank Jiuhai Chen, and Tianlin Liu for their suggestions on Zephyr result reproduction.

We thank Bingrui Li, and Weiyu Huang for their help with the experimental setup. We thank Github

user Wing Lian for integrating the NCA algorithm into the trl library. We thank Haosheng Zou for

providing feedback on our method.

This work was supported by NSFC Projects (Nos. 62350080, 92370124, 92248303, 62276149,

62061136001, 62076147), BNRist (BNR2022RC01006), Tsinghua Institute for Guo Qiang, and the

High Performance Computing Center, Tsinghua University. J. Zhu was also supported by the XPlorer

Prize.

10

## Page 11

References

[1] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland,

Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning

from human preferences. In International Conference on Artificial Intelligence and Statistics,

pages 4447–4455. PMLR, 2024.

[2] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the

method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.

[3] Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, and Guannan Zhang. Ulma: Unified

language model alignment with demonstration and point-wise human preference. arXiv preprint

arXiv:2312.02554, 2023.

[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,

Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul

Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke

Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad

Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias

Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex

Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,

William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,

Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,

Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech

Zaremba. Evaluating large language models trained on code, 2021.

[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework

for contrastive learning of visual representations. In International conference on machine

learning, pages 1597–1607. PMLR, 2020.

[6] Wenhu Chen, Ming Yin, Max W.F. Ku, Yixin Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi

Wang, and Pan Lu. Theoremqa: A theorem-driven question answering dataset. ArXiv preprint,

abs/2305.12524, 2023.

[7] Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-

Ling Mao, Heyan Huang, and Ming Zhou. InfoXLM: An information-theoretic framework

for cross-lingual language model pre-training. In Proceedings of the 2021 Conference of the

North American Chapter of the Association for Computational Linguistics: Human Language

Technologies, pages 3576–3588, Online, June 2021. Association for Computational Linguistics.

[8] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep

reinforcement learning from human preferences. Advances in neural information processing

systems, 30, 2017.

[9] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong

Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai

feedback. In Forty-first International Conference on Machine Learning, 2024.

[10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient

finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.

[11] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:

Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.

[12] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,

Xiao Bi, Yu Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When

the large language model meets programming - the rise of code intelligence. ArXiv preprint,

abs/2401.14196, 2024.

[13] Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing

Xie, Jie Zhou, Yankai Lin, et al. Controllable preference optimization: Toward controllable

multi-objective alignment. arXiv preprint arXiv:2402.19085, 2024.

[14] Michael U Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized

statistical models, with applications to natural image statistics. Journal of machine learning

research, 13(2), 2012.

11

## Page 12

[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for

unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on

computer vision and pattern recognition, pages 9729–9738, 2020.

[16] Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without

reference model. arXiv preprint arXiv:2403.07691, 2024.

[17] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised represen-

tations for reinforcement learning. In International Conference on Machine Learning, pages

## 5639–5650. Pmlr, 2020.

[18] Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: A comprehen-

sive benchmark for evaluating the robustness of llms as mathematical problem solvers. ArXiv

preprint, abs/2402.19255, 2024.

[19] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy

Liang, and Tatsunori B Hashimoto. AlpacaEval: An automatic evaluator of instruction-following

models, 2023.

[20] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive

energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning.

In Proceedings of the 40th International Conference on Machine Learning, 2023.

[21] Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for

conditional models: Consistency and statistical efficiency. arXiv preprint arXiv:1809.01812,

2018.

[22] Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and

developing English math word problem solvers. In Proc. of ACL, 2020.

[23] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-

sentations of words and phrases and their compositionality. Advances in neural information

processing systems, 26, 2013.

[24] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive

predictive coding. arXiv preprint arXiv:1807.03748, 2018.

[25] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,

Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to

follow instructions with human feedback. Advances in Neural Information Processing Systems,

35:27730–27744, 2022.

[27] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White.

Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint

arXiv:2402.13228, 2024.

[28] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple

math word problems? In Proceedings of the 2021 Conference of the North American Chapter

of the Association for Computational Linguistics: Human Language Technologies, 2021.

[29] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:

Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.

[30] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for

operational space control. In Proceedings of the 24th international conference on Machine

learning, pages 745–750, 2007.

[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,

Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual

models from natural language supervision. In International Conference on Machine Learning,

pages 8748–8763. PMLR, 2021.

[32] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q*: Your language model

is secretly a q-function. arXiv preprint arXiv:2404.12358, 2024.

[33] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and

Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.

In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

12

## Page 13

[34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[35] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan

Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing

language models for dialogue. OpenAI blog, 2022.

[36] Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, and Yang Liu. Improv-

ing reinforcement learning from human feedback using contrastive rewards. arXiv preprint

arXiv:2403.07708, 2024.

[37] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.

Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.

[38] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.

Preference ranking optimization for human alignment. In Proceedings of the AAAI Conference

on Artificial Intelligence, 2024.

[39] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won

Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei.

Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv preprint,

abs/2210.09261, 2022.

[40] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes

Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr:

Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.

[41] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan

Lambert, and Shengyi Huang. TRL: Transformer reinforcement learning, 2020.

[42] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl:

Generalizing direct preference optimization with diverse divergence constraints. arXiv preprint

arXiv:2309.16240, 2023.

[43] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF:

Rank responses to align language models with human feedback. In Thirty-seventh Conference

on Neural Information Processing Systems, 2023.

[44] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin

Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference

trees. arXiv preprint arXiv:2404.02078, 2024.

[45] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. SLIC-

HF: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425,

2023.

[46] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J

Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh

International Conference on Learning Representations, 2022.

[47] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,

Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and

chatbot arena. Advances in Neural Information Processing Systems, 36:46595–46623, 2023.

13

## Page 14

## A

Proof of Theorems

## A.1

InfoNCA Objective

Recall that our optimal language policy is

π∗(y|x) = µ(y|x)er(x,y)/α

Z(x)

.

(Eq. 2)

Consider a batch of K responses {yi}1:K for an instruction x. {yi}1:K consists of one optimal

response yν that is sampled from π∗(y|x) ∝µ(y|x)er(x,y)/α, and K −1 suboptimal noises indepen-

dently sampled from µ(y|x). ν ∈1 : K is the random index of that optimal response. The the joint

probability for {yi}1:K is

pjoint({yi}1:K|x, ν) = π∗(yν|x)

## Y

i̸=ν

µ(yi|x) = π∗(yν|x)

µ(yν|x)

## K

## Y

i=1

µ(yi|x).

Given that the prior satisfies p(ν = 1) = p(ν = 2) = ... = p(ν = K) = 1

K , the data posterior is

pjoint(ν|x, {yi}1:K) =

π∗(yν|x)/µ(yν|x)

## Pk

j=1 π∗(yj|x)/µ(yj|x)

.

p(O = yi|{y}1:K) =

p({y}1:K|O = yi)p(O = yi)

## Pk

j=1 p({y}1:K|O = yj)p(O = yj)

=

π∗(yi|x)/µ(yi|x)

## Pk

j=1 π∗(yj|x)/µ(yj|x)

=

er(yi)/α

## Pk

j=1 er(yj)/α

Define model policy as

πθ(y|x) := µ(y|x)erθ(x,y)

Zθ(x) .

The model posterior probability satisfies

pjoint

θ

(ν|x, {yi}1:K) =

erθ(x,yν)

## Pk

i=1 erθ(x,yi) .

Theorem A.1 (InfoNCA Objective). For any K > 1, α > 0, we have the following results.

(a) Equivalent objective.

min

θ

Epjoint(x,{yi})DKL[pjoint(ν|x, {yi})||pjoint

θ

(ν|x, {yi})]

⇐⇒min

θ

−Ep(x) Q µ(yi|x)

## K

## X

i=1

er(x,yi)/α

Z(x)

log

erθ(x,yi)

## Pk

j=1 erθ(x,yj) ,

(Eq. 5)

where Z(x) = Eµ(y|x)er(x,y)/α.

(b) Optimal solution. Assume unlimited model capacity and data samples. The optimal rθ∗and πθ∗

for solving Eq. 5 are

rθ∗(x, y) = r(x, y)/α + C(x),

and

πθ∗(x, y) ∝µ(y|x)er(x,y)/α,

where C(x) is an arbitrary function conditioning on x.

14

## Page 15

Proof. (a) Equivalent objective.

min

θ

Epjoint(x,{yi})DKL[pjoint(ν|x, {yi})||pjoint

θ

(ν|x, {yi})]

⇔min

θ

Epjoint(x,{yi})Epjoint(ν|x,{yi}) log pjoint(ν|x, {yi})

pjoint

θ

(ν|x, {yi})

⇔min

θ

−Epjoint(x,{yi})Epjoint(ν|x,{yi}) log pjoint

θ

(ν|x, {yi})

⇔min

θ

−Ep(x)p(ν)pjoint({yi}|x,ν) log pjoint

θ

(ν|x, {yi})

(Bayes’ rule)

⇔min

θ

−Ep(x)p(ν) QK

i=1 µ(yi|x)

π∗(yν|x)

µ(yν|x) log pjoint

θ

(ν|x, {yi})

(importance sampling)

⇔min

θ

−Ep(x) QK

i=1 µ(yi|x)



Ep(ν)

π∗(yν|x)

µ(yν|x) log pjoint

θ

(ν|x, {yi})



⇔min

θ

−Ep(x) QK

i=1 µ(yi|x)

"

1

## K

## K

## X

ν=1

π∗(yν|x)

µ(yν|x) log pjoint

θ

(ν|x, {yi})

#

⇔min

θ

−Ep(x) QK

i=1 µ(yi|x)

## " K

## X

ν=1

er(x,yν)/α

Z(x)

log pjoint

θ

(ν|x, {yi})

#

(based on Eq. 2)

⇔min

θ

−Ep(x) QK

i=1 µ(yi|x)

## " K

## X

i=1

er(x,yi)/α

Z(x)

log

erθ(x,yi)

## Pk

j=1 erθ(x,yj)

#

(change sum index)

(b) Optimal solution.

Given conclusions from (a). With unlimited model capacity, pjoint

θ

(ν|x, {yi}) could represent any

discrete distribution, such that we can arrive at the global optimal point given infinite training data.

Epjoint(x,{yi})DKL[pjoint(ν|x, {yi})||pjoint

θ∗(ν|x, {yi})] = 0

=⇒

pjoint(ν|x, {yi}) = pjoint

θ∗(ν|x, {yi})

∀x, ν, {yi}1:K

=⇒

π∗(yν|x)/µ(yν|x)

## Pk

i=1 π∗(yi|x)/µ(yi|x)

=

erθ∗(x,yν)

## Pk

i=1 erθ∗(x,yi)

∀x, ν, {yi}1:K

=⇒

rθ∗(x, y) = r(x, y)/α + C(x)

∀x, y, C

=⇒

πθ∗(x, y) ∝µ(y|x)er(x,y)/α

∀x, y

## A.2

NCA Objective

Recall the optimal language policy is

π∗(y|x) = µ(y|x)er(x,y)/α

Z(x)

.

(Eq. 2)

Consider a response y randomly sampled from either the optimal LM π∗(y|x) = µ(y|x) er(x,y)/α

Z(x)

, or

the pretrained LM µ(y|x). Let a binary variable ν = 1 indicates the response y is sampled from π∗.

Then the marginal distribution of y is

pjoint(y|x) := p(ν = 0)µ(y|x) + p(ν = 1)π∗(y|x).

Given the prior p(ν = 0) = p(ν = 1) = 1

2, using Bayes’ Rule, the data posterior satisfies

pjoint(ν = 0|x, y) =

µ(y|x)

µ(y|x) + π∗(y|x).

15

## Page 16

pjoint(ν = 1|x, y) =

π∗(y|x)

µ(y|x) + π∗(y|x).

Define model policy as πθ(y|x) := µ(y|x)erθ(x,y). The model posterior probability satisfies

pjoint

θ

(ν = 1|x, y) = σ(rθ(x, y)).

pjoint

θ

(ν = 0|x, y) = 1 −σ(rθ(x, y)) = σ(−rθ(x, y)).

(Eq. 8)

Theorem A.2 (NCA Objective). For any α > 0, we have the following results.

(a) Equivalent objective.

min

θ

Epjoint(x,y)DKL[pjoint(ν|x, y)||pjoint

θ

(ν|x, y)]

⇐⇒min

θ

−Ep(x)µ(y|x)

er(x,y)/α

Z(x)

log σ(rθ(x, y)) + log σ(−rθ(x, y)),

(Eq. 9)

where Z(x) = Eµ(y|x)er(x,y)/α.

(b) Optimal solution. Assume unlimited model capacity and data samples. The optimal rθ∗and πθ∗

for solving Eq. 5 are

rθ∗(x, y) = r(x, y)/α −log Eµ(y|x)er(x,y)/α,

and

πθ∗(x, y) ∝µ(y|x)er(x,y)/α.

Proof. (a) Equivalent objective.

min

θ

Epjoint(x,y)DKL[pjoint(ν|x, y)||pjoint

θ

(ν|x, y)]

⇔min

θ

Epjoint(x,y)Epjoint(ν|x,y) log pjoint(ν|x, y)

pjoint

θ

(ν|x, y)

⇔min

θ

−Epjoint(x,y)Epjoint(ν|x,y) log pjoint

θ

(ν|x, y)

⇔min

θ

−Ep(x)p(ν)pjoint(y|x,ν) log pjoint

θ

(ν|x, y)

(Bayes’ rule)

⇔min

θ

−[p(ν = 0)Ep(x)pjoint(y|x,ν=0) log pjoint

θ

(ν = 0|x, y)+

p(ν = 1)Ep(x)pjoint(y|x,ν=1) log pjoint

θ

(ν = 1|x, y)]

⇔min

θ

−



Ep(x)µ(y|x) log σ(−rθ(x, y)) + Ep(x)π∗(y|x) log σ(rθ(x, y))



(by Eq. 8)

⇔min

θ

−Ep(x)µ(y|x)

er(x,y)/α

Z(x)

log σ(rθ(x, y)) + log σ(−rθ(x, y))

(importance sampling)

(b) Optimal solution.

Given conclusions from (a). With unlimited model capacity, pjoint

θ

(ν|x, y) could represent any discrete

distribution, such that we can arrive at the global optimal point given infinite training data.

Epjoint(x,y)DKL[pjoint(ν|x, y)||pjoint

θ∗(ν|x, y)] = 0

=⇒

pjoint(ν|x, y) = pjoint

θ∗(ν|x, y)

∀x, ν, y

=⇒

pjoint(ν = 1|x, y) = pjoint

θ∗(ν = 1|x, y)

∀x, y

=⇒

π∗(y|x)

µ(y|x) + π∗(y|x) = σ(r∗

θ(x, y)) =

er∗

θ(x,y)

1 + er∗

θ(x,y)

∀x, y

=⇒

er∗

θ(x,y) = π∗(y|x)

µ(y|x)

∀x, y

=⇒

rθ∗(x, y) = r(x, y)/α −log Eµ(y|x)er(x,y)/α

∀x, y

=⇒

πθ∗(x, y) ∝µ(y|x)er(x,y)/α

∀x, y

16

## Page 17

## B

Pseudocode

PyTorch code for the InfoNCA/NCA loss for reward datasets is provided below:

import torch.nn.functional as F

def reward_loss(pi_logps, ref_logps, rewards, alpha, beta, loss_type):

"""

pi_logps: policy logprobs for K responses, shape (B, K)

ref_logps: reference logprobs for K responses, shape (B, K)

rewards: reward labels for K responses, shape (B, K)

alpha: the reward temperature controlling strength of KL penalty

beta: the parameterization coefficient that defines the reward model

loss_type: could be either "InfoNCA" or "NCA" loss

"""

soft_labels = (rewards / alpha).softmax(dim=-1) # (B, K)

model_rewards = (pi_logps - ref_logps) * beta # (B, K)

if loss_type == "InfoNCA":

model_logps = model_rewards.log_softmax(dim=-1) # (B, K)

losses = - (soft_labels * model_logps).sum(dim=-1) # (B,)

elif loss_type == "NCA":

optimization = - (soft_labels * F.logsigmoid(model_rewards)).sum(dim=-1) # (B,)

regularization = - F.logsigmoid(-model_rewards).mean(dim=-1) # (B,)

losses =

optimization + regularization # (B,)

return losses.mean()

The loss implementation under pairwise preference settings is equivalent to reward losses with K = 2

and α →0. We provide the code separately for easy comparison with DPO.

def preference_loss(chosen_pi_logps, chosen_ref_logps,

rejected_pi_logps, rejected_ref_logps,

beta, loss_type):

"""

chosen_pi_logps: policy logprobs for the preferred responses, shape (B, )

chosen_ref_logps: reference logprobs for the preferred responses, shape (B, )

rejected_pi_logps: policy logprobs for the dispreferred responses, shape (B, )

rejected_ref_logps: reference logprobs for the dispreferred responses, shape (B, )

beta: the parameterization coefficient that defines the reward model

loss_type: one of "InfoNCA", "NCA" or "DPO" loss

"""

chosen_rewards = (chosen_pi_logps - chosen_ref_logps) * beta # (B,)

rejected_rewards = (rejected_pi_logps - rejected_ref_logps) * beta # (B,)

if loss_type in ["DPO", "InfoNCA"]:

losses = -F.logsigmoid(chosen_rewards - rejected_rewards) # (B,)

elif loss_type == "NCA":

losses = - F.logsigmoid(chosen_rewards) \

- 0.5 * F.logsigmoid(-chosen_rewards) \

- 0.5 * F.logsigmoid(-rejected_rewards) # (B,)

return losses.mean()

17

## Page 18

## C

Experimental Details

Experiments with UltraFeedback. Our implementation is heavily based on the Transformer

Reinforcement Learning (TRL) library [41] and Zephyr’s official code base [40]. All models are

fine-tuned from the publicly accessible HuggingFaceH4/mistral-7B-SFT-beta model. Experiments

are run on Nvidia A40 or RTX 4090 GPUs using bfloat16 precision. We ablate β ∈{3e −4, 1e −

3, 3e−3, 1e−2, 3e−2, 1e−1, 3e−1, 1.0} and α ∈{0.01, 0.1, 0.33, 1.0, 3.33}. The default reward

temperature α is 0.01. The default parameterization coefficient β is also 0.01. We adopt the QLoRA

[10] fine-tuning technique with rank 16, αlora = 16, and a dropout rate of 0.05. We train all models

for 1 epoch. The batch size is 32. We use an AdamW optimizer with a learning rating of 5e-6. For

KTO and IPO baselines, we adopt exactly the same training pipeline for reporting their performance

except that we tune the β ∈{0.01, 0.1, 0.3, 0.5, 1.0}. We find the most suitable beta for KTO is 0.01,

and for IPO is 0.5.

Experiments with UltraInteract. We follow [44] and fine-tune all parameters of the pretrained

model with UltraInteract and UltraFeedback. Specifically, for HuggingFaceH4/mistral-7B-SFT-beta

model, we directly adopt the recipe of Eurus, with the same data mixture (all 220K multi-turn

trajectory pairs in UltraInteract and all 340K pairs in UltraFeedback) and hyperparameters. All β is

set to 0.1. The model is trained for 1 epoch with a cosine schedule. The learning rate is 5e-7 and the

warmup ratio is 0.1. For Mixtral-8x7B, we first SFT the model with the recipe for Eurus-70B-SFT,

and then applied the same preference learning recipe as aforementioned.

Figure 7: Rating system screenshot. Our rating system is adapted from [47].

18

## Page 19

## D

Additional Experiment Results

1.0

3e-1

1e-1

3e-2

1e-2

3e-3

1e-3

3e-4

Reparameterization Coefficient

3.33

1.0

0.33

0.1

0.01

Reward Tempearture

6.32

6.48

6.96

6.94

7.17

7.20

6.85

6.37

6.33

6.78

6.95

7.19

7.30

7.12

6.34

6.11

6.58

6.76

7.00

7.28

7.34

7.20

6.30

5.63

6.48

6.67

6.81

7.28

6.86

7.18

6.08

5.81

6.57

6.76

6.94

7.19

7.40

6.97

5.60

5.16

1.0

3e-1

1e-1

3e-2

1e-2

3e-3

1e-3

3e-4

Reparameterization Coefficient

3.33

1.0

0.33

0.1

0.01

Reward Tempearture

6.43

6.47

6.60

6.91

7.05

7.04

7.16

7.30

6.35

6.68

6.85

7.01

7.27

7.20

7.04

7.1

6.32

6.72

6.98

7.26

7.32

7.18

7.45

7.15

6.44

6.68

6.88

7.20

7.30

7.16

7.10

7.14

6.60

6.65

6.92

7.04

7.42

7.29

7.16

6.96

Figure 8: MT-bench score for InfoNCA (left) and NCA (right) regarding various α and β. Results

are averaged for K = 2 and K = 4 experiments. Overall, model performance is relatively more

sensitive to variations of β compared with α. NCA shows greater tolerance to hyperparameter

variations.

Method

Functional Form

MT-bench

KL Divergence

Mistral-7B-SFT

−

6.45

0.00

## +Dpo

rθ = β log πθ

µ

7.34

0.53

+InfoNCA

rθ = β log πθ

µ

7.63

0.47

rθ = β log πθ

4.65

4.39

rθ = β log πθ + SFT regularization

7.15

0.26

## +Nca

rθ = β log πθ

µ

7.52

0.39

rθ = β log πθ

5.82

3.32

rθ = β log πθ + SFT regularization

6.88

0.09

Table 4: Comparison of different functional forms of rθ on UltraFeeback datasets.

19

## Page 20

## E

Appendix F: Additional Empirical Results

We provide sampled responses of LM policies trained respectively with the DPO and NCA to compare

their data regularization feature. In general, generations from the NCA policy are more similar to the

preferred response in the dataset.

Prompt

You are given an answer. Create a question for the answer.

Grams to Ounces conversion 1 gram (g) is equal to 0.03527396195 ounces (oz).

Preferred Response in Dataset (r=8.0)

What is the conversion rate for grams to ounces for 1 gram?

Dispreferred Response in Dataset (r=7.0)

What is the relationship between grams and ounces in weight measurement?

Reponse generated by πDPO

θ

(r=2.0)

How many ounces are equivalent to 200 grams according to the given conversion rate of 1 gram (g)

to 0.03527396195 ounces (oz)?

Reponse generated by πNCA

θ

(r=10.0)

Question: What is the conversion rate from grams to ounces, and how many grams are one ounce?

Table 5: Sampled generations and the log-likelihood ratio of the NCA and the DPO/InfoNCA policy.

Prompt

If "A busy crosswalk somewhere in asia." does that mean that "The crosswalk was blocked off by

police."?

Options:

- yes; - it is not possible to tell ; - no

Preferred Response in Dataset (r=8.0)

It is not possible to tell.

Confidence: 95%

Dispreferred Response in Dataset (r=7.5)

Question:

If "A busy crosswalk somewhere in asia." does that mean that "The crosswalk was blocked off by

police."?

Answer:

It is not possible to tell.

Confidence: 90%

Reponse generated by πDPO

θ

(r=3.0)

- It is not possible to determine whether "The crosswalk was blocked off by police." in "A busy

crosswalk somewhere in Asia." To determine whether the crosswalk was blocked off by police, you

would need additional information or context.

Reponse generated by πNCA

θ

(r=8.0)

It is not possible to tell.

Confidence: 95%

Table 6: Sampled generations and the log-likelihood ratio of the NCA and the DPO/InfoNCA policy.

20

## Page 21

Prompt

In this task, you need to count the occurrences of the given word in the given sentence.

Sentence: ’a green double decker bus sitting on top of a parking lot’. Count the occurrences of the

word ’a’ in the given sentence.

Preferred Response in Dataset (r=9.0)

The word ’a’ appears 3 times in the given sentence.

Confidence: 100%

Dispreferred Response in Dataset (r=6.0)

The given sentence is "a green double decker bus sitting on top of a parking lot." To answer your

question about the occurrences of the word ’a’, I will count the number of times the word appears in

the sentence.

After counting the occurrences of the word ’a’ in the sentence, I found that it appears once. Therefore,

the answer to your question is that the word ’a’ appears once in the given sentence.

Reponse generated by πDPO

θ

(r=4.0)

Of course, I’m here to help! There are 11 occurrences of the word ’a’ in the given sentence. Here’s

how you can count them: 1. ’a’ in ’a’ 2. ’a’ in ’green’ 3. ’a’ in ’double’ 4. ’a’ in ’decker’ 5. ’a’ in

’bus’ 6. ’a’ in ’sitting’ 7. ’a’ in ’on ...

Confidence: 90%

Reponse generated by πNCA

θ

(r=8.0)

Sure! The word ’a’ appears 3 times in the given sentence.

Table 7: Sampled generations and the log-likelihood ratio of the NCA and the DPO/InfoNCA policy.

Prompt

if X2 + 3 = 12. What is X?

Preferred Response in Dataset (r=9.0)

To find the value of X, we’ll first need to isolate X2:

## X2 + 3 = 12

Subtract 3 from both sides:

## X2 = 9

Now, we can find the square root of both sides of the equation to solve for X:

## X = ±3

So, X can be either 3 or -3.

Dispreferred Response in Dataset (r=4.0)

I’m sorry, I cannot solve this algebra equation. Can I assist you with anything else?

Reponse generated by πDPO

θ

(r=7.0)

Sure, I’d love to help. By solving the equation, the value of X should be 3.

Reponse generated by πNCA

θ

(r=9.0)

To find the value of X, we need to solve the equation X2 + 3 = 12.

First, subtract 3 from both sides:

## X2 = 12 −3 X2 = 9

Next, take the square root of both sides:

## X = ±

√

9

So, X can be either 3 or −3.

Table 8: Sampled generations and the log-likelihood ratio of the NCA and the DPO/InfoNCA policy.

21

## Page 22

Appendix H: Comparing NCA with DPO on Data Regularization.

Suppose there are only 4 possible responses {ya, yb, yc, yd}. Each response has a reward r, and a

pretrained language model policy µ:

-

ya

yb

yc

yd

Likelihood µ(y)

40%

50%

5%

5%

Reward r(y)

10

3

7

0

Table 9: Initial probabilities and rewards

Given a preference dataset D = {ya > yb} (yc and yd do not exist in the dataset), the loss functions

are defined as:

## Ldpo

θ

= −log σ



log πθ(ya)

µ(ya) −log πθ(yb)

µ(yb)



## Lnca

θ

= −log σ



log πθ(ya)

µ(ya)



−1

2 log σ



−log πθ(ya)

µ(ya)



−1

2 log σ



−log πθ(yb)

µ(yb)



After fine-tuning, there are several possibilities for πθ:

-

πθ(ya)

πθ(yb)

πθ(yc)

πθ(yd)

log πθ(ya)

µ(ya) −log πθ(yb)

µ(yb)

¯r

(1)

20% ↓

10% ↓↓

5%

65% ↑

0.916 > 0

2.65 ↓

(2)

20% ↓

10% ↓↓

65% ↑

5%

0.916 > 0

6.85 ↑

(3) Wanted

60% ↑

30% ↓

5%

5%

0.916 > 0

7.25 ↑

Table 10: Post fine-tuning probabilities and rewards

In scenarios (1) and (2), we can see that the likelihood for both ya and yb decreases. However, (1)

and (2) satisfy the DPO loss function because the likelihood for yb decreases more, and the relative

likelihood margin between ya and yb becomes larger. In (1) and (2), the likelihood for either yc or

yd increases because π(ya) ↓+π(yb) ↓+π(yc)? + π(yd)? = 1. However, yc and yd are unreliable

because we do not know their quality (rewards). The LM policy could generalize to a low-quality

response like yd (case (1)).

In contrast, the NCA effectively prevents the winning response likelihood π(ya) from decreasing,

because it mainly optimizes the absolute data likelihood instead of just caring about the relative

likelihood margin log πθ(ya)

µ(ya) −log πθ(yb)

µ(yb) . Thus, we say NCA is more likely to assign a larger

likelihood to responses within the dataset.

Appendix G: Comparision with Related Works

We compare with a prior work SLiC-HF [45] in this section. SLiC is inspired by [46] and similarly

aims to calibrate sequence likelihood to align with human preferences. Given a preference data pair

{x →yw > yl}, the loss function for SLiC is

Lθ := max(0, δ −log πθ(yw|x) + log πθ(yw|x)) −λ log πθ(yw|x),

where δ is a hyperparameter that controls the likelihood margin of data, and λ controls the regulariza-

tion weight of the loss. The main difference between our proposed method and the SLiC loss can be

summarized as follows:

• Theoritical framework. SLiC is mainly adapted from the existing LM calibration methods

[46]. In contrast, our method is based on noise contrastive estimation methods [14, 24].

• Policy regularization. The training process of SLiC is regularized by the additional SFT loss

controlled by λ. In contrast, our proposed method is regularized through the parameterization

technique rθ = β log πθ

µ controlled by β.

• Learning target. SLiC directly optimizes the policy model πθ, while our method directly

optimizes the residual model rθ.

22
