# 2101.00190_Prefix-Tuning-Optimizing-Continuous-Prompts-for-Ge
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 2101.00190_Prefix-Tuning-Optimizing-Continuous-Prompts-for-Ge.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2



## Implementation Status

- ‚úÖ **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- üîÑ **Performance Monitoring**: Continuous validation of targets
- ‚úÖ **Documentation Standards**: Compliant with ACGS-2 requirements
- üîÑ **Cross-Reference Validation**: Ongoing link integrity maintenance

**Overall Status**: üîÑ IN PROGRESS - Systematic enhancement implementation

## Performance Targets

This component maintains the following performance requirements:

- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

These targets are validated continuously and must be maintained across all operations.

---

## Page 1

PreÔ¨Åx-Tuning: Optimizing Continuous Prompts for Generation

Xiang Lisa Li

Stanford University

xlisali@stanford.edu

Percy Liang

Stanford University

pliang@cs.stanford.edu

Abstract

Fine-tuning is the de facto way to leverage

large pretrained language models to perform

downstream tasks.

However, it modiÔ¨Åes all

the language model parameters and therefore

necessitates storing a full copy for each task.

In this paper, we propose preÔ¨Åx-tuning, a

lightweight alternative to Ô¨Åne-tuning for nat-

ural language generation tasks, which keeps

language model parameters frozen, but opti-

mizes a small continuous task-speciÔ¨Åc vector

(called the preÔ¨Åx). PreÔ¨Åx-tuning draws inspira-

tion from prompting, allowing subsequent to-

kens to attend to this preÔ¨Åx as if it were ‚Äúvir-

tual tokens‚Äù. We apply preÔ¨Åx-tuning to GPT-2

for table-to-text generation and to BART for

summarization. We Ô¨Ånd that by learning only

0.1% of the parameters, preÔ¨Åx-tuning obtains

comparable performance in the full data set-

ting, outperforms Ô¨Åne-tuning in low-data set-

tings, and extrapolates better to examples with

topics unseen during training.

1

Introduction

Fine-tuning is the prevalent paradigm for using

large pretrained language models (LMs) (Radford

et al., 2019; Devlin et al., 2019) to perform down-

stream tasks (e.g., summarization), but it requires

updating and storing all the parameters of the LM.

Consequently, to build and deploy NLP systems

that rely on large pretrained LMs, one currently

needs to store a modiÔ¨Åed copy of the LM parame-

ters for each task. This can be prohibitively expen-

sive, given the large size of current LMs; for exam-

ple, GPT-2 has 774M parameters (Radford et al.,

2019) and GPT-3 has 175B parameters (Brown

et al., 2020).

A natural approach to this problem is lightweight

Ô¨Åne-tuning, which freezes most of the pretrained

parameters and augments the model with small

trainable modules. For example, adapter-tuning

Figure 1:

Fine-tuning (top) updates all Transformer

parameters (the red Transformer box) and requires stor-

ing a full model copy for each task.

We propose

preÔ¨Åx-tuning (bottom), which freezes the Transformer

parameters and only optimizes the preÔ¨Åx (the red pre-

Ô¨Åx blocks). Consequently, we only need to store the

preÔ¨Åx for each task, making preÔ¨Åx-tuning modular and

space-efÔ¨Åcient. Note that each vertical block denote

transformer activations at one time step.

(RebufÔ¨Ået al., 2017; Houlsby et al., 2019) inserts

additional task-speciÔ¨Åc layers between the layers

of pretrained language models. Adapter-tuning

has promising performance on natural language

understanding and generation benchmarks, attain-

ing comparable performance with Ô¨Åne-tuning while

adding only around 2-4% task-speciÔ¨Åc parameters

(Houlsby et al., 2019; Lin et al., 2020).

On the extreme end, GPT-3 (Brown et al., 2020)

can be deployed without any task-speciÔ¨Åc tuning.

Instead, users prepend a natural language task in-

struction (e.g., TL;DR for summarization) and a

few examples to the task input; then generate the

output from the LM. This approach is known as

in-context learning or prompting.

In this paper, we propose preÔ¨Åx-tuning, a

lightweight alternative to Ô¨Åne-tuning for natural lan-

guage generation (NLG) tasks, inspired by prompt-

ing. Consider the task of generating a textual de-

arXiv:2101.00190v1  [cs.CL]  1 Jan 2021

## Page 2

scription of a data table, as shown in Figure 1,

where the task input is a linearized table (e.g.,

‚Äúname: Starbucks | type: coffee shop‚Äù) and the out-

put is a textual description (e.g., ‚ÄúStarbucks serves

coffee.‚Äù). PreÔ¨Åx-tuning prepends a sequence of

continuous task-speciÔ¨Åc vectors to the input, which

we call a preÔ¨Åx, depicted by red blocks in Figure 1

(bottom). For subsequent tokens, the Transformer

can attend to the preÔ¨Åx as if it were a sequence of

‚Äúvirtual tokens‚Äù, but unlike prompting, the preÔ¨Åx

consists entirely of free parameters which do not

correspond to real tokens. In contrast to Ô¨Åne-tuning

in Figure 1 (top), which updates all Transformer

parameters and thus requires storing a tuned copy

of the model for each task, preÔ¨Åx-tuning only op-

timizes the preÔ¨Åx. Consequently, we only need

to store one copy of the large Transformer and a

learned task-speciÔ¨Åc preÔ¨Åx, yielding a very small

overhead for each additional task (e.g., 250K pa-

rameters for table-to-text).

In contrast to Ô¨Åne-tuning, preÔ¨Åx-tuning is mod-

ular: we train an upstream preÔ¨Åx which steers a

downstream LM, which remains unmodiÔ¨Åed. Thus,

a single LM can support many tasks at once. In

the context of personalization where the tasks cor-

respond to different users (Shokri and Shmatikov,

2015; McMahan et al., 2016), we could have a sep-

arate preÔ¨Åx for each user trained only on that user‚Äôs

data, thereby avoiding data cross-contamination.

Moreover, the preÔ¨Åx-based architecture enables us

to even process examples from multiple users/tasks

in a single batch, something that is not possible

with other lightweight Ô¨Åne-tuning approaches.

We evaluate preÔ¨Åx-tuning on table-to-text gen-

eration using GPT-2 and abstractive summariza-

tion using BART. In terms of storage, preÔ¨Åx-tuning

stores 1000x fewer parameters than Ô¨Åne-tuning. In

terms of performance when trained on full datasets,

preÔ¨Åx-tuning and Ô¨Åne-tuning are comparable for

table-to-text (¬ß6.1), while preÔ¨Åx-tuning suffers a

small degradation for summarization (¬ß6.2). In low-

data settings, preÔ¨Åx-tuning on average outperforms

Ô¨Åne-tuning on both tasks (¬ß6.3). PreÔ¨Åx-tuning also

extrapolates better to tables (for table-to-text) and

articles (for summarization) with unseen topics

(¬ß6.4).

2

Related Work

Fine-tuning for natural language generation.

Current state-of-the-art systems for natural lan-

guage generation are based on Ô¨Åne-tuning pre-

trained LMs. For table-to-text generation, Kale

(2020) Ô¨Åne-tunes a sequence-to-sequence model

(T5; Raffel et al., 2020). For extractive and abstrac-

tive summarization, researchers Ô¨Åne-tune masked

language models (e.g., BERT; Devlin et al., 2019)

and encode-decoder models (e.g., BART; Lewis

et al., 2020) respectively (Zhong et al., 2020; Liu

and Lapata, 2019; Raffel et al., 2020). For other

conditional NLG tasks such as machine transla-

tion and dialogue generation, Ô¨Åne-tuning is also the

prevalent paradigm (Zhang et al., 2020c; Stickland

et al., 2020; Zhu et al., 2020; Liu et al., 2020). In

this paper, we focus on table-to-text using GPT-2

and summarization using BART, but preÔ¨Åx-tuning

can be applied to other generation tasks and pre-

trained models.

Lightweight

Ô¨Åne-tuning.

Lightweight

Ô¨Åne-

tuning freezes most of the pretrained parameters

and modiÔ¨Åes the pretrained model with small

trainable modules. The key challenge is to identify

high-performing architectures of the modules and

the subset of pretrained parameters to tune. One

line of research considers removing parameters:

some model weights are ablated away by training

a binary mask over model parameters (Zhao et al.,

2020; Radiya-Dixit and Wang, 2020). Another

line of research considers inserting parameters.

For example, Zhang et al. (2020a) trains a ‚Äúside‚Äù

network that is fused with the pretrained model via

summation; adapter-tuning inserts task-speciÔ¨Åc lay-

ers (adapters) between each layer of the pretrained

LM (Houlsby et al., 2019; Lin et al., 2020; RebufÔ¨Å

et al., 2017; Pfeiffer et al., 2020). Compared to

this line of work, which tunes around 3.6% of the

LM parameters, our method obtains a further 30x

reduction in task-speciÔ¨Åc parameters, tuning only

0.1% while maintaining comparable performance.

Prompting.

Prompting means prepending in-

structions and a few examples to the task input and

generating the output from the LM. GPT-3 (Brown

et al., 2020) uses manually designed prompts to

adapt its generation for different tasks, and this

framework is termed in-context learning. However,

since Transformers can only condition on a

bounded-length context (e.g., 2048 tokens for GPT-

3), in-context learning is unable to fully exploit

training sets longer than the context window. Sun

and Lai (2020) also prompt by keywords to control

for sentiment or topic of the generated sentence.

In natural language understanding tasks, prompt

## Page 3

engineering has been explored in prior works for

models like BERT and RoBERTa (Liu et al., 2019;

Jiang et al., 2020; Schick and Sch¬®utze, 2020). For

example, AutoPrompt (Shin et al., 2020) searches

for a sequence of discrete trigger words and con-

catenates it with each input to elicit sentiment or

factual knowledge from a masked LM. In contrast

with AutoPrompt, our method optimizes contin-

uous preÔ¨Åxes, which are more expressive (¬ß7.2);

moreover, we focus on language generation tasks.

Continuous vectors have been used to steer lan-

guage models; for example, Subramani et al. (2020)

showed that a pretrained LSTM language model

can reconstruct arbitrary sentences by optimizing

a continuous vector for each sentence, making the

vector input-speciÔ¨Åc. In contrast, preÔ¨Åx-tuning op-

timizes a task-speciÔ¨Åc preÔ¨Åx that applies to all in-

stances of that task. As a result, unlike the previous

work whose application is limited to sentence re-

construction, preÔ¨Åx-tuning can be applied to NLG

tasks.

Controllable generation.

Controllable genera-

tion aims to steer a pretrained language model to

match a sentence level attribute (e.g., positive senti-

ment or topic on sports). Such control can happen

at training time: Keskar et al. (2019) pretrains the

language model (CTRL) to condition on metadata

such as keywords or URLs. Additionally, the con-

trol can happen at decoding time, by weighted de-

coding (GeDi, Krause et al., 2020) or iteratively up-

dating the past activations (PPLM, Dathathri et al.,

2020). However, there is no straightforward way

to apply these controllable generation techniques

to enforce Ô¨Åne-grained control over generated con-

tents, as demanded by tasks like table-to-text and

summarization.

3

Problem Statement

Consider a conditional generation task where the

input is a context x and the output y is a sequence

of tokens. We focus on two tasks, shown in Fig-

ure 2 (right): In table-to-text, x corresponds to a

linearized data table and y is a textual description;

in summarization, x is an article and y is a short

summary.

3.1

Autoregressive LM

Assume we have an autoregressive language model

pœÜ(y | x) based on the Transformer (Vaswani et al.,

2017) architecture (e.g., GPT-2; Radford et al.,

2019) and parametrized by œÜ. As shown in Fig-

ure 2 (top), let z = [x; y] be the concatenation of x

and y; let Xidx denote the sequence of indices that

corresponds to x, and Yidx denote the same for y.

The activation at time step i is hi ‚ààRd, where

hi = [h(1)

i ; ¬∑ ¬∑ ¬∑ ; h(n)

i

] is a concatenation of all acti-

vation layers at this time step, and h(j)

i

is the acti-

vation of the j-th Transformer layer at time step i.1

The autoregressive Transformer model computes

hi as a function of zi and the past activations in its

left context, as follows:

hi = LMœÜ(zi, h<i),

(1)

where the last layer of hi is used to compute the

distribution for the next token: pœÜ(zi+1 | h‚â§i) =

softmax(WœÜ h(n)

i

) and WœÜ is a pretrained matrix

that map h(n)

i

to logits over the vocabulary.

3.2

Encoder-Decoder Architecture

We can also use an encoder-decoder architecture

(e.g., BART; Lewis et al., 2020) to model pœÜ(y | x),

where x is encoded by the bidirectional encoder,

and the decoder predicts y autoregressively (condi-

tioned on the encoded x and its left context). We

use the same indexing and activation notation, as

shown in Figure 2 (bottom). hi for all i ‚ààXidx

is computed by the bidirectional Transformer en-

coder; hi for all i ‚ààYidx is computed by the au-

toregressive decoder using the same equation (1).

3.3

Method: Fine-tuning

In the Ô¨Åne-tuning framework, we initialize with the

pretrained parameters œÜ. Here pœÜ is a trainable lan-

guage model distribution and we perform gradient

updates on the following log-likelihood objective:

max

œÜ

log pœÜ(y | x) =

## X

i‚ààYidx

log pœÜ(zi | h<i). (2)

4

PreÔ¨Åx-Tuning

We propose preÔ¨Åx-tuning as an alternative to

Ô¨Åne-tuning for conditional generation tasks. We

Ô¨Årst provide intuition in ¬ß4.1 before deÔ¨Åning our

method formally in ¬ß4.2.

1h(n)

i

is composed of a key-value pair. In GPT-2, the

dimension of each key and value is 1024.

## Page 4

Figure 2: An annotated example of preÔ¨Åx-tuning using an autoregressive LM (top) and an encoder-decoder model

(bottom). The preÔ¨Åx activations ‚àÄi ‚ààPidx, hi are drawn from a trainable matrix PŒ∏. The remaining activations are

computed by the Transformer.

4.1

Intuition

Based on intuition from prompting, we believe that

having a proper context can steer the LM without

changing its parameters. For example, if we want

the LM to generate a word (e.g., Obama), we can

prepend its common collocations as context (e.g.,

Barack), and the LM will assign much higher prob-

ability to the desired word. Extending this intuition

beyond generating a single word or sentence, we

want to Ô¨Ånd a context that steers the LM to solve

an NLG task. Intuitively, the context can inÔ¨Çu-

ence the encoding of x by guiding what to extract

from x; and can inÔ¨Çuence the generation of y by

steering the next token distribution. However, it‚Äôs

non-obvious whether such a context exists. Natural

language task instructions (e.g., ‚Äúsummarize the

following table in one sentence‚Äù) might guide an

expert annotator to solve the task, but fail for most

pretrained LMs.2 Data-driven optimization over

the discrete instructions might help, but discrete

optimization is computationally challenging.

Instead of optimizing over discrete tokens, we

can optimize the instruction as continuous word em-

beddings, whose effects will be propagated upward

to all Transformer activation layers and rightward

to subsequent tokens. This is strictly more expres-

sive than a discrete prompt which requires match-

ing the embedding of a real word. Meanwhile,

this is less expressive than intervening all layers of

the activations (¬ß7.2), which avoids long-range de-

pendencies and includes more tunable parameters.

PreÔ¨Åx-tuning, therefore, optimizes all layers of the

preÔ¨Åx.

2In our preliminary experiments, GPT-2 and BART fail in

this setting; the only exception is GPT-3.

4.2

Method

PreÔ¨Åx-tuning prepends a preÔ¨Åx for an autoregres-

sive LM to obtain z = [PREFIX; x; y], or prepends

preÔ¨Åxes for both encoder and encoder to obtain

z = [PREFIX; x; PREFIX‚Ä≤; y], as shown in Figure 2.

Here, Pidx denotes the sequence of preÔ¨Åx indices,

and we use |Pidx| to denote the length of the preÔ¨Åx.

We follow the recurrence relation in equa-

tion (1), except that the preÔ¨Åx are free parame-

ters. PreÔ¨Åx-tuning initializes a trainable matrix PŒ∏

(parametrized by Œ∏) of dimension |Pidx| √ó dim(hi)

to store the preÔ¨Åx parameters.

hi =

(

PŒ∏[i, :],

if i ‚ààPidx,

LMœÜ(zi, h<i),

otherwise.

(3)

The training objective is the same as equation (2),

but the set of trainable parameters changes: the lan-

guage model parameters œÜ are Ô¨Åxed and the preÔ¨Åx

parameters Œ∏ are the only trainable parameters.

Here, hi (for all i) is a function of the trainable

PŒ∏. When i ‚ààPidx, this is clear because hi copies

directly from PŒ∏. When i Ã∏‚ààPidx, hi still depends

on PŒ∏, because the preÔ¨Åx activations are always

in the left context and will therefore affect any

activations to its right.

4.3

Parametrization of PŒ∏

Empirically, directly updating the PŒ∏ parameters

leads to unstable optimization and a slight drop

in performance.3 So we reparametrize the matrix

PŒ∏[i, :] = MLPŒ∏(P ‚Ä≤

Œ∏[i, :]) by a smaller matrix (P ‚Ä≤

Œ∏)

composed with a large feedforward neural network

(MLPŒ∏). Note that PŒ∏ and P ‚Ä≤

Œ∏ has the same rows

3We Ô¨Ånd in preliminary experiments that directly opti-

mizing the preÔ¨Åx is very sensitive to the learning rate and

initialization.

## Page 5

dimension (i.e. the preÔ¨Åx length), but different

columns dimension.4 Once training is complete,

these reparametrization parameters can be dropped,

and only the preÔ¨Åx (PŒ∏) needs to be saved.

5

Experimental Setup

5.1

Datasets and Metrics

We evaluate on three standard neural generation

datasets for the table-to-text task: E2E (Novikova

et al., 2017), WebNLG (Gardent et al., 2017), and

DART (Radev et al., 2020). The datasets are or-

dered by increasing complexity and size. E2E only

has 1 domain (i.e. restaurant reviews); WebNLG

has 14 domains, and DART is open-domain, using

open-domain tables from Wikipedia.

The E2E dataset contains approximately 50K

examples with 8 distinct Ô¨Åelds; it contains multiple

test references for one source table, and the average

output length is 22.9. We use the ofÔ¨Åcial evaluation

script, which reports BLEU (Papineni et al., 2002),

NIST (Belz and Reiter, 2006), METEOR (Lavie

and Agarwal, 2007), ROUGE-L (Lin, 2004), and

CIDEr (Vedantam et al., 2015).

The WebNLG (Gardent et al., 2017) dataset con-

sists of 22K examples, and the input x is a sequence

of (subject, property, object) triples. The average

output length is 22.5. In the training and validation

splits, the input describes entities from 9 distinct

DBpedia categories (e.g., Monument). The test

split consists of two parts: the Ô¨Årst half contains

DB categories seen in training data, and the sec-

ond half contains 5 unseen categories. These un-

seen categories are used to evaluate extrapolation.

We use the ofÔ¨Åcial evaluation script, which reports

BLEU, METEOR and TER (Snover et al., 2006).

DART (Radev et al., 2020) is an open domain

table-to-text dataset, with similar input format

(entity-relation-entity triples) as WebNLG. The av-

erage output length is 21.6. It consists of 82K ex-

amples from WikiSQL, WikiTableQuestions, E2E,

and WebNLG and applies some manual or auto-

mated conversion. We use the ofÔ¨Åcial evaluation

script and report BLEU, METEOR, TER, Mover-

Score (Zhao et al., 2019), BERTScore (Zhang et al.,

2020b) and BLEURT (Sellam et al., 2020).

For the summarization task, we use the XSUM

(Narayan et al., 2018) dataset, which is an abstrac-

4PŒ∏ has a dimension of |Pidx| √ó dim(hi) while PŒ∏ has

a dimension of |Pidx| √ó k, where we choose k = 512 for

table-to-text and 800 for summarization. MLPŒ∏ maps from

dimension k to dim(hi)

tive summarization dataset on news articles. There

are 225K examples. The average length of the ar-

ticles is 431 words and the average length of the

summaries is 23.3. We report ROUGE-1, ROUGE-

2 and ROUGE-L.

5.2

Methods

For table-to-text generation, we compare preÔ¨Åx-

tuning with three other methods: Ô¨Åne-tuning (FINE-

TUNE), Ô¨Åne-tuning only the top 2 layers (FT-TOP2),

and adapter-tuning (ADAPTER).5 We also report

the current state-of-the-art results on these datasets:

On E2E, Shen et al. (2019) uses a pragmatically

informed model without pretraining. On WebNLG,

Kale (2020) Ô¨Åne-tunes T5-large. On DART, no

ofÔ¨Åcial models trained on this dataset version are

released.6 For summarization, we compare against

Ô¨Åne-tuning BART (Lewis et al., 2020).

5.3

Architectures and Hyperparameters

For table-to-text, we use GPT-2MEDIUM and GPT-

2LARGE; the source tables are linearized.7 For sum-

marization, we use BARTLARGE,8 and the source

articles are truncated to 512 BPE tokens.

Our implementation is based on the Hugging

Face Transformer models (Wolf et al., 2020).

At training time, we use the AdamW optimizer

(Loshchilov and Hutter, 2019) and a linear learn-

ing rate scheduler, as suggested by the Hugging

Face default setup. The hyperparameters we tune

include the number of epochs, batch size, learn-

ing rate, and preÔ¨Åx length. Hyperparameter details

are in the appendix. A default setting trains for 10

epochs, using a batch size of 5, a learning rate of

5 ¬∑ 10‚àí5 and a preÔ¨Åx length of 10. The table-to-text

models are trained on TITAN Xp or GeForce GTX

TITAN X machines. PreÔ¨Åx-tuning takes 0.2 hours

per epochs to train on 22K examples , whereas Ô¨Åne-

tuning takes around 0.3 hours. The summarization

models are trained on Tesla V100 machines, taking

1.25h per epoch on the XSUM dataset.

At decoding time, for the three table-to-text

datasets, we use beam search with a beam size

of 5. For summarization, we use a beam size of 6

5Same implementation as Lin et al. (2020).

6The ofÔ¨Åcial benchmark model is trained on v.1.0.0 while

the release dataset is v1.1.1.

7In comparison with natural language utterances, the lin-

earized table is in an unnatural format, which might be chal-

lenging for pretrained LMs.

8We didn‚Äôt include GPT-2 results for summarization be-

cause in our preliminary experiment, Ô¨Åne-tuning GPT-2 sig-

niÔ¨Åcantly underperforms Ô¨Åne-tuning BART on XSUM.

## Page 6

and length normalization of 0.8. Decoding takes

1.2 seconds per sentence (without batching) for

table-to-text, and 2.6 seconds per batch (using a

batch size of 10) for summarization.

6

Main Results

6.1

Table-to-text Generation

We Ô¨Ånd that adding only 0.1% task-speciÔ¨Åc param-

eters,9 preÔ¨Åx-tuning is effective in table-to-text gen-

eration, outperforming other lightweight baselines

(ADAPTER and FT-TOP2) and achieving a compa-

rable performance with Ô¨Åne-tuning. This trend is

true across all three datasets: E2E, WebNLG,10 and

## Dart.

For a fair comparison, we match the number of

parameters for preÔ¨Åx-tuning and adapter-tuning to

be 0.1%. Table 1 shows that preÔ¨Åx-tuning is sig-

niÔ¨Åcantly better than ADAPTER (0.1%), attaining

4.1 BLEU improvement per dataset on average.

Even when we compare with Ô¨Åne-tuning (100%)

and adapter-tuning (3.0%), which update signiÔ¨Å-

cantly more parameters than preÔ¨Åx-tuning, preÔ¨Åx-

tuning still achieves results comparable or better

than those two systems. This demonstrates that

preÔ¨Åx-tuning is more Pareto efÔ¨Åcient than adapter-

tuning, signiÔ¨Åcantly reducing parameters while im-

proving generation quality.

Additionally, attaining good performance on

DART suggests that preÔ¨Åx-tuning can generalize

to tables with diverse domains and a large pool

of relations. We will delve deeper into extrapo-

lation performance (i.e. generalization to unseen

categories or topics) in ¬ß6.4.

Overall, preÔ¨Åx-tuning is an effective and space-

efÔ¨Åcient method to adapt GPT-2 to table-to-text

generation. The learned preÔ¨Åx is expressive enough

to steer GPT-2 in order to correctly extract contents

from an unnatural format and generate a textual

description. PreÔ¨Åx-tuning also scales well from

GPT-2MEDIUM to GPT-2LARGE, suggesting it has

the potential to scale to even larger models with a

similar architecture, like GPT-3.

6.2

Summarization

As shown in Table 2, with 2% parameters, preÔ¨Åx-

tuning obtains slightly lower performance than Ô¨Åne-

9250K for E2E, 250K for WebNLG, and 500K for DART

vs. 345M GPT-2 parameters.

10The S,U,A columns in WebNLG represents SEEN, UN-

SEEN, and ALL respectively; SEEN categories appear at

training time; UNSEEN categories only appears at test time;

and ALL is the combination of the two.

tuning (36.05 vs. 37.25 in ROUGE-L). With only

0.1% parameters, preÔ¨Åx-tuning underperforms full

Ô¨Åne-tuning (35.05 vs. 37.25). There are several

differences between XSUM and the three table-to-

text datasets which could account for why preÔ¨Åx-

tuning has comparative advantage in table-to-text:

(1) XSUM contains 4x more examples than the

three table-to-text datasets on average; (2) the in-

put articles are 17x longer than the linearized table

input of table-to-text datasets on average; (3) sum-

marization might be more complex than table-to-

text because it requires reading comprehension and

identifying key contents from an article.

6.3

Low-data Setting

Based on the results from table-to-text (¬ß6.1)

and summarization (¬ß6.2), we observe that preÔ¨Åx-

tuning has a comparative advantage when the

number of training examples is smaller. To con-

struct low-data settings, we subsample the full

dataset (E2E for table-to-text and XSUM for

summarization) to obtain small datasets of size

{50, 100, 200, 500}. For each size, we sample 5

different datasets and average over 2 training ran-

dom seeds. Thus, we average over 10 models to

get an estimate for each low-data setting.11

Figure 3 (right) shows that preÔ¨Åx-tuning outper-

forms Ô¨Åne-tuning in low-data regimes by 2.9 BLEU

on average, in addition to requiring many fewer pa-

rameters, but the gap narrows as the dataset size

increases.

Qualitatively, Figure 3 (left) shows 8 examples

generated by both preÔ¨Åx-tuning and Ô¨Åne-tuning

models trained on different data levels. While both

methods tend to undergenerate (missing table con-

tents) in low data regimes, preÔ¨Åx-tuning tends to be

more faithful than Ô¨Åne-tuning. For example, Ô¨Åne-

tuning (100, 200)12 falsely claims a low customer

rating while the true rating is average, whereas

preÔ¨Åx-tuning (100, 200) generates a description

that is faithful to the table.

6.4

Extrapolation

We now investigate extrapolation performance to

unseen topics for both table-to-text and summariza-

tion. In order to construct an extrapolation setting,

we split the existing datasets so that training and

test cover different topics. For table-to-text, the

11We also sample a dev split (with dev size = 30% √ó train-

ing size ) for each training set. We use the dev split to choose

hyperparameters and do early stopping.

12The number in the parenthesis refers to the training size.

## Page 7

## E2E

WebNLG

## Dart

BLEU NIST MET R-L CIDEr

## Bleu

## Met

## Ter ‚Üì

BLEU MET TER ‚ÜìMover BERT BLEURT

## S

## U

## A

## S

## U

## A

## S

## U

## A

## Gpt-2Medium

## Fine-Tune

68.2

8.62

46.2

71.0

2.47

64.2 27.7 46.5 0.45 0.30 0.38 0.33 0.76 0.53

46.2

0.39

0.46

0.50

0.94

0.39

## Ft-Top2

68.1

8.59

46.0

70.8

2.41

53.6 18.9 36.0 0.38 0.23 0.31 0.49 0.99 0.72

41.0

0.34

0.56

0.43

0.93

0.21

## Adapter(3%)

68.9

8.71

46.1

71.3

2.47

60.4 48.3 54.9 0.43 0.38 0.41 0.35 0.45 0.39

45.2

0.38

0.46

0.50

0.94

0.39

## Adapter(0.1%)

66.3

8.41

45.0

69.8

2.40

54.5 45.1 50.2 0.39 0.36 0.38 0.40 0.46 0.43

42.4

0.36

0.48

0.47

0.94

0.33

## Prefix(0.1%)

69.7

8.81

46.1

71.4

2.49

62.9 45.6 55.1 0.44 0.38 0.41 0.35 0.49 0.41

46.4

0.38

0.46

0.50

0.94

0.39

## Gpt-2Large

## Fine-Tune

68.5

8.78

46.0

69.9

2.45

65.3 43.1 55.5 0.46 0.38 0.42 0.33 0.53 0.42

47.0

0.39

0.46

0.51

0.94

0.40

PreÔ¨Åx

70.3

8.85

46.2

71.7

2.47

63.4 47.7 56.3 0.45 0.39 0.42 0.34 0.48 0.40

46.7

0.39

0.45

0.51

0.94

0.40

## Sota

68.6

8.70

45.3

70.8

2.37

63.9 52.8 57.1 0.46 0.41 0.44

-

-

-

-

-

-

-

-

-

Table 1: Metrics (higher is better, except for TER) for table-to-text generation on E2E (left), WebNLG (middle)

and DART (right). With only 0.1% parameters, PreÔ¨Åx-tuning outperforms other lightweight baselines and achieves

a comparable performance with Ô¨Åne-tuning. The best score is boldfaced for both GPT-2MEDIUM and GPT-2LARGE.

Source

name : The Eagle | type : coffee shop | food : Chinese | price : cheap | customer

rating : average | area : riverside | family friendly : no | near : Burger King

PreÔ¨Åx (50)

The Eagle is a cheap Chinese coffee shop located near Burger King.

PreÔ¨Åx (100)

The Eagle is a cheap coffee shop located in the riverside near Burger King. It

has average customer ratings.

PreÔ¨Åx (200)

The Eagle is a cheap Chinese coffee shop located in the riverside area near

Burger King. It has average customer ratings.

PreÔ¨Åx (500)

The Eagle is a coffee shop that serves Chinese food. It is located in the riverside

area near Burger King. It has an average customer rating and is not family

friendly.

## Ft (50)

The Eagle coffee shop is located in the riverside area near Burger King.

## Ft (100)

The Eagle is a cheap coffee shop near Burger King in the riverside area. It has

a low customer rating and is not family friendly.

## Ft (200)

The Eagle is a cheap Chinese coffee shop with a low customer rating. It is

located near Burger King in the riverside area.

## Ft (500)

The Eagle is a cheap Chinese coffee shop with average customer ratings. It is

located in the riverside area near Burger King.

100

200

300

400

500

training_data_size

32

34

36

rouge-1

method

## Ft

## Pt

100

200

300

400

500

training_data_size

10

11

12

13

14

15

rouge-2

method

## Ft

## Pt

100

200

300

400

500

training_data_size

0.50

0.55

0.60

## Bleu

method

## Ft

## Pt

100

200

300

400

500

training_data_size

0.60

0.62

0.64

0.66

## Rouge

method

## Ft

## Pt

Figure 3: (Left) qualitative examples in lowdata settings. (Right) preÔ¨Åx-tuning (orange) outperforms Ô¨Åne-tuning

(blue) in low-data regimes in addition to requiring many fewer parameters. The top two plots correspond to sum-

marization, measured by ROUGE-1 and ROUGE-2. The bottom two plots correspond to table-to-text, measured

by BLEU and ROUGE-L. The x-axis is the training size and the y-axis is the evaluation metric (higher is better).

## R-1 ‚Üë

## R-2 ‚Üë

## R-L ‚Üë

FINE-TUNE(Lewis et al., 2020)

45.14

22.27

37.25

## Prefix(2%)

43.80

20.93

36.05

## Prefix(0.1%)

42.92

20.03

35.05

Table 2: Metrics for summarization on XSUM. PreÔ¨Åx-

tuning slightly underperforms Ô¨Åne-tuning.

WebNLG dataset is labeled with table topics. There

are 9 categories that appear in training and dev, de-

noted as SEEN and 5 categories that only appear at

test time, denoted as UNSEEN. So we evaluate ex-

trapolation by training on the SEEN categories and

testing on the UNSEEN categories. For summariza-

tion, we construct two extrapolation data splits13:

In news-to-sports, we train on news articles,

13XSUM dataset is drawn from BBC news, and we iden-

tify the topic of each article based on their URLs. Since

‚Äúnews‚Äù and ‚Äúsports‚Äù are the two domains with the most arti-

cles, we create our Ô¨Årst train/test split. Additionally, ‚Äúnews‚Äù

has subdomains such as ‚ÄúUK‚Äù, ‚Äúworld‚Äù, and ‚Äútechnology‚Äù.

Consequently, we create a second data split, using the top 3

news subdomains as training data and the rest as test data.

news-to-sports

within-news

## R-1 ‚Üë

## R-2 ‚Üë

## R-L ‚Üë

## R-1 ‚Üë

## R-2 ‚Üë

## R-L ‚Üë

## Fine-Tune

38.15

15.51

30.26

39.20

16.35

31.15

## Prefix

39.23

16.74

31.51

39.41

16.87

31.47

Table 3: Extrapolation performance on XSUM. PreÔ¨Åx-

tuning outperforms Ô¨Åne-tuning on both news-to-sports

and within-news splits.

and test on sports articles. In within-news, we

train on {world, UK, business} news, and test on

the remaining news categories (e.g., health, tech-

nology).

On both table-to-text and summarization, preÔ¨Åx-

tuning has better extrapolation than Ô¨Åne-tuning un-

der all metrics, as shown in Table 3 and the ‚ÄòU‚Äô

columns of Table 1 (middle).

We also Ô¨Ånd that adapter-tuning achieves good

extrapolation performance, comparable with preÔ¨Åx-

tuning, as shown in Table 1. This shared trend

suggests that preserving LM parameters indeed has

a positive impact on extrapolation. However, the

## Page 8

0

100

200

300

Prefix Length (XSUM)

18.5

19.0

19.5

20.0

20.5

21.0

## Rouge-2

33.5

34.0

34.5

35.0

35.5

36.0

## Rouge-L

## Rouge-2

## Rouge-L

0

10

20

30

40

Prefix Length (DART)

44.0

44.5

45.0

45.5

46.0

## Bleu

0.460

0.465

0.470

0.475

0.480

## Ter

## Bleu

## Ter

Figure 4:

PreÔ¨Åx length vs. performance on summer-

ization (left) and table-to-text (right). Performance in-

creases as the preÔ¨Åx length increases up to a threshold

(200 for summarization and 10 for table-to-text) and

then a slight performance drop occurs. Each plot re-

ports two metrics (on two vertical axes).

reason for such gains is an open question and we

will discuss further in ¬ß8.

7

Intrinsic Evaluation

We compare different variants of preÔ¨Åx-tuning.

¬ß7.1 studies the impact of the preÔ¨Åx length. ¬ß7.2

studies tuning only the embedding layer, which is

more akin to tuning a discrete prompt. ¬ß7.3 com-

pares preÔ¨Åxing and inÔ¨Åxing, which inserts trainable

activations between x and y. ¬ß7.4 studies the im-

pact of various preÔ¨Åx initialization strategies.

7.1

PreÔ¨Åx Length

A longer preÔ¨Åx means more trainable parameters,

and therefore more expressive power. Figure 4

shows that performance increases as the preÔ¨Åx

length increases up to a threshold (200 for sum-

marization, 10 for table-to-text) and then a slight

performance drop occurs.14

Empirically, longer preÔ¨Åxes have a negligible

impact on inference speed, because attention com-

putation over the entire preÔ¨Åx is parallellized on

GPUs.

7.2

Full vs Embedding-only

Recall in ¬ß4.1, we discuss the option of optimizing

the continuous embeddings of the ‚Äúvirtual tokens.‚Äù

We instantiate that idea and call it embedding-only

ablation. The word embeddings are free parame-

ters, and the upper activation layers are computed

by the Transformer. Table 4 (top) shows that the

performance drops signiÔ¨Åcantly, suggesting that

tuning only the embedding layer is not sufÔ¨Åciently

expressive.

The embedding-only ablation upper bounds the

performance of discrete prompt optimization (Shin

14PreÔ¨Åxes longer than the threshold lead to lower training

loss, but slightly worse test performance, suggesting that they

tend to overÔ¨Åt the training data.

## E2E

## Bleu

## Nist

## Met

## Rouge

CIDEr

## Prefix

69.7

8.81

46.1

71.4

2.49

Embedding-only: EMB-{PreÔ¨ÅxLength}

## Emb-1

48.1

3.33

32.1

60.2

1.10

## Emb-10

62.2

6.70

38.6

66.4

1.75

## Emb-20

61.9

7.11

39.3

65.6

1.85

InÔ¨Åx-tuning: INFIX-{PreÔ¨ÅxLength}

## Infix-1

67.9

8.63

45.8

69.4

2.42

## Infix-10

67.2

8.48

45.8

69.9

2.40

## Infix-20

66.7

8.47

45.8

70.0

2.42

Table 4: Intrinsic evaluation of Embedding-only (¬ß7.2)

and InÔ¨Åxing (¬ß7.3). Both Embedding-only ablation and

InÔ¨Åx-tuning underperforms full preÔ¨Åx-tuning.

random

"active"

"elephant"

"summarize"

"table-to-text:"

"banana"

"beautiful"

"divide"

"keep"

0.45

0.50

0.55

0.60

## Bleu

Figure 5: Initializing the preÔ¨Åx with activations of real

words signiÔ¨Åcantly outperforms random initialization,

in low-data settings.

et al., 2020), because discrete prompt restricts the

embedding layer to exactly match the embedding

of a real word. Consequently, we have this chain

of increasing expressive power: discrete prompting

< embedding-only ablation < preÔ¨Åx-tuning.

7.3

PreÔ¨Åxing vs InÔ¨Åxing

We also investigate how the trainable activations‚Äô

position in the sequence affects performance. In

preÔ¨Åx-tuning, we place them at the beginning

[PREFIX; x; y]. We can also place the trainable

activations between x and y (i.e. [x; INFIX; y]) and

call this inÔ¨Åx-tuning. Table 4 (bottom) shows that

inÔ¨Åx-tuning slightly underperforms preÔ¨Åx-tuning.

We believe this is because preÔ¨Åx-tuning can affect

the activations of x and y whereas inÔ¨Åx-tuning can

only inÔ¨Çuence the activations of y.

7.4

Initialization

We Ô¨Ånd that how the preÔ¨Åx is initialized has a large

impact in low-data settings. Random initialization

leads to low performance with high variance.

Initializing the preÔ¨Åx with activations of real words

## Page 9

signiÔ¨Åcantly improves generation, as shown in Fig-

ure 5. In particular, initializing with task relevant

words such as ‚Äúsummarization‚Äù and ‚Äútable-to-text‚Äù

obtains slightly better performance than task

irrelevant words such as ‚Äúelephant‚Äù and ‚Äúdivide‚Äù,

but using real words is still better than random.

Since we initialize the preÔ¨Åx with activations

of real words computed by the LM, this initial-

ization strategy is concordant with preserving the

pretrained LM as much as possible.

8

Discussion

In this section, we will discuss several favorable

properties of preÔ¨Åx-tuning and some open prob-

lems.

8.1

Personalization

As we note in ¬ß1, preÔ¨Åx-tuning is advantageous

when there are a large number of tasks that needs

to be trained independently. One practical setting is

user privacy (Shokri and Shmatikov, 2015; McMa-

han et al., 2016). In order to preserve user privacy,

each user‚Äôs data needs to be separated and a per-

sonalized model needs to be trained independently

for each user. Consequently, each user can be re-

garded as an independent task. If there are millions

of users, preÔ¨Åx-tuning can scale to this setting and

maintain modularity, enabling Ô¨Çexible addition or

deletion of users by adding or deleting their pre-

Ô¨Åxes without cross-contamination.

8.2

Batching Across Users

Under the same personalization setting, preÔ¨Åx-

tuning allows batching different users‚Äô queries even

though they are backed by different preÔ¨Åxes. When

multiple users query a cloud GPU device with their

inputs, it is computationally efÔ¨Åcient to put these

users in the same batch. PreÔ¨Åx-tuning keeps the

shared LM intact; consequently, batching requires

a simple step of prepending the personalized preÔ¨Åx

to user input, and all the remaining computation

is unchanged. In contrast, we can‚Äôt batch across

different users in adapter-tuning, which has person-

alized adapters between shared Transformer layers.

8.3

Inductive Bias of PreÔ¨Åx-tuning

Recall that Ô¨Åne-tuning updates all pretrained pa-

rameters, whereas preÔ¨Åx-tuning and adapter-tuning

preserve them. Since the language models are pre-

trained on general purpose corpus, preserving the

LM parameters might help generalization to do-

mains unseen during training. In concordance with

this intuition, we observe that both preÔ¨Åx-tuning

and adapter-tuning have signiÔ¨Åcant performance

gain in extrapolation settings (¬ß6.4); however, the

reason for such gain is an open question.

While preÔ¨Åx-tuning and adapter-tuning both

freeze the pretrained parameters, they tune different

sets of parameters to affect the activation layers of

the Transformer. Recall that preÔ¨Åx-tuning keeps the

LM intact and uses the preÔ¨Åx and the pretrained at-

tention blocks to affect the subsequent activations;

adapter-tuning inserts trainable modules between

LM layers, which directly add residual vectors to

the activations. Moreover, we observe that preÔ¨Åx-

tuning requires vastly fewer parameters compared

to adapter-tuning while maintaining comparable

performance. We think this gain in parameter efÔ¨Å-

ciency is because preÔ¨Åx-tuning keeps the pretrained

LM intact as much as possible, and therefore ex-

ploits the LM more than adapter-tuning.

Concurrent work by Aghajanyan et al. (2020)

uses intrinsic dimension to show that there exists

a low dimension reparameterization that is as ef-

fective for Ô¨Åne-tuning as the full parameter space.

This explains why good accuracy on downstream

task can be obtained by updating only a small num-

ber of parameters. Our work echoes the Ô¨Ånding by

showing that good generation performance can be

attained by updating a very small preÔ¨Åx.

9

Conclusion

We have proposed preÔ¨Åx-tuning, a lightweight al-

ternative to Ô¨Åne-tuning that prepends a trainable

continuous preÔ¨Åx for NLG tasks. We discover that

despite learning 1000x fewer parameters than Ô¨Åne-

tuning, preÔ¨Åx-tuning can maintain a comparable

performance in a full data setting and outperforms

Ô¨Åne-tuning in both low-data and extrapolation set-

tings.

References

Armen Aghajanyan, Luke Zettlemoyer, and Sonal

Gupta. 2020. Intrinsic dimensionality explains the

effectiveness of language model Ô¨Åne-tuning.

Anja Belz and Ehud Reiter. 2006.

Comparing auto-

matic and human evaluation of NLG systems.

In

11th Conference of the European Chapter of the

Association for Computational Linguistics, Trento,

Italy. Association for Computational Linguistics.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie

Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind

Neelakantan, Pranav Shyam, Girish Sastry, Amanda

## Page 10

Askell,

Sandhini Agarwal,

Ariel Herbert-Voss,

Gretchen Krueger, Tom Henighan, Rewon Child,

Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,

Clemens Winter, Christopher Hesse, Mark Chen,

Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin

Chess, Jack Clark, Christopher Berner, Sam Mc-

Candlish, Alec Radford, Ilya Sutskever, and Dario

Amodei. 2020. Language models are few-shot learn-

ers.

Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane

Hung, Eric Frank, Piero Molino, Jason Yosinski, and

Rosanne Liu. 2020. Plug and play language mod-

els: A simple approach to controlled text generation.

In International Conference on Learning Represen-

tations.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2019.

BERT: Pre-training of

deep bidirectional transformers for language under-

standing.

In Proceedings of the 2019 Conference

of the North American Chapter of the Association

for Computational Linguistics: Human Language

Technologies, Volume 1 (Long and Short Papers),

pages 4171‚Äì4186, Minneapolis, Minnesota. Associ-

ation for Computational Linguistics.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,

and Laura Perez-Beltrachini. 2017. The WebNLG

challenge: Generating text from RDF data. In Pro-

ceedings of the 10th International Conference on

Natural Language Generation, pages 124‚Äì133, San-

tiago de Compostela, Spain. Association for Compu-

tational Linguistics.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,

Bruna Morrone, Quentin De Laroussilhe, Andrea

Gesmundo, Mona Attariyan, and Sylvain Gelly.

2019. Parameter-efÔ¨Åcient transfer learning for NLP.

In Proceedings of the 36th International Conference

on Machine Learning, volume 97 of Proceedings

of Machine Learning Research, pages 2790‚Äì2799,

Long Beach, California, USA. PMLR.

Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham

Neubig. 2020.

How can we know what language

models know? Transactions of the Association for

Computational Linguistics, 8:423‚Äì438.

Mihir Kale. 2020. Text-to-text pre-training for data-to-

text tasks.

N. Keskar, B. McCann, L. R. Varshney, Caiming Xiong,

and R. Socher. 2019.

Ctrl: A conditional trans-

former language model for controllable generation.

ArXiv, abs/1909.05858.

Ben Krause, Akhilesh Deepak Gotmare, Bryan Mc-

Cann, Nitish Shirish Keskar, ShaÔ¨Åq Joty, Richard

Socher, and Nazneen Fatema Rajani. 2020. GeDi:

Generative Discriminator Guided Sequence Genera-

tion. arXiv preprint arXiv:2009.06367.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An

automatic metric for mt evaluation with high levels

of correlation with human judgments. In Proceed-

ings of the Second Workshop on Statistical Machine

Translation, StatMT ‚Äô07, pages 228‚Äì231, Strouds-

burg, PA, USA. Association for Computational Lin-

guistics.

Mike

Lewis,

Yinhan

Liu,

Naman

Goyal,

Mar-

jan Ghazvininejad, Abdelrahman Mohamed, Omer

Levy, Veselin Stoyanov, and Luke Zettlemoyer.

2020. BART: Denoising sequence-to-sequence pre-

training for natural language generation, translation,

and comprehension. In Proceedings of the 58th An-

nual Meeting of the Association for Computational

Linguistics, pages 7871‚Äì7880, Online. Association

for Computational Linguistics.

Chin-Yew Lin. 2004.

ROUGE: A package for auto-

matic evaluation of summaries. In Text Summariza-

tion Branches Out, pages 74‚Äì81, Barcelona, Spain.

Association for Computational Linguistics.

Zhaojiang Lin, Andrea Madotto, and Pascale Fung.

2020.

Exploring versatile generative language

model via parameter-efÔ¨Åcient transfer learning. In

Findings of the Association for Computational Lin-

guistics: EMNLP 2020, pages 441‚Äì459, Online. As-

sociation for Computational Linguistics.

Yang Liu and Mirella Lapata. 2019. Text summariza-

tion with pretrained encoders.

In Proceedings of

the 2019 Conference on Empirical Methods in Nat-

ural Language Processing and the 9th International

Joint Conference on Natural Language Processing

(EMNLP-IJCNLP), pages 3730‚Äì3740, Hong Kong,

China. Association for Computational Linguistics.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey

Edunov, Marjan Ghazvininejad, Mike Lewis, and

Luke Zettlemoyer. 2020.

Multilingual denoising

pre-training for neural machine translation.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-

dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,

Luke Zettlemoyer, and Veselin Stoyanov. 2019.

Roberta: A robustly optimized BERT pretraining ap-

proach. CoRR, abs/1907.11692.

Ilya Loshchilov and Frank Hutter. 2019.

Decoupled

weight decay regularization. In International Con-

ference on Learning Representations.

H. Brendan McMahan, Eider Moore, Daniel Ramage,

and Blaise Ag¬®uera y Arcas. 2016. Federated learn-

ing of deep networks using model averaging. Pro-

ceedings of the 20 th International Conference on

ArtiÔ¨Åcial Intelligence and Statistics (AISTATS) 2017,

abs/1602.05629.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.

2018. Don‚Äôt give me the details, just the summary!

Topic-aware convolutional neural networks for ex-

treme summarization. In Proceedings of the 2018

Conference on Empirical Methods in Natural Lan-

guage Processing, Brussels, Belgium.

## Page 11

Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.

2017. The E2E dataset: New challenges for end-to-

end generation. CoRR, abs/1706.09254.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-

Jing Zhu. 2002. Bleu: A method for automatic eval-

uation of machine translation.

In Proceedings of

the 40th Annual Meeting on Association for Com-

putational Linguistics, ACL ‚Äô02, pages 311‚Äì318,

Stroudsburg, PA, USA. Association for Computa-

tional Linguistics.

Jonas Pfeiffer, Aishwarya Kamath, Andreas R¬®uckl¬¥e,

Kyunghyun

Cho,

and

Iryna

Gurevych.

2020.

Adapterfusion:

Non-destructive task composition

for transfer learning.

Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand

Sivaprasad, Chiachun Hsieh, Nazneen Fatema Ra-

jani, Xiangru Tang, Aadit Vyas, Neha Verma,

Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto,

Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori

Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu,

Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, and

Richard Socher. 2020.

Dart: Open-domain struc-

tured data record to text generation.

A. Radford, Jeffrey Wu, R. Child, David Luan, Dario

Amodei, and Ilya Sutskever. 2019. Language mod-

els are unsupervised multitask learners.

Evani Radiya-Dixit and Xin Wang. 2020. How Ô¨Åne can

Ô¨Åne-tuning be? learning efÔ¨Åcient language models.

In Proceedings of the Twenty Third International

Conference on ArtiÔ¨Åcial Intelligence and Statistics,

volume 108 of Proceedings of Machine Learning Re-

search, pages 2435‚Äì2443, Online. PMLR.

Colin Raffel, Noam Shazeer, Adam Roberts, Kather-

ine Lee, Sharan Narang, Michael Matena, Yanqi

Zhou, Wei Li, and Peter J. Liu. 2020.

Exploring

the limits of transfer learning with a uniÔ¨Åed text-to-

text transformer. Journal of Machine Learning Re-

search, 21(140):1‚Äì67.

Sylvestre-Alvise RebufÔ¨Å, Hakan Bilen, and Andrea

Vedaldi. 2017.

Learning multiple visual domains

with residual adapters. In Advances in Neural Infor-

mation Processing Systems, volume 30, pages 506‚Äì

516. Curran Associates, Inc.

Timo Schick and Hinrich Sch¬®utze. 2020. Exploiting

cloze questions for few shot text classiÔ¨Åcation and

natural language inference.

Thibault Sellam, Dipanjan Das, and Ankur Parikh.

2020.

BLEURT: Learning robust metrics for text

generation. In Proceedings of the 58th Annual Meet-

ing of the Association for Computational Linguistics,

pages 7881‚Äì7892, Online. Association for Computa-

tional Linguistics.

Sheng Shen, Daniel Fried, Jacob Andreas, and Dan

Klein. 2019.

Pragmatically informative text gen-

eration.

In Proceedings of the 2019 Conference

of the North American Chapter of the Association

for Computational Linguistics: Human Language

Technologies, Volume 1 (Long and Short Papers),

pages 4060‚Äì4067, Minneapolis, Minnesota. Associ-

ation for Computational Linguistics.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV

au2, Eric Wallace, and Sameer Singh. 2020. Auto-

prompt: Eliciting knowledge from language models

with automatically generated prompts.

Reza Shokri and Vitaly Shmatikov. 2015.

Privacy-

preserving deep learning.

In Proceedings of

the 22nd ACM SIGSAC Conference on Computer

and Communications Security, CCS ‚Äô15, page

1310‚Äì1321, New York, NY, USA. Association for

Computing Machinery.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-

nea Micciulla, and Ralph Weischedel. 2006. A study

of translation error rate with targeted human annota-

tion. In In Proceedings of the Association for Ma-

chine Transaltion in the Americas (AMTA 2006.

Asa

Cooper

Stickland,

Xian

Li,

and

Marjan

Ghazvininejad.

2020.

Recipes

for

adapting

pre-trained monolingual and multilingual models to

machine translation.

Nishant

Subramani,

Samuel

## R.

Bowman,

and

Kyunghyun Cho. 2020.

Can unconditional lan-

guage models recover arbitrary sentences?

Fan-Keng Sun and Cheng-I Lai. 2020.

Conditioned

natural language generation using only uncondi-

tioned language model: An exploration.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob

Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz

Kaiser, and Illia Polosukhin. 2017. Attention is all

you need. In Advances in Neural Information Pro-

cessing Systems, volume 30, pages 5998‚Äì6008. Cur-

ran Associates, Inc.

Ramakrishna Vedantam, C. Lawrence Zitnick, and

Devi Parikh. 2015. Cider: Consensus-based image

description evaluation. In CVPR, pages 4566‚Äì4575.

IEEE Computer Society.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien

Chaumond, Clement Delangue, Anthony Moi, Pier-

ric Cistac, Tim Rault, R¬¥emi Louf, Morgan Funtow-

icz, Joe Davison, Sam Shleifer, Patrick von Platen,

Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,

Teven Le Scao, Sylvain Gugger, Mariama Drame,

Quentin Lhoest, and Alexander M. Rush. 2020.

Transformers: State-of-the-art natural language pro-

cessing. In Proceedings of the 2020 Conference on

Empirical Methods in Natural Language Processing:

System Demonstrations, pages 38‚Äì45, Online. Asso-

ciation for Computational Linguistics.

Jeffrey

## O

Zhang,

Alexander

Sax,

Amir

Zamir,

Leonidas Guibas, and Jitendra Malik. 2020a. Side-

tuning: A baseline for network adaptation via addi-

tive side networks.

## Page 12

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.

Weinberger, and Yoav Artzi. 2020b.

BERTScore:

Evaluating text generation with bert.

In Interna-

tional Conference on Learning Representations.

Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,

Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing

Liu, and Bill Dolan. 2020c. DIALOGPT : Large-

scale generative pre-training for conversational re-

sponse generation. In Proceedings of the 58th An-

nual Meeting of the Association for Computational

Linguistics:

System Demonstrations, pages 270‚Äì

278, Online. Association for Computational Linguis-

tics.

Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-

rich Sch¬®utze. 2020. Masking as an efÔ¨Åcient alterna-

tive to Ô¨Ånetuning for pretrained language models.

Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-

tian M. Meyer, and Steffen Eger. 2019. MoverScore:

Text generation evaluating with contextualized em-

beddings and earth mover distance. In Proceedings

of the 2019 Conference on Empirical Methods in

Natural Language Processing and the 9th Interna-

tional Joint Conference on Natural Language Pro-

cessing (EMNLP-IJCNLP), pages 563‚Äì578, Hong

Kong, China. Association for Computational Lin-

guistics.

Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,

Xipeng Qiu, and Xuanjing Huang. 2020.

Extrac-

tive summarization as text matching. In Proceedings

of the 58th Annual Meeting of the Association for

Computational Linguistics, pages 6197‚Äì6208, On-

line. Association for Computational Linguistics.

Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,

Wengang Zhou, Houqiang Li, and Tieyan Liu. 2020.

Incorporating bert into neural machine translation.

In International Conference on Learning Represen-

tations.

## Page 13

learning rate

# epoch

batch size

preÔ¨Åx length

PreÔ¨Åx:

## E2E

8e-05

5

10

5

WebNLG

5e-05

5

5

5

## Dart

5e-05

10

5

10

## Xsum

5e-05

30

14

100

Adapter:

## E2E (3%)

5e-05

5

5

-

## E2E (0.1%)

8e-05

10

5

WebNLG (3%)

5e-05

5

5

-

WebNLG (0.1%)

5e-05

10

5

-

## Dart (3%)

5e-05

5

5

-

## Dart (0.1%)

8e-05

5

5

-

Fine-tune:

## E2E

5e-05

5

10

-

WebNLG

1e-05

10

6

-

## Dart

1e-05

10

6

-

FT-top2:

## E2E

5e-05

5

10

-

WebNLG

5e-05

10

9

-

## Dart

5e-05

5

5

-

Table 5: Hyperparameter settings for our method and

baseline methods.

## A

Supplementary Material

## A.1

Hyperparameters

In Table 5, we report the hyperparameters used

to train the models documented in the experiment

section.

## A.2

Additional Results for Low-data Settings

Figure 6 supplements the low-data performance

curves in Figure 3 by plotting the relationship be-

tween training size and generation metrics for both

preÔ¨Åx-tuning and Ô¨Åne-tuning.

## A.3

Additional Results for the Initialization

Experiment

Figure 7 supplements Figure 3 by plotting addi-

tional metrics for our initialization technique ¬ß7.4.

It validates that random initialization (from a uni-

form (0,1) distirbution) signiÔ¨Åcantly underperforms

initializing with real words; Additionally, initializ-

ing with task-relevant words (e.g., ‚Äúsummarization‚Äù

and ‚Äútable-to-text‚Äù) attains slightly better gener-

ation scores than initializing with task-irrelevant

words (e.g., ‚Äúelephant‚Äù and ‚Äúbanana‚Äù).

## A.4

Qualitative Examples for Extrapolation

Table 6 contains qualitative examples from both

seen and unseen categories in WebNLG. We Ô¨Ånd

that for unseen categories, both preÔ¨Åx-tuning and

Ô¨Åne-tuning tend to undergenerate (generated out-

put do not cover full table contents) or generate

untruthfully (generated output is inconsistent with

table contents). In particular, preÔ¨Åx-tuning tends to

undergenerate whereas Ô¨Åne-tuning tends to gener-

ate untruthfully. For seen categories, both perform

fairly well in terms of coverage and truthfulness.

## Page 14

100

200

300

400

500

training_data_size

32

34

36

rouge-1

method

## Ft

## Pt

100

200

300

400

500

training_data_size

10

11

12

13

14

15

rouge-2

method

## Ft

## Pt

100

200

300

400

500

training_data_size

24

26

28

rouge-L

method

## Ft

## Pt

100

200

300

400

500

training_data_size

3

4

5

6

7

## Nist

method

## Ft

## Pt

100

200

300

400

500

training_data_size

0.32

0.34

0.36

0.38

## Meteor

method

## Ft

## Pt

100

200

300

400

500

training_data_size

1.2

1.4

1.6

1.8

CIDEr

method

## Ft

## Pt

Figure 6: PreÔ¨Åx-tuning (orange) outperforms Ô¨Åne-tuning (blue) in low-data regimes in addition to requiring many

fewer parameters. The top three plots correspond to summarization, measured by ROUGE-1, ROUGE-2, and

ROUGE-L. The bottom three plots correspond to table-to-text, measured by NIST, METEOR, and CIDEr. The

x-axis is the training size and the y-axis is the evaluation metric (higher is better).

random init

"active"

"elephant"

"summarize"

"table-to-text:"

"banana"

"beautiful"

"divide"

"keep"

2

3

4

5

6

7

## Nist

random init

"active"

"elephant"

"summarize"

"table-to-text:"

"banana"

"beautiful"

"divide"

"keep"

0.30

0.32

0.34

0.36

0.38

## Meteor

random init

"active"

"elephant"

"summarize"

"table-to-text:"

"banana"

"beautiful"

"divide"

"keep"

0.58

0.60

0.62

0.64

0.66

## Rouge

random init

"active"

"elephant"

"summarize"

"table-to-text:"

"banana"

"beautiful"

"divide"

"keep"

1.0

1.2

1.4

1.6

1.8

CIDEr

Figure 7: Initializing the preÔ¨Åx with activations of real words signiÔ¨Åcantly outperforms random initialization, in a

low-data setting with 100 training data.

## Page 15

Source [Unseen, Athelete]

(Al Kharaitiyat SC, club, Alaa Abdul-Zahra), (Al Khor, ground, Al Kharaitiyat SC), (Shabab Al-Ordon Club,

club, Alaa Abdul-Zahra) (Amar Osim, manager, Al Kharaitiyat SC)

PreÔ¨Åx-tuning

Al Kharaitiyat SC are managed by Amar Osim and play at their ground at Al Khor. Al Kharaitiyat SC are also

the club for which Alaa Abdul-Zahra is a player.

Fine-tuning

Alaa Abdul-Zahra plays for Al-Kharaitiyat SC and Shabab Al-Ordon Club. He also plays for Al-Khor and

manages Al-Kharaitiyat SC.

Reference

Alaa Abdul Zahra plays for Al Kharaitiyat SC which is located at Al Khor and managed by Amar Osim. The

Shabab Al-Ordon club is associated with Alaa Abdul-Zahra.

Source [Unseen, Transportation]

(Genoa, location, Costa Crociere), (AIDA Cruises, operator, AIDAstella), (Costa Crociere, owner, AIDAstella)

PreÔ¨Åx-tuning

AID Astella is operated by Aida Cruises and is owned by the Costa Rican tourist resort of Genoa.

Fine-tuning

AID Astella, operated by AIDA-Cruises, is located in Genoa and is owned by the Costa Rican government.

Reference

Costa Crociere is the owner of the AIDAstella and are based in Genoa. The operator of AIDAstella is AIDA

Cruises.

Source [Unseen, Politician]

(Euro, currency, Netherlands), (Stellendam, birthPlace, Ab Klink ), (Netherlands, nationality, Ab Klink)

PreÔ¨Åx-tuning

Ab Klink was born in Stellendam and is a national of the Netherlands where the currency is the Euro.

Fine-tuning

Ab Klink is a national of the Netherlands where the currency is the Euro. He was born in Stellendam.

Reference

Ab Klink was born in Stellendam in the Netherlands, where the national currency is the euro.

Source [Unseen, Politician]

(Robert E, Lee, commander, Battle of Salem Church), (American Civil War, isPartOfMilitaryConÔ¨Çict, Battle of

Salem Church), (Battle of Salem Church, battles, Aaron S. Daggett)

PreÔ¨Åx-tuning

Robert E. Lee was the commander of the Battle of Salem Church which was part of the military conÔ¨Çict in the

American Civil war.

Fine-tuning

The Battle of Salem Church is part of the American Civil War and was commanded by Robert E. Lee.

Reference

Robert E Lee was a commander in the Battle of Salem Church, which was one of the military conÔ¨Çicts in the

American Civil War. Aaron S Daggett fought in the same battle.

Source [Unseen, Artist]

(Christian alternative rock, musicSubgenre, Alternative rock), (Alternative rock, genre, Andrew White (musi-

cian))

PreÔ¨Åx-tuning

Andrew White is a Christian alternative rock musician.

Fine-tuning

Andrew White, a Christian alternative rocker, performs.

Reference

The musician Andrew White‚Äôs genre is alternative rock, the genre which has the sub genre Christian alternative

rock.

Source [Unseen, Artist]

(Hip hop music, genre, Allen Forrest), (solo singer, background, Allen Forrest)

PreÔ¨Åx-tuning

Allen Forrest is a solo singer.

Fine-tuning

Born in

Reference

Allen Forrest is a solo singer whose genre is Hip Hop music.

Source [Seen, ComicsCharacter]

(Americans, nationality, Ducan Rouleau), (Ducan Rouleau, creator, Baymax),(Alan Tudyk, starring, Big Hero 6

(Ô¨Ålm)), (Steven T Segle, creator, Baymax), (Big Hero 6 (Ô¨Ålm), serires, Baymax)

PreÔ¨Åx-tuning

Baymax is a character in Big Hero 6 which stars Alan Tudyk. He was created by Steven T. Seagle and the

American, Duncan Rouleau.

Fine-tuning

Alan Tudyk stars in the Ô¨Ålm Big Hero 6 in which Baymax is a character created by Steven T. Seagle and the

American, Duncan Rouleau.

Reference

Baymax is a character who appeared in Big Hero 6 starring Alan Tudyk. It was created by Steven T Seagle and

the American, Duncan Rouleau.

Source [Seen, City]

(Washington, D.C., capital, United States), (White Americans, ethnicGroup, United States), (United States,

country, New Jersey), (New York City, largest City, United States), (New Jersy, isPartOf, Atlantic City)

PreÔ¨Åx-tuning

Washington D.C. is the capital of the United States where the largest city is New York City and the White

Americans are an ethnic group. Atlantic City, New Jersey is also part of the United States.

Fine-tuning

Atlantic City, New Jersey is part of New Jersey in the United States. The capital city is Washington D.C. and

one of the ethnic groups is White Americans.

Reference

New York City (NYC) is the largest U.S. city. Atlantic City, New Jersey are also part of the United States with

its capital as Washington, DC and home to White Americans.

Table 6: Qualitative examples from WebNLG. The Ô¨Årst 6 examples are from the unseen categories, labeled next

to source; the last two examples are from the seen categories. For unseen categories, both preÔ¨Åx-tuning and Ô¨Åne-

tuning tend to undergenerate (generated output do not cover full table contents) or generate untruthfully (generated

output is inconsistent with table contents). In particular, preÔ¨Åx-tuning tends to undergenerate more often than

generate untruthfully whereas Ô¨Åne-tuning tends to generate untruthfully. For seen categories, both perform fairly

well in terms of coverage and truthfulness.
