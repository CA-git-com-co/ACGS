# 2402.01766_LLM-Voting-Human-Choices-and-AI-Collective-Decisio
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 2402.01766_LLM-Voting-Human-Choices-and-AI-Collective-Decisio.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2



## Implementation Status

- ‚úÖ **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- üîÑ **Performance Monitoring**: Continuous validation of targets
- ‚úÖ **Documentation Standards**: Compliant with ACGS-2 requirements
- üîÑ **Cross-Reference Validation**: Ongoing link integrity maintenance

**Overall Status**: üîÑ IN PROGRESS - Systematic enhancement implementation

## Performance Targets

This component maintains the following performance requirements:

- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

These targets are validated continuously and must be maintained across all operations.

---

## Page 1

LLM Voting: Human Choices and AI Collective Decision-Making

Joshua C. Yang, Damian Dailisan, Marcin Korecki, Carina I. Hausladen, Dirk Helbing

Computational Social Science

ETH Zurich

Z√ºrich, Switzerland

Abstract

This paper investigates the voting behaviors of Large Lan-

guage Models (LLMs), specifically GPT-4 and LLaMA-2,

their biases, and how they align with human voting patterns.

Our methodology involved using a dataset from a human

voting experiment to establish a baseline for human prefer-

ences and conducting a corresponding experiment with LLM

agents. We observed that the choice of voting methods and

the presentation order influenced LLM voting outcomes. We

found that varying the persona can reduce some of these bi-

ases and enhance alignment with human choices. While the

Chain-of-Thought approach did not improve prediction accu-

racy, it has potential for AI explainability in the voting pro-

cess. We also identified a trade-off between preference diver-

sity and alignment accuracy in LLMs, influenced by different

temperature settings. Our findings indicate that LLMs may

lead to less diverse collective outcomes and biased assump-

tions when used in voting scenarios, emphasizing the need for

cautious integration of LLMs into democratic processes.

1

Introduction

Recent breakthroughs in generative models have marked a

significant achievement in Artificial Intelligence (AI): the

creation of machines capable of fluently processing human

language. Large Language Models (LLMs) are increasingly

being integrated into a variety of services. As with many

discoveries before, a great deal of enthusiasm has been as-

sociated with LLMs and their potential applications. As ex-

citing and potentially useful LLMs may be, however, they

are also limited in various ways, and their na√Øve integration

may bring many unforeseen consequences.

LLMs have been used to reproduce classic economic,

psycho-linguistic, and social psychology experiments with

humans (Aher, Arriaga, and Kalai 2023; Argyle et al. 2023b;

Horton 2023), as well as behavior on social media plat-

forms (T√∂rnberg et al. 2023). Some propose using LLMs

to improve online democratic conversations (Argyle et al.

2023a). Another proposal is to use LLMs to perform human-

related scientific research (Boiko, MacKnight, and Gomes

2023; Bran et al. 2023; Sourati and Evans 2023).

Despite the impressive development of LLMs, however,

it is clear that LLMs are currently still limited and prone to

Copyright ¬© 2024, Association for the Advancement of Artificial

Intelligence (www.aaai.org). All rights reserved.

errors (Sobieszek and Price 2022; Floridi 2023). The ‚Äúhal-

lucinations‚Äù (Yao et al. 2023), proclivity for lies (Azaria

and Mitchell 2023), effects such as a strong primacy ef-

fect (Wang et al. 2023b), and clear political biases (Feng

et al. 2023) all question that LLMs can be safely used in

social settings.

In the area of digital democracy, an application of AI that

has sparked considerable debate is the concept of assisted

real-time voting. The proposal to use AI ‚Äúdigital twins‚Äù to

replace politicians has been raised (Hidalgo 2018). These AI

systems, including customized LLM agents, envisioned to

vote and mirror individual voter preferences (Gersbach and

Martinelli 2023). Proponents believe that such agents could

enable more nuanced and granular voting under voter super-

vision (Tang, Weyl, and the Plurality Community 2023). Yet,

ethical concerns about automation, democratic integrity, and

agent bias necessitate careful consideration. Allen and Weyl

(2024) argue that Generative Foundation Models (GFMs),

such as GPT-4, present unprecedented challenges for demo-

cratic institutions. The deployment of such technology in

democratic processes would require a cautious approach, en-

suring that the core values of democracy are upheld (Helbing

et al. 2022).

In our study, we explore the limitations and potentials of

integrating LLMs into collective decision-making processes.

We focus on examining LLM behavior in voting scenarios

by contrasting the voting patterns of human participants with

those of LLaMA-2 and GPT-4 agents. This analysis uses

data from a Participatory Budgeting (PB) voting experimen-

tal study conducted by Yang et al. (2024). The overall set-

ting of the study is illustrated in Fig 1. By investigating the

behavior of LLMs in this context, we seek to contribute to

the broader discourse on LLM decision-making, value align-

ment, AI explainability, and ethical use of AI in society.

2

Related Literature

LLM Autonomous Agents

The popularity of LLMs sparked a trend wherein researchers

used LLMs to model "agents" as entities with several sub-

modules. In their landmark paper, Park et al. (2023) show

LLM agents interacting with each other in a sandbox com-

munity exhibits human-like behavior. Boiko, MacKnight,

and Gomes (2023) suggests combining multiple LLMs for

arXiv:2402.01766v3  [cs.CL]  14 Aug 2024

## Page 2

Figure 1: Overview of the LLM voting experimental setup

the autonomous design, planning, and execution of scientific

experiments. For a literature survey on creating autonomous

LLM agents, we refer the reader to Wang et al. (2023a).

Research into multi-agent LLM systems, drawing inspi-

ration from social and biological cooperative systems, ex-

plores the dynamics of cooperative artificial agents (Axel-

rod and Hamilton 1981; Shoham and Leyton-Brown 2009;

Bonabeau, Dorigo, and Theraulaz 1999). Studies have in-

vestigated various LLM agents in both software and embod-

ied forms for complex tasks, such as text evaluation (Chan

et al. 2023), image captioning, and text-to-image synthe-

sis (Zhuge et al. 2023; Talebirad and Nadiri 2023; Zhang

et al. 2023b). Innovations include proactivity and adaptabil-

ity enhancement (Zhang et al. 2023a), standardized prompt-

ing mechanisms (Hong et al. 2023), roleplaying frameworks

for cooperation study (Li et al. 2023), and debating meth-

ods to improve answer quality (Wei et al. 2022; Hao et al.

2023; Du et al. 2023). Furthermore, researchers have ex-

plored the use of different personas within a single agent

to enhance complex task performance (Wang et al. 2023c).

While most studies focus on surpassing benchmarks in com-

plex tasks, some also focus on the intrinsic characteristics of

multi-agent LLM systems (Serapio-Garc√≠a et al. 2023; Liang

et al. 2023).

LLM Societies

One particular example of a multi-agent LLM system is

the creation of LLM societies. Such systems create envi-

ronments for studying debates and societal simulations. Liu

et al. (2023) introduces an open-source platform for sim-

ulating artificial human societies, while Hao et al. (2023)

proposes a ChatLLM network system for more objective

and comprehensive decision-making. Zhuge et al. (2023)

explores the concept of natural language-based societies of

mind, questioning their optimal governance structure. In

Wang et al. (2023c), the idea of Solo Performance Prompt-

ing (SPP) is proposed, transforming a single LLM into a

‚Äúcognitive synergist‚Äù through multi-turn self-collaboration

with multiple personas. Du et al. (2023) presents a method

where multiple language model instances propose, debate,

and refine their responses to reach consensus.

Liang et al. (2023) addresses the Degeneration-of-

Thought (DoT) problem with a Multi-Agent Debate frame-

work, where agents engage in ‚Äútit for tat‚Äù arguments and a

judge oversees the debate to derive a final solution. Talebi-

rad and Nadiri (2023) introduces a collaborative environ-

ment, where intelligent agents with distinct attributes and

roles communicate. Wu et al. (2023) discusses ‚ÄúAutogen‚Äù,

an open-source library facilitating conversation among mul-

tiple LLM agents. Furthermore, Ferland (2015) and Rank

and Mushtare (2019) explore the use of personas in civic

communication and the potential of digital twins to under-

stand voter behavior and address biases in democratic pro-

cesses.

Social Choice and Rationality

Social welfare functions play a critical role in aggregating

individual preferences into a collective social ranking (Sen

1995). This aggregation, denoted by R, derived from indi-

vidual rankings {Ri}, focuses on forming a coherent social

order while balancing fairness and rational decision-making

(Arrow 1951).

Rationality in social choice theory impacts voting behav-

iors and preference aggregation. It encompasses:

Internal Consistency of Choice: This aspect, fundamen-

tal in decision theory and economics, necessitates that an

individual‚Äôs choices are coherent and logically consistent

(Arrow 1951). In our research, this aspect of rationality is

tested by varying voting methods. Previous studies find that

the voting methods impacts voter preferences and behaviors

to a varying extent depends on the context (Yang et al. 2024;

Hausladen et al. 2023).

## Page 3

Self-Interest Maximization: Central to classical eco-

nomic theory is the assumption that individuals act to max-

imize their self-interest (Becker 1976). Additionally to con-

sistency, self-interest is added to the characterization of an

agent (Walsh 1994). The latter principle is typically mod-

eled as a utility function, where individuals choose options

that maximize their utility.

Maximization in General: Rationality extends to the

maximization of a defined objective function (Sen 1995). In

the context of voting, this often implies maximizing collec-

tive utility or welfare, where an individual selects the option

that yields the greatest collective benefit. This is connected

to the idea of other-regarding preferences (Blanco, Engel-

mann, and Normann 2011), describing a situation where an

individual‚Äôs utility is impacted by the well-being of others.

Our study explores these three dimensions of rational

choice, analyzing their influence on individual and collec-

tive voting behaviors.

Voting in Multi-Winner Systems

Collective actions in modern democracies are becoming

increasingly individualized and issue-driven, leading to a

state of "chaotic pluralism" that challenges traditional demo-

cratic processes (Margetts et al. 2016). Multi-winner sys-

tems, wherein voters select multiple candidates from a broad

field with the possibility of multiple winners (Skowron, Fal-

iszewski, and Slinko 2019; Elkind et al. 2017), offer a poten-

tial solution by aligning with pluralistic societal values and

diverse demands.

Multi-winner systems translates individual voter prefer-

ences into a collective decision, often seeking to maximize

collective utility or fairness (Aziz and Huang 2017). Ap-

proval voting is one common method, where the top candi-

dates receiving the most votes are elected (Brams and Fish-

burn 1983; Laslier and Van Der Straeten 2016).

In the context of LLMs, the presentation of voting options

as a list is a critical factor. Similar to how the arrangement

of candidates on a ballot can influence voter perception, the

order and format in which LLMs present multiple options

may affect the selection process, reflecting the need to un-

derstand how LLMs mimic these complex multi-candidate

and multi-winner decisions.

Opinion Sampling and Persona Creation

Our methodology, which compares actual population opin-

ions with LLM-generated responses, is in line with the work

of Durmus et al. (2023). They developed the GlobalOpin-

ionQA dataset and a metric for comparing LLM and human

responses, which is crucial to assess how well LLMs reflect

human opinions. Similarly, T√∂rnberg et al. (2023) simulated

social media environments using digital personas based on

demographic data. Their findings suggest that this approach

leads to healthier, less divisive online discussions.

LLMs and Human Behaviour

Exploring the intricacies of political biases and personality

traits in LLMs is crucial for their ethical application in hu-

man interactions. Serapio-Garc√≠a et al. (2023) investigate the

role of personality traits in LLMs, developing methods to

assess and customize them in advanced models. Feng et al.

(2023) delve into political biases of LLMs, examining their

origins from diverse pretraining data and effects on tasks

like hate speech detection. Meanwhile, Argyle et al. (2023b)

explore how LLMs can positively influence online politi-

cal discourse, with AI chat assistants improving conversa-

tion quality and receptiveness to different views. In addition,

Wang et al. (2023b) examine the primacy effect in ChatGPT,

finding that: i) ChatGPT‚Äôs decisions are sensitive to the or-

der of labels in prompts, and ii) there is a higher tendency

for ChatGPT to select labels in earlier positions as the an-

swer. (Abdurahman et al. 2024) also finds that AI models

resemble a single WEIRD (Western, Educated, Industrial-

ized, Rich, Democratic) individual rather than simulating a

diverse pool of participants. In summary, these studies un-

derscore the significance of recognizing personality traits in

LLMs and managing biases to ensure their ethical and ef-

fective use. The study by Wei et al. (2022) demonstrates that

‚ÄúChain-of-Thought‚Äù(CoT) prompting, which involves pro-

viding intermediate reasoning steps, enhances the problem-

solving abilities of LLMs. In their experiments, CoT signifi-

cantly improved performance on arithmetic, commonsense,

and symbolic reasoning tasks.

LLMs and Collective Intelligence

As language models (LLMs) are increasingly used to mimic

human behavior, recent research has also begun explor-

ing the collective behavior of LLMs. The work of Jarrett

et al. (2023) formalizes the problem of digital representa-

tion as the simulation of an agent‚Äôs behavior to yield equiv-

alent outcomes from the collective decision-making mech-

anism. Chuang et al. (2024) examines whether groups of

LLMs can mimic human behavior when role-playing as par-

tisan personas (like Democrats or Republicans). The authors

prompted the LLMs to role-play as different personas cre-

ated with varying levels of background detail. The research

finds that incorporating CoT reasoning or a lack of detailed

persona tends to diminish the wisdom of partisan crowds

effect, making the group less likely to converge to more ac-

curate beliefs.

Fish et al. (2023) merges social choice theory with the text

generation abilities of LLMs. This framework is designed to

facilitate complex decision-making, such as selecting tex-

tual statements that represent collective preferences. A re-

cent work by Gudi√±o-Rosero, Grandi, and Hidalgo (2024)

suggests that LLMs using augmented data can more accu-

rately predict the preferences of an entire participant pop-

ulation compared to probabilistic samples, which may not

be representative. They indicates that LLMs have significant

potential for constructing systems of augmented democracy.

3

Methods

In this paper, we compare the behavior of human voters in a

PB setting to that of LLM voters. For human voters, we used

data from Yang et al. (2024) who simulated a Participatory

Budgeting (PB) scenario in Zurich in an online experiment.

## Page 4

In this participatory budgeting program, you will be

voting for the allocation of the 60,000 CHF on city

projects in Zurich. You are a university student in

Zurich.

<DESCRIPTION | if Persona>*

Think about what projects you would personally like

to be funded here. Make sure your votes reflect

your preference. The explanation should be very

short.

Look at the 24 projects in the following table and

think about your preferences in urban projects

before you start voting.

## <List All Projects>

Select projects out of your personal interest using

‚Äò#‚Äô and project id.

## <Instruction For Voting Method>

Consider your personal interests and explain how

they might influence your decisions when choosing

urban projects in a participatory budgeting

program in Zurich.

Chain-of-thought response

Vote response

Initial Context

Chain-of-Thoughts*

Voting Prompt

*Persona and Chain-of-Thought templates are optional

Figure 2: Overview of the LLM prompt template.

The dataset1 contains 180 university student participants‚Äô

votes on urban projects, reflecting real urban interests for

allocating a CHF 60,000 budget across 24 projects. We sim-

ulated the voting of human participants using LLaMA-2 and

GPT-4 Turbo models. These models take in an Initial Con-

text with instructions and background that the human voters

also received in their experiements, followed by a Voting

Prompt for casting votes on the projects in a format that can

be processed (see Fig. 2).

Both the human participants and LLM agents were pre-

sented the same list with 24 projects (Table 3 in the ap-

pendix) in the voting process, detailed by ID, Name, Dis-

trict (Nord, S√ºd, Ost, West), Urban Category (Nature, Cul-

ture, Transportation), and Cost (CHF 5,000 or CHF 10,000).

There is exactly one project for each combination of these

characteristics.

Testing Different LLMs:

Our study uses two types of

agents: LLaMA-2 70B (Q8_0) and GPT-4 Turbo (GPT-4-

1106-preview). We chose the open-source LLaMA-2 to en-

sure other researchers can replicate and verify our experi-

ments, thereby enhancing the reliability of our research. In

1https://github.com/joshuay1/zurich-pb-voting

contrast, GPT-4 Turbo was selected for its widespread use

and relevance in current applications, making our findings

more applicable to real-world scenarios.

Aggregation:

In each experimental setting, we replicated

the voting process with 180 LLM voters, matching the sam-

ple size of the human experiment. LLM votes, formatted

with a ‚Äò#‚Äô and project ID in the textual responese, were

parsed using regex. The Borda Count method was ap-

plied to Ranked votes to convert ranks into points, while

the 10-point Cumulative votes were normalized to ensure

equal weighting. Outcomes were then aggregated, ordering

projects by total votes or points for a direct comparison with

human voter results.

Voting Methods

We explore the four most representative voting input meth-

ods in multi-winner settings, as identified by Yang et al.

(2024) in their voting experiment from which the data orig-

inate. Among these, 5-Approval voting is used as our base-

line due to its common usage in PB settings and its simplic-

ity, which facilitates the comparison of outcomes when each

voter is constrained to select an equal number of projects.

For each voting method, we use the same textual voting

instructions that were given to human voters for the LLM

agents. The voting instructions are as follows:

‚Ä¢ Approval: ‚ÄúSelect any number of projects. Here, in this

vote, you can select all the projects you approve of."

‚Ä¢ 5-Approval: ‚ÄúSelect exactly 5 projects."

‚Ä¢ Cumulative: ‚ÄúDistribute 10 points among the projects

you like. List the projects and the points you allocate,

separated by a colon."

‚Ä¢ Ranked: ‚ÄúSelect 5 projects and rank them from the most

preferred to the 5th most preferred."

Note: The conversion from ranks to points in Ranked vot-

ing follows the conventional Borda‚Äôs Method. (1st rank: 5

points, 2nd rank: 4 points, and so forth)

Evaluation of Voting Outcomes

In our study, we use three indices to compare voting out-

comes from LLM agents and human participants:

‚Ä¢ Aggregated preferences: We employ Kendall‚Äôs œÑ to

assess the similarity between LLMs and humans:

œÑ =

2

n(n‚àí1)

## P

i<j sgn(xi ‚àíxj)sgn(yi ‚àíyj), where n is

the number of observations, and xi, xj, yi, yj represent

ranked preferences.

‚Ä¢ Individual vote comparison: We utilize the Jaccard

Similarity index, defined as Ji(A, B) =

## |A‚à©B|

|A‚à™B|, where

A and B are the vote sets from the ith LLM and human

agents, respectively.

‚Ä¢ Preference diversity: The complement of Jaccard Simi-

larity (i.e. Jaccard Distance), DJi(A, B) = 1‚àíJi(A, B),

quantifies the diversity of preferences of voters. A higher

value indicates greater heterogeneity in voting patterns.

## Page 5

Experiments

Variation of the Temperature Parameter:

The tempera-

ture parameter t modulates the probability of choosing the

next token following p(tokeni) =

exp logiti/t

## P

j exp logitj/t. We sys-

tematically vary the temperature parameter within the range

t ‚àà[0, 2] with 0.5 increments, noting that the standard tem-

perature setting in ChatGPT is 1. This helps assess the im-

pact of different randomness levels on the model‚Äôs outputs.

Variation of List Presentation:

Our study focuses on two

list presentation variations in the context of multi-candidate

elections:

1. Primacy Effect: This investigates how the positioning of

items at the beginning of a list affects their choices.

2. Numerical Labeling Effect: This investigates the im-

pact of numerical IDs on the LLM‚Äôs decision-making

process.

By examining both the primacy effect and the numerical

labeling effect, the study aims to disentangle the influences

of item ordering and numerical labeling on LLM decision-

making.

Addition of Persona

We construct persona descriptions

from participant responses in the survey data conducted in

the same study of human votes. From the survey data, we

have the self-reported rating on urban category preferences

(Culture, Nature, Transport), district (Nord, S√ºd, Ost, West),

and importance of decision-making factors (district, urban

category, cost). These details are incorporated into the Initial

Context to introduce a variation of persona. By relying on

self-reported preferences rather than personal demographic

data, we effectively simulate diverse voter profiles for the

assisted-voting scenario, thus avoiding the ethical complex-

ities and privacy concerns associated with AI stereotyping

and the use of sensitive personal information.

Each entry consists of the participant‚Äôs ID, urban cate-

gory preferences, district information, connection to the city,

and importance ratings. The process of persona generation

involves processing survey data and using the numerical

scores to generate descriptive human language labels that

reflect the participants‚Äô preference intensity. The function

further combines this with participants‚Äô district preferences

and their prioritization of decision factors like district im-

portance, urban category significance, and cost, creating a

comprehensive and personalized narrative for each partici-

pant. Here is the persona constructed based on the example

in Appendix C:

You are a university student from Nord district in Zurich. In urban top-

ics, you have a strong preference for transport. When deciding on

projects, you find the district to be moderately important, the urban

category very important, and the cost of the project not important at

all.

Addition of Chain-of-Thought (CoT):

In the attempt

to improve performance without further complicating the

comparison between the results of human voters and LLM

agents, we applied CoT reasoning technique to the best-

performing model. In this experiment, the model used was

Figure 3: Histograms showing the frequency percentage of

a certain number of selected projects out of 24 projects in

Approval Voting between human and LLM voters.

focused on GPT-4. The agents would be prompted an addi-

tional time before the voting prompt with the project list (see

Fig. 2).

We specifically prompted agents to reflect on their pref-

erences before presenting the PB projects. This approach

aimed to mitigate the issue of ‚Äúunfaithful‚Äù CoT explana-

tions, as described by Turpin et al. (2024). By withholding

project information during the initial CoT phase, we pre-

vented the LLM agents from using pre-existing choices to

rationalize their decisions.

For reproducibility, the code and data used in our ex-

periments are available in this GitHub repository: github.

com/ethz-coss/LLM_voting. For technical appendices and

the appendix referenced in this paper, see the extended ver-

sion of this work on arXiv: arxiv.org/abs/2402.01766.

4

Results

Number of Selected Projects

The three voter types show distinct behaviors with respect

to approval and cumulative voting. As shown in figure 3,

human participants display a broad spectrum of approval

patterns, ranging from a minimum of 2 to a maximum of

all 24 projects. In contrast, LLaMA‚Äôs approval patterns form

bell-shaped distribution, peaking around 7 projects. GPT4

demonstrates an even narrower distribution, with a median

of 5 projects. In cumulative voting, LLaMA‚Äôs point alloca-

tions frequently surpassed the prescribed maximum of 10

points, reflecting a deficiency in the model‚Äôs numerical rea-

soning capabilities. In contrast, GPT4 adhered strictly to the

voting instruction of assigning 10 points.

Consistency across voting methods

Figure 4A shows the voting consistency of humans and

LLMs across voting methods measured by Kendall‚Äôs œÑ. This

metric ranges from ‚àí1 for complete disagreement, 0 indicat-

ing no discernible pattern in voting, to 1 for perfect agree-

ment across the various voting methods.

## Page 6

Human Average

œÑ = 0.81

GPT-4 Average

œÑ = 0.71

LLaMA-2 Average

œÑ = 0.46

## A

## B

Figure 4: A: Heatmap of Kendall‚Äôs œÑ across different voting methods with votes cast by human (‚Äú_h‚Äù), GPT-4 (‚Äú_g‚Äù), and

LLaMA-2 (‚Äú_l‚Äù) voters, showing the similarity between ranking orders of 24 projects, across different groups and methods.

Voting methods include 5-Approval (kapp), Approval (appr), Cumulative (10 points) (cumu), and Ranked (rank). Higher values

of œÑ (green) show greater agreement between the compared voting methods. B: Comparative analysis of the primacy effect

on LLM ranking behaviors in 5-Approval voting. The outcomes of human votes (baseline) are juxtaposed against results with

reversed presentation order or ID sequence, showing how the presentation sequence affects ranking. Project rankings are color-

coded to reflect their relative positioning, with green representing projects in the top half, and blue in the bottom half. In addition,

the top and bottom three projects are shown with darker hues. Disparities in œÑ values underscore the variable susceptibility of

each LLM to ordering effects in vote aggregation.

Human

voters

displayed

high

average

consistency

(œÑ: 0.81) across different voting input methods, that is, hu-

man voters are clear with their preference and vote consis-

tently when different voting input methods are presented.

Particularly, ranked voting (rank_h) exhibited the highest

consistency values for human voters, suggesting that the out-

come of ranked voting is more representative of the collec-

tive human preferences.

GPT-4 also showed considerable consistency (œÑ: 0.71)

across voting input methods, with its preferences most dis-

tinctly aligned in cumulative voting (cumu_g). LLaMA-2

had a lower average consistency score as compared to hu-

mans and GPT-4 agents (œÑ: 0.45). The LLaMA-2 œÑ value

in ranked voting (rank_l) was notably lower, indicating the

least alignment with other voting methods in their selection

of projects

Consistency across list order variations

To investigate how sensitive the LLMs are to the order of

projects, we presented the 24 projects in varying sequences

when prompting the LLM: original order, reversed order,

and original order with reversed project IDs. Upon exam-

ining the responses individually, each agent provided a con-

vincing rationale behind their votes, as denoted in example

responses in the appendix. However, the aggregated votes re-

veal a real shift in the collective outcome that is not obvious

in individual votes.

LLaMA-2 agents demonstrated considerable volatility in

preferences due to changing order; the reversed order caused

a substantial reshuffling (œÑ = ‚àí0.2). When the IDs were re-

versed, LLaMA-2 agents with reversed IDs displayed more

stability by keeping two of the top three projects (dark green)

at the top (Fig. 4B). GPT-4 agents showed a stronger consis-

tency when the order is reversed, but the reversed ID sce-

nario resulted in a notable disturbance, elevating the lowest-

ranked projects (dark blue). Kendall‚Äôs œÑ coefficients at the

bottom of Fig. 4B show that both LLMs experienced a sig-

nificant drop with reversed orders and IDs.

These results highlight the sensitivity of LLMs to list pre-

sentation, aligning with findings where ChatGPT showed a

similar sensitivity to label order (Wang et al. 2023b). How-

ever, our study did not observe the expected Primacy Effect,

where top-listed items are typically preferred. As shown in

Table 1 in the appendix, in our voting scenarios, there was

no significant correlation between project rankings and their

IDs, which represented their order on the list. These findings

also emphasize the importance of considering the aggre-

gated outcome to understand the potential impact of LLM

agents in democratic settings, beyond individual voting be-

haviors and LLM-generated rationales.

## Page 7

Incorporating Persona into LLM Voting

The personas of the LLM agents are constructed based on

human survey responses regarding the importance of ur-

ban categories and residential districts as well as the pri-

oritization of district characteristics, urban categories, and

decision-related costs.

When personas are applied via a static prompt to the LLM

models, individual votes become more aligned to human

votes for both, LLaMA-2 (from J = 0.14 to 0.21) and GPT-

4 (J = 0.18 to 0.30).

The incorporation of personas not only influences individ-

ual sets of votes but also increases the similarity in collec-

tive project rankings. Notably, GPT-4 with persona achieves

the highest œÑ of 0.54. Moreover, GPT-4 voters without per-

sona adjustments (œÑ = 0.39) still outperform LLaMA-2 vot-

ers with persona variations (œÑ = 0.12). LLaMA-2 with-

out persona creates an outcome (œÑ = ‚àí0.04) that is akin

to a random outcome. Qualitatively, GPT-4 without persona

exhibits a pronounced bias towards transportation projects.

With persona adjustments, however, GPT-4‚Äôs voting distri-

bution across various project types becomes more diverse

and therefore more aligned with human outcomes (Fig. 5B).

Incorporating Chain-of-Thought into LLM Voting

We analyze whether the inclusion of CoT reasoning en-

hances collective decision-making in LLMs. Our findings

indicate that CoT has minimal impact on both individual

and collective decision-making levels. As illustrated in Fig-

ure 5A, GPT-4 and LLaMA-2 agents that underwent the CoT

process demonstrate similar collective preferences to those

without CoT prompting. Figure 5B reveals that for GPT-4,

Jaccard Similarity values remain consistent with or without

CoT. For LLaMA-2, there‚Äôs even a slight decrease in simi-

larity (0.21 to 0.19) when CoT is used. The statistical anal-

ysis reinforces these observations, showing no significant

differences for either mode (GPT-4: T-statistic = 0.0119,

P-value

=

0.9906; LLaMA-2: T-statistic

=

1.6506,

P-value = 0.0997).

However, in analyzing the responses qualitatively, we find

that the addition of CoT has great potential to improve the

explanability of AI decision-making processes.

For example, GPT-4 agents in 5-approval voting scenarios

return simple selections of projects, such as:

#19,#20,#21,#22,#7

This output fails to clarify the rationale behind the de-

cisions. By contrast, incorporating CoT provides a more

comprehensive thought response. For example, here is the

thought response of agent 1 before voting (Full response in

appendix):

As a university student from the Nord district in Zurich with a strong

preference for transport within urban topics, my decisions in a par-

ticipatory budgeting program would be heavily influenced by propos-

als tackling issues of mobility and connectivity. Hence, I would favor

projects that aim to improve the transportation infrastructure, such as

the development of bike lanes, enhancement of public transport ser-

vices, and the integration of sustainable transport solutions. Projects

that specifically target improvements within the Nord district would be

of slightly less importance to me than the overall category of urban

transport, but I would still consider them favorably, especially if they

promise to better the daily commute and accessibility for residents,

including my fellow students. The cost of the project would not in-

fluence my decision, as I am focused on the potential benefits and

improvements in the transport sector rather than financial constraints.

My main criterion for voting for a project would be its potential to make

travel within Zurich more efficient, safer, and environmentally friendly.

This additional thought response offers valuable insights

into the reasoning behind the decisions for AI explainability.

The implications of this effect are further discussed in the

subsequent discussion section.

Qualitative Vote Comparison to Human Voters

There are some notable differences in the proportion of votes

allocated to certain types of projects by the LLaMA-2, GPT-

4, and human voters (Fig. 5A). LLaMA-2 agents demon-

strated a tendency to favor more budget-conscious options,

shown in the cost bar plot in Fig. 5A, often selecting projects

costing around CHF 5,000. Human voters are the least cost-

conscious voter in this study, with the highest votes favoring

the more expensive CHF 10,000 projects.

GPT-4 with no persona variation, in contrast, exhibits

a strong preference in urban category for transportation-

related projects. The intensity of this preference towards

transportation is not shared with human voters. This discrep-

ancy may stem from an over-reliance on the default ‚Äúuniver-

sity student‚Äù demographic profile, coupled with a stereotyp-

ical assumption about the preferences of this group. Incor-

porating persona variations into GPT-4 agents helps miti-

gate this bias, leading to voting outcomes that more closely

aligned with those of human voters.

For a more detailed comparison, see Figure 8 in the ap-

pendix, which lists project-specific differences. The voting

patterns of GPT-4 agents align slightly more with human

preferences than those of LLaMA-2 agents, with average

deviations of 101.9% and 141.9%, respectively. LLaMA-

2 agents exhibit a distinct preference for kid-oriented

projects (#3 and #9), diverging from human choices. Mean-

while, GPT-4 agents without persona variation heavily favor

projects enhancing bicycle infrastructure (#5, #7, #11).

Alignment between Stated Preference and Votes

We also analyzed how votes align with surveyed preferences

of human voters. As shown in Figure 6, human voters typ-

ically allocate nearly half of their votes to projects within

their self-reported district and preferred urban category, indi-

cating openness to supporting projects outside their primary

interests.

In contrast, LLaMA-2 agents without persona variation

exhibit voting patterns akin to random outcomes. For both

LLaMA-2 and GPT-4 agents, integrating personas leads to

a shift toward choices more aligned with stated preferences

and human votes. However, when personas are incorporated,

GPT-4 agents tend to over-align with self-reported districts

and urban interests, surpassing the human tendency for self-

prioritization. This overfitting suggests an excessive adher-

ence to the characteristics of the provided human prefer-

ences.

## Page 8

## A

## B

Figure 5: A: Stacked bar plots displaying the distribution of 5-approval votes among different districts (left) and urban project

categories (right) for various voter types, including human participants, LLaMA-2, GPT-4, and their respective enhancements

(persona, CoT). The addition of persona and CoT is denoted as ‚Äú+p‚Äù and ‚Äú+c‚Äù respectively. Each bar‚Äôs segment denotes the pro-

portion of votes contributed by each voter type to the district, category, or cost. B: Box plot of Jaccard Similarity of Individual

human votes against the agent votes. The addition of persona and CoT is denoted as ‚Äú+p‚Äù and ‚Äú+c‚Äù respectively. The short red

lines and annotated numbers denote mean values. The closer agents‚Äô votes are to human votes, the closer the value is to 1.0.

random

human

llama2

llama2+p

gpt4

gpt4+p

gpt4+p+c

0

20

40

60

80

100

Vote for Self Interest (%)

Interest type

District

Category

Figure 6: Box plots illustrating the distribution of vote per-

centages towards the voters‚Äô self-identified district and cat-

egory interests. The red lines represent the median values,

while the diamond symbols denote the mean values for each

group. A random voting situation is added for a clearer com-

parison with voting outcomes that consist of a particular

preference.

Diversity in Different Temperature Settings

Analysis of LLM agents‚Äô voting behaviors across differ-

ent temperature settings reveals distinct trends in how these

settings influence collective decision-making processes. At

lower temperatures, both GPT-4 and LLaMA-2 agents

demonstrate highly concentrated preferences, consistently

selecting similar project sets. This behavior aligns with the

deterministic nature of lower temperatures, where the LLMs

favor the most probable outcomes based on their training,

leading to limited diversity in choices.

As temperatures increase, there is a notable shift towards

broader and more diverse preferences. This is particularly

evident at a temperature of 2, where LLM agents begin to

explore a wider array of options, reflecting a range of hu-

man interests (Fig. 7B). The increased randomness allows

the models to simulate a more human-like variance in prefer-

ences, although this comes at the cost of reduced predictabil-

ity in outcomes.

For our analysis, we simplified our description by equat-

ing the limit as temperature approaches zero (t ‚Üí+0) with

a temperature of 0. Mathematically, as t ‚Üí+0, the softmax

function converges to argmax, selecting the token in a per-

fectly deterministic manner. This behavior was observed in

our experiments with LLAMA-2 but notably not with GPT-4

in Figure 7B, suggesting a possible divergence in their soft-

max implementation.

The alignment of LLaMA-2 and GPT-4 agents across dif-

ferent temperature settings with human voting patterns is

illustrated in Figure 7A. Among the LLaMA-2 and GPT-4

voters themselves, outcomes generated with the temperature

setting of 1.0 are found to be the most stable, as they consis-

tently exhibit high Kendall‚Äôs œÑ values above 0.8 compared

to settings of 0.5 and 1.5.

When we compare LLM outcomes with human outcome,

however, the Kendall‚Äôs œÑ values are relatively low across

all temperature settings for LLaMA-2, indicating a consis-

tent lack of alignment with human voting outcomes. GPT-4

agents achieve a better alignment with human outcome at

a temperature setting of 1.0, as evidenced by the highest œÑ

values, 0.39, in the first column of Figure 7A.

As expected, increasing the temperature setting in GPT-

4 introduces greater diversity in the voting outcomes but

also leads to more pronounced deviations from human-

like decision-making. For instance, when the temperature

is raised from 1.0 to 1.5, there is only a minor reduction

## Page 9

## A

## B

0.00

Figure 7: A: Heatmap displaying Kendall‚Äôs œÑ coefficient across various temperature settings of LLaMA-2 and GPT-4 models,

compared directly with human vote outcomes. Each cell represents the similarity between the collective ranked results of voting

sets, with temperature denoted by t. Values closer to 1 indicate a higher similarity in the overall ordering of outcomes, whereas

values closer to 0 indicate randomness. B: Box plot illustrating the Jaccard Dissimilarity between all individual votes from

human voters and various temperature settings of LLaMA-2 and GPT-4 models. The mean for each group is indicated by a

short red line, with the numerical value displayed. Dissimilarity values closer to 1 suggest greater average deviation between

vote sets, indicating less similarity in voting patterns.

in Kendall‚Äôs œÑ from 0.39 to 0.37, which suggests a slight

decrease in alignment with human outcomes. At the same

time, this increase in temperature causes a significant rise in

the Jaccard Dissimilarity of the votes, from 0.44 to 0.67, as

shown in Figure 7B. In the subsequent discussion, we will

assess whether the modest reduction in alignment is an ac-

ceptable compromise for the substantial gain in response di-

versity.

The alignment of LLaMA-2 and GPT-4 agents across dif-

ferent temperature settings with human voting patterns is

illustrated in Figure 7A. Among the LLaMA-2 and GPT-4

voters, outcomes generated with the temperature setting of

1.0 seems to be the most stable, as they are relatively con-

sistent with that of 0.5 or 1.5 with œÑ all above 0.8.

For LLaMA-2 agents, the Kendall‚Äôs œÑ values are close to

0 across all temperature settings, indicating a consistent lack

of alignment with human voting outcomes. In contrast, GPT-

4 agents achieve optimal alignment at a temperature setting

of 1, as evidenced by the highest œÑ values. Overall, when a

higher temperature introduces more diversity into the voting

outcomes, it also results in a greater deviation from human-

like decision-making patterns. However, increasing the tem-

perature to 1.5 results in only a slight decrease in œÑ from 0.39

to 0.37, yet affects the Jaccard Dissimilarity or diversity of

the votes, as shown in Figure 7B, jumps from 0.44 to 0.67.

The temperature of 1.5 seems to balance the need for diver-

sity and the desire to maintain coherence with human-like

decision-making.

5

Discussion

We compared the voting behaviors of LLMs and humans

using voting data from real-world PB experimental study.

Our results reveal distinct differences in human and LLM

voting patterns and collective outcomes. Humans display

significant variation in the number of approved projects,

while LLM agents tend to approve fewer and exhibit more

uniform behavior, even with the addition of varying per-

sonas. This insight is consistent with the finding of (Abdu-

rahman et al. 2024) that synthetic AI-simulated sampling has

a WEIRD (Western, Educated, Industrialized, Rich, Demo-

cratic) bias and often fail to show meaningful variance (or

diversity) in their judgments.

The results also show that the choice of voting method in-

fluence LLM votes more so than it affects humans votes.

Humans demonstrated proficiency in ranking and relative

comparisons. For the LLM agents, GPT-4 excelled in quan-

tifying and distributing preferences using cumulative voting,

whereas LLaMA-2 notably showed inconsistent voting, with

ranked voting outcome being the least consistent outcome.

This suggests that for LLM agents to make collective de-

cisions, cumulative voting, where agents can assign precise

points, might be a better choice than ranked voting.

Additionally, list ordering also influenced the LLM vot-

ing. Although these agents often provide convincing justi-

fications for their votes in their responses (Appendix E), a

quantifiable change in collective votes reveals an inconsis-

tency in their preferences. This highlights the importance of

## Page 10

looking into aggregated votes on top of individual LLM re-

sponses in understanding LLM behavior in the democratic

context.

Without adding a persona, LLMs display distinct prefer-

ences not shared by humans. For example, LLaMa-2 prefers

kids-related projects, while GPT-4 shows a strong inclina-

tion towards transportation projects. The addition of per-

sonas partly offsets these innate preferences but introduces

new challenges, leading LLMs to base their votes predom-

inantly on persona-specific reported districts and urban in-

terests. In contrast, human votes rely less on these factors,

suggesting that their choices reflect not only self-interest but

are also driven by other-regarding preferences, such as con-

sidering the community‚Äôs interest.

Several strategies proposed in the literature to improve

LLM behavior did not help in aligning them more closely

with humans: Chain-of-thought (CoT) reasoning neither

aligns LLM individual preferences nor collective outcomes

more with humans, consistent with previously reported re-

sults (Chuang et al. 2024). Furthermore, varying the temper-

ature introduces a trade-off between diversity and accuracy

in human alignment. At a temperature of 1, where GPT-4

shows the highest similarity to human voting, the preference

range is overly concentrated. At higher temperatures, LLM

agents match the human preference diversity range but pro-

duce more random and less accurate votes.

Some aspects of LLMs could support human voting pat-

terns in PB settings. Specifically, unlike human voters who

are often less cost-conscious in their selection of projects,

LLMs show a higher preference for low-budget projects.

LLMs are also relatively effective in quantifying their pref-

erences, as shown in the consistency of cumulative vot-

ing compared to the outcomes with other voting methods.

This suggests that LLMs could potentially assist humans in

decision-making processes involving cost considerations or

more accurately present human preferences in a quantifiable

manner.

Limitations:

(1) Context Specificity: Our findings concerning participa-

tory budgeting and multi-winner elections may not be so

generalizable, as it does not directly apply to the more com-

mon single-winner election formats (such as presidential

elections). (2) Order Effects: We find that LLM votes are

susceptible to order effects. While humans also exhibit pri-

macy and recency effects (Van Schaik, Kusev, and Julius-

son 2011), our PB voting data did not include a treatment

for measuring these order effects in humans. As a result,

we could not quantify the difference between humans and

LLMs. Future research is needed to address this gap.

Future Directions:

(1) Persona Construction: Our results show that personas

align LLM preferences more closely with human prefer-

ences in terms of topic preference. However, they also cause

a divergence from human preferences regarding the balance

between self-interest and other-regarding preferences. Fu-

ture research should focus on creating persona descriptions

that balance both aspects to better mirror human voting be-

havior. (2) Performance and Diversity: Varying temperature

revealed a trade-off between model performance and diver-

sity in LLM voting. Future research should investigate the

extent to which improved alignment with human votes justi-

fies the lack of diversity among decision-making agents. (3)

Exploring Various Election Types: We tested a multi-winner

election format in a PB setting. Testing additional election

formats in different contexts would broaden the understand-

ing of LLM applicability in democratic processes.

6

Conclusion

This study analyzed and compared the voting behaviors of

LLM agents and human voters using data from real-world

Participatory Budgeting experimental study. Our findings re-

veal significant differences between LLM and human voting

behaviors in several key areas: the number of projects ap-

proved, consistency across different voting methods, qual-

itative preferences, and the degree of self-interest exhib-

ited in voting patterns. We explored various strategies to

align LLM behaviors more closely with human behaviors,

but each strategy presented notable trade-offs. Incorporat-

ing personas increased similarity in preferences for certain

topics but also made LLMs more self-interested compared

to humans. Furthermore, no temperature setting effectively

maintained both the diversity and the human-like nature of

preferences. A notable advantage of LLMs in the context of

PB is their cost awareness and their ability to rationalize de-

cisions. While humans often lack cost-consciousness in par-

ticipatory budgeting settings, LLMs demonstrate a greater

consideration of costs. Additionally, LLMs using the Chain-

of-Thought approach can provide reasoning before voting

decisions, making them potentially useful for explaining

collective decision-making outcomes, which can be difficult

for voters with varying opinions to understand.

Overall, we empirically quantified some the biases and

limitations of using LLMs in a democratic context. While

LLM agents with personas can be useful in predicting vot-

ing patterns, our results indicate that the biases and lack of

diversity in LLM decisions require critical evaluation be-

fore their deployment in democratic decision-making pro-

cesses. Instead, we propose that LLMs be more effectively

utilized within a human-in-the-loop framework. In this con-

text, LLMs can function as support tools, assisting humans

in addressing specific deficiencies in their decision-making

processes, such as enhancing cost-consciousness, overcom-

ing common human cognitive biases, summarization, expla-

nation, or providing nuances and additional context for pub-

lic policy and public sentiment.

Impact Statement

This study critically examines the use of LLMs in demo-

cratic systems, particularly in the voting process. It reveals

that the current capabilities of LLMs are inadequate for cap-

turing the full spectrum of human perspectives, which can

threaten the integrity of democratic outcomes. The findings

highlight the need for strict ethical guidelines to manage

the integration of AI into democratic contexts. A human-

## Page 11

centered approach is essential to ensure that AI deployment

supports rather than compromises the collective intelligence

derived from diverse human preferences in society.

Acknowledgments

JY would like to express gratitude to the Swiss National

Science Foundation (SNSF) for the financial support pro-

vided for the previously conducted PB voting experiment in

Yang et al. (2024), which was part of the National Research

Programme NRP77 on Digital Transformation (project no.

187249). JY also thanks Dominik Peters, Regula H√§nggli

Fricker, and Evangelos Pournaras for their intellectual con-

tributions. MK, CIH, and DH acknowledge the support from

the ‚ÄúCoCi: Co-Evolving City Life‚Äù project, funded by the

European Research Council (ERC) under the EU‚Äôs Horizon

2020 research and innovation program (grant no. 833168).

DD acknowledges the support from the Distributed Intelli-

gence and Technology for Traffic and Mobility Management

(DIT4TraM) project, funded by the EU‚Äôs Horizon 2020 Re-

search and Innovation Programme (grant no. 953783).

References

Abdurahman, S.; Atari, M.; Karimi-Malekabadi, F.; Xue,

M. J.; Trager, J.; Park, P. S.; Golazizian, P.; Omrani, A.; and

Dehghani, M. 2024. Perils and opportunities in using large

language models in psychological research. PNAS Nexus, 3.

Aher, G.; Arriaga, R. I.; and Kalai, A. T. 2023. Using Large

Language Models to Simulate Multiple Humans and Repli-

cate Human Subject Studies. arXiv:2208.10264.

Allen, D.; and Weyl, E. 2024. The Real Dangers of Genera-

tive AI. Journal of Democracy, 35(1): 147‚Äì162.

Argyle, L. P.; Bail, C. A.; Busby, E. C.; Gubler, J. R.;

Howe, T.; Rytting, C.; Sorensen, T.; and Wingate, D. 2023a.

Leveraging AI for democratic discourse: Chat interventions

can improve online political conversations at scale.

Pro-

ceedings of the National Academy of Sciences, 120(41):

e2311627120.

Publisher: Proceedings of the National

Academy of Sciences.

Argyle, L. P.; Busby, E. C.; Fulda, N.; Gubler, J. R.; Ryt-

ting, C.; and Wingate, D. 2023b. Out of One, Many: Using

Language Models to Simulate Human Samples. Political

Analysis, 31(3): 337‚Äì351.

Arrow, K. J. 1951. Social Choice and Individual Values.

Yale University Press.

Axelrod, R.; and Hamilton, W. D. 1981. The Evolution of

Cooperation. Basic Books.

Azaria, A.; and Mitchell, T. 2023. The Internal State of an

LLM Knows When It‚Äôs Lying. arXiv:2304.13734.

Aziz, H.; and Huang, S. 2017. A Polynomial-time Algo-

rithm to Achieve Extended Justified Representation. ar5iv.

Becker, G. S. 1976. The Economic Approach to Human Be-

havior. University of Chicago Press.

Blanco, M.; Engelmann, D.; and Normann, H. T. 2011.

A within-subject analysis of other-regarding preferences.

Games and Economic Behavior, 72(2): 321‚Äì338.

Boiko, D. A.; MacKnight, R.; and Gomes, G. 2023. Emer-

gent autonomous scientific research capabilities of large lan-

guage models. ArXiv:2304.05332 [physics].

Bonabeau, E.; Dorigo, M.; and Theraulaz, G. 1999. Swarm

Intelligence: From Natural to Artificial Systems.

Oxford

University Press.

Brams, S. J.; and Fishburn, P. C. 1983. Approval Voting.

Boston: Birkh√§user. ISBN 9783764331085.

Bran, A. M.; Cox, S.; Schilter, O.; Baldassari, C.;

White, A. D.; and Schwaller, P. 2023.

ChemCrow:

Augmenting large-language models with chemistry tools.

arXiv:2304.05376.

Chan, C.-M.; Chen, W.; Su, Y.; Yu, J.; Xue, W.; Zhang,

S.; Fu, J.; and Liu, Z. 2023.

ChatEval: Towards Bet-

ter LLM-based Evaluators through Multi-Agent Debate.

arXiv:2308.07201.

Chuang, Y.-S.; Harlalka, N.; Suresh, S.; Goyal, A.; Hawkins,

R.; Yang, S.; Shah, D.; Hu, J.; and Rogers, T. T. 2024. The

Wisdom of Partisan Crowds: Comparing Collective Intelli-

gence in Humans and LLM-based Agents. In Proceedings

of the Annual Meeting of the Cognitive Science Society, vol-

ume 46.

Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch,

I. 2023. Improving Factuality and Reasoning in Language

Models through Multiagent Debate. ArXiv:2305.14325 [cs].

Durmus, E.; Nyugen, K.; Liao, T. I.; Schiefer, N.; Askell, A.;

Bakhtin, A.; Chen, C.; Hatfield-Dodds, Z.; Hernandez, D.;

Joseph, N.; Lovitt, L.; McCandlish, S.; Sikder, O.; Tamkin,

A.; Thamkul, J.; Kaplan, J.; Clark, J.; and Ganguli, D. 2023.

Towards Measuring the Representation of Subjective Global

Opinions in Language Models. ArXiv:2306.16388 [cs].

Elkind, E.; Faliszewski, P.; Skowron, P.; and et al. 2017.

Properties of Multiwinner Voting Rules. Social Choice and

Welfare, 48(3): 599‚Äì632.

Feng, S.; Park, C. Y.; Liu, Y.; and Tsvetkov, Y. 2023. From

Pretraining Data to Language Models to Downstream Tasks:

Tracking the Trails of Political Biases Leading to Unfair

NLP Models. ArXiv:2305.08283 [cs].

Ferland, B. 2015. A rational or a virtuous citizenry? ‚Äì The

asymmetric impact of biases in votes-seats translation on cit-

izens‚Äô satisfaction with democracy. Electoral Studies, 40:

394‚Äì408.

Fish, S.; G√∂lz, P.; Parkes, D. C.; Procaccia, A. D.; Rusak,

G.; Shapira, I.; and W√ºthrich, M. 2023. Generative social

choice. arXiv preprint arXiv:2309.01291.

Floridi, L. 2023. AI as agency without intelligence: on Chat-

GPT, large language models, and other generative models.

Philosophy & Technology, 36(1): 15.

Gersbach, H.; and Martinelli, C. 2023. Supported democ-

racy: reinventing direct democracy, AI and voting twice. Ac-

cessed: 2024-01-24.

Gudi√±o-Rosero, J.; Grandi, U.; and Hidalgo, C. A. 2024.

Large Language Models (LLMs) as Agents for Augmented

Democracy. arXiv preprint arXiv:2405.03452.

## Page 12

Hao, R.; Hu, L.; Qi, W.; Wu, Q.; Zhang, Y.; and Nie, L.

2023. ChatLLM Network: More brains, More intelligence.

ArXiv:2304.12998 [cs].

Hausladen, C. I.; H√§nggli, R.; Helbing, D.; Kunz, R.; Wang,

J.; and Pournaras, E. 2023. On the Legitimacy of Voting

Methods. Available at SSRN 4372245.

Helbing, D.; Mahajan, S.; H√§nggli, R.; Musso, A.; Haus-

laden, C. I.; Carissimo, C.; Carpentras, D.; Stockinger, E.;

Argota S√°nchez-Vaquerizo, J.; Yang, J.; et al. 2022. Democ-

racy by Design: Perspectives for Digitally Assisted, Partici-

patory Upgrades of Society. Participatory Upgrades of So-

ciety (November 2, 2022), 71.

Hidalgo, C. 2018. Augmented Democracy. Accessed: 2024-

01-12.

Hong, S.; Zhuge, M.; Chen, J.; Zheng, X.; Cheng, Y.; Zhang,

C.; Wang, J.; Wang, Z.; Yau, S. K. S.; Lin, Z.; Zhou, L.; Ran,

C.; Xiao, L.; Wu, C.; and Schmidhuber, J. 2023. MetaGPT:

Meta Programming for A Multi-Agent Collaborative Frame-

work. arXiv:2308.00352.

Horton, J. J. 2023. Large Language Models as Simulated

Economic Agents: What Can We Learn from Homo Silicus?

ArXiv:2301.07543 [econ, q-fin].

Jarrett, D.; Pislar, M.; Bakker, M. A.; Tessler, M. H.; Koster,

R.; Balaguer, J.; Elie, R.; Summerfield, C.; and Tacchetti, A.

2023. Language agents as digital representatives in collec-

tive decision-making. In NeurIPS 2023 Foundation Models

for Decision Making Workshop.

Laslier, J.-F.; and Van Der Straeten, K. 2016. Strategic vot-

ing in multi-winner elections with approval balloting: a the-

ory for large electorates. Social Choice and Welfare, 47(3):

559‚Äì587.

Li, G.; Hammoud, H. A. A. K.; Itani, H.; Khizbullin, D.;

and Ghanem, B. 2023. CAMEL: Communicative Agents

for "Mind" Exploration of Large Language Model Society.

arXiv:2303.17760.

Liang, T.; He, Z.; Jiao, W.; Wang, X.; Wang, Y.; Wang, R.;

Yang, Y.; Tu, Z.; and Shi, S. 2023. Encouraging Divergent

Thinking in Large Language Models through Multi-Agent

Debate.

Liu, R.; Yang, R.; Jia, C.; Zhang, G.; Zhou, D.; Dai,

A. M.; Yang, D.; and Vosoughi, S. 2023. Training Socially

Aligned Language Models on Simulated Social Interactions.

ArXiv:2305.16960 [cs].

Margetts, H.; John, P.; Hale, S.; and Yasseri, T. 2016. Politi-

cal Turbulence: How Social Media Shape Collective Action.

Princeton University Press. ISBN 9780691159225.

Park, J. S.; O‚ÄôBrien, J. C.; Cai, C. J.; Morris, M. R.; Liang,

P.; and Bernstein, M. S. 2023. Generative Agents: Interac-

tive Simulacra of Human Behavior. ArXiv:2304.03442 [cs].

Rank, A. D.; and Mushtare, R. 2019. Using Personas as a

Tool for Teaching Civic Communication. The Journal of

General Education, 68(3-4): 252‚Äì262.

Sen, A. 1995. Rationality and social choice. The American

economic review, 85(1): 1.

Serapio-Garc√≠a, G.; Safdari, M.; Crepy, C.; Sun, L.; Fitz,

S.; Romero, P.; Abdulhai, M.; Faust, A.; and Matari¬¥c,

## M. 2023.

Personality Traits in Large Language Models.

ArXiv:2307.00184 [cs].

Shoham, Y.; and Leyton-Brown, K. 2009. Multiagent Sys-

tems: Algorithmic, Game-Theoretic, and Logical Founda-

tions. Cambridge University Press.

Skowron, P.; Faliszewski, P.; and Slinko, A. 2019.

Ax-

iomatic characterization of committee scoring rules. Journal

of Economic Theory, 180: 244‚Äì273.

Sobieszek, A.; and Price, T. 2022. Playing Games with Ais:

The Limits of GPT-3 and Similar Large Language Models.

Minds and Machines, 32(2): 341‚Äì364.

Sourati, J.; and Evans, J. A. 2023.

Accelerating science

with human-aware artificial intelligence. Nature Human Be-

haviour, 7(10): 1682‚Äì1696.

Talebirad, Y.; and Nadiri, A. 2023.

Multi-Agent Collab-

oration: Harnessing the Power of Intelligent LLM Agents.

ArXiv:2306.03314 [cs].

Tang, A.; Weyl, G.; and the Plurality Community. 2023. Plu-

rality: The Future of Collaborative Technology and Democ-

racy.

Turpin, M.; Michael, J.; Perez, E.; and Bowman, S. 2024.

Language models don‚Äôt always say what they think: unfaith-

ful explanations in chain-of-thought prompting. Advances

in Neural Information Processing Systems, 36.

T√∂rnberg, P.; Valeeva, D.; Uitermark, J.; and Bail, C.

2023.

Simulating Social Media Using Large Language

Models to Evaluate Alternative News Feed Algorithms.

ArXiv:2310.05984 [cs].

Van Schaik, P.; Kusev, P.; and Juliusson, A. 2011. Human

preferences and risky choices.

Walsh, V. 1994. Rationality as Self-Interest versus Rational-

ity as Present Aims. The American Economic Review, 84(2):

401‚Äì405.

Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.;

Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; Zhao, W. X.; Wei, Z.;

and Wen, J.-R. 2023a. A Survey on Large Language Model

based Autonomous Agents. ArXiv:2308.11432 [cs].

Wang, Y.; Cai, Y.; Chen, M.; Liang, Y.; and Hooi, B. 2023b.

Primacy Effect of ChatGPT. ArXiv:2310.13206 [cs].

Wang, Z.; Mao, S.; Wu, W.; Ge, T.; Wei, F.; and Ji, H.

2023c. Unleashing Cognitive Synergy in Large Language

Models: A Task-Solving Agent through Multi-Persona Self-

Collaboration. ArXiv:2307.05300 [cs].

Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;

Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022.

Chain-of-

thought prompting elicits reasoning in large language mod-

els. Advances in Neural Information Processing Systems,

35: 24824‚Äì24837.

Wu, Q.; Bansal, G.; Zhang, J.; Wu, Y.; Li, B.; Zhu, E.; Jiang,

L.; Zhang, X.; Zhang, S.; Liu, J.; Awadallah, A. H.; White,

R. W.; Burger, D.; and Wang, C. 2023. AutoGen: Enabling

Next-Gen LLM Applications via Multi-Agent Conversation.

ArXiv:2308.08155 [cs].

Yang, J. C.; Hausladen, C. I.; Peters, D.; Pournaras, E.;

Fricker, R. H.; and Helbing, D. 2024.

Designing Digital

## Page 13

Voting Systems for Citizens: Achieving Fairness and Legiti-

macy in Digital Participatory Budgeting Voting Mechanism.

ACM Digital Government: Research and Practice.

Yao, J.-Y.; Ning, K.-P.; Liu, Z.-H.; Ning, M.-N.; and Yuan,

L. 2023. LLM Lies: Hallucinations are not Bugs, but Fea-

tures as Adversarial Examples. arXiv:2310.01469.

Zhang, C.; Yang, K.; Hu, S.; Wang, Z.; Li, G.; Sun, Y.;

Zhang, C.; Zhang, Z.; Liu, A.; Zhu, S.-C.; Chang, X.; Zhang,

J.; Yin, F.; Liang, Y.; and Yang, Y. 2023a. ProAgent: Build-

ing Proactive Cooperative AI with Large Language Models.

arXiv:2308.11339.

Zhang, H.; Du, W.; Shan, J.; Zhou, Q.; Du, Y.; Tenenbaum,

J. B.; Shu, T.; and Gan, C. 2023b. Building Cooperative

Embodied Agents Modularly with Large Language Models.

arXiv:2307.02485.

Zhuge, M.; Liu, H.; Faccio, F.; Ashley, D. R.; Csord√°s, R.;

Gopalakrishnan, A.; Hamdi, A.; Hammoud, H. A. A. K.;

Herrmann, V.; Irie, K.; Kirsch, L.; Li, B.; Li, G.; Liu, S.;

Mai, J.; PiÀõekos, P.; Ramesh, A.; Schlag, I.; Shi, W.; Stani¬¥c,

A.; Wang, W.; Wang, Y.; Xu, M.; Fan, D.-P.; Ghanem,

B.; and Schmidhuber, J. 2023.

Mindstorms in Natural

Language-Based Societies of Mind. ArXiv:2305.17066 [cs].

## Page 14

## A

Additional Figures

Figure 8: Histograms comparing the voting behavior of Human and LLMs. Left: The number of selected projects in Approval

Voting varies across the types of voters. Right: Sum of assigned points in Cumulative Voting for GPT4 and LLaMA-2 voters.

GPT4 adheres strictly to the instruction of assigning 10 points, LLaMA-2 does not.

Figure 9: Comparative analysis of voting outcomes, highlighting the percentage differences between human and LLM voters.

The 24 projects, spanning various districts and categories, are listed with corresponding vote percentage differences. A positive

percentage indicates a project‚Äôs greater priority for LLM voters compared to human voters.

## Page 15

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24

Projects Ranked by Vote Percentage

0

5

15

Percentage of Total Votes (%)

gpt4 t=0

gpt4 t=2

llama2 t=0

llama2 t=2

human

Model/Temperature

gpt4 t=0

gpt4 t=0.5

gpt4 t=1

gpt4 t=1.5

gpt4 t=2

llama2 t=0

llama2 t=0.5

llama2 t=1

llama2 t=1.5

llama2 t=2

human

Figure 10: Vote percentage distribution across projects ranked by collective preference for human, GPT-4 and LLaMA-2 voters

at varying temperature. The lines represent the normalized distribution of votes for each entity, highlighting the decision-making

patterns and diversity of preferences.

## B

Additional Tables

Table 1: Pearson correlation between the ranking order and project IDs across various voting scenarios. In the LLM experiments,

settings where project lists with reversed IDs (rev_id) and reversed order (rev_order) are included.

Correlation

P-Value

Significant

human

0.100

0.642

False

gpt4

0.013

0.952

False

gpt4_rev_id

-0.089

0.680

False

gpt4_rev_order

-0.020

0.926

False

llama2

-0.092

0.668

False

llama2_rev_id

0.189

0.377

False

llama2_rev_order

0.331

0.114

False

## Page 16

## C

Persona Description Construction

We construct persona descriptions from participant responses in a survey. Each entry consists of the participant‚Äôs ID, urban

category preferences, district information, connection to the city, and importance ratings.

Data Structure and Example Entry:

Table 2: Example Survey Response for Persona Description Construction

Field

Value

## Id

## Yk3Tdkdg

1st Urban Category

Transport

1st Score

60

2nd Urban Category

Nature

2nd Score

30

3rd Urban Category

Culture

3rd Score

10

District

Nord

How Connected

A lot

District Importance

Moderately important

Urban Category Importance

Very important

Cost Importance

Not important at all

Likelihood Importance

Moderately important

Based on this data, we then create a narrative description for each persona. The process of persona generation involves

processing survey data and using the numerical scores to generate descriptive human language labels that reflect the participants‚Äô

preference intensity.

In the survey, participants rated the importance of each urban category, with the total score summing to exactly 100. To

construct personas that reflect these nuances, the function interprets scores: if the importance score exceeds 50, it uses the

descriptor ‚Äúhaving a strong preference for" the category. Scores between 33 and 50 lead to ‚Äúprefer,‚Äù indicating a moderate

preference. Scores below 33 result in no specific preference being highlighted. This nuanced interpretation ensures that each

persona‚Äôs narrative description accurately mirrors their interests and priorities in urban topics.

The function further combines this with participants‚Äô district preferences and their prioritization of decision factors like

district importance, urban category significance, and cost, creating a comprehensive and personalized narrative for each partic-

ipant.

Corresponding Persona Description:

You are a university student from Nord district in Zurich. In urban topics, you have a strong preference for transport. When

deciding on projects, you find the district to be moderately important, the urban category very important, and the cost of

the project not important at all.

## Page 17

## D

List of Projects

Table 3: List of 24 projects presented in the participatory budgeting experiment, as provided to both the 180 human participants

and the LLM agents.

Id

Name

District

Category

Cost

#1

Planting Workshops at Oerlikon

Nord

Nature

5000

#2

Footpath Gardens around Train Stations

Nord

Nature

10000

#3

Kid Festival at Leutschenpark

Nord

Culture

5000

#4

Music Studio at Kulturbahnhof Affoltern

Nord

Culture

10000

#5

Safe Bike Paths around Irchel Park

Nord

Transportation

5000

#6

More Night Buses to Oerlikon

Nord

Transportation

10000

#7

Free Open Badi Space in Wollishofen

S√ºd

Nature

5000

#8

A Neighborhood Garden for Wiedikon

S√ºd

Nature

10000

#9

Environmental Film Session for Kids

S√ºd

Culture

5000

#10

Car-free Sunday Festivals on Mutschellenstrasse

S√ºd

Culture

10000

#11

Free Bike Fixing Workshops

S√ºd

Transportation

5000

#12

Car Sharing System for Young People

S√ºd

Transportation

10000

#13

Transforming City Spaces under Trees into Gardens

Ost

Nature

5000

#14

More Trees in Bellevue & Sechsel√§utenplatz

Ost

Nature

10000

#15

Plant Festival in the City Centre

Ost

Culture

5000

#16

Multicultural Festival at Sechsel√§utenplatz

Ost

Culture

10000

#17

Bike Lanes on Seefeldstrasse

Ost

Transportation

5000

#18

Speed bumps in the City and the Lake Front

Ost

Transportation

10000

#19

Bird Houses for Zurich-H√∂ngg

West

Nature

5000

#20

Wet Biotope as Learning Garden in Frankental

West

Nature

10000

#21

Dingtheke: Community Things Exchange Library in Wipkingen

West

Culture

5000

#22

Sustainable Cooking Workshop with Kids

West

Culture

10000

#23

Public Bicycle Moving Trailer to be Borrowed

West

Transportation

5000

#24

Car-free Langstrasse

West

Transportation

10000

## E

Example LLM Responses

GPT-4 Responses: agent #0, temp=1, no persona

‚Ä¢ Approval Voting:

#5, #11, #17, #23

‚Ä¢ 5-Approval Voting:

#2, #5, #11, #17, #24

‚Ä¢ Cumulative Voting:

#5 ‚àí3 points #11 ‚àí2 points #17 ‚àí3 points #23 ‚àí2 points

As a university student, I am focused on sustainable transportation and fostering a bike‚àífriendly city, hence my preference for safe bike paths, free bike

fixing workshops, bike lanes, and a public bicycle moving trailer. These projects would directly benefit my daily life and the lives of my peers, promoting

health, sustainability, and convenience.

‚Ä¢ Ranked Voting:

#10,#22,#18,#24,#7

1. #10 Car‚àífree Sunday Festivals on Mutschellenstrasse ‚àíCultural events are important for community building and reducing traffic.

2. #22 Sustainable Cooking Workshop with Kids ‚àíEducation on sustainability, especially for kids, is crucial for the future.

3. #18 Speed bumps in the City and the Lake Front ‚àíSafety is important, and speed bumps can help reduce accidents.

4. #24 Car‚àífree Langstrasse ‚àíReducing traffic can improve quality of life and it‚Äôs an initiative towards a greener city.

5. #7 Free Open Badi Space in Wollishofen ‚àíPublic spaces like this contribute to the well‚àíbeing and are essential for the community during summer.

LLaMA-2 Responses: agent #3, temp=1, no persona

‚Ä¢ Approval Voting:

## Page 18

Sure! As a university student in Zurich, I would like to vote for the following projects:

#5 Safe Bike Paths around Irchel Park ‚àíThis project would improve the safety of bike paths in the area, making it easier for students like me to get around

by bike.

#10 Car‚àífree Sunday Festivals on Mutschellenstrasse ‚àíThis project would create a fun and environmentally‚àífriendly event that I could enjoy with my

friends and fellow students.

#17 Bike Lanes on Seefeldstrasse ‚àíThis project would improve the infrastructure for cyclists in the city, making it easier for me to get around by bike.

#22 Sustainable Cooking Workshop with Kids ‚àíThis project would promote sustainability and healthy eating habits among young people, which is an

important issue for me as a student.

I hope that my votes will help make these projects a reality!

‚Ä¢ 5-Approval Voting:

Sure! Here are my votes:

#3 ‚àíKid Festival at Leutschenpark (Nord) ‚àíI think it‚Äôs important to invest in events that bring the community together and provide fun activities for kids.

#6 ‚àíMore Night Buses to Oerlikon (Nord) ‚àíAs a student, I often have late nights studying or socializing, and having more night buses would make it easier

for me to get home safely.

#13 ‚àíTransforming City Spaces under Trees into Gardens (Ost) ‚àíI love the idea of turning unused spaces into beautiful gardens. It would make the city

more enjoyable to live in and help the environment.

#17 ‚àíBike Lanes on Seefeldstrasse (Ost) ‚àíAs a cyclist, I think it‚Äôs important to have designated bike lanes to ensure safety and encourage more people to

cycle.

#22 ‚àíSustainable Cooking Workshop with Kids (West) ‚àíI believe it‚Äôs important to teach kids about sustainability and healthy eating habits from a young

age. This project aligns with my values and interests.

‚Ä¢ Cumulative Voting:

Sure! Here are my preferences for the participatory budgeting program in Zurich, with three projects that I believe would greatly benefit the city and its

residents:

1. #5 ‚àíSafe Bike Paths around Irchel Park (Nord) ‚àí4 points As a university student, I believe that improving cycling infrastructure is crucial for a safer and

more sustainable commute. This project would enhance the safety of bike paths around Irchel Park, making it easier for students and residents to get

around the city without relying on cars.

2. #17 ‚àíBike Lanes on Seefeldstrasse (Ost) ‚àí3 points This project would create dedicated bike lanes on a busy street in the eastern part of Zurich,

providing a safer and more convenient route for cyclists. As a student, I believe that investing in cycling infrastructure is essential for reducing traffic

congestion and promoting sustainable transportation.

3. #22 ‚àíSustainable Cooking Workshop with Kids (West) ‚àí3 points This project would offer cooking workshops for children, focusing on sustainable food

practices and healthy eating habits. As a young adult, I believe that it‚Äôs important to educate the next generation about the impact of their food choices on

the environment and their health. By supporting this project, I hope to encourage sustainable living practices among young people in Zurich.

Total points allocated: 10

‚Ä¢ Ranked Voting:

Sure! Here are my votes:

#1 ‚àíPlanting Workshops at Oerlikon (Nord) ‚àíNature ‚àí5000 CHF

#2 ‚àíFree Open Badi Space in Wollishofen (S\u00fcd) ‚àíNature ‚àí5000 CHF

#3 ‚àíEnvironmental Film Session for Kids (S\u00fcd) ‚àíCulture ‚àí5000 CHF

#4 ‚àíCar‚àífree Sunday Festivals on Mutschellenstrasse (S\u00fcd) ‚àíCulture ‚àí10,000 CHF

#5 ‚àíBike Lanes on Seefeldstrasse (Ost) ‚àíTransportation ‚àí5,000 CHF

My ranking is:

1. #1 ‚àíPlanting Workshops at Oerlikon (Nord) ‚àíNature ‚àí5000 CHF

2. #3 ‚àíEnvironmental Film Session for Kids (S\u00fcd) ‚àíCulture ‚àí5,000 CHF

3. #4 ‚àíCar‚àífree Sunday Festivals on Mutschellenstrasse (S\u00fcd) ‚àíCulture ‚àí10,000 CHF

4. #2 ‚àíFree Open Badi Space in Wollishofen (S\u00fcd) ‚àíNature ‚àí5,000 CHF

5. #5 ‚àíBike Lanes on Seefeldstrasse (Ost) ‚àíTransportation ‚àí5,000 CHF

I ranked projects based on my personal interest in environmental sustainability, cultural events, and transportation infrastructure. I believe that investing in

planting workshops, environmental education for kids, and car‚àífree festivals will contribute to a greener and more livable city. Additionally, I think it‚Äôs

important to provide free open spaces for residents to enjoy, especially in densely populated areas like Wollishofen. Finally, I support the development of

bike lanes on Seefeldstrasse to promote sustainable transportation options.

GPT-4 Responses: agent #0, temp=1, with persona

‚Ä¢ 5-Approval Voting:

#19,#20,#21,#22,#7

## Page 19

LLaMA-2 Responses: agent #3, temp=1, with persona

‚Ä¢ 5-Approval Voting:

Sure! Based on my preferences, I would vote for the following projects:

#5 Safe Bike Paths around Irchel Park (Nord, Transportation, 5000 CHF)

#6 More Night Buses to Oerlikon (Nord, Transportation, 10000 CHF)

#17 Bike Lanes on Seefeldstrasse (Ost, Transportation, 5000 CHF)

#23 Public Bicycle Moving Trailer to be Borrowed (West, Transportation, 5000 CHF)

#24 Car‚àífree Langstrasse (West, Transportation, 10000 CHF)

These projects align with my interests in transportation and improve the city‚Äôs infrastructure for cyclists and public transportation.

GPT-4 Chain-of-Thought Responses: agent #1, temp=1, with persona

‚Ä¢ First thought response

As a university student from the Nord district in Zurich with a strong preference for transport within urban topics, my decisions in a participatory budgeting

program would be heavily influenced by proposals tackling issues of mobility and connectivity. Hence, I would favor projects that aim to improve the

transportation infrastructure, such as the development of bike lanes, enhancement of public transport services, and the integration of sustainable transport

solutions. Projects that specifically target improvements within the Nord district would be of slightly less importance to me than the overall category of

urban transport, but I would still consider them favorably, especially if they promise to better the daily commute and accessibility for residents, including my

fellow students. The cost of the project would not influence my decision, as I am focused on the potential benefits and improvements in the transport sector

rather than financial constraints. My main criterion for voting for a project would be its potential to make travel within Zurich more efficient, safer, and

environmentally friendly.

‚Ä¢ 5-Approval Voting

Considering my strong preference for transport projects, I would allocate my votes to the following:

#5, Safe Bike Paths around Irchel Park

#6, More Night Buses to Oerlikon

#11, Free Bike Fixing Workshops

#17, Bike Lanes on Seefeldstrasse

#24, Car‚àífree Langstrasse

These projects support an improvement in transportation, which aligns closely with my personal interests. Four of these projects are in my home district (

Nord), further reinforcing my moderate preference for district‚àírelated improvements. Even though project #24 is not in my district, it supports the transport

category, which I consider very important.

GPT-4 Chain-of-Thought Responses: agent #39, temp=1, with persona

‚Ä¢ First thought response

As a university student from the West district of Zurich with a strong preference for culture, my decisions regarding urban projects in a participatory

budgeting program would be influenced by how well the projects align with the cultural enrichment of my district and the city at large. Given my background,

I would prioritize funding for projects that promote cultural activities, such as the establishment of art galleries, support for local theaters, music festivals, or

public art installations, especially those that are within or benefit the West district, as I find the district of execution very important.Moreover, since the

urban category is very important to me, I would look favorably on projects that enhance urban livability through cultural means, such as creating cultural

community centers or artist‚àíin‚àíresidence programs that could bring a diverse range of cultural experiences to the residents.The cost of the project is

moderately important, suggesting that while I am cost‚àíconscious and desire effective use of funds, I might be willing to support slightly more expensive

projects if they offer significant cultural benefits to the community. Nevertheless, overly expensive projects might be deprioritized in favor of those that offer

better cost‚àíto‚àíbenefit ratios within my interests. To sum up, cultural relevance, location within the West district, and a reasonable cost structure would

guide my decisions in the participatory budgeting process.

‚Ä¢ 5-Approval Voting

Based on my preferences for cultural projects, their location in the West district, and moderate consideration of the project cost, I would select the following:

#19 ‚àíBird Houses for Zurich‚àíH\u00f6ngg (West, Nature, 5000)

#20 ‚àíWet Biotope as Learning Garden in Frankental (West, Nature, 10000)

#21 ‚àíDingtheke: Community Things Exchange Library in Wipkingen (West, Culture, 5000)

#22 ‚àíSustainable Cooking Workshop with Kids (West, Culture, 10000)

#24 ‚àíCar‚àífree Langstrasse (West, Transportation, 10000)

These projects reflect a strong emphasis on cultural enrichment and environmental considerations in my home district.
