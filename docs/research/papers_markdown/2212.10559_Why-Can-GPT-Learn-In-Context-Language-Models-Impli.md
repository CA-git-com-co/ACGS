# 2212.10559_Why-Can-GPT-Learn-In-Context-Language-Models-Impli
**Constitutional Hash: cdd01ef066bc6cf2**


**Original PDF**: 2212.10559_Why-Can-GPT-Learn-In-Context-Language-Models-Impli.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2



## Implementation Status

- ‚úÖ **Constitutional Hash Validation**: Active enforcement of `cdd01ef066bc6cf2`
- üîÑ **Performance Monitoring**: Continuous validation of targets
- ‚úÖ **Documentation Standards**: Compliant with ACGS-2 requirements
- üîÑ **Cross-Reference Validation**: Ongoing link integrity maintenance

**Overall Status**: üîÑ IN PROGRESS - Systematic enhancement implementation

## Performance Targets

This component maintains the following performance requirements:

- **P99 Latency**: <5ms (constitutional requirement)
- **Throughput**: >100 RPS (minimum operational standard)
- **Cache Hit Rate**: >85% (efficiency requirement)
- **Constitutional Compliance**: 100% (hash: cdd01ef066bc6cf2)

These targets are validated continuously and must be maintained across all operations.

---

## Page 1

Why Can GPT Learn In-Context?

Language Models Implicitly Perform Gradient Descent as

Meta-Optimizers

Damai Dai‚Ä†‚àó, Yutao Sun‚à•‚àó, Li Dong‚Ä°, Yaru Hao‚Ä°, Shuming Ma‚Ä°, Zhifang Sui‚Ä†, Furu Wei‚Ä°

‚Ä† MOE Key Lab of Computational Linguistics, Peking University

‚à•Tsinghua University

‚Ä° Microsoft Research

{daidamai,szf}@pku.edu.cn

{lidong1,fuwei}@microsoft.com

Abstract

Large pretrained language models have shown

surprising in-context learning (ICL) ability.

With a few demonstration input-label pairs,

they can predict the label for an unseen input

without parameter updates. Despite the great

success in performance, its working mecha-

nism still remains an open question. In this

paper, we explain language models as meta-

optimizers and understand in-context learning

as implicit Ô¨Ånetuning. Theoretically, we Ô¨Åg-

ure out that Transformer attention has a dual

form of gradient descent. On top of it, we un-

derstand ICL as follows: GPT Ô¨Årst produces

meta-gradients according to the demonstration

examples, and then these meta-gradients are

applied to the original GPT to build an ICL

model. We comprehensively compare the be-

haviors of in-context learning and explicit Ô¨Åne-

tuning on real tasks to provide empirical evi-

dence that supports our understanding. Exper-

imental results show that in-context learning

behaves similarly to explicit Ô¨Ånetuning from

multiple perspectives.

Inspired by the dual

form between Transformer attention and gradi-

ent descent, we design a momentum-based at-

tention by analogy with gradient descent with

momentum. The improved performance over

vanilla attention further supports our under-

standing from another perspective, and more

importantly, shows the potential to utilize our

understanding for future model design. The

code is available at https://aka.ms/icl.

1

Introduction

In recent years, large pretrained language models,

especially in Transformer-based architectures (e.g.,

GPT; Brown et al. 2020), have shown strong emer-

gent in-context learning (ICL) ability (Wei et al.,

2022; Dong et al., 2023). Different from Ô¨Ånetuning

which needs additional parameter updates, ICL just

needs several demonstration examples prepended

‚àóContribution during internship at Microsoft Research.

(Sentence, ?)

‚Ä¶

Demonstration Examples

Query Example

Feed-Forward Network

Self-Attention

(Sentence1, Answer1) (Sentence2, Answer2)

Meta-Gradients

Answer

‚Ä¶

## Gpt

In-Context Learning

Finetuning

## Gpt

(Sentence1, Answer1)

## Gpt

(Sentence2, Answer2)

Back-Propagation

Forward

Computation

## ùö´ùö´ùëæùëæùêÖùêÖùêÖùêÖ

## ùö´ùö´ùëæùëæùêàùêàùêàùêàùêàùêà

Gradients

Dual

View

Figure 1: According to the demonstration examples,

GPT produces meta-gradients for in-context learning

(ICL) through forward computation. ICL works by ap-

plying these meta-gradients to the model through atten-

tion. The meta-optimization process of ICL shares a

dual view with Ô¨Ånetuning that explicitly updates the

model parameters with back-propagated gradients.

before the query input, and then the model can pre-

dict labels for unseen inputs. On numerous down-

stream tasks, large GPT models can achieve sur-

prising performance, which even exceeds smaller

models with supervised Ô¨Ånetuning. However, al-

though ICL has achieved great performance, its

working mechanism is still an open question to be

investigated.

In this paper, we explain in-context learning as a

process of meta-optimization and analyze connec-

tions between GPT-based in-context learning and

Ô¨Ånetuning. Concentrating on the attention mod-

ules, we Ô¨Ågure out that the Transformer attention

has a dual form of gradient descent. On top of

it, we propose a novel perspective to explain in-

arXiv:2212.10559v3  [cs.CL]  15 May 2023

## Page 2

context learning: (1) a pretrained GPT serves as

a meta-optimizer; (2) it produces meta-gradients

according to the demonstration examples through

forward computation; (3) the meta-gradients are

applied to the original language model through at-

tention to build an ICL model. As illustrated in

Figure 1, in-context learning and explicit Ô¨Ånetun-

ing share a dual view of gradient descent, where

ICL produces meta-gradients through forward com-

putation, while Ô¨Ånetuning computes gradients by

back-propagation. Therefore, it is reasonable to un-

derstand in-context learning as implicit Ô¨Ånetuning.

In order to provide empirical evidence to sup-

port our understanding, we conduct comprehensive

experiments based on real tasks. On six classi-

Ô¨Åcation tasks, we compare the model predictions,

attention outputs, attention weights to query tokens,

and attention weights to training tokens between

in-context learning and Ô¨Ånetuning. Experimental

results validate that the behavior of in-context learn-

ing is similar to explicit Ô¨Ånetuning from multiple

perspectives. These results are strong evidence

to prove the reasonability of our understanding of

in-context learning as implicit Ô¨Ånetuning.

Further, inspired by the dual form between Trans-

former attention and gradient descent, we design

a momentum-based attention, which regards the

attention values as meta-gradients and applies the

momentum mechanism (Polyak, 1964; Sutskever

et al., 2013) to them. Experiments on both lan-

guage modeling and in-context learning show that

our momentum-based attention consistently outper-

forms vanilla attention, which supports our under-

standing of meta-optimization again from another

perspective. We note that beyond this preliminary

attempt, our understanding may have more poten-

tial to enlighten model design, which is worth in-

vestigating in the future.

Our contributions are summarized as follows:

‚Ä¢ We Ô¨Ågure out a dual form between Trans-

former attention and gradient descent, and ex-

plain ICL as a process of meta-optimization.

‚Ä¢ We analyze connections between in-context

learning and explicit Ô¨Ånetuning and propose

to understand ICL as implicit Ô¨Ånetuning.

‚Ä¢ We provide several lines of empirical evidence

to prove that ICL and explicit Ô¨Ånetuning be-

have similarly from multiple perspectives.

‚Ä¢ We design a momentum-based attention and

validate its effectiveness, which supports our

understanding of meta-optimization again and

shows the potential of our understanding to

enlighten future model design.

2

Background

2.1

In-Context Learning with GPT

In this paper, we focus on ICL for classiÔ¨Åca-

tion tasks using GPT (Brown et al., 2020).

## A

GPT model is stacked with L identical Trans-

former (Vaswani et al., 2017) decoder layers where

each layer consists of an attention module and a

feed-forward network. For a classiÔ¨Åcation task,

given a query input text x and a candidate an-

swer set Y = {y1, y2, . . . , ym}, we need to pre-

dict a label ÀÜy conditional on n demonstration

examples C = {(x‚Ä≤

1, y‚Ä≤

1), (x‚Ä≤

2, y‚Ä≤

2), . . . , (x‚Ä≤

n, y‚Ä≤

n)},

where (x‚Ä≤

i, y‚Ä≤

i) is an input-label pair different from

the query one. Formally, given a GPT model M,

we Ô¨Årst compute the probability of each answer yj:

PM(yj | C, x).

(1)

Since the label space is restricted for classiÔ¨Åca-

tion, we predict the Ô¨Ånal answer ÀÜy by selecting

the answer with the highest probability from the

candidate answer set Y :

ÀÜy = arg max

yj

PM(yj | C, x).

(2)

In practice, we usually use a pre-deÔ¨Åned template

to format the demonstrations and prepend them

before the query input. Let T (¬∑) be the function

that formats an example, e.g.:

T (x, y) = Sentence: x. Sentiment: y.

(3)

The contextual model input I is organized like

T (x‚Ä≤

1, y‚Ä≤

1) T (x‚Ä≤

2, y‚Ä≤

2) ... T (x‚Ä≤

n, y‚Ä≤

n) T (x, _). (4)

Feeding this contextual input into M, the probabil-

ity of an answer yj is computed as

lj = M(I) ¬∑ eyj,

(5)

PM(yj | C, x) = softmax(lj),

(6)

where M(I) denotes the output hidden state at the

last token position; eyj denotes the output word

embedding of yj; and lj is the logit corresponding

to the j-th answer.

## Page 3

2.2

Dual Form Between Attention and Linear

Layers Optimized by Gradient Descent

The idea in this paper to explain language models

as meta-optimizers is inspired by Aizerman et al.

(1964); Irie et al. (2022). They present that linear

layers optimized by gradient descent have a dual

form of linear attention. Let W0, ‚àÜW ‚ààRdout√ódin

be the initialized parameter matrix and the update

matrix, respectively, and x ‚ààRdin be the input rep-

resentation. A linear layer optimized by gradient

descent can be formulated as

F(x) = (W0 + ‚àÜW) x.

(7)

In the back-propagation algorithm, ‚àÜW is com-

puted by accumulating the outer products of his-

toric input representations x‚Ä≤T

i

‚ààRdin and the error

signals ei ‚ààRdout of their corresponding outputs:

## ‚àÜW =

## X

i

ei ‚äóx‚Ä≤

i,

(8)

where ei is derived from the historic output gradi-

ents by multiplying ‚àíŒ≥, the negative learning rate.

Combing Equation (7) and Equation (8), we can

derive the dual form of linear layers optimized by

gradient descent:

F(x) = (W0 + ‚àÜW) x

=W0x + ‚àÜWx

=W0x +

## X

i

 ei ‚äóx‚Ä≤

i



x

=W0x +

## X

i

ei



x‚Ä≤T

i x



=W0x + LinearAttn

 E, X‚Ä≤, x



,

(9)

where LinearAttn(V, K, q) denotes the linear at-

tention operation, in which we regard the historic

output error signals E as values, the historic inputs

X‚Ä≤ as keys, and the current input x as the query.

3

Understanding In-Context Learning

(ICL) as Implicit Finetuning

We Ô¨Årst qualitatively analyze the Transformer atten-

tion under a relaxed linear attention form to Ô¨Ågure

out a dual form between it and gradient descent.

Then, we compare in-context learning with explicit

Ô¨Ånetuning to analyze connections between these

two optimization forms. Based on these theoreti-

cal Ô¨Åndings, we propose to understand in-context

learning as implicit Ô¨Ånetuning.

3.1

Understanding Transformer Attention as

Meta-Optimization

Let x ‚ààRd be the input representation of a query

token t, and q = WQx ‚ààRd‚Ä≤ be the attention

query vector. In the ICL setting, the attention result

of a head is formulated as

FICL(q) = Attn(V, K, q)

=WV [X‚Ä≤; X] softmax

(WK[X‚Ä≤; X])T q

‚àö

d

!

, (10)

where WQ, WK, WV ‚ààRd‚Ä≤√ód are the projection

matrices for computing the attention queries, keys,

and values, respectively;

‚àö

d denotes the scaling

factor; X denotes the input representations of query

tokens before t; X‚Ä≤ denotes the input representa-

tions of the demonstration tokens; and [X‚Ä≤; X] de-

notes the matrix concatenation. For ease of qualita-

tive analysis, we approximate the standard attention

to relaxed linear attention by removing the softmax

operation and the scaling factor:

FICL(q) ‚âàWV [X‚Ä≤; X]

##  Wk[X‚Ä≤; X]

T q

= WV X (WKX)T q + WV X‚Ä≤  WKX‚Ä≤T q

= e

FICL(q).

(11)

We deÔ¨Åne WZSL = WV X (WKX)T as the ini-

tialized parameters to be updated since WZSLq is

the attention result in the zero-shot learning (ZSL)

setting, where no demonstrations are given. Fol-

lowing the reverse direction of Equation (9), we

derive a dual form of the Transformer attention:

e

FICL(q) = WZSLq + WV X‚Ä≤  WKX‚Ä≤T q

=WZSLq + LinearAttn

 WV X‚Ä≤, WKX‚Ä≤, q



=WZSLq +

## X

i

WV x‚Ä≤

i

 WKx‚Ä≤

i

T q



=WZSLq +

## X

i

 (WV x‚Ä≤

i) ‚äó

 WKx‚Ä≤

i



q

=WZSLq + ‚àÜWICLq

= (WZSL + ‚àÜWICL) q.

(12)

As shown in the above equations, the attention to

the demonstration tokens is equivalent to param-

eter updates ‚àÜWICL that take effect on WZSL. In

addition, by analogy with E in Equation (9), we

regard WV X‚Ä≤ as meta-gradients, which are used to

compute the update matrix ‚àÜWICL.

In summary, we explain in-context learning as a

process of meta-optimization: (1) a pretrained GPT

model serves as a meta-optimizer; (2) it produces

meta-gradients according to the demonstration ex-

amples through forward computation; (3) through

attention, the meta-gradients are applied to the orig-

inal language model to build an ICL model.

## Page 4

3.2

Comparing ICL with Finetuning

Based on the above understanding of in-context

learning, we further compare the meta-optimization

of in-context learning with the explicit optimiza-

tion of Ô¨Ånetuning to analyze connections between

them. Considering that ICL directly takes effect

on only the attention keys and values, we design

a speciÔ¨Åc Ô¨Ånetuning setting as the compared base-

line, which also updates only the parameters for the

key and value projection. Also in the relaxed linear

attention form, the attention result of a Ô¨Ånetuned

head is formulated as

e

FFT(q) = (WV + ‚àÜWV )XXT (WK + ‚àÜWK)T q

= (WZSL + ‚àÜWFT) q,

(13)

where ‚àÜWK and ‚àÜWV denote the parameter up-

dates to WK and WV , respectively, which are

acquired by back-propagation from task-speciÔ¨Åc

training objectives; and ‚àÜWFT is the updates to

WZSL introduced by Ô¨Ånetuning.

For a more fair comparison with in-context learn-

ing, we further restrict the Ô¨Ånetuning setting as fol-

lows: (1) we specify the training examples as the

demonstration examples for in-context learning;

(2) we train each example for only one step in the

same order as demonstrated for in-context learning;

(3) we format each training example with the same

template used for ICL T (x‚Ä≤

i, y‚Ä≤

i) and use the causal

language modeling objective for Ô¨Ånetuning.

Comparing in-context learning and this Ô¨Ånetun-

ing setting, we Ô¨Ånd that ICL has many properties

in common with Ô¨Ånetuning. We organize these

common properties into the following four aspects.

Both Perform Gradient Descent

Comparing

Equation (12) and Equation (13), we Ô¨Ånd that both

in-context learning and Ô¨Ånetuning introduce up-

dates (‚àÜWICL v.s. ‚àÜWFT) to WZSL, which drive

from implicit and explicit gradient descent, respec-

tively. The main difference is that ICL produces

meta-gradients by forward computation while Ô¨Åne-

tuning acquires real gradients by back-propagation.

Same

Training

Information

The

meta-

gradients of ICL are produced according to

the demonstration examples.

The gradients of

Ô¨Ånetuning are also derived from the same training

examples.

That is to say, in-context learning

and Ô¨Ånetuning share the same source of training

information.

Same Causal Order of Training Examples

In-

context learning and our Ô¨Ånetuning setting share

the same causal order of training examples. ICL

uses decoder-only Transformers so the subsequent

tokens in the demonstrations will not affect the pre-

ceding ones. For our Ô¨Ånetuning setting, we use the

same order of training examples and train only one

epoch, so we can also guarantee that the subsequent

examples have no effect on the preceding ones.

Both Aim at Attention

Compared with zero-

shot learning, the direct effect of in-context learn-

ing and our Ô¨Ånetuning are both restricted to the

computation of attention keys and values. For ICL,

the model parameters are unchanged and it encodes

demonstration information into additional keys and

values to change the attention behavior. For Ô¨Ånetun-

ing, due to our restriction, the training information

can be introduced to only the projection matrices

for attention keys and values as well.

Considering the above common properties be-

tween in-context learning and Ô¨Ånetuning, we show

that it is reasonable to understand in-context learn-

ing as implicit Ô¨Ånetuning. In the rest of this paper,

we compare ICL and explicit Ô¨Ånetuning empirically

from multiple perspectives to provide quantitative

results to support this understanding.

4

Experiments

4.1

Experimental Settings

We analyze two off-the-shelf pretrained GPT mod-

els with 1.3 billion and 2.7 billion model parame-

ters, respectively, which are released by fairseq1.

In the rest of this paper, we call them GPT 1.3B and

GPT 2.7B for short. All experiments are conducted

on NVIDIA V100 GPUs with 32 GB memory.

For each task, we use the same template to for-

mat examples for zero-shot learning (ZSL), Ô¨Åne-

tuning (FT), and in-context learning (ICL). Details

of the templates used for each task are provided

in Appendix A. The answer prediction processes

for ZSL and Ô¨Ånetuning are the same with ICL as

described in Section 2.1, except that they do not

have demonstration examples.

For in-context learning, we Ô¨Åx the max num-

ber of demonstration examples to 32 and tune the

random seed for each task to Ô¨Ånd a set of demon-

stration examples that achieves the best validation

performance. For explicit Ô¨Ånetuning, we use the

same demonstration examples for in-context learn-

ing as the training examples and use SGD as the

optimizer. For a fair comparison, we Ô¨Åne-tune the

1https://github.com/facebookresearch/fairseq

## Page 5

## Sst2

## Sst5

## Mr

Subj

AGNews

## Cb

# Validation Examples

872

1101

1066

2000

7600

56

# Label Types

2

5

2

2

4

3

ZSL Accuracy (GPT 1.3B)

70.5

39.3

65.9

72.6

46.3

37.5

FT Accuracy (GPT 1.3B)

73.9

39.5

73.0

77.8

65.3

55.4

ICL Accuracy (GPT 1.3B)

92.7

45.0

89.0

90.0

79.2

57.1

ZSL Accuracy (GPT 2.7B)

71.4

35.9

60.9

75.2

39.8

42.9

FT Accuracy (GPT 2.7B)

76.9

39.1

80.0

86.1

65.7

57.1

ICL Accuracy (GPT 2.7B)

95.0

46.5

91.3

90.3

80.3

55.4

Table 1: Statistics of six classiÔ¨Åcation datasets (rows 1-2) and validation accuracy in the zero-shot learning (ZSL),

Ô¨Ånetuning (FT), and in-context learning (ICL) settings on these datasets (rows 3-8).

Model

## Sst2

## Sst5

## Mr

Subj

AGNews

## Cb

Average

## Gpt 1.3B

91.84

66.67

97.08

87.17

83.08

87.50

85.56

## Gpt 2.7B

96.83

71.60

95.83

87.63

84.44

100.00

89.39

Table 2: Rec2FTP for two GPT models on six datasets. From the perspective of model prediction, ICL can cover

most of the correct behavior of Ô¨Ånetuning.

model for only one epoch and the training exam-

ples are provided in the same order as demonstrated

for in-context learning. We tune the learning rate

for Ô¨Ånetuning and select the one that achieves the

best validation performance. Details of the search

range and selected value for the random seeds and

learning rates are shown in Appendix B.

4.2

Evaluation Datasets

We compare in-context learning and Ô¨Ånetuning

based on six datasets spanning three sorts of

classiÔ¨Åcation tasks. SST2 (Socher et al., 2013),

SST5 (Socher et al., 2013), MR (Pang and

Lee, 2005) and Subj (Pang and Lee, 2004) are

four datasets for sentiment classiÔ¨Åcation; AG-

News (Zhang et al., 2015) is a topic classiÔ¨Åcation

dataset; and CB (De Marneffe et al., 2019) is used

for natural language inference. Statistics of the

number of validation examples and label types are

summarized in Table 1.

For reference, we present the validation accuracy

in the ZSL, Ô¨Ånetuning, and ICL settings on six

classiÔ¨Åcation datasets in Table 1. Compared with

ZSL, ICL and Ô¨Ånetuning both achieve considerable

improvements, which means the optimizations they

make are both helpful to these downstream tasks.

4.3

ICL Covers Most of Correct Predictions

of Finetuning

We compute a recall to Ô¨Ånetuning prediction

(Rec2FTP) to measure ICL can cover how much

behavior of Ô¨Ånetuning from the perspective of the

model prediction. We Ô¨Årst count NFT>ZSL, the

number of query examples that Ô¨Ånetuning can pre-

dict correctly but ZSL cannot. Then, among these

examples, we count N(FT>ZSL)‚àß(ICL>ZSL), the num-

ber that ICL can also predict correctly. Finally, we

compute the Rec2FTP score as

## N(Ft>Zsl)‚àß(Icl>Zsl)

## Nft>Zsl

.

A higher Rec2FTP score suggests that ICL cov-

ers more correct behavior of Ô¨Ånetuning from the

perspective of the model prediction.

We show the Rec2FTP scores for two GPT mod-

els on six datasets in Table 2. As shown in the table,

on average, ICL can correctly predict more than

85% of the examples that Ô¨Ånetuning can correct

from ZSL. These results indicate that from the per-

spective of model prediction, ICL can cover most

of the correct behavior of Ô¨Ånetuning.

4.4

ICL Tends to Change Attention Outputs

in the Same Direction as Finetuning

From the perspective of representation, we com-

pute a similarity of the attention output updates

(SimAOU) to measure the similarity between the

updates that ICL and Ô¨Ånetuning make. For a query

example, let h(l)

X denote the normalized output rep-

## Page 6

Model

Metric

## Sst2

## Sst5

## Mr

Subj

AGNews

## Cb

Average

## Gpt 1.3B

SimAOU (Random ‚àÜ)

0.002

0.003

0.001

0.002

0.002

0.003

0.002

SimAOU (‚àÜFT)

0.110

0.080

0.222

0.191

0.281

0.234

0.186

## Gpt 2.7B

SimAOU (Random ‚àÜ)

0.000

-0.002

0.000

0.001

-0.002

0.000

-0.001

SimAOU (‚àÜFT)

0.195

0.323

0.157

0.212

0.333

0.130

0.225

Table 3: SimAOU for two GPT models on six datasets. ICL updates are much more similar to Ô¨Ånetuning updates

than to random updates. From the perspective of representation, ICL tends to change attention output representa-

tions in the same direction as Ô¨Ånetuning changes.

Model

Metric

## Sst2 Sst5

## Mr

Subj

AGNews

## Cb

Average

GPT 1.3B SimAM (Before Finetuning)

0.555

0.391 0.398 0.378

0.152

0.152

0.338

SimAM (After Finetuning)

0.585

0.404 0.498 0.490

0.496

0.177

0.442

GPT 2.7B SimAM (Before Finetuning)

0.687

0.380

0.314 0.346

0.172

0.228

0.355

SimAM (After Finetuning)

0.687

0.492 0.347 0.374

0.485

0.217

0.434

Table 4: SimAM for two models on six datasets. From the perspective of attention behavior, compared with atten-

tion weights before Ô¨Ånetuning, ICL is more inclined to generate similar attention weights to those after Ô¨Ånetuning.

resentation of the last token at the l-th attention

layer in setting X. The updates of ICL and Ô¨Åne-

tuning compared with ZSL are h(l)

ICL ‚àíh(l)

ZSL and

h(l)

FT ‚àíh(l)

ZSL, respectively. We compute the cosine

between these two updates to get SimAOU (‚àÜFT)

at the l-th layer. A higher SimAOU (‚àÜFT) means

ICL is more inclined to update the attention output

in the same direction as Ô¨Ånetuning. For comparison,

we also compute a baseline metric called SimAOU

(Random ‚àÜ) that computes the similarity between

ICL updates and randomly generated updates.

We present the SimAOU scores averaged across

examples and layers for two GPT models on six

datasets in Table 3.

From the table, we Ô¨Ånd

that SimAOU (Random ‚àÜ) is always around zero,

while SimAOU (‚àÜFT) remains much more positive.

These results indicate that ICL updates are much

more similar to Ô¨Ånetuning updates than to random

updates. From the perspective of representation,

we prove that ICL tends to change the attention

outputs in the same direction as Ô¨Ånetuning.

4.5

ICL Is Inclined to Generate Similar

Attention Weights to Finetuning

From the perspective of attention behavior, we com-

pute a similarity of the attention map (SimAM)

to measure the similarity of the attention map to

query tokens for ICL and Ô¨Ånetuning. For a query

example, let m(l,h)

## X

denote the attention weights be-

fore softmax of the last token at the h-th attention

head in the l-th attention layer in setting X. For ICL,

we omit the attention to the demonstration tokens

and only monitor the attention weights to the query

tokens. First, before Ô¨Ånetuning, we compute the

cosine between m(l,h)

ICL and m(l,h)

ZSL and then average

the similarity across attention heads to get SimAM

(Before Finetuning) at each layer. Similarly, after

Ô¨Ånetuning, we compute the cosine between m(l,h)

## Icl

and m(l,h)

## Ft

to get SimAM (After Finetuning). A

higher SimAM (After Finetuning) over SimAM

(Before Finetuning) indicates that the attention be-

havior of ICL is more similar to a Ô¨Ånetuned model

than a non-Ô¨Ånetuned one.

Table 4 demonstrates the SimAM scores aver-

aged across examples and layers for two GPT mod-

els on six datasets. We observe that compared with

attention weights before Ô¨Ånetuning, ICL is more

inclined to generate similar attention weights to

attention weights after Ô¨Ånetuning. Again, from the

perspective of attention behavior, we prove that

ICL behaves similarly to Ô¨Ånetuning.

4.6

ICL and Finetuning Tend to Pay Similar

Attention to Training Tokens

Since we understand ICL as a process of meta-

optimization, we also compare the attention to

training tokens for ICL and Ô¨Ånetuning with the

Kendall rank correlation coefÔ¨Åcient (Kendall,

1948). For a query example, let m(l)

ICL denote the

ICL attention weights to the demonstration tokens

## Page 7

Model

Metric

## Sst2

## Sst5

## Mr

Subj

AGNews

## Cb

Average

## Gpt 1.3B

Kendall (ICL, Random)

0.000

-0.001

0.000

0.001

-0.001

0.000

0.000

Kendall (ICL, FT)

0.192

0.151

0.173

0.181

0.190

0.274

0.193

## Gpt 2.7B

Kendall (ICL, Random)

-0.001

0.000

0.000

0.000

0.000

-0.001

0.000

Kendall (ICL, FT)

0.213

0.177

0.264

0.203

0.201

0.225

0.214

Table 5: Kendall rank correlation coefÔ¨Åcients for two GPT models on six datasets. Compared with random atten-

tion weights, ICL attention weights to training tokens are much more similar to Ô¨Ånetuning attention weights.

of the last query token in the l-th attention layer,

which is summed across attention heads. For Ô¨Åne-

tuning, we Ô¨Årst record all the attention queries

Q‚Ä≤(l,h) ‚ààRd‚Ä≤√óN of the training tokens, and then

use the inner product between them and the atten-

tion query q(l,h) ‚ààRd‚Ä≤ of the last token in the

query example as the Ô¨Ånetuning attention weights

to the training tokens: m(l)

## Ft = P

h Q‚Ä≤(l,h)T q(l,h),

which is also summed across attention heads. The

Kendall coefÔ¨Åcient between m(l)

ICL and m(l)

FT is com-

puted as Kendall (ICL, FT) =

Pc‚àíPd

N(N‚àí1)/2, where

N denotes the number of training tokens, Pc de-

notes the number of concordant pairs, and Pd de-

notes the number of discordant pairs. A higher

Kendall coefÔ¨Åcient means that the orders of atten-

tion weights to training tokens of ICL and Ô¨Ånetun-

ing are more similar. For comparison, we also com-

pute the Kendall coefÔ¨Åcient between m(l)

ICL and ran-

domly generated attention weights m(l)

Random, which

we call Kendall (ICL, Random).

Table 5 shows the Kendall correlation coefÔ¨Å-

cients averaged across examples and layers for two

GPT models on six datasets. We Ô¨Ånd that Kendall

(ICL, Random) is always near zero, while Kendall

(ICL, FT) always maintains a distinctly positive

value. These results suggest that ICL and Ô¨Ånetun-

ing tend to pay similar attention to training tokens.

5

Momentum-Based Attention Inspired

by Dual Form of Transformer

Attention

We have Ô¨Ågured out the dual form between Trans-

former attention and gradient descent. As illus-

trated in Figure 2, inspired by this dual view,

we investigate whether we can utilize momen-

tum (Polyak, 1964; Sutskever et al., 2013), a widely

used technique for optimization algorithms, to im-

prove Transformer attention.

Gradient descent with momentum averages gra-

Momentum-Based

Attention

Gradient Descent

Gradient Descent

with Momentum

Attention

(Dual Form)

(Analogy)

Figure 2: Inspired by the dual form between atten-

tion and gradient descent, we introduce the momentum

mechanism into Transformer attention by analogy with

gradient descent with momentum.

dients among timestamps:

Œòt = Œòt‚àí1 ‚àíŒ≥

t‚àí1

## X

i=1

Œ∑t‚àíi‚àáfŒòi,

(14)

where Œ≥ is the learning rate and Œ∑ is a scalar be-

tween 0 and 1. As stated in Section 3.1, the atten-

tion values serve as meta-gradients. By analogy

with gradient descent with momentum, we try to

use Exponential Moving Average (EMA; Hunter

1986) to average the attention values to build the

momentum-based attention:

MoAttn(V, K, qt) = Attn(V, K, qt) + EMA(V )

= V softmax(KT qt

‚àö

d

) +

t‚àí1

## X

i=1

Œ∑t‚àíivi,

where vi is the i-th attention value vector. The

momentum of attention value vectors explicitly

strengthens the recency bias of attention, which has

been shown helpful for language modeling (Press

et al., 2022). Therefore, we assume that introducing

momentum into attention will contribute to faster

convergence and better performance.

Experiments on Language Modeling

First, we

evaluate the effect of momentum-based attention

on language modeling. We train two GPT models

with 350M parameters from scratch, where one is

the vanilla Transformer, and another applies mo-

mentum to attention. More training details are pro-

vided in Appendix C. We evaluate the perplexity

## Page 8

Model

Train1024

Valid256

Valid512

Valid1024

Transformer

17.61

19.50

16.87

15.14

TransformerMoAttn

17.55

19.37

16.73

15.02

Table 6: Perplexity on the training set and validation sets with different input lengths for language modeling.

Momentum-based attention achieves a consistent perplexity improvement compared with the vanilla Transformer.

Model

## Sst5

## Imdb

## Mr

## Cb

## Arc-E

## Piqa

Average

Transformer

25.3

64.0

61.2

43.9

48.2

68.7

51.9

TransformerMoAttn

27.4

70.3

64.8

46.8

50.0

69.0

54.7

Table 7: Accuracy on six in-context learning datasets. Introducing momentum into attention improves the accuracy

of the vanilla Transformer by 2.8 on average.

of these two models on the training set and three

validation sets with input lengths of 256, 512, and

1024, respectively. The results are shown in Table 6.

On all of the validation sets, applying momentum

to attention introduces a consistent perplexity im-

provement compared with the vanilla Transformer.

Experiments on In-Context Learning

We also

evaluate the in-context learning ability of the above

language models to verify the effectiveness of

momentum-based attention on downstream tasks.

We consider six datasets for sentiment analysis

(SST5 (Socher et al., 2013), IMDB (Maas et al.,

2011), and MR (Pang and Lee, 2005)), natural lan-

guage inference (CB (De Marneffe et al., 2019)),

and multi-choice selection (ARC-E (Clark et al.,

2018) and PIQA (Bisk et al., 2020)). For all of

these datasets, we use up to 32 examples as demon-

strations. As shown in Table 7, compared with

vanilla Transformer, using momentum-based atten-

tion achieves consistently higher accuracy on all of

these datasets.

The performance improvements on both lan-

guage modeling and in-context learning prove our

deduction that introducing momentum will im-

prove Transformer attention. From another perspec-

tive, these results further support our understanding

of Transformer attention as meta-optimization.

6

Related Work

Recently, some pieces of work have attempted to

understand the inference mechanism of in-context

learning. Xie et al. (2022) explain in-context learn-

ing as implicit Bayesian inference. They state that

in-context learning emerges when language mod-

els can infer the shared latent concept among the

demonstration examples, which is learned during

pretraining. On another aspect, Olsson et al. (2022)

focus on speciÔ¨Åc modules in Transformers. They

Ô¨Ånd some induction heads in Transformers that re-

fer to abstract patterns in previous sequences to

help predict the next token. They indicate that

the induction heads drive the ability of in-context

learning. Different from them, we concentrate on

the learning algorithm of ICL and explain it as a

process of meta-optimization.

Some other work also studies the learning algo-

rithm of ICL. As a case study, Garg et al. (2022)

show that Transformers can be trained to in-context

learn a class of linear functions and the perfor-

mance is comparable to the least squares estimator.

Based on linear regression, Aky√ºrek et al. (2022)

prove that they can construct parameters of Trans-

formers to implement gradient-descent-based learn-

ing algorithms. Further, they show that models

trained with an in-context learning objective tend

to match the behavior of models computed by ex-

plicit learning algorithms. Also based on regres-

sion tasks, von Oswald et al. (2022) show that lin-

ear attention-only Transformers with constructed

parameters that implement gradient descent and

models learned by an in-context learning objective

are highly related. Compared with them, we are

the Ô¨Årst ones to explain in-context learning in real

scenarios. To be speciÔ¨Åc, (1) we analyze in-context

learning for off-the-shelf GPT models, instead of

models trained from scratch by an ICL objective;

(2) our experiments are based on real NLP tasks,

instead of toy ones like linear regression.

7

Conclusion

In this paper, we aim to explain the working mech-

anism of GPT-based ICL. Theoretically, we Ô¨Ågure

## Page 9

out a dual form between Transformer attention and

gradient descent, and propose to understand ICL

as a process of meta-optimization. Further, we

analyze connections between ICL and explicit Ô¨Åne-

tuning and show the reasonability to regard ICL as

implicit Ô¨Ånetuning. Empirically, we comprehen-

sively compare ICL and Ô¨Ånetuning based on six

real NLP tasks. The results prove that ICL behaves

similarly to explicit Ô¨Ånetuning from multiple per-

spectives. Further, inspired by our understanding of

meta-optimization, we design a momentum-based

attention that achieves consistent performance im-

provements over vanilla attention. We believe our

understanding will have more potential to enlighten

ICL applications and model design in the future.

Limitations

Although the ability of in-context learning has been

found for different architectures (e.g., Transformer

and LSTM), we consider only Transformer-based

in-context learning in this paper because Trans-

former is the current mainstream architecture of

NLP. However, as for in-context learning itself, Ô¨Åg-

uring out how it works for other architectures is

also a meaningful problem, which we encourage to

study in the future.

As for the dual form we point out between Trans-

former attention and gradient descent, we consider

a relaxed form of linear attention for qualitative

analysis. Although the experimental results sup-

port our understanding well, the mechanism of stan-

dard Transformer attention without approximation

may be more complex and should be studied more

clearly in the future.

As for empirical experiments, our analysis needs

to record a large number of intermediate results

(e.g., attention output representations, and atten-

tion weights to query tokens and demonstration

tokens) for thousands of validation examples. Con-

sidering the storage space and computational cost

of analysis, we only analyze GPT models with up

to 2.7B parameters and leave larger models such as

GPT 13B for future work. In addition, for the clar-

ity of the problem deÔ¨Ånition and the convenience

of experiments, our analysis is based on only clas-

siÔ¨Åcation tasks. Although classiÔ¨Åcation is a repre-

sentative application of in-context learning, other

tasks like multiple choice and open-ended genera-

tion are not considered in this paper and could be

investigated in the future.

Acknowledgement

Damai Dai and Zhifang Sui are supported by the

National Key Research and Development Program

of China 2020AAA0106700 and NSFC project

## U19A2065.

References

Mark A Aizerman, Emmanuil M Braverman, and Lev I

Rozonoer. 1964. Theoretical foundation of potential

functions method in pattern recognition. Avtomatika

i Telemekhanika, 25(6):917‚Äì936.

Ekin Aky√ºrek, Dale Schuurmans, Jacob Andreas,

Tengyu Ma, and Denny Zhou. 2022. What learning

algorithm is in-context learning? investigations with

linear models. CoRR, abs/2211.15661.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng

Gao, and Yejin Choi. 2020. PIQA: reasoning about

physical commonsense in natural language. In The

Thirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelli-

gence, AAAI 2020, pages 7432‚Äì7439. AAAI Press.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie

Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind

Neelakantan, Pranav Shyam, Girish Sastry, Amanda

Askell,

Sandhini Agarwal,

Ariel Herbert-Voss,

Gretchen Krueger, Tom Henighan, Rewon Child,

Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,

Clemens Winter, Christopher Hesse, Mark Chen,

Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin

Chess, Jack Clark, Christopher Berner, Sam Mc-

Candlish, Alec Radford, Ilya Sutskever, and Dario

Amodei. 2020. Language models are few-shot learn-

ers. In Advances in Neural Information Processing

Systems 33: Annual Conference on Neural Informa-

tion Processing Systems 2020, NeurIPS 2020.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,

Ashish Sabharwal, Carissa Schoenick, and Oyvind

Tafjord. 2018.

Think you have solved question

answering?

try arc, the AI2 reasoning challenge.

CoRR, abs/1803.05457.

Marie-Catherine De Marneffe, Mandy Simons, and Ju-

dith Tonhauser. 2019. The commitmentbank: Inves-

tigating projection in naturally occurring discourse.

In proceedings of Sinn und Bedeutung, volume 23,

pages 107‚Äì124.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-

ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei

Li, and Zhifang Sui. 2023. A survey for in-context

learning. CoRR, abs/2301.00234.

Shivam Garg, Dimitris Tsipras, Percy Liang, and Gre-

gory Valiant. 2022. What can transformers learn in-

context? A case study of simple function classes.

CoRR, abs/2208.01066.

J Stuart Hunter. 1986.

The exponentially weighted

moving average.

Journal of quality technology,

18(4):203‚Äì210.

## Page 10

Kazuki Irie, R√≥bert Csord√°s, and J√ºrgen Schmidhuber.

2022. The dual form of neural networks revisited:

Connecting test time predictions to training patterns

via spotlights of attention.

In International Con-

ference on Machine Learning, ICML 2022, volume

162 of Proceedings of Machine Learning Research,

pages 9639‚Äì9659. PMLR.

Maurice George Kendall. 1948. Rank correlation meth-

ods.

Louis Kirsch, James Harrison, Jascha Sohl-Dickstein,

and Luke Metz. 2022. General-purpose in-context

learning by meta-learning transformers.

CoRR,

abs/2212.04458.

Louis Kirsch and J√ºrgen Schmidhuber. 2021.

Meta

learning backpropagation and improving it. In Ad-

vances in Neural Information Processing Systems

34: Annual Conference on Neural Information Pro-

cessing Systems 2021, NeurIPS 2021, pages 14122‚Äì

14134.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,

Dan Huang, Andrew Y. Ng, and Christopher Potts.

2011. Learning word vectors for sentiment analy-

sis. In Proceedings of the 49th Annual Meeting of

the Association for Computational Linguistics: Hu-

man Language Technologies, pages 142‚Äì150, Port-

land, Oregon, USA. Association for Computational

Linguistics.

Catherine

Olsson,

Nelson

Elhage,

Neel

Nanda,

Nicholas Joseph, Nova DasSarma, Tom Henighan,

Ben Mann, Amanda Askell, Yuntao Bai, Anna

Chen, Tom Conerly, Dawn Drain, Deep Ganguli,

Zac HatÔ¨Åeld-Dodds, Danny Hernandez, Scott John-

ston, Andy Jones, Jackson Kernion, Liane Lovitt,

Kamal Ndousse, Dario Amodei, Tom Brown, Jack

Clark, Jared Kaplan, Sam McCandlish, and Chris

Olah. 2022. In-context learning and induction heads.

CoRR, abs/2209.11895.

Bo Pang and Lillian Lee. 2004.

A sentimental edu-

cation: Sentiment analysis using subjectivity sum-

marization based on minimum cuts.

In Proceed-

ings of the 42nd Annual Meeting of the Association

for Computational Linguistics (ACL-04), pages 271‚Äì

278, Barcelona, Spain.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-

ing class relationships for sentiment categorization

with respect to rating scales. In ACL 2005, 43rd An-

nual Meeting of the Association for Computational

Linguistics, Proceedings of the Conference, pages

115‚Äì124. The Association for Computer Linguistics.

Boris T Polyak. 1964. Some methods of speeding up

the convergence of iteration methods.

Ussr com-

putational mathematics and mathematical physics,

4(5):1‚Äì17.

OÔ¨År Press, Noah A. Smith, and Mike Lewis. 2022.

Train short, test long: Attention with linear biases en-

ables input length extrapolation. In The Tenth Inter-

national Conference on Learning Representations,

ICLR 2022. OpenReview.net.

Richard Socher, Alex Perelygin, Jean Wu, Jason

Chuang, Christopher D. Manning, Andrew Ng, and

Christopher Potts. 2013.

Recursive deep models

for semantic compositionality over a sentiment tree-

bank.

In Proceedings of the 2013 Conference on

Empirical Methods in Natural Language Processing,

pages 1631‚Äì1642, Seattle, Washington, USA. Asso-

ciation for Computational Linguistics.

Ilya Sutskever, James Martens, George E. Dahl, and

Geoffrey E. Hinton. 2013.

On the importance of

initialization and momentum in deep learning. In

Proceedings of the 30th International Conference on

Machine Learning, ICML 2013, volume 28 of JMLR

Workshop and Conference Proceedings, pages 1139‚Äì

1147. JMLR.org.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob

Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz

Kaiser, and Illia Polosukhin. 2017.

Attention is

all you need.

In Advances in Neural Information

Processing Systems, pages 5998‚Äì6008. Curran As-

sociates, Inc.

Johannes von Oswald, Eyvind Niklasson, Ettore Ran-

dazzo, Jo√£o Sacramento, Alexander Mordvintsev,

Andrey Zhmoginov, and Max Vladymyrov. 2022.

Transformers learn in-context by gradient descent.

ArXiv preprint, abs/2212.07677.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-

fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-

gatama, Maarten Bosma, Denny Zhou, Donald Met-

zler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,

Percy Liang, Jeff Dean, and William Fedus. 2022.

Emergent abilities of large language models. CoRR,

abs/2206.07682.

Sang Michael Xie, Aditi Raghunathan, Percy Liang,

and Tengyu Ma. 2022. An explanation of in-context

learning as implicit bayesian inference. In The Tenth

International Conference on Learning Representa-

tions, ICLR 2022. OpenReview.net.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.

Character-level convolutional networks for text clas-

siÔ¨Åcation. In Advances in Neural Information Pro-

cessing Systems 28: Annual Conference on Neural

Information Processing Systems 2015, pages 649‚Äì

657.

## Page 11

Appendix

## A

Templates for In-Context Learning

We demonstrate the templates used to format exam-

ples and the candidate answer sets for six classiÔ¨Å-

cation datasets used in our experiments in Table 8.

## B

Hyper-Parameters for In-Context

Learning and Finetuning

We perform grid search to Ô¨Ånd the best random seed

for ICL and the best learning rate for Ô¨Ånetuning.

The search range for all the datasets is the same.

For random seeds, we search in {1, 2, 3, 4, 5, 6, 7}.

For learning rates, the search base values are

{1, 2, 3, 4, 5, 6, 7, 8, 9} and we scale them to 0.1,

0.01, 0.001, and 0.0001 times, i.e., we have 9√ó4 =

36 values to search. As an exception, for GPT 1.3B

Ô¨Ånetuned on SST5, we perform a more Ô¨Åne-grained

search and Ô¨Ånally set its learning rate to 0.00016

since the Ô¨Ånetuned model cannot outperform the

zero-shot learning with the above 36 learning rates.

In Table 9, we present the details of the selected

random seeds and learning rates for two GPT mod-

els on six classiÔ¨Åcation datasets.

## C

Hyper-Parameters for Training

Language Models from Scratch

The hyper-parameters for training two language

models from scratch are summarized in Table 10.

## Page 12

Dataset

Template

Candidate Answer Set

## Sst2

Sentence: {Sentence}

{ Negative, Positive }

Label: {Label}

## Sst5

Sentence: {Sentence}

{ terrible, bad, neutral, good, great }

Label: {Label}

## Mr

Review: {Sentence}

{ Negative, Positive }

Sentiment: {Label}

Subj

Input: {Sentence}

{ objective, subjective }

Type: {Label}

AGNews

Classify the news articles into the categories

of World, Sports, Business, and Technology.

{ World, Sports, Business, Technology }

News: {Sentence}

Type: {Label}

## Cb

{Premise}

{ True, False, Neither }

Question: {Hypothesis} True, False, or Nei-

ther?

Answer: {Label}

Table 8: Formatting templates and candidate answer sets for six classiÔ¨Åcation datasets.

Hyper-Parameter

Dataset

## Gpt 1.3B

## Gpt 2.7B

Random Seed

## Sst2

2

7

## Sst5

5

5

## Mr

5

1

Subj

4

4

AGNews

3

3

## Cb

3

3

Learning Rate

## Sst2

0.0005

0.007

## Sst5

0.00016

0.04

## Mr

0.003

0.001

Subj

0.003

0.002

AGNews

0.2

0.2

## Cb

0.08

0.01

Table 9: Selected random seeds and learning rates for two GPT models on six classiÔ¨Åcation datasets.

## Page 13

Hyper-parameter

Value

Embedding & Hidden Dimension

1024

FFN Inner Hidden Dimension

4096

Number of Attention Heads

16

Number of Transformer Layers

24

Number of Parameters

## 350M

Sequence Length

1024

Batch Size

512K Tokens

Optimizer

Adam

Adam Betas

(0.9, 0.98)

Adam Epsilon

1e-6

Maximum Learning Rate

3e-4

Learning Rate Scheduler

Polynomial Decay

Total Training Steps

## 500K

Warm-up Steps

## 20K

Gradient Clip Norm

2.0

Table 10: Hyper-parameters for training two language models from scratch.
