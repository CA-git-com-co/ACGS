# 2501.13007_PairJudge-RM-Perform-Best-of-N-Sampling-with-Knock

**Original PDF**: 2501.13007_PairJudge-RM-Perform-Best-of-N-Sampling-with-Knock.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2

---

## Page 1

PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament

Yantao Liu1, Zijun Yao2, Rui Min3, Yixin Cao1, Lei Hou2, Juanzi Li2

1Fudan University, 2Tsinghua University, 3Hong Kong University of Science and Technology

ricardoliu@outlook.com, yaozj20@mails.tsinghua.edu.cn

Code: https://github.com/THU-KEG/PairJudgeRM/

Model: https://huggingface.co/THU-KEG/PairJudgeRM

Dataset: https://huggingface.co/datasets/THU-KEG/PairJudge-432K

Abstract

Best-of-N (BoN) sampling, a common strategy

for test-time scaling of Large Language Mod-

els (LLMs), relies on reward models to select

the best candidate solution from multiple gen-

erations. However, traditional reward models

often assign arbitrary and inconsistent scores,

limiting their effectiveness. To address this,

we propose a Pairwise Judge Reward Model

(PairJudge RM) combined with a knockout

tournament for BoN sampling. Instead of as-

signing absolute scores, given one math prob-

lem, PairJudge RM judges two candidate so-

lutions’ correctness with chain-of-thought rea-

soning simultaneously. This approach elimi-

nates the need for scoring and enables cross-

validation of solutions through parallel judg-

ment. In the knockout tournament, PairJudge

RM conducts pairwise Judgment between can-

didate solutions and eliminates the incorrect

ones iteratively. We construct PAIRJUDGE-

432K, a large-scale dataset of 432K pairwise

judgments derived from NumiaMath and anno-

tated using gemini-1.5-flash, and train

the PairJudge RM via supervised fine-tuning.

Experiments on MATH-500 and the Olympiad

Bench demonstrate significant improvements

over baseline reward models. And a 40% to

60% relative improvement is achieved on the

top 50% challenging problems.

1

Introduction

Recently, test-time scaling has garnered significant

attention from the research community, as it draw

a blueprint for the next stage of scaling of Large

Language Models (LLMs) (Snell et al., 2024; Wu

et al., 2024; OpenAI, 2024). One of the most com-

mon practice to achieve test-time scaling is to use

reward models (RMs) to perform the Best-of-N

(BoN) Sampling at test time (Wang et al., 2023;

Lightman et al., 2023; Wang et al., 2024b; Zhang

et al., 2024b; Yuan et al., 2024b): the LLM gener-

ates N candidate solutions for a given problem, and

a learned reward model, scoring each candidate so-

lution, selects the best one as the final output. The

effectiveness of this strategy hinges on how accu-

rate the score assigned by the reward model is to

the candidate solutions.

However, assigning accurate and consistent

scores is inherently challenging, even for human

experts (Jonsson and Svingby, 2007; Abdul Gafoor

and Jisha, 2014). An experiment conducted in

NeurIPS 2021 shows that for different human ex-

perts guided by the same rubric, the scores as-

signed to the same candidate paper can vary signifi-

cantly (Beygelzimer et al., 2021). This limitation is

particularly pronounced in reward models, which

are typically trained to assign relative scores rather

than absolute, meaningful scores (Lambert et al.,

2024; Liu et al., 2024). As a result, the scores as-

signed by reward models are often arbitrary and

inconsistent, hindering the performance of BoN

sampling (Liu et al., 2024; Kim et al., 2024).

To address this limitation, we propose a Pairwise

Judge Reward Model (PairJudge RM) combined

with a knockout tournament for BoN sampling. In-

stead of assigning absolute scores, PairJudge RM

judges two candidate solutions’ correctness simul-

taneously for a given reasoning problem.

In this setting, our approach eliminates the need

for arbitrary scoring and enables cross-validation

of solutions through pairwise judgment. To per-

form BoN sampling, we organize candidate so-

lutions into a knockout tournament, where each

pairwise judgment acts as a match. Rounds of

matches are played until only one candidate re-

mains, which is selected as the final output. Fig-

ure 1 presents the knockout tournament process

with our PairJudge RM. Specifically, inlining with

existing work (Lightman et al., 2023; Wang et al.,

2023; Snell et al., 2024; Wu et al., 2024), math

reasoning tasks are used as the testbed to evaluate

the BoN sampling performance.

We construct PAIRJUDGE-432K, a large-scale

arXiv:2501.13007v2  [cs.CL]  19 Feb 2025

## Page 2

✓Advanced

✗Eliminated

Question: If one equilateral triangle in a regular hexagon has a perimeter

of 21 inches, what is the hexagon’s perimeter?

Response ■: Each side of triangle is 21 ÷ 3 = 7. The hexagon, made of

six such sides, has a perimeter of 6 × 7 = 42 .

Response ■: The triangle’s perimeter is 21. The hexagon, made of six

such triangles, has a perimeter of 21 × 6 = 126 .

Response ■: First calculate the side length of the triangle = 7 (correct). Second,

recall the hexagon is made of six such sides = 42 (correct). The final answer is

correct.

Advanced!

Response ■: First, the triangle’s perimeter is 21 (correct). Second, the hexagon

is made of six such triangles, leading to 21 × 6 = 126 (incorrect interpretation,

mix up the area with perimeter). The final answer is incorrect.

Eliminated!

## Input

## Prompt

## Pairwise

## Judgment

■Ans. 42

■Ans. 84

■Ans. 126 ■Ans. 21

a) Input Prompt and Pairwise Judgment in PairJudge RM.

b) Knockout with PairJudge RM.

Figure 1: An example of the knockout tournament using the PairJudge RM. Panel (a) shows a pairwise judgment in

the knockout tournament (Panel b) between response ■and ■. The PairJudge RM takes a question and responses

■, ■as input, then judges their correctness pairwise. In the judgment, the PairJudge RM correctly identifies the

response ■as correct and response ■as incorrect, leading to the elimination of the ■. The pairwise judgment

process continues iteratively in the knockout tournament until only one response remains. For this question, the

response ■is selected as the best candidate solution and Answer 42 is the final answer through the knockout.

dataset of 432K pairwise judgments derived from

NumiaMath (LI et al., 2024) and annotated using

gemini-1.5-flash. Using this dataset, we

train the PairJudge RM via supervised fine-tuning.

Experiments on MATH-500 and the Olympiad

Bench demonstrate that PairJudge RM significantly

outperforms traditional discriminative reward mod-

els.

On the top 50% challenging problems in

MATH-500, PairJudge RM achieves a 40% to 60%

relative improvement over the baseline. Further-

more, our method outperforms the recently pro-

posed Critic Model (Gao et al., 2024a; McAleese

et al., 2024; Zhang et al., 2024b) under the same

computational budget.

In summary, our contributions are as follows:

• We propose a Pairwise Judge Reward Model

(PairJudge RM) combined with a knockout

tournament for BoN sampling. This approach

avoids the limitations of arbitrary scoring in

traditional reward models and enables cross-

validation of candidate solutions.

• We release PAIRJUDGE-432K, a large-scale

dataset for training PairJudge RMs containing

432K annotated pairwise judgments, along

with its construction pipeline.

• Experiments on MATH-500 and the Olympiad

Bench demonstrate significant improvements

compared to baselines in BoN sampling.

Specifically, on the top 50% difficult prob-

lems, PairJudge RM achieves a 40% to 60%

relative improvement over baseline models.

2

Preliminaries

Best-of-N Sampling in Math Reasoning Given

a math problem x ∈X and the N candidate solu-

tions {y1, y2, . . . , yN} sampled from a Large Lan-

guage Model (LLM), the BoN Sampling aims to

select the best candidate solution y∗from the N

candidate solutions based on an external selection

mechanism. Typically, there are two types of re-

ward models (RMs) serving as the external selec-

tion mechanism: the Outcome Reward Model and

the Process Reward Model.

Outcome Reward Model Given a math problem

x and a candidate solution y, the Outcome Reward

Model assigns a numerical score s(y) to the candi-

date solution y. The Outcome Reward Model se-

lects the candidate solution with the highest score

as the final output:

y∗= arg

max

y∈{y1,y2,...,yN} s(y).

(1)

The Outcome Reward Model is typically trained

on a preference dataset D, consisting of pairs

(x, yc, yr), where yc is the chosen response and

yr is the rejected response. The model is trained to

assign a higher reward to yc than to yr, minimizing

the following objective:

L = −E [log σ(Rψ(x, yc) −Rψ(x, yr))] ,

(2)

## Page 3

where σ(·) is the sigmoid function and ψ is the pa-

rameters of the Reward Model. L is the loss func-

tion for preference learning indicating the probabil-

ity of the chosen response yc being preferred over

the rejected response yr. This objective ensures

that the reward model learns to identify responses

that align better with human preferences.

Process Reward Model Given a math problem x

and a corresponding candidate solution y, the Pro-

cess Reward Model first requires to split the candi-

date solution y into a sequence of reasoning steps

{a1, a2, . . . , aM}. The Process Reward Model as-

signs a numerical score s(ai) to each reasoning

step ai. The score of the entire candidate solution

y is the mean of the scores of all reasoning steps:

s(y) = 1

## M

## M

## X

i=1

s(ai).

(3)

The Process Reward Model selects the candidate

solution with the highest score as the final output

with the same mechanism as the Outcome Reward

Model in Equation 1.

The Process Reward Model is typically trained

on a dataset with process labels Dproc, where each

solution y to a problem x, the dataset contains

a series of process labels {l1, l2, . . . , lM}, where

li ∈{0, 1} indicates whether the reasoning step ai

is correct or incorrect. Then the Process Reward

Model is trained to predict the correctness of each

reasoning step ai.

3

PairJudge RM and Knockout

In this section, we introduce the PairJudge RM and

the knockout tournament, which are the core com-

ponents of our proposed method for performing

BoN Sampling at test time.

3.1

Pairwise Judge Reward Model

Definition Given a math problem x and two can-

didate solutions y1 and y2, the PairJudge RM is

designed to simultaneously check the correctness

of the two candidate solutions. Specifically, the

PairJudge RM is trained to judge the correctness of

the two candidate solutions, denoted as c1 and c2,

respectively.

c1, c2 = PairJudge(x, y1, y2),

(4)

where c1, c2 ∈{0, 1} indicate whether the candi-

date solutions y1 and y2 are correct or incorrect.

Implementation Inspired by the Generative Re-

ward Model (GenRM) (Zhang et al., 2024b) and

LLM-as-a-Judge (Zheng et al., 2023), we imple-

ment the PairJudge RM as a generative model.

Specifically, given a math problem x and two

candidate solutions y1 and y2, the PairJudge RM

first generates a reasoning text using chain-of-

thought (Wei et al., 2022) to verify the correctness

of the two candidate solutions. Based on the rea-

soning text, the PairJudge RM then predicts the

correctness of the two candidate solutions by di-

rectly generating the correctness labels c1 and c2.

The detailed prompt for performing pairwise verifi-

cation with chain-of-thought is provided in Table 4

in the Appendix.

3.2

Knockout Tournament

To perform BoN Sampling with the PairJudge RM,

we introduce a knockout tournament to select the

best candidate solution.

Algo 1: Knockout for Best-of-N Sampling

Input: Math problem x,

N candidate solutions

Y = {y1, y2, . . . , yN},

Pairwise Judge Reward Model: PairJudge

Output: Best candidate solution ybest

Step 1: Group candidates into teams

Partition Y into k teams, where members of

a team share the same final answer.

Step 2: Initialize the knockout pool

Add all N candidates to the initial pool P.

Step 3: Perform the knockout rounds

while |P| > 1 do

Pair each candidate yi with an unpaired

yj from a different team.

Remove yi and yj from P.

foreach pair (yi, yj) do

Compute correctness scores ci, cj

using PairJudge(x, yi, yj).

if ci > cj then

yi advances.

else if cj > ci then

yj advances.

else if ci, cj both correct then

Randomly select one to advance.

else

Both incorrect and eliminated.

Add advancing candidates back to P.

Step 4: Return the best solution

Output the last remaining y in P as ybest.

## Page 4

Specifically, we first group the N candidate solu-

tions into k teams, where candidates that share the

same answer are placed in the same team. Then,

we pair up the candidate solutions from each team

to compete with candidate solutions from other

teams. In each match, only the candidate solution

that receives the correct label from the PairJudge

RM advances to the next round. If both candidate

solutions receive the correct label, one is randomly

selected to advance. This process continues until

only one candidate solution remains or early ter-

mination occurs when all candidate solutions are

from the same team.

The detailed procedure of the knockout tourna-

ment is shown in Algorithm 1.

4

PAIRJUDGE-432K dataset collection

To train the PairJudge RM, we collect a large-

scale dataset named PAIRJUDGE-432K, which

contains 432K annotated pairwise judgments de-

rived from NumiaMath (LI et al., 2024) with

gemini-1.5-flash. In the following, we de-

scribe the detailed procedure of collecting the

PAIRJUDGE-432K dataset.

4.1

Dataset Format

Since the PairJudge RM is designed as a genera-

tive model to judge the correctness of candidate

solutions, the training dataset has the same format

as the one for Supervised Fine-Tuning, consist-

ing of prompt-completion pairs. Specifically, each

prompt is constructed by filling the template shown

in Table 4 with a math problem x and two candidate

solutions y1 and y2. The completion is a chain-of-

thought reasoning text that judges the correctness

of the two solutions and provides the correctness

labels c1 and c2.

4.2

Math Problem Collection

We first collect math problems from the Numia-

Math dataset (LI et al., 2024), which contains 860K

problems ranging from high school math exercises

and international mathematics olympiad competi-

tion problems. Because these data are primarily

collected from online exam paper PDFs and mathe-

matics discussion forums, we remove low-quality

problems with messy formatting, OCR errors, or

missing answers. We also remove multiple-choice

(MCQ) and True/False questions to avoid random

guessing in candidate solutions. Following com-

munity conventions, we remove proof problems as

well, due to the difficulty of verifying candidate

Dataset

Original Count

Filtered Count

## Amc/Aime

4,070

289

AoPS Forum

30,192

9,017

Chinese K-12

276,554

63,779

## Gsm8K

7,342

6,539

Math

7,477

5,988

Olympiads

150,563

52,766

ORCA Math

153,314

149,550

Synthetic AMC

62,108

94

Synthetic Math

167,874

136,921

Total

859,494

425,943

Table 1: Statistics of the datasets before and after filter-

ing. AMC-related datasets shrink significantly because

most AMC problems are multiple-choice.

solutions. The detailed filtering criteria are listed

in Table 5 of the Appendix.

4.3

Candidate Solution Generation

For each math problem x, we generate k = 24

candidate solutions {y1, y2, . . . , yk} using Llama-

3.1-8B-instruct (AI, 2025). We employ the same

four-shot in-context examples for all problems as

the prompt. The candidate solutions are decoded

with a temperature of 1.0 and a Top-P value of 0.5

to balance diversity and quality.

4.4

Pairwise Judgment Annotation

We use gemini-1.5-flash to annotate the

PairJudge RM training data on the NumiaMath

dataset. To align the generated training data distri-

bution with the solution-judgment distribution in

the knockout tournament, we conduct a knockout

tournament for each math problem x and its can-

didate solutions {y1, y2, . . . , yk} to select the best

solution ybest. During the knockout tournament, we

record all pairwise judgments among candidate so-

lutions and retain only those records that correctly

judge solution correctness for the PairJudge RM.

Specifically, due to cost considerations, we only

run the knockout tournament for questions whose

candidate solutions are not all correct or all incor-

rect. As a result, we conducted 343K tournaments

and recorded 2.2M comparisons. Among these,

1.3M correctly evaluated both candidate solutions

and were used as raw training data for the Pair-

Judge RM. Finally, we filtered out samples where

the response did not strictly follow the instructions

in Table 4, ending up with 432K training samples.

5

Experiments

In this section, we demonstrate the effectiveness of

the PairJudge RM and the knockout tournament

## Page 5

Type

Reward Model

Llama-3.1-8B-Inst

Qwen-2.5-7B-Inst

Llama-3.1-70B-Inst

Avg.

@16

@32

@64

@16

@32

@64

@16

@32

@64

## Math-500

## Orm

ArmoRM-Llama3-8B

51.6

49.2

49.8

77.6

77.4

76.4

64.8

64.8

65.8

64.2

SkyworkRM-Llama3.1-8B

51.4

51.0

51.0

77.6

76.4

78.0

66.4

66.6

67.4

65.1

EurusRM-7B

55.2

53.4

53.4

76.6

77.0

77.4

68.0

66.6

67.6

66.1

Pair-Preference-Llama3-8B

48.0

47.6

49.0

76.0

77.4

75.6

64.0

63.4

60.2

62.4

## Prm

Math-Shepherd-7B

49.5

50.1

49.2

74.7

75.3

75.9

63.5

62.8

63.6

62.7

RLHFlow-8B-Mistral-Data

51.0

51.0

50.2

75.4

76.2

76.6

64.0

63.0

64.8

63.6

RLHFlow-8B-DS-Data

55.2

57.0

56.2

75.8

76.0

76.2

66.2

66.4

65.4

66.0

RLHFlow-8B-LLaMA-Data

55.5

56.8

56.0

76.0

76.3

76.5

66.7

67.0

66.0

66.3

Majority Voting

57.0

58.8

58.8

77.4

77.6

78.0

70.2

72.8

73.6

69.4

PairJudge RM & Knockout

61.0

64.6

65.6

80.2

79.8

80.4

72.2

75.6

77.4

73.0

Olympiad Bench

## Orm

ArmoRM-Llama3-8B

16.1

15.9

16.7

39.3

40.1

40.4

29.2

29.8

30.1

28.7

SkyworkRM-Llama3.1-8B

19.9

20.0

18.7

39.9

40.0

41.0

29.8

30.4

29.8

29.4

EurusRM-7B

20.4

19.6

20.1

37.9

39.4

39.1

30.1

30.7

32.4

30.0

Pair-Preference-Llama3-8B

17.7

19.1

17.2

39.4

38.9

38.1

26.5

27.4

25.5

27.8

## Prm

Math-Shepherd-7B

15.2

13.7

13.1

34.8

34.5

35.1

25.3

26.0

24.1

24.6

RLHFlow-8B-Mistral-Data

16.4

14.5

14.5

36.1

35.9

36.3

26.7

27.1

25.2

25.9

RLHFlow-8B-DS-Data

18.5

19.6

19.3

35.4

34.8

34.2

28.9

29.5

30.1

27.8

RLHFlow-8B-LLaMA-Data

18.7

20.0

19.7

35.8

35.2

34.7

29.1

29.4

30.3

28.1

Majority Voting

20.3

22.4

23.3

40.0

40.7

39.9

35.6

35.9

36.7

32.8

PairJudge RM & Knockout

22.7

24.9

25.5

41.9

40.2

41.2

33.9

36.7

37.8

33.9

Table 2: Different reward models’ best-of-N sampling performance on MATH-500 and Olympiad Bench with three

different LLMs: Llama-3.1-8B-Inst, Qwen-2.5-7B-Inst, and Llama-3.1-70B-Inst. The results are reported in terms

of accuracy. The pass@1 accuracy of these three LLMs are 42.0, 73.6, and 59.2 on MATH-500, and 12.3, 35.7, and

25.9 on Olympiad Bench, respectively. @16, @32, and @64 denote the accuracy with Best-of-16, Best-of-32, and

Best-of-64 sampling, respectively. The best results are in bold, and the second-best results are underlined.

in performing BoN Sampling at test time. We

first introduce the experimental setup, including the

dataset, evaluation metrics, and baselines. Then,

we present the experimental results and analysis.

5.1

Experimental Setup

Dataset We evaluate BoN Sampling on MATH-

500 (Hendrycks et al., 2021) and Olympiad

Bench (He et al., 2024) to coverage from the high-

school-level math problems to the olympiad-level

math problems. To study the generalizability of

our PairJudge RM, we test it with three LLMs that

have different capabilities and come from different

model families: Llama-3.1-8B-Instruct (AI@Meta,

2024), Llama-3.1-70B-Instruct (AI@Meta, 2024),

and Qwen2.5-7B-Instruct (Qwen Team, 2024).

Training Details We use Qwen2.5-7B-Instruct as

the base model and perform supervised fine-tuning

on our PAIRJUDGE-432K dataset to obtain the

PairJudge RM. We set the learning rate to 1×10−5

with the Adam optimizer and a batch size of 128.

The model is trained for 8 epochs.

Baselines We compare our PairJudge RM with

both outcome and process reward model, which

is trained to assign a score to each candidate

solution and then select the candidate solution

with the highest score as the final output.

For

the Outcome Reward Model, we use EurusRM-

7B (Yuan et al., 2024a), SkyworkRM-Llama3.1-

8B (Liu and Zeng, 2024), and ArmoRM-Llama3-

8B (Gao et al., 2024b) as representatives of state-

of-the-art outcome reward models. In Outcome

Reward Model, we also include a Pair-Preference-

Llama3-8B as the representative of the pairwise

preference model (Dong et al., 2024; Zhao et al.,

2023; Ye et al., 2024; Jiang et al., 2023; Lee

et al., 2024). These models also take the candi-

## Page 6

0%-25%

25%-50%

50%-75%

75%-100%

Difficulty Percentile Range

0

20

40

60

80

100

Accuracy

100

93

62

7

100

94

62

14

100

98

89

22

(a) Generated by LLaMA-3.1-70B-Instr

Process RM

Outcome RM

PairJudge RM

0%-25%

25%-50%

50%-75%

75%-100%

Difficulty Percentile Range

100

78

43

3

98

80

30

6

99

91

62

10

(b) Generated by LLaMA-3.1-8B-Instr

Process RM

Outcome RM

PairJudge RM

Figure 2: Comparison of Process RM, Outcome RM, and PairJudge RM across difficulty percentiles in MATH-500.

Candidate solutions are generated by (a) LLaMA-3.1-70B-Instr and (b) LLaMA-3.1-8B-Instr. Process RM and

Outcome RM refer to EurusRM-7B and RLHFlow-8B-DS-Data, respectively. As shown, PairJudge RM consistently

outperforms both, except on the easiest problems. Notably, for the hardest 50% problems, PairJudge RM achieves a

40%–60% relative improvement.

date solution pairs as input, but they are trained

as same as the outcome reward model to assign

scores to the candidate solutions, instead of judg-

ing the correctness of the candidate solutions via

chain-of-thought reasoning. For the Process Re-

ward Model, we leverage three off-the-shelf open-

source models: Math-Shepherd (Wang et al., 2023),

RLHFlow-8B-Mistral-Data, and RLHFlow-8B-

Deepseek-Data (Dong et al., 2024). For fair com-

parison, we also reimplement the Math-Shepherd

model with MCTS data generated by Llama-3.1-

8B-Instruct, denoted as RLHFlow-8B-LLaMA-

Data. We select the candidate solution with the

highest reward-model score as the final output of

BoN Sampling. Moreover, we include a majority-

voting baseline, which selects the candidate solu-

tion that receives the most votes from the N candi-

date solutions as the final output.

5.2

Results

The experimental results are summarized in Table 2.

Our proposed method, PairJudge RM, consistently

outperforms baseline models, including majority

voting, across all datasets and generation models.

Notably, PairJudge RM achieves an average im-

provement of 6.7% on MATH-500 and 3.9% on

Olympiad Bench compared to the strongest base-

line model (excluding majority voting). Interest-

ingly, majority voting outperforms the baseline re-

ward model on MATH-500, suggesting that exist-

ing reward models may lack robustness in scoring

candidate solutions. These findings align with pre-

vious research (Liu et al., 2024; Kim et al., 2024),

which highlights the limitations of baseline reward

models in reliably assessing solution correctness.

5.3

Difficulty Analysis

To further investigate scenarios in which the Pair-

Judge RM outperforms the baseline reward model,

we analyze the performance of the PairJudge RM

and the baseline reward model on math problems

with different levels of difficulty. We define the

difficulty of a math problem as the fraction of in-

correct answers among the candidate solutions:

Difficulty = #incorrect answers

#candidate solutions.

(5)

Specifically, we calculate this difficulty when the

number of candidate solutions is n = 64. We

then divide the math problems into four percentile

groups based on their difficulty level and evaluate

the performance of the PairJudge RM and base-

line models on each percentile in the MATH-500

dataset. Figure 2 shows the results. Except for the

easiest problems, the PairJudge RM consistently

outperforms the baseline models across all diffi-

culty levels. On the challenging problems (Diffi-

culty > 0.5), the PairJudge RM achieves a relative

## Page 7

BoN@16

BoN@32

BoN@64

70

75

80

Accuracy

77.6

78.2

78.6

78.6

77.8

76.4

78.3

78.9

79.2

Majority Vote

Prob Score

PairJudge RM

Figure 3: Comparison of the Critic Model with Majority

vote/ Prob Score bewteen PairJudge RM on BoN sam-

pling. Accuracy is reported in percentage.

improvement of 40% to 60% over the baseline mod-

els. These findings indicate that the PairJudge RM

has strong potential to enhance BoN Sampling on

challenging math problems.

6

Comparison with Critic Model

Critic Model (Gao et al., 2024a; McAleese et al.,

2024), also known as LLM-as-a-Judge (Zheng

et al., 2023; Bai et al., 2023), uses one LLM to

critique the response of another LLM to a given

prompt. In contrast to ordinary RMs, the output

of the Critic Model is a critique in the form of

chain-of-thought reasoning rather than a numer-

ical score. This setting is similar to that of the

PairJudge RM, as both methods generate a chain-

of-thought reasoning text to evaluate the response.

The key difference is that PairJudge RM performs

pairwise judgment, while Critic Model uses point-

wise judgment. Recently, Critic Model has been

applied in the math and code reasoning domains

to verify candidate solutions and assign numerical

scores (McAleese et al., 2024; Gao et al., 2024a).

In this section, we compare the effectiveness of

PairJudge RM and Critic Model in correctness ver-

ification and Best-of-N Sampling at the test time.

6.1

Comparison on Correctness Verification

We compare the performance of the PairJudge RM

and the Critic Model on correctness verification.

Specifically, given one question and two candidate

solutions, both models are tasked with judging the

correctness of these solutions. For a fair compar-

ison, we train the PairJudge RM and the Critic

Model using the same computational budget and

training data. In particular, we use the same ques-

tions from the MATH-500 training set (Hendrycks

et al., 2021) and candidate solutions generated by

Llama-3.1-8B-Instruct (AI@Meta, 2024) for both

Model

## Math

Olympiad

Avg.

Critic Model

67.7

56.9

62.3

PairJudge RM

70.4

64.2

67.3

Table 3: Comparison of the PairJudge RM and LLM-as-

a-Judge on the MATH-500 and Olympiad datasets on

correctness verification task. Candidates are generated

by Qwen-2.5-7B-Instruct. Accuracy is reported.

models. Since each training example for the Pair-

Judge RM contains two candidate solutions, the

training data for the Critic Model is twice as large.

All other training details follow Section 5.1.

After training, we evaluate both models on the

MATH-500 and Olympiad datasets. We sample

8,000 candidate solutions from each dataset to form

the test set for the Critic Model. To avoid bias,

these candidate solutions are generated by Qwen-

2.5-7B-Instruct (Qwen Team, 2024) using the test

split. We then pair each solution with another solu-

tion that produces a different answer for the same

question, yielding 4,000 pairs for evaluating the

PairJudge RM.

As shown in Table 3, the PairJudge RM outper-

forms the Critic Model on both the MATH-500

and Olympiad datasets. This result suggests that

pairwise judgment is more effective than pointwise

judgment when judging correctness. Notably, on

the more challenging Olympiad dataset, the Pair-

Judge RM achieves a larger improvement, high-

lighting its potential for difficult math problems.

6.2

Comparison on Best-of-N Sampling

As described in Section 6.1, the primary difference

between the PairJudge RM and the Critic Model is

the judgment process. The PairJudge RM performs

pairwise judgment, while the Critic Model per-

forms pointwise judgment. This difference makes

it challenging to apply the Critic Model for BoN

Sampling at test time. If the Critic Model verifies

two candidate solutions with different answers as

correct, it remains unclear which one is superior.

In contrast, the PairJudge RM directly compares

correctness to select the better solution.

To enable BoN Sampling with the Critic Model,

two approaches are typically adopted.

Combine with Majority Voting One option is

to combine the Critic Model with majority vot-

ing. The Critic Model first judges the correctness

of each candidate and removes those marked as

incorrect. Majority voting is then applied to the

remaining solutions to determine the final output.

## Page 8

Problem: A regular hexagon can be divided into six equilateral triangles. If the perimeter of one of the triangles is

21 inches, what is the perimeter, in inches, of the regular hexagon?

Solution 1: Since the perimeter of the triangle is 21, each side of the triangle has length 21

3 = 7. The hexagon is

made up of six of these triangles, so the perimeter of the hexagon is 6 · 7 = 42. So the final answer is 42 .

Critic Model Probabilistic Score: 0.0006

PairJudge RM Correctness Judgment: True

Solution 2: Since the perimeter of one of the triangles is 21 inches, the side length of the triangle is 21

3 = 7 inches.

Since there are six equilateral triangles, the perimeter of the hexagon is six times the side length of one triangle,

which is 6 · 7 = 42 inches. So the final answer is 42 .

Critic Model Probabilistic Score: 0.9999

PairJudge RM Correctness Judgment: True

Figure 4: Comparison of the Critic Model with Probabilistic Score and PairJudge RM in evaluating two correct

solutions to a math problem. The Critic Model assigns drastically different probabilistic scores (0.0006 vs. 0.9999),

highlighting its inconsistency, while PairJudge RM consistently identifies both as correct.

Use Probabilistic Score Another approach is to

use the probabilistic score assigned by the Critic

Model to each candidate solution. In this method,

the Critic Model is prompted to generate a to-

ken—either “correct” or “incorrect”—within its

reasoning text to indicate correctness. Zhang et al.

(2024b) suggest that the probability of generating

the token “correct” can serve as the score for each

candidate solution. The candidate with the highest

score is then selected as the final output.

To prevent data leakage, we use the MATH-

500 test split and candidate solutions generated

by Qwen-2.5-7B-Instruct (Qwen Team, 2024) to

evaluate both models on BoN Sampling. For a fair

comparison, we reuse the Critic Model and Pair-

Judge RM trained in Section 6.1 for this evaluation.

As shown in Figure 3, the PairJudge RM consis-

tently outperforms the Critic Model on the MATH-

500 dataset. This result demonstrates that, under

the same training budget, the PairJudge RM is more

effective at BoN Sampling than the Critic Model.

Additionally, the Critic Model using a probabilistic

score underperforms compared to majority voting,

likely due to its tendency to assign highly polar-

ized scores. As illustrated in Figure 4, even similar

candidate solutions receive substantially different

probabilistic scores. This observation suggests that

the probabilistic scoring method faces robustness

and stability issues similar to those observed in

reward models (Liu et al., 2024; Kim et al., 2024).

7

Related Work

7.1

Test Time Scaling and Best-of-N Sampling

Test-time scaling, introduced with o1(OpenAI,

2024), improves model performance during in-

ference by dedicating more computation (Snell

et al., 2024; Wu et al., 2024) Approaches include

Monte Carlo Tree Search (Zhang et al., 2024a; Gao

et al., 2024b) and long-chain-of-thought (Min et al.,

2024; Qwen Team, Alibaba, 2023) Best-of-N Sam-

pling is one such approach, generating N candi-

date solutions and selecting the best via a reward

model (Wang et al., 2023; Lightman et al., 2023;

Wang et al., 2024b; Zhang et al., 2024b) Perform-

ing BoN sampling with tournament-style selec-

tion was first introduced for instruction-following

tasks (Lee et al., 2024) Our knockout method dif-

fers by grouping candidates with identical answers

to avoid unnecessary comparisons and using gener-

ative judgment via CoT reasoning (Wei et al., 2022)

rather than discriminative scoring.

7.2

Reward Models and Critic Models

Reward models (RMs) assign numerical scores to

LLM outputs for feedback during training and infer-

ence (Lambert et al., 2024; Liu et al., 2024; Wang

et al., 2024a; Lightman et al., 2023). They can

operate in pointwise or pairwise manners (Yuan

et al., 2024a; Jiang et al., 2023). Critic models eval-

uate response quality, especially in reasoning tasks

like math and code (Gao et al., 2024a; McAleese

et al., 2024; Zheng et al., 2023; Li et al., 2024). Un-

like RMs, Critic Models function offering textual

feedback instead of numerical score. PairJudge

RM differs from them by simultaneously judging

the correctness of two responses using chain-of-

thought reasoning instead of pointwise evaluation

like Critic Models and numerical scoring like RMs.

8

Conclusion

We propose the Pairwise Judge Reward Model

(PairJudge RM) with a knockout tournament for

BoN Sampling. PairJudge RM uses CoT reason-

ing to simultaneously evaluate two candidate so-

lutions, eliminating arbitrary scoring and enabling

cross-validation. The knockout tournament itera-

tively eliminates incorrect solutions through pair-

## Page 9

wise judgments. We create a 432K pairwise judg-

ment dataset to train PairJudge RM. Experiments

show PairJudge RM significantly outperforms base-

line RMs in BoN Sampling on benchmarks.

Limitation

The main limitation of the proposed method lies on

the inference time. To conduct the BoN Sampling

with the PairJudge RM, serval rounds of pairwise

verification are required to select the best candidate

solution. This process is time-consuming and may

not be suitable for latency-sensitive applications.

However, the proposed method can be potentially

accelerated by parallel computing or other opti-

mization techniques to reduce the inference time.

For example, the multiple pairwise verification can

easily be parallelized to multiple GPUs to speed up

the inference process since they are independent of

each other. Moreover, with popularization of the

inference-time scaling, it is a common practice to

increase the computational resources to improve

the performance of the model in solving complex

reasoning tasks like math problems (Snell et al.,

2024; Wu et al., 2024).

Ethical Considerations

In this work, all the data and models are acquired

from public datasets and pre-trained models, and

no human subjects are involved in the experiments.

Considering the potential hallucination and bias

in the pre-trained models, it is worth nothing that

the user should be cautious when applying the pro-

posed method to real-world applications such as

use PairJudge RM to check human student’s home-

work in the educational system.

References

K Abdul Gafoor and P Jisha. 2014. A study of relia-

bility of marking and absolute grading in secondary

schools. Online Submission, 2(2):292–298.

Meta AI. 2025. Introducing llama 3.1: Our most capa-

ble models to date. Accessed: 2025-02-01.

AI@Meta. 2024. Llama 3 model card.

Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze

He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia

Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei

Hou. 2023. Benchmarking foundation models with

language-model-as-an-examiner. In Thirty-seventh

Conference on Neural Information Processing Sys-

tems Datasets and Benchmarks Track.

Alina Beygelzimer, Yann Dauphin, Percy Liang,

and Jennifer Wortman Vaughan. 2021.

The

neurips 2021 consistency experiment.

https:

//blog.neurips.cc/2021/12/08/

the-neurips-2021-consistency-experiment/.

Accessed: 2024-12-26.

Karel Devriesere, László Csató, and Dries Goossens.

2024. Tournament design: A review from an op-

erational research perspective. European Journal

of Operational Research, In Press, Corrected Proof.

Available online 9 November 2024.

Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang,

Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo,

Caiming Xiong, and Tong Zhang. 2024. RLHF work-

flow: From reward modeling to online RLHF. Trans-

actions on Machine Learning Research.

Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang,

Ce Zheng, Runji Lin, Keming Lu, Junyang Lin,

Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu,

and Baobao Chang. 2024a. Llm critics help catch

bugs in mathematics: Towards a better mathemati-

cal verifier with natural language feedback. CoRR,

abs/2406.14024.

Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu,

Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie

Wen. 2024b. Interpretable contrastive monte carlo

tree search reasoning. Preprint, arXiv:2410.01707.

Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu,

Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie

Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan

Liu, and Maosong Sun. 2024.

OlympiadBench:

A challenging benchmark for promoting AGI with

olympiad-level bilingual multimodal scientific prob-

lems. In Proceedings of the 62nd Annual Meeting of

the Association for Computational Linguistics (Vol-

ume 1: Long Papers), pages 3828–3850, Bangkok,

Thailand. Association for Computational Linguistics.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul

Arora, Steven Basart, Eric Tang, Dawn Song, and

Jacob Steinhardt. 2021. Measuring Mathematical

Problem Solving With the MATH Dataset. NeurIPS.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,

Elena Buchatskaya, Trevor Cai, Eliza Rutherford,

Diego de las Casas, Lisa Anne Hendricks, Johannes

Welbl, Aidan Clark, Tom Hennigan, Eric Noland,

Katherine Millican, George van den Driessche, Bog-

dan Damoc, Aurelia Guy, Simon Osindero, Karen

Simonyan, Erich Elsen, Oriol Vinyals, Jack William

Rae, and Laurent Sifre. 2022. An empirical analysis

of compute-optimal large language model training.

In Advances in Neural Information Processing Sys-

tems.

Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.

Llm-blender: Ensembling large language models

with pairwise comparison and generative fusion. In

Proceedings of the 61th Annual Meeting of the Asso-

ciation for Computational Linguistics (ACL 2023).

## Page 10

Anders Jonsson and Gunilla Svingby. 2007. The use

of scoring rubrics: Reliability, validity and educa-

tional consequences. Educational research review,

2(2):130–144.

Sunghwan Kim, Dongjin Kang, Taeyoon Kwon,

Hyungjoo Chae, Jungsoo Won, Dongha Lee, and

Jinyoung Yeo. 2024. Evaluating robustness of reward

models for mathematical reasoning. arXiv preprint

arXiv:2410.01729.

Nathan Lambert, Valentina Pyatkin, Jacob Morrison,

LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,

Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,

et al. 2024.

Rewardbench:

Evaluating reward

models for language modeling.

arXiv preprint

arXiv:2403.13787.

Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Min-

joon Seo, Kang Min Yoo, and Youngjae Yu. 2024.

Aligning large language models by on-policy self-

judgment. In Proceedings of the 62nd Annual Meet-

ing of the Association for Computational Linguis-

tics (Volume 1: Long Papers), pages 11442–11459,

Bangkok, Thailand. Association for Computational

Linguistics.

Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad

Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhat-

tacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu,

Kai Shu, Lu Cheng, and Huan Liu. 2024. From gen-

eration to judgment: Opportunities and challenges of

llm-as-a-judge. arXiv preprint arXiv: 2411.16594.

Jia LI, Edward Beeching,

Lewis Tunstall,

Ben

Lipkin, Roman Soletskyi, Shengyi Costa Huang,

Kashif Rasul, Longhui Yu, Albert Jiang, Ziju

Shen, Zihan Qin, Bin Dong, Li Zhou, Yann

Fleureau, Guillaume Lample, and Stanislas Polu.

2024. Numinamath. [https://huggingface.

co/AI-MO/NuminaMath-CoT](https:

//github.com/project-numina/

aimo-progress-prize/blob/main/

report/numina_dataset.pdf).

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri

Edwards, Bowen Baker, Teddy Lee, Jan Leike,

John Schulman, Ilya Sutskever, and Karl Cobbe.

2023. Let’s Verify Step by Step. arXiv preprint

arXiv:2305.20050.

Chris Yuhao Liu and Liang Zeng. 2024.

Skywork

reward model series. https://huggingface.

co/Skywork.

Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou,

and Juanzi Li. 2024. Rm-bench: Benchmarking re-

ward models of language models with subtlety and

style. arXiv preprint arXiv:2410.16184.

Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron

Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan

Leike. 2024. Llm critics help catch llm bugs. arXiv

preprint arXiv:2407.00215.

Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen,

Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xi-

aoxue Cheng, Huatong Song, et al. 2024. Imitate,

explore, and self-improve: A reproduction report

on slow-thinking reasoning systems. arXiv preprint

arXiv:2412.09413.

OpenAI.

2024.

Introducing

openai

o1

pre-

view.

https://openai.com/index/

introducing-openai-o1-preview/.

Accessed: 2024-09-17.

Adam Paszke, Sam Gross, Francisco Massa, Adam

Lerer, James Bradbury, Gregory Chanan, Trevor

Killeen, Zeming Lin, Natalia Gimelshein, Luca

Antiga, Alban Desmaison, Andreas Kopf, Edward

Yang, Zachary DeVito, Martin Raison, Alykhan Te-

jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,

Junjie Bai, and Soumith Chintala. 2019. Pytorch:

An imperative style, high-performance deep learning

library. In Advances in Neural Information Process-

ing Systems 32, pages 8024–8035. Curran Associates,

Inc. Accessed: 2025-02-12.

Alibaba Qwen Team. 2024. Qwen2. 5 technical report.

arXiv preprint arXiv:2412.15115.

Qwen Team,

Alibaba. 2023.

QwQ-32B Pre-

view.

https://qwenlm.github.io/zh/

blog/qwq-32b-preview/.

Accessed: 2024-

12-28.

Jeffrey Rasley, Zhizhou He, Jianwen Song, and Mikhail

Smelyanskiy. 2020.

Deepspeed:

Extreme-scale

model training for everyone.

https://www.

deepspeed.ai. Accessed: 2025-02-12.

Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku-

mar. 2024. Scaling llm test-time compute optimally

can be more effective than scaling model parameters.

arXiv preprint arXiv:2408.03314.

Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan

Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu

Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao

Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao

Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen,

Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang,

Zuxuan Wu, and Yu-Gang Jiang. 2024a. Secrets of

rlhf in large language models part ii: Reward model-

ing. Preprint, arXiv:2401.06080.

Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Ji-

achen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei

Chen, Lionel M Ni, et al. 2024b. Openr: An open

source framework for advanced reasoning with large

language models. arXiv preprint arXiv:2410.09671.

Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai,

Yifei Li, Deli Chen, Y Wu, and Zhifang Sui. 2023.

Math-shepherd: A label-free step-by-step verifier

for llms in mathematical reasoning. arXiv preprint

arXiv:2312.08935.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten

Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,

## Page 11

and Denny Zhou. 2022. Chain of thought prompt-

ing elicits reasoning in large language models. In

Advances in Neural Information Processing Systems.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien

Chaumond, Clément Delangue, and Alexander

Maisonneuve.

2020.

Hugging

face’s

trans-

formers:

State-of-the-art

natural

language

processing.

https://huggingface.co/

transformers. Accessed: 2025-02-12.

Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck,

and Yiming Yang. 2024. An empirical analysis of

compute-optimal inference for problem-solving with

language models. arXiv preprint arXiv:2408.00724.

Chenlu Ye, Wei Xiong, Yuheng Zhang, Hanze Dong,

Nan Jiang, and Tong Zhang. 2024. Online iterative

reinforcement learning from human feedback with

general preference model. In The Thirty-eighth An-

nual Conference on Neural Information Processing

Systems.

Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding,

Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen,

Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen

Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun.

2024a. Advancing llm reasoning generalists with

preference trees. Preprint, arXiv:2404.02078.

Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning

Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu,

and Hao Peng. 2024b. Free process rewards without

process labels. arXiv preprint arXiv:2412.01981.

Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang

Li, and Wanli Ouyang. 2024a.

Accessing gpt-4

level mathematical olympiad solutions via monte

carlo tree self-refine with llama-3 8b. arXiv preprint

arXiv:2406.07394.

Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran

Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024b.

Generative verifiers: Reward modeling as next-token

prediction. arXiv preprint arXiv:2408.15240.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,

Mohammad Saleh, and Peter J Liu. 2023. Slic-hf: Se-

quence likelihood calibration with human feedback.

arXiv preprint arXiv:2305.10425.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan

Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,

Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.

Judging LLM-as-a-judge with MT-Bench and Chat-

bot Arena. arXiv preprint arXiv:2306.05685.

## A

Dataset Information

In this section, we provide detailed information

about the datasets used in this work. The datasets

employed for evaluating mathematical reasoning

tasks come from various sources, each contributing

unique characteristics for comprehensive bench-

marking.

MATH-500 is a collection of problems de-

signed to test mathematical reasoning capabili-

ties.

It covers a wide range of mathematical

concepts

and

problem

types,

from

basic

algebra to more advanced topics.

Specifi-

cally,

MATH-500 is newer,

IID version of

MATH (Hendrycks et al., 2021), which is a widely

used benchmark for avoide the data leakage

issue.

Size: 500 problems.

Source: https:

//github.com/openai/prm800k/tree/

main/prm800k/math_splits.

License:

MIT License.

Olympiad Bench is derived from a collection of

problems from various international mathematical

olympiads. It includes a broad range of challeng-

ing problems, covering topics like number theory,

combinatorics, geometry, and algebra. This dataset

is particularly useful for testing a model’s ability

to handle competition-level mathematical reason-

ing tasks. Size: 8,476 problems Source: https:

//arxiv.org/abs/2402.14008. License:

MIT License

NuminaMath-CoT is a dataset that includes 860k

math problems, where each solution is format-

ted in a Chain of Thought (CoT) manner. The

sources of the dataset range from Chinese high

school math exercises to US and international math-

ematics olympiad competition problems. The data

were primarily collected from online exam paper

PDFs and mathematics discussion forums. The

processing steps include (a) OCR from the origi-

nal PDFs, (b) segmentation into problem-solution

pairs, (c) Translation into English, (d) realign-

ment to produce a CoT reasoning format, and

(e) final answer formatting.

Size: 860k prob-

lems. Source: https://huggingface.co/

datasets/AI-MO/NuminaMath-CoT.

Li-

cense: Apache License 2.0.

## B

Computational Resources

This section provides detailed information about

the computational resources used in our exper-

iments.

The training of Qwen-2.5-7B-Instr on

## Page 12

the PAIRJUDGE-432K was conducted on an

8-GPU H100 server, with an estimated train-

ing duration of approximately 24 hours.

The

construction of the PAIRJUDGE-432K relies on

the gemini-1.5-flash API server through

Google Cloud. The associated API costs for this

project amounted to approximately 2,000 USD.

## C

Future Work

Application in Reinforcement Learning

In this work, we mainly focus on how to perform

the BoN Sampling at test time with the PairJudge

RM. This experiment setting follows the existing

work (Wang et al., 2023; Lightman et al., 2023;

Wang et al., 2024b; Zhang et al., 2024b) and helps

us to compare the performance of the PairJudge

RM with baseline models and verify the effective-

ness of the proposed method. However, the Pair-

Judge RM can also be applied at the Reinforcement

Learning (RL) training stage to improve the perfor-

mance of the model in solving complex reasoning

tasks like math problems. To apply in the training

stage, the PairJudge RM need to assign a numer-

ical score to the candidate solutions just like the

discriminative reward model (Lambert et al., 2024;

Liu et al., 2024). Such a numerical score could

acquired by the winning rate of the candidate so-

lutions in the knockout tournament, which can be

used as the reward signal to guide the training of

the model. In the future, we plan to explore the

application of the PairJudge RM in the RL training

stage to improve the performance of the model in

solving complex reasoning tasks like math prob-

lems.

Alternative Tournament Strategies

In this work, we introduce the knockout tourna-

ment to select the best candidate solution, where

the candidate solutions are viewed as players in

the tournament and each pairwise comparison is

viewed as a match between two players. The main

reason for choosing the knockout tournament is

that it is one of the most naive tournament design

that could select the best candidate under time com-

plexity O(N), where N is the number of candidate

solutions. It worth noting that there are tons of

alternative tournament strategies that could be used

to select the best candidate solution, such as the

round-robin tournament, the Swiss-system tourna-

ment, and the double-elimination tournament (De-

vriesere et al., 2024) Such alternative tournament

strategies could be potentially used to improve the

performance of the PairJudge RM in selecting the

best candidate solution, and we plan to explore the

application of the alternative tournament strategies

in the future work.

## D

Potential Improvement

Due to the computational limitation and resource

constraints, there are several potential improve-

ments that could be made to further improve the

performance of the PairJudge RM.

• Bigger Model Capacity: Due to the com-

putational limitation, although we presents a

promising and scalable dataset contruction in

Section 4, the PairJudge RM is trained with

the Qwen-2.5-7B-Instr, which is a relatively

small model compared to the state-of-the-art

models like the Qwen-2.5-70B-Instr, LLaMA-

3.1-70B-Instr and QwQ-32B. According to

the Chinchilla Law (Hoffmann et al., 2022),

under the same training data and training time,

a model with larger capacity can achieve bet-

ter performance than a model with smaller

capacity.

• More Data Scaling Dimension: The Pair-

Judge RM is trained with the PAIRJUDGE-

432K dataset, which contains 343K train-

ing data for the PairJudge RM. Now there

are two directions to further improve the

performance of the PairJudge RM: 1) use

more models rather than only LLama-3.1-8B-

Instruct to generate the candidate solutions,

and 2) Use more models rather than only

gemini-1.5-flash to annotate the train-

ing data. This two directions could poten-

tially magnitudes the size of the training data

and improve the performance of the PairJudge

## Rm.

• Long-Cot Base Model: The PairJudge RM is

trained with the Qwen-2.5-7B-Instruct. Con-

sidering the recent success of the Long-Cot

models such as the QwQ-32B (Qwen Team,

Alibaba, 2023) in reasoning task, it is worth

exploring the application of the PairJudge RM

with the Long-Cot models to further improve

the performance of the PairJudge RM.

## E

Artifacts in Our Research

Our work is built upon several key artifacts that

have played a crucial role in enabling the devel-

opment and evaluation of the proposed Pairwise

Judge Reward Model (PairJudge RM).

PyTorch (Paszke et al., 2019) is a widely used

## Page 13

deep learning framework that provides flexible and

efficient tools for building neural networks, making

it an essential artifact for modern NLP research. Its

dynamic computational graph and GPU accelera-

tion have been pivotal in enabling rapid prototyping

and experimentation, especially in the context of

transformer-based models and reinforcement learn-

ing.

The Hugging Face Transformers library (Wolf

et al., 2020) is another critical tool that has revolu-

tionized NLP. It provides an extensive collection

of pre-trained models and tools for working with

transformer architectures, making it easier for re-

searchers and practitioners to fine-tune models on

domain-specific tasks. The library’s user-friendly

API, combined with its comprehensive model hub,

has democratized access to state-of-the-art models

such as BERT, GPT, T5, and more.

DeepSpeed (Rasley et al., 2020) is a library de-

veloped by Microsoft that aims to optimize large-

scale model training. It introduces techniques such

as mixed-precision training and model parallelism

to significantly reduce memory usage and speed

up training, making it an essential tool for train-

ing large transformer models. DeepSpeed’s sup-

port for efficient distributed training has enabled

researchers to scale up their models while reducing

computational costs.

The Llama model (AI@Meta, 2024) is a family

of large language models developed by Meta AI.

Trained on approximately 15 trillion tokens, Llama

models are available in sizes ranging from 8 billion

to 405 billion parameters. They have demonstrated

superior performance across various NLP bench-

marks, making them a valuable resource for tasks

such as text generation, translation, and summa-

rization.

The Qwen model (Qwen Team, 2024) is a se-

ries of large language models developed by Alibaba

Cloud. The Qwen series includes models with vary-

ing parameter counts ranging from 0.5B to 72B.

These models have shown competitive performance

across diverse benchmarks, including language un-

derstanding, generation, multilingual proficiency,

coding, mathematics, and reasoning. The Qwen

series has been instrumental in advancing the capa-

bilities of LLMs in various applications.

All these artifacts have played a crucial role in

enabling our research and have significantly con-

tributed to our research.

## F

Experiments Replication

To facilitate the replication of our experiments, we

plan to release the codebase, model checkpoints,

and dataset used in this work. The codebase will

be made available on GitHub, along with detailed

instructions on how to reproduce the experiments.

As for the model checkpoints and dataset, we plan

to provide in Huggingface. All the experiments

will be conducted on three times and the average

results will be reported to minimize the randomness

in the experiments.

## G

Prompt Templates

Here we provide the prompt templates for the Pair-

Judge RM in Table 4 , which is used to guide the

PairJudge RM to judge the correctness of two can-

didate solutions to a given math problem.

## H

Math Problem Filtering Criteria

Here we provide the filtering criteria applied to the

dataset in Table 5, which is used to remove low-

quality, proof-based, or multiple-choice problems.

## Page 14

Table 4: Prompt Template for PairJudge RM, the {question}, {response_a}, and {response_b} are placeholders for the math

question, response A, and response B, respectively.

Task Objective:

Evaluate the correctness of two responses (Response A and Response B) to a given math question. Perform a step-by-step verification of each

response’s accuracy. After completing the step-by-step checks, provide a final correctness judgment for each response.

Steps to Follow:

0. Extract Answers from both Responses:

- Read and both responses to identify the final answers provided.

- If the responses provide different answers, make sure there are is no possible way that both responses can be correct. It must be the case that

one response is correct and the other is incorrect or both are incorrect.

1. Step-by-Step Verification of Correctness:

- For each response (Response A and Response B):

Carefully examine each step of the solution provided. Check the following:

- Mathematical accuracy: Ensure all calculations, algebraic simplifications, and mathematical operations are correct.

- Logical consistency: Verify that each step follows logically from the previous one and that the reasoning is sound.

- Completeness: Make sure that all necessary steps are included to fully solve the problem and reach the final answer.

While performing this step-by-step evaluation, refer to the Additional Tips section for helpful techniques to validate each response’s accuracy.

Attention: When checking the correctness of a single step, you should never first conclude the correctness of this step (for example, *"This step

is incorrect because..."* is strictly forbidden). You should neutrally check this step, provide evidence about its correctness, and then finally

draw a conclusion about the correctness of this step. In other words, you should first employ the techniques in Additional Tips to check the

correctness of this step, and then draw a conclusion about the correctness of this step.

2. Final Conclusion:

- After completing the step-by-step verification for each response, sum up the information you have now, then finally determine whether each

response’s answer is correct or incorrect.

- Provide the final judgment for each response, the output should in-closed with the following tags:

- If Response A’s answer is correct:

<resp_a_judge>Correct</resp_a_judge>

- If Response A’s answer is incorrect:

<resp_a_judge>Incorrect</resp_a_judge>

- If Response B’s answer is correct:

<resp_b_judge>Correct</resp_b_judge>

- If Response B’s answer is incorrect:

<resp_b_judge>Incorrect</resp_b_judge>

- Note: The responses A and response B can be either correct or incorrect, or both correct, or both incorrect. You should provide the final

judgment for each response. There is no guarantee that at least one response is correct or incorrect.

Additional Tips:

- Key Validation Techniques (to apply during Step 1):

- Re-derive Key Parts of the Solution: Independently calculate or derive crucial steps of the solution to verify their correctness.

- Verify Calculations: Double-check all mathematical operations (e.g., addition, multiplication, division) to confirm accuracy.

- Compare Responses: If needed, compare similar steps between Response A’s and Response B’s answers to identify discrepancies or

inconsistencies.

- The final output format should be as follows:

Final Judgment:

Response A: <resp_a_judge>Correct/Incorrect</resp_a_judge>

Response B: <resp_b_judge>Correct/Incorrect</resp_b_judge>

Question: <question> {question} </question>

Response A: <response_a> {response_a} </response_a>

Response B: <response_b> {response_b} </response_b>

Table 5: Filtering criteria applied to the dataset to remove low-quality, proof-based, or multiple-choice problems.

Filter Type

Criteria

Bad Quality Problems

Problems with messy formatting, OCR errors, or empty ground truth (gt).

Equations in Ground Truth

gt contains “=” (indicating it might be an equation rather than a clear ground true).

Multiple Questions

Problems with patterns indicating multiple sub-questions (MULTI_QUESTION).

Yes/No Questions

Solutions with patterns indicating yes/no, true/false (YESNO_QUESTIONS).

Text Answers

Ground truth containing patterns indicating textual answers (TEXT_ANSWER).

Proof Problems

Problems with patterns indicating proof problems (PROVE_PATTERN).

Multiple Choice Questions

Problems with patterns indicating multiple-choice questions (MCQ_OPTIONS).
