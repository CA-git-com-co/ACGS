# 2402.05749_Generalized-Preference-Optimization-A-Unified-Appr

**Original PDF**: 2402.05749_Generalized-Preference-Optimization-A-Unified-Appr.pdf
**Conversion Method**: PyMuPDF
**Constitutional Hash**: cdd01ef066bc6cf2

---

## Page 1

Generalized Preference Optimization: A

Unified Approach to Offline Alignment

Yunhao Tang𝛽, Zhaohan Daniel Guo𝛽, Zeyu Zheng𝛽, Daniele Calandriello𝛽, Rémi Munos𝛽, Mark Rowland𝛽,

Pierre Harvey Richemond𝛽, Michal Valko𝛽, Bernardo Ávila Pires𝛽and Bilal Piot𝛽

𝛽Google DeepMind

Offline preference optimization allows fine-tuning large models directly from offline data, and has

proved effective in recent alignment practices. We propose generalized preference optimization (GPO),

a family of offline losses parameterized by a general class of convex functions. GPO enables a unified

view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as

special cases, while naturally introducing new variants. The GPO framework also sheds light on how

offline algorithms enforce regularization, through the design of the convex function that defines the

loss. Our analysis and experiments reveal the connections and subtle differences between the offline

regularization and the KL divergence regularization intended by the canonical RLHF formulation. In a

controlled setting akin to Gao et al. (2023), we also show that different GPO variants achieve similar

trade-offs between regularization and performance, though the optimal values of hyper-parameter might

differ as predicted by theory. In all, our results present new algorithmic toolkits and empirical insights

to alignment practitioners.

1. Introduction

Reinforcement learning from human feedback (RLHF) has been a canonical paradigm for aligning

powerful AI systems along human values (Christiano et al., 2017; Ouyang et al., 2022), as demon-

strated by recent advances in large language models (LLMs) (Achiam et al., 2023; Team et al., 2023).

RLHF consists of two steps: reward modeling, which trains a reward model 𝑟𝜙to capture human

preferences from a dataset of pairwise comparison; and regularized policy optimization, which aligns

the AI systems against the learned reward model, more formally as below

max

𝜃

𝔼𝑦∼𝜋𝜃



𝑟𝜙(𝑦)



|           {z           }

reward maximization

−𝛽𝕂𝕃(𝜋𝜃, 𝜋ref)

|          {z          }

regularization

.

Lately, directly aligning AI systems from pairwise comparison datasets has become increasingly

common (e.g., Azar et al., 2024; Rafailov et al., 2023; Zhao et al., 2023), as evidenced by progress in

open source models (e.g., Jiang et al., 2024). Compared to canonical RL algorithms, such methods

are more computationally efficient as they do not require expensive sampling from the models. They

also avoid learning reward models altogether, and effectively replace RLHF with a supervised learning

problem, which is convenient from various practical perspectives. We refer to such methods as offline

preference optimization, as they seek to optimize human preferences using offline datasets. Here,

offline stresses the fact that such datasets are not generated by interactive data collections from the

learned model.

Our first contribution is to provide a unifying view over notable existing offline preference optimization

algorithms, such as DPO (Rafailov et al., 2023), IPO (Azar et al., 2024) and SLiC (Zhao et al., 2023).

To this end, we propose GPO (Generalized Preference Optimization), which parameterizes preference

optimization losses via a family of convex functions 𝑓, with DPO, IPO, and SLiC as special cases (see

Figure 1 for a preview of the instantiations). The central insight to our derivation is that one can

arXiv:2402.05749v2  [cs.LG]  28 May 2024

## Page 2

Generalized Preference Optimization: A Unified Approach to Offline Alignment

1.0

0.5

0.0

0.5

1.0

1.5

2.0

0.00

0.25

0.50

0.75

1.00

1.25

1.50

1.75

2.00

f( )

## Dpo

## Ipo

## Slic

0-1

Figure 1 | Illustration of offline preference optimization losses 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[ 𝑓(𝜌𝜃)] as a function of the difference

of log ratio 𝜌𝜃= log 𝜋𝜃(𝑦𝑤)/𝜋ref(𝑦𝑤) −log 𝜋𝜃(𝑦𝑙)/𝜋ref(𝑦𝑙). DPO applies the (scaled) logistic loss

1

log 2 log(1 +

exp(−𝜌𝜃)), SLiC applies the hinge loss max(0, 1 −𝜌𝜃), while IPO applies the squared loss (𝜌𝜃−1)2. As a

result, many popular offline losses can be understood as convex approximations to the 0-1 loss that measures

the binary classification accuracy. Any other convex loss alternatives to the above examples provide offline

preference optimization losses not in the existing literature, as we show in Table 1.

interpret the problem of reward modeling as a supervised binary classification problem (Hastie et al.,

2009). The rich literature on supervised binary classification paves the way to unifying existing offline

preference optimization algorithms, and naturally introduces new algorithms not yet in the current

literature. The GPO formulation also helps better understand the algorithmic trade-offs between

different variants, particularly, the strength of regularization, which we further dive into.

With a unifying view over offline preference optimization algorithms, our second contribution is to

dive into the regularization mechanism induced by offline losses. We see that the tail behavior of the

convex function 𝑓, governs the effective strength of regularization induced between 𝜋𝜃and 𝜋ref, which

offers insight on the choice of hyper-parameters such as 𝛽. We identify the offline regularization,

computed based on the offline dataset, and show how it generally differs from the KL divergence

intended in the initial formulation. Our analysis and empirical results hint at some challenges to

enforcing the KL divergence constraints with offline losses, revealing some of the subtleties of the

‘equivalence’ arguments adopted in prior work to derive offline losses (see also Theorem 1 for a more

general version of the equivalence argument).

The paper is organized as follows:

• In Section 2, we present GPO, generalized policy optimization, which parameterizes offline

preference optimization algorithms through a convex function. This recovers a few popular

algorithms as special cases and offers insights to offline alignment algorithms in general.

• In Section 3, we expand on the derivation of reward modeling as a binary classification problem.

Our insight allows for connecting a rich literature on supervised classification to the designs of

offline alignment, which paves the way to the GPO formulation.

• In Section 4, we dive into how offline preference optimization induces regularization between 𝜋𝜃

and 𝜋ref during optimization. We identify an offline regularization loss, the effective regularization

that offline algorithms enforce, and show how it differs from the KL divergence through analysis

2

## Page 3

Generalized Preference Optimization: A Unified Approach to Offline Alignment

and experimental study. We also show how the design of 𝑓introduces different strength of

regularization, and how hyper-parameters should be chosen adaptive to 𝑓.

• In Section 5, we start with a controlled setting akin to Gao et al. (2023) and show the regularization

vs. performance trade-off for different GPO variants. By varying 𝛽and learning stages during

training, the policy performance initially increases followed by decrease, as predicted by the

Goodhart’s law. We observe similar trade-offs across different GPO variants, though the best

hyper-parameter can differ significantly due to different inherent strengths of the regularization,

as suggested by theory. In a LLM summarization task, we also confirm similar performance across

different GPO variants (up to tuning in 𝛽).

2. A general family of offline preference optimization losses

In the case of language model alignment, we optimize a policy 𝜋𝜃that outputs response 𝑦∼𝜋𝜃(·|𝑥)

given prompt 𝑥. Given two responses 𝑦, 𝑦′ ∈Y, a human rater provides feedback by picking out the

preferred response. This allows relabeling the two responses as (𝑦𝑤, 𝑦𝑙) corresponding to the win-loss

responses. Such pairwise preference data is usually collected offline and can come from a variety of

sources in practice, which we denote as a behavior policy 𝜇. Henceforth, when the context is clear we

remove the dependency on the prompt 𝑥for simplicity.

Importantly, we do not make any assumption on the preference structure 𝑝(𝑦≻𝑦′), e.g., it may not

come from a Bradley-Terry (BT) model (Bradley and Terry, 1952), a common assumption made in

prior work (Rafailov et al., 2023). Below, we unify ways to derive various existing offline preference

optimization losses for learning from pairwise human feedback.

2.1. A recipe to derive preference optimization losses

Assuming access to a reward function 𝑟𝜙, the regularized policy optimization objective (Ouyang et al.,

2022) is

max

𝜋𝜃𝔼𝑦∼𝜋𝜃



𝑟𝜙(𝑦)



−𝛽𝕂𝕃(𝜋𝜃, 𝜋ref) .

(1)

To be clear about the KL definition, we have for any two distributions 𝜋, 𝜋′: 𝕂𝕃(𝜋, 𝜋′) B 𝔼𝑦∼𝜋

h

log 𝜋(𝑦)

𝜋′(𝑦)

i

.

The solution to the regularized objective above can be written analytically as 𝜋∗

𝜃(𝑦) ∝𝜋ref(𝑦) exp  𝛽−1𝑟𝜙(𝑦).

Given a pair of responses (𝑦𝑤, 𝑦𝑙), we can train the reward model 𝑟𝜙through supervised learning. A

convenient class of loss function is defined through the difference 𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙): we can think of

𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙) as predicting how likely 𝑦𝑤is preferred to 𝑦𝑙. From the discussion above, we see that

this difference is equivalent to the log ratio difference of the optimal policy to Eqn (1)

𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙) = 𝛽



log

𝜋∗

𝜃(𝑦𝑤)

𝜋ref(𝑦𝑤) −log

𝜋∗

𝜃(𝑦𝑙)

𝜋ref(𝑦𝑙)



.

(2)

Hence intuitively, any loss defined through the reward difference 𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙) can introduce a loss

over 𝜋𝜃.

A central insight of this work is framing reward learning as a supervised binary classification problem.

We leave a more detailed derivation to Section 3, which provides additional insights. Letting 𝑓: ℝ→ℝ

be a scalar function, in general the reward learning loss (to be minimized) can be written as

𝔼(𝑦𝑤,𝑦𝑙)∼𝜇



𝑓 𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙)

.

(3)

Before moving on, note that the difference 𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙) is reminiscent of the BT model assump-

tion. However, we argue that it is more sensible to relate this parametric form to the fact that the

3

## Page 4

Generalized Preference Optimization: A Unified Approach to Offline Alignment

Table 1 | Side-by-side correspondence between existing offline preference optimization losses and

convex supervised learning losses. Among a rich variety of convex supervised learning losses developed

in the literature, logistic log loss (Hastie et al., 2009), hinge loss (Cortes and Vapnik, 1995) and

squared loss (Rosasco et al., 2004) have offline preference optimization algorithmic counterparts.

Other notable losses, such as the exponential loss (Freund and Schapire, 1995), truncated quadratic

loss (Bartlett et al., 2006) and Savage loss (Masnadi-Shirazi and Vasconcelos, 2008) can form novel

offline preference optimization algorithms.

Supervised learning losses

𝑓(𝛽𝜌𝜃)

Offline preference optimization

Logistic log loss

log (1 + exp(−𝛽𝜌𝜃))

DPO (Rafailov et al., 2023)

Hinge loss

max (0, 1 −𝛽𝜌𝜃)

SLiC (Zhao et al., 2023)

squared loss

(𝛽𝜌𝜃−1)2

IPO (Azar et al., 2024)

Exponential loss

exp(−𝛽𝜌𝜃)

## N/A

Truncated quadratic loss

(max(0, 1 −𝛽𝜌𝜃))2

## N/A

Savage loss

1/(1 + exp(𝛽𝜌𝜃))2

## N/A

RLHF formulation (Eqn 1) is a maximization problem, and hence imply that each response can be

characterized as a single scalar 𝑟𝜙(𝑦). We provide a more detailed discussion in Section 3.

Many existing offline preference optimization losses can be cast in this general form by replacing the

reward difference by the log ratio difference,

𝔼(𝑦𝑤,𝑦𝑙)∼𝜇



𝑓



𝛽·



log 𝜋𝜃(𝑦𝑤)

𝜋ref(𝑦𝑤) −log 𝜋𝜃(𝑦𝑙)

𝜋ref(𝑦𝑙)



.

(4)

Henceforth, we denote the log ratio difference as 𝜌𝜃B log 𝜋𝜃(𝑦𝑤)

𝜋ref(𝑦𝑤) −log 𝜋𝜃(𝑦𝑙)

𝜋ref(𝑦𝑙) and the above loss can

be rewritten as 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[ 𝑓(𝛽𝜌𝜃)]. A general recipe to derive offline preference optimization losses

is to start with a supervised learning loss function 𝑓for reward learning, and replace the reward

difference by 𝜌𝜃(see, e.g., Hastie et al., 2009 for a nice overview of such loss functions). We can

identify the specific functions 𝑓for the most common choices; see illustrations of the losses in Figure 1

with 𝛽= 1.

• DPO: 𝑓(𝛽𝜌𝜃) = −log 𝜎(𝛽𝜌𝜃) with 𝜎being the sigmoid function, applies the logistic loss (Hastie

et al., 2009). The loss can also be written as log(1 + exp(−𝛽𝜌𝜃)).

• IPO: 𝑓(𝛽𝜌𝜃) = (𝛽𝜌𝜃−1)2, the squared function (Rosasco et al., 2004), can be understood as

applying linear regression to the probability that 𝑦𝑤is preferred (Hastie et al., 2009).

• SLiC: 𝑓(𝛽𝜌𝜃) = max(0, 1−𝛽𝜌𝜃) is the hinge loss function, stemming from the max-margin (support

vector machine) paradigm (Boser et al., 1992; Cortes and Vapnik, 1995). The original SliC

algorithm (Zhao et al., 2023) also includes a supervised learning component, which we do not

discuss here.

2.2. GPO: A generalized family of offline preference optimization algorithms

Building on the discussion above, in general, any properly defined supervised learning loss 𝑓for

reward modeling can translate into a preference optimization objective 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[ 𝑓(𝛽𝜌𝜃)]. We provide

a table of a few notable supervised learning losses developed in the decades-old literature, each loss

mapping into an offline preference optimization algorithm.

As discussed above, some of them have already translated into existing methods. We note a few

examples without offline preference optimization counterparts:

4

## Page 5

Generalized Preference Optimization: A Unified Approach to Offline Alignment

1.0

0.5 0.0

0.5

1.0

1.5

2.0

2.5

3.0

0.00

0.25

0.50

0.75

1.00

1.25

1.50

1.75

2.00

f( )

Logistic

Squared

Hinge

Exponential

Truncated squared

Savage

0-1

Figure 2 | Illustration of notable examples of binary classification loss functions, including both examples

(logistic, squared and hinge) that have led to existing offline preference optimization algorithms, as well as

others (exponential, truncated squared, Savage) that produce novel losses.

• Exponential loss: 𝑓(𝛽𝜌𝜃) = exp(−𝛽𝜌𝜃), the loss function for the AdaBoost algorithm (Freund and

Schapire, 1995).

• Truncated quadratic: 𝑓(𝛽𝜌𝜃) = (max (0, 1 −𝛽𝜌𝜃))2 (Bartlett et al., 2006), a truncated variant of

the squared loss, is also a smooth approximation to the hinge loss.

• Savage loss: 𝑓(𝛽𝜌𝜃) = 1/(1 + exp(𝛽𝜌𝜃))2 (Masnadi-Shirazi and Vasconcelos, 2008) which have

proved robust to outliers in data and found applications in boosting algorithms.

Bartlett et al. (2006); Rosasco et al. (2004) give a more exhaustive list of convex supervised learning

losses and their discussions.

A key motivating argument for the offline preference optimization algorithms (Azar et al., 2024;

Rafailov et al., 2023; Zhao et al., 2023) is that minimizing the offline losses for the policy 𝜋𝜃is

equivalent to obtaining the optimal regularized policy against a loss minimizing reward model. We

can extend the conclusion to this general family of offline preference optimization algorithms.

Theorem 1. (Equivalence of optimal solutions) Let 𝜋∗

𝜃be the global minimizer of the offline

preference optimization loss in Eqn (4). 𝜋∗

𝜃is the same as the optimal regularized policy (according

to Eqn (1)) for a reward function that globally minimizes the loss Eqn (3).

3. Reward modeling viewed as a binary classification problem

Here, we take a step back and dive into the derivation that converts reward modeling into a supervised

binary classification problem. We provide a brief background on the basic setup, and how it relates to

reward modeling (see, e.g., Hastie et al., 2009 for a more comprehensive introduction).

In binary classification, given a pair of feature and label (𝑧, 𝑙) with 𝑧∈ℝ𝑘and 𝑙∈{−1, 1}, the

aim is to predict ˆℓ(𝑧) ∈ℝas a function of the feature, and use sign



ˆℓ(𝑧)



as the classifier, in the

hope that it can match the ground truth label 𝑦. The classification accuracy can be written as

5

## Page 6

Generalized Preference Optimization: A Unified Approach to Offline Alignment

1

## 2𝔼

h

sign



ˆℓ(𝑧) · 𝑙

i

+ 1

2 ∈[0, 1] and an equivalent loss function is

## 𝔼

h

1 −sign



ˆℓ(𝑧) · ℓ

i

.

(5)

The above loss, known as the 0-1 loss (see the dotted dark curve in Figure 1) is non-convex. Instead

of directly optimizing it, we can take smooth convex functions 𝑓: ℝ→ℝand approximate the loss as

## 𝔼

h

𝑓



ˆℓ(𝑧) · ℓ

i

.

Taking this back to the case of reward modeling, given a pair of responses (𝑦1, 𝑦2), we construct a

sample for binary classification by setting the label ℓ= 1 if 𝑦1 ≻𝑦2 and ℓ= −1 otherwise.

Thinking of (𝑦1, 𝑦2) as the feature from which to make prediction, in general the prediction would

be a bi-variate function ˆℓ(𝑦1, 𝑦2) that can depend on both 𝑦1 and 𝑦2 in an arbitrary form. For a

pointwise reward model that depends on a single response 𝑟𝜙: Y →ℝ, an intuitive parameterization

would be to take the difference of two rewards ˆℓ(𝑦1, 𝑦2) = 𝑟𝜙(𝑦1) −𝑟𝜙(𝑦2). The corresponding binary

classification loss is

𝔼𝑦1∼𝜇,𝑦2∼𝜇



𝕀[𝑦1 ≻𝑦2] 𝑓 𝑟𝜙(𝑦1) −𝑟𝜙(𝑦2)

+ 𝔼𝑦1∼𝜇,𝑦2∼𝜇



𝕀[𝑦2 ≻𝑦1] 𝑓 𝑟𝜙(𝑦2) −𝑟𝜙(𝑦1)

.

Equivalently, we can write the loss as in Eqn (3)

𝔼(𝑦𝑤,𝑦𝑙)∼𝜇



𝑓 𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙)

.

The above result offers a number of interesting implications, which we expand on in the next section.

3.1. Characterizing what the reward model learns

Drawing inspiration from the supervised learning literature, we can reason about properties of the

reward models obtained by minimizing the convex loss function 𝑓. This can translate into effects on

the downstream optimized policies due to the equivalence in Eqn (2). Some discussions are in order

below.

The Bradley-Terry assumption and analytic forms of reward models.

As alluded to earlier, the

design of the reward modeling loss as a function of the reward difference 𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙) should be

interpreted as a result of the reward maximization formulation of RLHF. Implicitly, the maximization

formulation assumes that there is a total order on all the responses (i.e., they can be ranked in a

monotonic order), which intuitively is captured by the BT assumption to a large extent. Meanwhile

when there is no total order, the formulation Eqn (1) would not be perfect, and one might need to

resort to alternative solution concepts such as Nash equilibrium (Munos et al., 2024; Swamy et al.,

2024).

In general, one should train a pairwise preference model ˆℓ(𝑦1, 𝑦2) = 𝑟𝜙(𝑦1, 𝑦2) rather than pointwise

reward models, for which there could be characterizations on the properties of the learned model that

we discuss below. For pointwise models the analytic forms are only available in a few special cases

drawn from prior work. We discuss two notable examples: (1) the logistic loss, under the assumption

that the ground truth preference satisfies a BT model 𝑝(𝑦1 ≻𝑦2) = 𝜎(𝑟∗(𝑦1) −𝑟∗(𝑦2)), then the optimal

reward obtained by minimizing Eqn (3) is a constant shift from 𝑟∗(Rafailov et al., 2023); (2) For the

squared loss, where the optimal reward is a constant away from 𝑝(𝑦≻𝜇) = 𝔼𝑦′∼𝜇[𝑝(𝑦≻𝑦′)] without

further assumptions on the ground truth preference. For interested readers, note that the discussion

here also provides an alternative way to derive the IPO algorithm distinct from the original derivation

in Azar et al. (2024).

6

## Page 7

Generalized Preference Optimization: A Unified Approach to Offline Alignment

0.00

0.25

0.50

0.75

1.00

(yi)

logistic

action

y1

y2

y3

square

hinge

exponential

truncated quadratic

beta = 1.0

savage

0.00

0.25

0.50

0.75

1.00

(yi)

beta = 2.0

0

2000

4000

6000

8000

Step

0.00

0.25

0.50

0.75

1.00

(yi)

0

2000

4000

6000

8000

Step

0

2000

4000

6000

8000

Step

0

2000

4000

6000

8000

Step

0

2000

4000

6000

8000

Step

0

2000

4000

6000

8000

Step

beta = 10.0

Figure 3 | Bandit example (Azar et al., 2024) to illustrate the regularization effect of different GPO variants.

Convex loss functions with a fast decaying tail or upwards tail (hinge, truncated quadratic and squared loss) will

penalize response-level deviations from 𝜋𝜃to 𝜋ref, effectively enforcing a stronger regularization. Other convex

losses we exhibit here generally have a slower decaying tail, and will more likely converge to deterministic

policies in pathological cases (e.g., deterministic preference).

A case study of logistic loss vs. hinge loss.

Considering the special case when the preferred

and non-preferred samples are separable, the hinge loss will find the optimal separating hyperplane

that maximizes the margin between the two sets of samples. Drawing inspiration from the classic

comparison between logistic regression and support vector machine (Hastie et al., 2009), we note

that the logistic loss will find a similar decision boundary (i.e., sign of the prediction), but it will

try to increase the magnitude of the prediction ˆℓ(𝑦𝑤, 𝑦𝑙) to infinity. Such behavior is alluded to in

the IPO work (Azar et al., 2024) as a failure case of DPO. In general, convex loss functions with a

fast-decaying tail (e.g., hinge loss for SLiC) or upwards tail (e.g., squared loss for IPO) will alleviate

such issues. In Section 4, we will illustrate such insights in combination with policy optimization.

General requirement on the convex function 𝑓.

Not all convex functions 𝑓can lead to valid loss

functions for binary classification. For our study, we further assume 𝑓′(0) < 0, i.e., 𝑓locally decreases

at 𝜌𝜃= 0. This means that the minimizer of 𝑓is obtained at some 𝜌𝜃> 0, and intuitively would push

the reward difference 𝑟𝜙(𝑦𝑤) −𝑟𝜙(𝑦𝑙) in the right direction. Intriguingly, this condition is related to

Bayes consistency (Bartlett et al., 2006; Rosasco et al., 2004), i.e., under which condition can the

prediction function ˆℓ(𝑦1, 𝑦2) recover the same sign as the preference probability sign (2𝑝(𝑦1 ≻𝑦2) −1).

We provide discussions for interested readers in Appendix C.

4. Understanding regularization in offline preference optimization

In this section, we seek to gain a better understanding of the regularization implicitly enforced by the

offline preference optimization algorithms.

Though in general it is challenging to characterize the full learning dynamics of the offline algorithms,

we provide analysis from a few angles, which might shed light on how the regularization works.

Recall that in the RLHF formulation (Eqn 1), the KL regularization is a key element; we will see its

connections to the offline regularization.

4.1. How do offline losses enforce regularization

As hinted at before, henceforth will we consider the class of convex loss functions that are locally

decreasing at 𝜌𝜃= 0, i.e., 𝑓′(0) < 0. All the examples in Table 1 satisfy this property.

To shed light on how such loss functions entail preference optimization while enforcing regularizers,

7

## Page 8

Generalized Preference Optimization: A Unified Approach to Offline Alignment

we consider the Taylor expansion around 𝜌𝜃= 0, which is a valid approximation when 𝜌𝜃is small,

i.e., 𝜋𝜃does not deviate much from 𝜋ref.

𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[ 𝑓(𝛽𝜌𝜃)]

|                   {z                   }

offline loss

≈𝑓(0) + 𝑓′(0)𝛽· 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[𝜌𝜃]

|                        {z                        }

preference optimization

+ 𝑓′′(0)𝛽2

2

· 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇



𝜌2

𝜃



|                           {z                           }

offline regularization

,

The expansion implies that when the approximation is valid, the offline algorithms all resemble

the case where 𝑓is the squared loss (i.e., the IPO loss (Azar et al., 2024)). We provide more

discussion in Appendix B. Minimizing the Taylor-expanded objective achieves two purposes: preference

optimization and regularization towards the reference policy. Indeed, minimizing the first-order term

𝑓′(0)𝛽· 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[𝜌𝜃]

encourages 𝜋𝜃to place more weight on the preferred response 𝑦𝑤over 𝑦𝑙, hence maximizing pairwise

human preference.

To see the effect of the regularization, when 𝑓′′(0) > 0 observe that the second-order term

𝑓′′(0)𝛽2 · 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇

1

2 𝜌2

𝜃



(6)

is minimized at 𝜌𝜃= 0, in which case 𝜋𝜃(𝑦) = 𝜋ref(𝑦) for all 𝑦in the support of 𝜇. In general, this

loss will encourage 𝜋𝜃to stay close to 𝜋ref. We call the above 𝜇-weighted squared loss. Importantly,

the global minimizer of the KL divergence between 𝜋𝜃and 𝜋ref is also a minimizer of the 𝜇-weighted

squared loss (i.e., both minimized when 𝜋𝜃= 𝜋ref).

When the approximation is valid, the GPO problem with a regularizer 𝛽is corresponds to the IPO

problem with regularizer | 𝑓′′(0)/ 𝑓′(0)| · 𝛽, and this quantity determines the relative strength of the

regularization. The coefficient | 𝑓′′(0)/ 𝑓′(0)| interestingly relates to how convex loss functions are

theoretically built-in to be regularized for better generalization (Masnadi-Shirazi and Vasconcelos,

2015). This may inform the design of offline preference optimization algorithms with another

theoretical perspective.

Intuition about the full gradient update.

The Taylor expansion is only valid near 𝜌𝜃= 0 and except

for the special case of squared loss (IPO), drops higher order terms. For example, the expansion does

not work natively for SLiC, which employs a non-smooth convex function. Though understanding the

full learning dynamics is challenging, we can provide some intuitions about how the full gradient

update enforces 𝜋𝜃to stay close to 𝜋ref: consider the gradient update for when 𝛽= 1,

𝜃←𝜃−𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[ 𝑓′(𝜌𝜃)∇𝜃𝜌𝜃] .

(7)

Starting from 0, suppose 𝜌𝜃takes a very high value. This means potentially 𝜋𝜃places many more

weights on certain responses than 𝜋ref, which is what the KL divergence regularization seeks to

prevent. For the offline update, since 𝑓is convex, a few cases are possible: case I: 𝑓′(𝜌𝜃) < 0 (for

logistic, exponential and Savage loss), 𝜌𝜃will continue to increase but with a vanishing gradient;

hence the regularization is still in place. Meanwhile for case II: 𝑓′(𝜌𝜃) ≤0 (for hinge, smoothed

quadratic and squared loss), 𝜌𝜃will stop updating or be pushed downwards. As a result, in case II

the gradient update explicitly does not allow 𝜋𝜃(𝑦) to deviate from 𝜋ref(𝑦) for individual responses 𝑦,

effectively enforcing a stronger regularization with a fixed value of 𝛽.

In Figure ??, we illustrate the effect of strong regularization using the 3-action bandit example

presented in (Azar et al., 2024), where a simple offline dataset with three pairs of examples are

8

## Page 9

Generalized Preference Optimization: A Unified Approach to Offline Alignment

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

shift of

from

ref

4

2

0

2

4

6

8

log loss

(

,

ref)

y

[

1

2

2]

Figure 4 | An example of 𝜇-weighted squared loss and KL divergence for mixture of Gaussians. The squared

loss has local minimizers different from the KL divergence. This means locally descending on the squared loss

may not lead to decreases in the KL divergence, and may not find the global minimizer of the KL divergence.

See Appendix A for the pdf of 𝜋ref and 𝜇.

used for training softmax parameterized policies: (𝑦1, 𝑦2), (𝑦2, 𝑦3), (𝑦1, 𝑦3). Examples are uniformly

sampled from the distribution. Since 𝑦1 is the strongest response, we expect the algorithms to assign

high weights to 𝜋𝜃(𝑦1), causing deviation from 𝜋ref which is uniform. The example is meant to

illustrate the undesirable behavior of DPO, which tends to push up the probability of 𝑦1, despite the

intended regularization. See Appendix A for more details on the setup.

We generalize their observations by noting that for any given values of 𝛽, case I losses will keep

pushing up the probability of a winning action 𝑦1, whereas case II losses enforce the constraint much

more conservatively, preventing deterministic policies. In practice where preferences over responses

are almost never deterministic, we will see that case I losses are also reasonably well behaved.

Choosing the right value for 𝛽.

if we understand the tail behavior of the convex function as

determining the natural regularization strength of the offline algorithm, the hyper-parameter 𝛽needs

to chosen accordingly, if one desires a fixed level of regularization. For example, the logistic loss (i.e.,

DPO) requires a higher value of 𝛽to enforce the same level of regularization as the squared loss (i.e.,

IPO) and the hinge loss (i.e., SLiC), as also exemplified in Figure ??.

4.2. Offline regularization vs. KL regularization

Henceforth we will resort back to the offline regularization: 𝜇-weighted squared loss, and understand

its difference against the KL divergence regularization. We start with the gradient of the 𝜇-weighted

squared loss

𝔼𝑦∼𝜇



∇𝜃

1

2 𝜌2

𝜃



which seeks to decrease the squared error that measures the discrepancy between 𝜋𝜃and 𝜋ref, at

samples generated by 𝜇. For the KL divergence, we can show that its gradient is equivalent to the

9

## Page 10

Generalized Preference Optimization: A Unified Approach to Offline Alignment

𝜇-weighted squared loss with 𝜇= 𝜋𝜃

∇𝜃𝕂𝕃(𝜋𝜃, 𝜋ref) = 𝔼𝑦∼𝜋𝜃



∇𝜃

1

2 𝜌2

𝜃



.

(8)

In other words, we can understand the gradient to the KL divergence as minimizing the discrepancy

with on-policy samples under 𝜋𝜃, rather than offline samples from 𝜇. We detail the derivation in

Appendix B; note a highly similar result was also derived in (Richter et al., 2020).

In summary, both losses enforce the squared penalty on samples from 𝜇vs. online samples from 𝜋𝜃.

We can envision cases when the 𝜇-weighted squared loss is being minimized, the KL divergence might

not decrease as desired.

A mixture of Gaussians counterexample.

To show the fact that, during minimization of the

squared loss, we may not necessarily observe global minimization of the KL divergence, we provide a

low-dimensional toy counterexample using mixture of Gaussians. We set up an example where both

𝜋ref and 𝜇are mixtures of three Gaussians. The optimized policy 𝜋𝜃is just a constant shift away from

𝜋ref with the shift being parameterized by a trainable parameter 𝑐. When 𝑐= 0, we have 𝜋ref = 𝜋𝜃

and both the squared loss and KL divergence are minimized to 0.

In Figure 4, we show the KL divergence and the 𝜇-weighted squared loss, both in log scales, as a

function of 𝑐∈[−1, 1]. The squared loss has a few minima, with some of them being remote from

𝑐= 0. This means gradient descent on the squared loss may not lead to smaller KL in general, though

they are both globally minimized at 𝜋𝜃= 𝜋ref for 𝑐= 0. See Appendix A for the plot of the pdfs of 𝜇

and 𝜋.

This example is meant to illustrate that the arguments used in prior work on offline preference

optimization (Rafailov et al., 2023), which heavily rely on the global minimization of objectives,

may not always be true in practice: locally minimizing the 𝜇-weighted squared loss might not lead

to decrease in the KL divergence. However, the silver lining is that near 𝜋𝜃= 𝜋ref, the two losses

are highly correlated; we will validate the observations on such a low-dimensional example with a

language modeling study.

4.3. Analyzing a language modeling example

In the case of language modeling, where 𝜋𝜃, 𝜋ref, 𝜇are sequential categorical distributions, we measure

the correlation between the KL divergence 𝕂𝕃(𝜋𝜃, 𝜋ref) and the 𝜇-weighted squared loss 𝔼𝑦∼𝜇

 1

2 𝜌2

𝜃



during offline training. We consider the summarization task similar to (Roit et al., 2023), where

the offline dataset is an open source summarization dataset collected with human feedback labels

(Stiennon et al., 2020). We give more details in Appendix A.

For each experiment, we choose a fixed value of regularization 𝛽. Then, we initialize 𝜋𝜃from 𝜋ref

and minimize the offline preference losses over the dataset. As the training progresses, we record

sample-based estimates of the KL divergence and 𝜇-weighted squared loss over time, and trace them

out in Figure 5 left plot for when 𝑓is a squared function. We show both loss functions in the log scale.

Importantly, we have dropped from the plot the initial data point for which 𝜋𝜃= 𝜋ref and both losses

are zero, otherwise the whole plot will look unbalanced (since log 0 ≈−inf). See the full plot in

Appendix A. We make a few comments regarding the current plot.

Correlation between the two losses.

There appears to be two phases in Figure 5 left plot. When 𝛽is

large, and when the 𝜇-weighted squared loss is maintained at a lower level, we see a better correlation

between the two losses. Meanwhile, when 𝛽is small and the 𝜇-weighted squared loss grows quickly

10

## Page 11

Generalized Preference Optimization: A Unified Approach to Offline Alignment

0.5

0.0

0.5

1.0

1.5

2.0

2.5

log

(

,

ref)

3

2

1

0

1

2

3

4

log Ey

[

1

2

2]

= 0.001

= 0.01

= 0.1

= 0.5

= 1.0

= 5.0

= 10.0

= 100.0

1.2

1.4

1.6

1.8

2.0

2.2

2.4

2.6

log

(

,

ref)

1.0

0.5

0.0

0.5

1.0

1.5

2.0

Logistic

Square

Hinge

Exponential

Truncated quadratic

Savage

Figure 5 | Tracing out KL divergence vs. 𝜇-weighted squared loss during offline preference optimization. (Left)

With 𝑓being the squared function, we show the trajectories for a range of 𝛽s. Importantly, the initial data point

for which 𝜋𝜃= 𝜋ref is dropped for better visualization, see Appendix A for the complete plot. Note that as 𝛽

increases, the algorithm maintains a better constraint on the 𝜇-weighted squared loss, which also induces a

constraint on the KL divergence. (Right) We pool over different 𝛽s and show trajectories for different GPO

variants. See Appendix A for individual plots for each variant. Overall, all algorithmic variants enjoy similar

constraint properties, with most variants being slightly more stable than the logistic variant.

during optimization, its correlation with KL divergence becomes more elusive (see purple and blue

data points on the left plot). Such observations echo the mixture of Gaussian examples, where in the

vicinity of 𝜋𝜃= 𝜋ref, the two losses have similar trends; the misalignment happens when we deviate

too much from the origin.

Though the correlation between the two losses seem to break when 𝜋𝜃is too far away from 𝜋ref, the

silver lining is that for offline algorithms, the optimization always starts with the origin 𝜋𝜃= 𝜋ref, and

one may expect a better control over the KL divergence through the 𝜇-weighted squared loss.

More variations in KL compared to 𝜇-weighted loss.

For Figure 5 left plot, in the regime where

the KL divergence and 𝜇-weighted squared loss are better correlated (areas inside the grey bounding

box), we see an order of magnitude more drastic variations in the KL divergence (10−0.5 →101.5)

than the 𝜇-weighted squared loss (10−1.5 →100.5).

This hints at the challenge of maintaining the KL divergence constraint by controlling the 𝜇-weighted

squared loss. Indeed, since the offline preference optimization algorithms directly optimize for the

𝜇-weighted squared loss in the vicinity of the origin 𝜋𝜃= 𝜋ref, even small changes in the 𝜇-weighted

squared loss can induce much bigger changes in the KL divergence. This might become a source of

instability during optimization. However, the degree to which such instability can be mitigated by

other hyper-parameter choices such as learning rate, might vary case-by-case.

Comparison across different GPO variants.

In Figure 5 right plot we compare the constraint

contours across different GPO variants listed in Table 1. For each variant we sweep the 𝛽s but for

visualization we pool across results from all 𝛽s, see Appendix A for individual plots.

11

## Page 12

Generalized Preference Optimization: A Unified Approach to Offline Alignment

0.5

0.0

0.5

1.0

1.5

2.0

2.5

3.0

log

(

,

sft)

0.00

0.02

0.04

0.06

0.08

0.10

0.12

0.14

Win rate against golden policy

Logistic

Square

Hinge

Exponential

Truncated square

Savage

0.5

1.0

1.5

2.0

2.5

log

(

,

sft)

0.00

0.02

0.04

0.06

0.08

0.10

0.12

= 0.01

= 0.05

= 0.1

= 0.5

= 1.0

= 5

= 10

= 50

= 100

Figure 6 | Left: Tracing KL divergence vs. golden win rate performance for different GPO variants.

Each data point corresponds to a policy obtained during training with a particular value of 𝛽and

convex function loss. For each loss variant, we pool data points across 𝛽s and different stages of

training. Overall, the trade-off curves of GPO variants look similar. Right: Tracing the trade-off for

the logistic loss (DPO), grouped according to the regularization coefficient 𝛽. As 𝛽increases, the

regularization effect is larger and during training, and the policies tend to have smaller KL divergence

against 𝜋sft.

Overall, different variants follow a similar pattern, with most variants being slightly more robust

compared to the logistic loss, which seems to induce slightly bigger variations in the KL divergence

compared to other alternatives.

5. Empirical study of GPO variants

We now carry out a set of experimental comparison between different GPO algorithms, and to study

their empirical behavior and validate theoretical insights.

5.1. Trade-offs between KL divergence and performance

As the offline alignment optimization progresses, the policy 𝜋𝜃starts to drift away from the initial

anchor policy 𝜋sft. When measured in terms of the ground truth performance, there is a trade-off

between model performance and KL divergence from the initialization. We adopt a synthetic setting

similar to (Gao et al., 2023) to study this trade-off.

Concretely, we take the summarization task introduced above and train a XXL model (11 billion

parameters) as the golden preference model, using similar training setting as Munos et al. (2024).

This preference model will be used as the golden judgement. Since the preference model carries out

side by side comparison, we also train a golden policy as the fixed baseline to compare against. We

provide more technical details in Appendix A. For each fixed convex loss function, we sweep over

values of the regularization coefficient 𝛽. For each 𝛽, we train the model for 2 · 104 steps with a

constant learning rate (10−5 and 3 · 10−5). We evaluate checkpoints every 2𝑘steps for a total of 20𝑘

training steps.

In Figure 6 (left), we trace the performance of trained checkpoints over time, plotting their golden

12

## Page 13

Generalized Preference Optimization: A Unified Approach to Offline Alignment

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0

log10

0.02

0.04

0.06

0.08

0.10

0.12

0.14

Peak win rate

Logistic

Square

Hinge

Exponential

Truncated square

Savage

Best  for square

Best  for logistic

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0

log10

0.0

0.5

1.0

1.5

2.0

2.5

Median log

(

,

sft)

Best performing KL divergence

Figure 7 | Left: 90%-th percentile performance during training for different values of 𝛽s. We use the

90%-th percentile as an estimate of the best possible performance under a fixed 𝛽. Different GPO

variants seem to peak at different values of 𝛽: noticeably, squared loss and truncated squared loss

peak at about 𝛽= 1 while others mostly peak at slightly larger values 𝛽∼10. Right: Median values

of KL divergence during training, as a function of 𝛽for different GPO variants. When 𝛽is small,

different variants have little distinction; when 𝛽is large (strong regularization) and fixed, squared

and truncated squared loss tend to incur smaller KL divergence compared to other variants.

evaluation performance against the golden policy. Each dot corresponds to a checkpoint evaluation,

for a particular value of 𝛽, learning rate and convex function loss. We group the results by the convex

function loss. A few observations are in order: (1) We observe the over-optimization effect compatible

with Goodhart’s law Gao et al. (2023), wherein as the KL divergence increases, the golden performance

evaluation first increases and then decreases as a result of over-optimization. The key difference is

that (Gao et al., 2023) is for online RLHF, while our case is offline optimization; (2) For different loss

functions, the overall trade-off curves look similar. Concretely, the peak performance is similar and

is obtained at a similar level of KL divergence. This suggests that for any choice of the convex loss

function, a choice of 𝛽and training step can lead to a specified level of performance.

In Figure 6 (right), we break down the trade-off curve with respect to the regularization coefficient 𝛽.

We show the case for the logistic loss, though other losses have a similar breakdown (see Appendix A

for full results). For each 𝛽(with a unique color), different data points correspond to different

stage of training for the same experiment and hence tracing out a trend of KL divergence vs. win

rate. We make a few observations: (1) Data points seem to piece together seamlessly at the soft

boundaries between 𝛽s, this means given a fixed value of 𝛽, one can probably obtain a specified level

of KL divergence and win rate performance, by training the policy for a certain number of steps.

However, different 𝛽s are not equal: in the case of logistic loss, 𝛽∼1 seems to obtain the best overall

performance across training, while 𝛽= 0.01 can easily train the policy to have large KL divergence,

resulting in degraded performance; meanwhile, 𝛽= 100 puts a larger constrain the policy near 𝜋sft,

making it difficult to obtain the best performance across training.

Impact of 𝛽.

We now closely investigate the impact that 𝛽has on the performance and KL regu-

larization dynamics of various GPO variants. Figure 7 (left) shows the peak performance of various

13

## Page 14

Generalized Preference Optimization: A Unified Approach to Offline Alignment

algorithms as a function of 𝛽. As seen from the plot, the peak performance of squared and truncated

squared loss is obtained at generally lower 𝛽∼1, whereas the peak performance for other variants

are obtained at higher 𝛽∼10. There is some variations of the peak win rate (e.g., exponential seems

to be slightly better than others) but this might not be statistically significant.

While the observation suggests the fact that different algorithms require different values of 𝛽s to

perform the best, it can be explained by the fact that different loss functions induce distinct strengths

of regularization as a function of 𝛽, as predicted by theory. In Figure 7 (right) we show the median KL

divergence during training as a function of 𝛽, for different convex loss functions. When 𝛽is small and

regularization is weak, there is little distinction between different variants. This is compatible with

the results in Figure 5: the offline algorithm enforces regularization through the weighted squared

loss, and its correlation with KL divergence is weak when the regularization is small. At large values

of 𝛽s, the correlation between offline regularization and KL divergence is much stronger. And indeed,

we see squared and truncated squared loss enforce stronger regularization than other variants, with

logistic, exponential and Savage being in the same league and hinge loss in the middle.

5.2. Model-based side by side evaluation

The synthetic setting has provided many insights into the trade-offs between regularization and

policy performance, and how they are modulated by choices of 𝛽and convex loss functions. We now

carry out a final set of experiments on the summarization task, using settings described in prior work

(Calandriello et al., 2024; Munos et al., 2024).

We consider the side-by-side comparison metric used by Munos et al. (2024), where we compare

the checkpoint performance against a fixed opponent 𝜋ref. The comparison is made by a prompted

PaLM-2 model (Anil et al., 2023) over an evaluation set of 2000 summary samples. The prompted

model judges which response is of higher quality. See Appendix A for evaluation details.

Examining the performance across 𝛽s, we see that when 𝛽is small, the optimization tends to be

more effective, achieving the best performance at about 𝛽∈[0.1, 1] across the board, with similar

peak performance. The performance experiences a bigger drop when 𝛽becomes large. When making

pairwise comparison across different GPO variants, we see that their performance is generally on par

with one another; choosing the right 𝛽appears more critical. Due to space limits, we present these

comparisons in Appendix A.

6. Discussions and conclusion

We have presented GPO, a generalized approach to deriving offline preference optimization losses

for LLM alignment. GPO presents a continuous spectrum of loss functions, encompassing DPO, IPO

and SLiC as special instances. By deriving GPO through the rich literature on binary classification,

we have presented a more unified way to reason about the strength of regularization and what the

optimized policy seeks to capture.

We have shown the connections between the offline regularization and the KL regularization, which

the RLHF formulation seeks to enforce. The two types of regularization are different in general.

However, optimizing from the origin, we see empirical evidence that the two losses are correlated,

alluding to the fact that enforcing KL divergence through offline optimization is possible though

maybe more challenging.

We have also showed the regularization vs. performance trade-off between different GPO variants.

Overall, the regularization vs. performance trade-off is similar for different algorithms. As predicted

by theory, different convex loss variants induce inherently distinct strengths for regularization, which

impacts the optimal value of 𝛽for each algorithm (i.e., squared loss needs a smaller 𝛽than logistic

14

## Page 15

Generalized Preference Optimization: A Unified Approach to Offline Alignment

loss).

Our results have a number of limitations and provide avenues for future work. Our framework is

based on the reward maximization formulation of RLHF, and hence still encounters theoretical issues

when the ground truth preference structure is complex. A future direction would be to connect GPO

with alternative solution concepts for alignment such as Nash equilibrium (Munos et al., 2024). Our

framework also only deals with offline losses with a contrastive form, and does not handle supervised

learning based losses (Zhao et al., 2023).

Acknowledgements.

We thank Ivo Danihelka for providing very valuable feedback to an earlier

draft of the paper. We are thankful to the Google DeepMind teams that build the infrastructure which

facilitates the research in this work.

References

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,

Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report.

arXiv preprint arXiv:2303.08774, 2023.

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak

Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. arXiv preprint

arXiv:2305.10403, 2023.

Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal

Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human

preferences. In Proceedings of the International Conference on Artificial Intelligence and Statistics,

2024.

Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk bounds.

Journal of the American Statistical Association, 101(473):138–156, 2006.

Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal

margin classifiers. In Proceedings of the Workshop on Computational Learning Theory, 1992.

Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of

paired comparisons. Biometrika, 39(3/4):324–345, 1952.

Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires,

Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng,

and Bilal Piot. Human alignment of large language models through online preference optimisation.

In Proceedings of the International Conference on Machine Learning, 2024.

Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep

reinforcement learning from human preferences. In Advances in Neural Information Processing

Systems, 2017.

Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20:273–297, 1995.

Yoav Freund and Robert E. Schapire. A desicion-theoretic generalization of on-line learning and

an application to boosting. In Proceedings of the European conference on Computational Learning

Theory, 1995.

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In

Proceedings of the International Conference on Machine Learning, 2023.

15

## Page 16

Generalized Preference Optimization: A Unified Approach to Offline Alignment

Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The elements of statistical learning: Data

mining, inference, and prediction. Springer, 2009.

Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,

Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of

experts. arXiv preprint arXiv:2401.04088, 2024.

Hamed Masnadi-Shirazi and Nuno Vasconcelos. On the design of loss functions for classification:

Yheory, robustness to outliers, and SavageBoost. In Advances in Neural Information Processing

Systems, 2008.

Hamed Masnadi-Shirazi and Nuno Vasconcelos. A view of margin losses as regularizers of probability

estimates. Journal of Machine Learning Research, 16(1):2751–2795, 2015.

Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Daniel

Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin,

Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, and Bilal Piot. Nash learning

from human feedback. In Proceedings of the International Conference on Machine Learning, 2024.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong

Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow

instructions with human feedback. In Advances in Neural Information Processing Systems, 2022.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea

Finn. Direct preference optimization: Your language model is secretly a reward model. In Advances

in Neural Information Processing Systems, 2023.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi

Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text

transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.

Lorenz Richter, Ayman Boustati, Nikolas Nüsken, Francisco Ruiz, and Omer Deniz Akyildiz. VarGrad:

A low-variance gradient estimator for variational inference. In Advances in Neural Information

Processing Systems, 2020.

Adam Roberts, Hyung Won Chung, Gaurav Mishra, Anselm Levskaya, James Bradbury, Daniel Andor,

Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scaling up models and data

with t5x and seqio. Journal of Machine Learning Research, 24(377):1–8, 2023.

Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist,

Sertan Girgin, Léonard Hussenot, Orgad Keller, et al. Factually consistent summarization via

reinforcement learning with textual entailment feedback. In Proceedings of the Annual Meeting of

the Association for Computational Linguistics, 2023.

Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. Are loss

functions all the same? Neural computation, 16(5):1063–1076, 2004.

Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.

In Proceedings of the International Conference on Machine Learning, 2018.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,

Dario Amodei, and Paul F. Christiano. Learning to summarize with human feedback. In Advances

in Neural Information Processing Systems, 2020.

Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaxi-

16

## Page 17

Generalized Preference Optimization: A Unified Approach to Offline Alignment

malist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056,

2024.

Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu

Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A family of highly capable

multimodal models. arXiv preprint arXiv:2312.11805, 2023.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. SLiC-HF:

Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.

17

## Page 18

Generalized Preference Optimization: A Unified Approach to Offline Alignment

A. Experiment details and additional results

We provide further details and additional results on experiments across the paper.

A.1. Bandit experiment

To illustrate the regularization properties of various GPO variants, we have employed the bandit

experiment introduced in (Azar et al., 2024). We consider a 3-action bandit problem where the

dataset consists of three possibilities

(𝑦1, 𝑦2), (𝑦2, 𝑦3), (𝑦1, 𝑦3).

Sampling from the offline dataset consists in uniformly sampling from the pairs. We then train softmax

parameterized policies with exactly the same setup as (Azar et al., 2024). Note that with logistic,

exponential and Savage loss, because the tail does not vanish fast enough, the policy converges to

the greedy action 𝑦1 even with regularization at 𝛽= 1. While for the other three losses, thanks to

stronger regularization, 𝜋𝜃(𝑦1) maintains closer distance to 𝜋ref(𝑦1).

A.2. A mixture of Gaussian counterexample

We find the counterexample by parameterizing all related distributions as mixtures of Gaussians with

3 modes. It is not difficult to construct numerical counterexamples as shown in the paper, with ≤5

simulations.

The offline distribution 𝜇is parameterized as 𝜇= 1

3N (𝑢1, 0.052)+ 1

3N (𝑢2, 0.052)+ 1

3N (𝑢3, 0.052) where

𝑢1, 𝑢2, 𝑢3 are i.i.d. uniform between −1 and 1. The reference policy is fixed as 𝜋ref =

3

## 10N (−0.8, 0.12) +

4

## 10N (0, 0.12) +

3

10N (0.8, 0.12). The optimized policy 𝜋𝜃is a constant shift away from 𝜋ref. The

particular choice of the parameters are fairly ad-hoc and other choices of hyper-parameters should

lead to clear counterexamples as well. Since for mixtures of Gaussians, both the KL divergence and

the 𝜇-weighted squared loss do not yield analytic forms. Instead, we draw 2000 samples to estimate

both losses as unbiased estimates.

Figure 8 (left) shows the probability density function (pdf) for 𝜋ref and 𝜇, in the counterexample

that we presented.

A.3. Language modeling experiments

We consider the summarization task similar to (Roit et al., 2023), where the offline dataset is an

open source summarization dataset collected with human feedback labels (Stiennon et al., 2020).

The base model is T5X (Roberts et al., 2023), a family of LLMs based on encoder-decoder transformer

architecture. Throughout, we train large-sized models with 700𝑀parameters. During training,

we apply a constant learning rate of 10−5 with batch size 𝑏= 32. We use the Adafactor optimizer

(Shazeer and Stern, 2018) with a decay rate of 0.8. Each model is trained for 2 × 105 steps in total.

The evaluation follows from (Munos et al., 2024) where they consider the side-by-side comparison

metric between two models. A default baseline model is the supervised fine-tuned baseline 𝜋ref.

The comparison is made by a prompted PALM-2 model (Anil et al., 2023), where the model judges

which response is of higher quality. The evaluation set consists of 2000 examples, each containing a

paragraph to summarize. The prompted model is given the paragraph, as well as the two summaries

generated by the two compared models, to deliver a final verdict.

Tracing KL divergence and 𝜇-weighted squared loss.

For each experiment (with a fixed convex

function 𝑓and fixed 𝛽), we evaluate intermittently the 𝜇-weighted squared loss on the learner and

the KL divergence on the evaluator. The evaluator is carried out every 2000 steps where we train for

18

## Page 19

Generalized Preference Optimization: A Unified Approach to Offline Alignment

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

x

0.0

0.5

1.0

1.5

2.0

2.5

density

ref

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

shift of

from

ref

4

2

0

2

4

6

8

log loss

(

,

ref)

y

[

1

2

2]

Figure 8 | Full results for the mixture of Gaussian counterexample. (Left) The probability density

function (pdf) for 𝜇and 𝜋ref, both are designed to be mixtures of Gaussian with 3 modes; (Right)

The same plot as Figure 4.

a total of 20000 steps. We also evaluate every 200 steps for the first 2000 steps since the initial stage

during training presents the most salient changes in the 𝜇-weighted squared losses.

The tracing plot for individual GPO variant is shown in Figure 10 for better visualization.

A.4. Trade-off between performance and KL divergence

All experiments are carried out with T5X models (Raffel et al., 2020) with the T5X data and compute

framework (Roberts et al., 2023). To create a synthetic setup similar to Gao et al. (2023), we take

the summarization dataset and train a golden preference model with the XXL model (11 billion

parameters). Then we use the XXL model to relabel the offline dataset, and all offline experiments

going forward are carried out with this relabeled dataset.

Since the preference model requires side by side comparison, we also train a golden policy using

online IPO (Calandriello et al., 2024) using the golden preference model. This policy is denoted

golden because it makes use of the golden preference model during training, and should arguably

obtain the best possible performance over time. We use this policy as the reference policy during

evaluation.

All policies are trained with the Large T5X model (110 million parameters) using offline preference

optimization variants outlined in the paper.

Full results on the breakdown of KL divergence vs. win rate.

Figure 9 shows the win rate

performance and KL divergence trade-off curves across different algorithmic variants of GPO. For

each algorithmic variant, the data points are grouped by the regularization coefficient 𝛽. Overall,

different algorithmic variants exhibit trade-off pattern and their dependency on 𝛽is similar too.

It is worth noting that compatible with results reported in Figure 7, all algorithmic variants achieve

the peak performance at the same value of KL divergence but with a different value of 𝛽. This is the

result of the fact that different loss functions have different natural strength of regularization.

19

## Page 20

Generalized Preference Optimization: A Unified Approach to Offline Alignment

0.5

1.0

1.5

2.0

2.5

0.00

0.02

0.04

0.06

0.08

0.10

0.12

Win rate against golden policy

f = Logistic

= 0.01

= 0.05

= 0.1

= 0.5

= 1.0

= 5

= 10

= 50

= 100

0.5

0.0

0.5

1.0

1.5

2.0

2.5

3.0

0.00

0.02

0.04

0.06

0.08

0.10

0.12

f = Square

0.0

0.5

1.0

1.5

2.0

2.5

0.00

0.02

0.04

0.06

0.08

0.10

0.12

f = Hinge

0.0

0.5

1.0

1.5

2.0

2.5

log

(

,

sft)

0.00

0.02

0.04

0.06

0.08

0.10

0.12

0.14

Win rate against golden policy

f = Exponential

0.0

0.5

1.0

1.5

2.0

2.5

3.0

log

(

,

sft)

0.00

0.02

0.04

0.06

0.08

0.10

0.12

f = Truncated square

0.5

1.0

1.5

2.0

2.5

log

(

,

sft)

0.00

0.02

0.04

0.06

0.08

0.10

0.12

f = Savage

Figure 9 | Tracing the trade-off between performance and KL divergence for the various loss functions.

For each loss function, the data points are grouped according to the regularization coefficient 𝛽. We

see that different algorithmic variants exhibit similar patterns both in terms of the general trade-off

curves, as well as the dependency of the curves on 𝛽.

20

## Page 21

Generalized Preference Optimization: A Unified Approach to Offline Alignment

1.4

1.6

1.8

2.0

2.2

2.4

2.6

0.5

0.0

0.5

1.0

1.5

2.0

log Ey

[

1

2

2]

f = Logistic

1.4

1.6

1.8

2.0

2.2

2.4

0.5

0.0

0.5

1.0

1.5

2.0

f = Square

1.2

1.4

1.6

1.8

2.0

2.2

2.4

0.5

0.0

0.5

1.0

1.5

2.0

f = Hinge

1.2

1.4

1.6

1.8

2.0

2.2

log

(

,

ref)

1.0

0.5

0.0

0.5

1.0

1.5

2.0

log Ey

[

1

2

2]

f = Exponential

1.4

1.6

1.8

2.0

2.2

2.4

2.6

log

(

,

ref)

0.5

0.0

0.5

1.0

1.5

2.0

f = Truncated quadratic

1.2

1.4

1.6

1.8

2.0

2.2

2.4

log

(

,

ref)

1.0

0.5

0.0

0.5

1.0

1.5

2.0

f = Savage

Figure 10 | Tracing out KL divergence vs. 𝜇-weighted squared loss during offline preference optimiza-

tion, for individual GPO variants. This plot separates the data from Figure 5 for better visualization.

Side by side evaluation.

We subsample 256 prompts from the training set and generate responses

from both the golden policy and the target policy to compare against. We then use the preference

model to judge the win rate between the two sets of responses, and average across the subsampled

prompt set.

A.5. Model-based side by side evaluation

We now discuss experimental results on the summarization task with model-based side by side

evaluation. While previous study on the KL divergence vs. win rate trade-off is carried out in a

synthetic setting, here we train models with the open sourced summarization dataset (Stiennon et al.,

2020) and prompt a PALM-2 model (Anil et al., 2023) for side by side evaluation. We adopt identical

evaluation setup as in (Munos et al., 2024) and (Calandriello et al., 2024).

Win rate results.

In Figure 11, we show the win rate of various algorithmic variants in a side-by-side

comparison against the supervised fine-tuned checkpoint 𝜋ref. For two identical models, the win rate

should be 0.5. We observe that the best performance is usually obtained at 𝛽∈[0.1, 1], with similar

performance across different 𝑓s. Interestingly, when 𝛽becomes too large, the win rate drops more

quickly across all methods.

In Figure 12, we show the side by side comparison across GPO variants. For each variant, we take

the checkpoint with 𝛽= 0.1 since this appears to be a value where all algorithms work reasonably,

according to the win rate against the supervised fine-tuned checkpoint. The win rate comparison

across GPO variants suggests that they perform mostly similar.

21

## Page 22

Generalized Preference Optimization: A Unified Approach to Offline Alignment

3.0

2.5

2.0

1.5

1.0

0.5

0.0

0.5

log10

0.5

0.6

0.7

0.8

0.9

Win rate against

ref

Logistic

Square

Hinge

Exponential

Truncated square

Savage

Figure 11 | Win rate of various GPO methods against the supervised fine-tuned baseline 𝜋ref, as a

function of 𝛽. Almost all algorithmic variants obtain the best performance at 𝛽∈[0.1, 1], with similar

peak performance.

B. Proof and derivations of theoretical results

We provide more detailed proof to a few important theoretical results in the paper.

Theorem 1. (Equivalence of optimal solutions) Let 𝜋∗

𝜃be the global minimizer of the offline

preference optimization loss in Eqn (4). 𝜋∗

𝜃is the same as the optimal regularized policy (according

to Eqn (1)) for a reward function that globally minimizes the loss Eqn (3).

Proof. The proof is straightforward. Indeed, note that if we seek to minimize Eqn (3) with 𝑟𝜙, we can

reparameterize the reward function as 𝑟𝜙(𝑦) = 𝛽log 𝜋𝜃(𝑦)

𝜋ref(𝑦) +𝑧with normalizing constant 𝑧that depends

on 𝜋𝜃. Then if 𝑟∗

𝜙is the global minimizer to Eqn (3), the corresponding 𝜋𝜃(𝑦) ∝𝜋ref(𝑦) exp(𝛽−1𝑟∗

𝜙(𝑦))

must be the global minimizer to Eqn (4).

B.1. Derivation of the gradient of KL divergence and 𝜇-weighted squared loss

By definition we have 𝕂𝕃(𝜋𝜃, 𝜋ref) = 𝔼𝑦∼𝜋𝜃

h

log 𝜋𝜃(𝑦)

𝜋ref(𝑦)

i

, its gradient contains two terms

∇𝜃𝕂𝕃(𝜋𝜃, 𝜋ref) = 𝔼𝑦∼𝜋𝜃



log 𝜋𝜃(𝑦)

𝜋ref(𝑦) ∇𝜃log 𝜋𝜃(𝑦)



+ 𝔼𝑦∼𝜋𝜃[∇𝜃log 𝜋𝜃(𝑦)]

|                     {z                     }

=0

The second term vanishes because it is the expectation of a score function with respect to the

distribution itself. Meanwhile, for the 𝜇-weighted squared loss, we rewrite the original definition as

1

2𝔼(𝑦𝑤,𝑦𝑙)∼𝜇



𝜌2

𝜃



= 1

2𝔼(𝑦1,𝑦2)∼𝜇

"

log 𝜋𝜃(𝑦1)

𝜋ref(𝑦1) −log 𝜋𝜃(𝑦2)

𝜋ref(𝑦2)

2#

,

22

## Page 23

Generalized Preference Optimization: A Unified Approach to Offline Alignment

Logistic

Square

Hinge

Exponential

Truncated quadratic

Savage

Logistic

Square

Hinge

Exponential

Truncated quadratic

Savage

50%

48%

50%

47%

50%

48%

51%

50%

53%

50%

53%

50%

49%

46%

50%

47%

50%

48%

52%

49%

52%

50%

51%

48%

49%

46%

49%

48%

50%

46%

51%

49%

51%

51%

53%

50%

Win rate of row vs. column

Figure 12 | Win rate of various GPO methods against one another. We take all checkpoints at 𝛽= 0.1

since this is a value where all variants have reasonable performance. We show the color coded win

rates in a matrix.

where the equality is based on the fact that the order of (𝑦𝑤, 𝑦𝑙) does not impact the expectation.

Now, taking the gradient of the above loss with 𝜇= 𝜋𝜃,

𝔼(𝑦1,𝑦2)∼𝜋𝜃



∇𝜃

1

2 𝜌2

𝜃



= 𝔼(𝑦1,𝑦2)∼𝜇

1

2



log 𝜋𝜃(𝑦1)

𝜋ref(𝑦1) −log 𝜋𝜃(𝑦2)

𝜋ref(𝑦2)



(∇𝜃log 𝜋𝜃(𝑦1) −∇𝜃log 𝜋𝜃(𝑦2))



,

=(𝑎)

1

2𝔼(𝑦1,𝑦2)∼𝜋𝜃



log 𝜋𝜃(𝑦1)

𝜋ref(𝑦1) ∇𝜃log 𝜋𝜃(𝑦1) + log 𝜋𝜃(𝑦2)

𝜋ref(𝑦2) ∇𝜃log 𝜋𝜃(𝑦2)



=(𝑏) 𝔼𝑦∼𝜋𝜃



log 𝜋𝜃(𝑦)

𝜋ref(𝑦) ∇𝜃log 𝜋𝜃(𝑦)



.

Here, (a) follows from the fact the cross term vanishes because 𝑦1, 𝑦2 are independent; (b) follows

from the fact that 𝑦1, 𝑦2 are identically distributed. This proves the desired equality in Eqn (8).

Relation to results from (Richter et al., 2020).

A highly related result has been derived in (Richter

et al., 2020), relating the gradient of the KL divergence to the gradient of the variance of the log ratio.

We provide a simple derivation here. Note that when 𝜇= 𝜋𝜃, the 𝜇-weighted squared loss indeed

evaluates to a variance

𝔼(𝑦𝑤,𝑦𝑙)∼𝜇

1

2 𝜌2

𝜃



## = 𝕍



log 𝜋𝜃(𝑦)

𝜋ref(𝑦)



.

To see this note that if 𝑌, 𝑌′ are i.i.d. samples then 1

## 2𝔼



## (𝑌−𝑌′)2

## = 𝕍[𝑌].

□

B.2. Discussion on Taylor expansions of the GPO losses

Assume that 𝑓is smoothly differentiable and convex, and 𝑓′(0) < 0, then the GPO problem with the

second order Taylor expansion recovers the squared loss with 𝛽′ = 𝑓′′(0) 𝛽

| 𝑓′(0) | . Note that the squared loss

is effectively the IPO loss.

23

## Page 24

Generalized Preference Optimization: A Unified Approach to Offline Alignment

To see this, by the second order Taylor approximation to 𝑓around 𝜌𝜃= 0, we have

𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[ 𝑓(𝛽𝜌𝜃)] ≈𝑓(0) + 𝑓′(0)𝛽· 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[𝜌𝜃] + 𝑓′′(0)𝛽2

2

· 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇



𝜌2

𝜃



= 𝑓(0) + 𝑓′(0)2

2 𝑓′′(0)

 𝑓′′(0)

| 𝑓′(0)| 𝛽𝔼(𝑦𝑤,𝑦𝑙)∼𝜇[𝜌𝜃] −1

2

−𝑓′(0)2

2 𝑓′′(0)

≡(𝑎) 𝔼(𝑦𝑤,𝑦𝑙)∼𝜇

" 𝑓′′(0)𝛽

| 𝑓′(0)| 𝜌𝜃−1

2#

,

where for (a) we have rearranged terms and the equivalence is up to constants. Indeed, we see that

the Taylor-expanded GPO loss is equivalent to the IPO loss with 𝛽′ as defined above.

C. Discussion on Bayes consistency for the learned reward model

Here we provide a brief background on Bayes consistency. Using the notation from Section 3, we

consider binary classification loss of the following form with a convex function 𝑓

## 𝔼

h

𝑓



ˆℓ(𝑧) · ℓ

i

where 𝑙∈{−1, 1} is the ground-truth label and ˆℓ(𝑧) is the prediction. The Bayes optimal classifier,

which minimizes the 0-1 classification error, depends on the probability 𝑝(ℓ= 1|𝑧), which is ˆℓ∗(𝑧) =

sign (2𝑝(ℓ= 1|𝑧) −1). The Bayes consistency result (Bartlett et al., 2006; Rosasco et al., 2004) state

the following.

Theorem 2. (Bayes consistency) Assume 𝑓is convex, and continuously differentiable and 𝑓′(0) < 0.

Then let ˆℓ(𝑧) be the global minimizer to the binary classification loss, then sign



ˆℓ(𝑧)



= ˆℓ∗(𝑧).

We refer readers to Rosasco et al. (2004) for the easy-to-follow proof. The high level idea is to show

that at the global minimizer, assuming 𝑝(ℓ= 1|𝑧) > 1/2, we should expect ˆℓ(𝑧) > 0. Intuitively, this

should be the case since 𝑓′(0) < 0 and is convex, so the minimizer should be at the right hand side of

the origin.

C.1. Discussion of pairwise preference model

We now discuss properties of the pairwise preference model, where the prediction ˆℓ(𝑦1, 𝑦2) is parame-

terized as a general bi-variate function ˆℓ(𝑦1, 𝑦2) = 𝑟𝜙(𝑦1, 𝑦2) of 𝑦1, 𝑦2 rather than the difference of two

univariate functions 𝑟𝜙(𝑦1) −𝑟𝜙(𝑦2). We conjecture that some of the results will transfer to pointwise

reward models in practice, e.g., when the BT assumption approximately makes sense. Making precise

of such approximations is left to future work.

An intuitive requirement for the prediction ˆℓ(𝑦1, 𝑦2) is that it gets the sign of the preference correct,

which is defined through 𝑝(𝑦1 ≻𝑦2). More concretely, one might seek the follow property

sign



ˆℓ(𝑦1, 𝑦2)



= sign (𝑝(𝑦1 ≻𝑦2) −1/2)

(9)

Interestingly, the right-hand side of Eqn (9) corresponds to the Bayes optimal classifier, which

minimizes the classification loss in Eqn (5). The convex loss functions we consider in this work

(e.g., all examples in Table 1) all satisfy the property that if 𝑙(𝑦1, 𝑦2) is parameterized as a general

preference model (rather than a pointwise reward model, see e.g., (Munos et al., 2024)), then by

minimizing the loss we find ˆℓ(𝑦1, 𝑦2) that satisfies Eqn (9), a result stemming from Bayes consistency

(Bartlett et al., 2006; Rosasco et al., 2004).

24

## Page 25

Generalized Preference Optimization: A Unified Approach to Offline Alignment

However, even if different loss functions produce the same sign, the predictions ˆℓ(𝑦1, 𝑦2) can differ

drastically depending on 𝑓. In the main paper we have provided a case study example of logistic loss

vs. hinge loss, borrowing inspirations from the study in Hastie et al. (2009).

25
