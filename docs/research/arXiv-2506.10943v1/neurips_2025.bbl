\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Grattafiori et~al.(2024)Grattafiori, Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Vaughan, et~al.]{grattafiori2024llama3herdmodels}
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et~al.
\newblock The {L}lama 3 herd of models, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.21783}.

\bibitem[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, Arora, Atkinson, Authur, Chandu, Cohan, Dumas, Elazar, Gu, Hessel, Khot, Merrill, Morrison, Muennighoff, Naik, Nam, Peters, Pyatkin, Ravichander, Schwenk, Shah, Smith, Strubell, Subramani, Wortsman, Dasigi, Lambert, Richardson, Zettlemoyer, Dodge, Lo, Soldaini, Smith, and Hajishirzi]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, and Hannaneh Hajishirzi.
\newblock {OLM}o: {A}ccelerating the science of language models.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics}. Association for Computational Linguistics, 2024.
\newblock URL \url{https://aclanthology.org/2024.acl-long.841/}.

\bibitem[Qwen et~al.(2025)Qwen, Yang, Yang, Zhang, Hui, Zheng, Yu, Li, Liu, Huang, Wei, Lin, Yang, Tu, Zhang, Yang, Yang, Zhou, Lin, Dang, Lu, Bao, Yang, Yu, Li, Xue, Zhang, Zhu, Men, Lin, Li, Tang, Xia, Ren, Ren, Fan, Su, Zhang, Wan, Liu, Cui, Zhang, and Qiu]{qwen2025qwen25technicalreport}
Qwen, An~Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le~Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu~Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu.
\newblock Qwen2.5 technical report, 2025.
\newblock URL \url{https://arxiv.org/abs/2412.15115}.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo, Beltagy, Downey, and Smith]{gururangan2020finetune}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug Downey, and Noah~A. Smith.
\newblock Don't stop pretraining: Adapt language models to domains and tasks.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}. Association for Computational Linguistics, 2020.
\newblock URL \url{https://aclanthology.org/2020.acl-main.740/}.

\bibitem[Zhu et~al.(2020)Zhu, Rawat, Zaheer, Bhojanapalli, Li, Yu, and Kumar]{zhu2021modifying}
Chen Zhu, Ankit~Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.
\newblock Modifying memories in transformer models, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.00363}.

\bibitem[Chollet et~al.(2025)Chollet, Knoop, Kamradt, and Landers]{chollet2024arc}
Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers.
\newblock {ARC} prize 2024: Technical report, 2025.
\newblock URL \url{https://arxiv.org/abs/2412.04604}.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2022finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Rozière et~al.(2024)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Sauvestre, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier, Scialom, and Synnaeve]{rozière2024codellamaopenfoundation}
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian~Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.
\newblock Code {L}lama: {O}pen foundation models for code, 2024.
\newblock URL \url{https://arxiv.org/abs/2308.12950}.

\bibitem[Chen et~al.(2023)Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, Köpf, Mohtashami, Sallinen, Sakhaeirad, Swamy, Krawczuk, Bayazit, Marmet, Montariol, Hartley, Jaggi, and Bosselut]{chen2023meditron70bscalingmedicalpretraining}
Zeming Chen, Alejandro~Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut.
\newblock Medi{T}ron-70{B}: {S}caling medical pretraining for large language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.16079}.

\bibitem[Colombo et~al.(2024)Colombo, Pires, Boudiaf, Culver, Melo, Corro, Martins, Esposito, Raposo, Morgado, and Desa]{colombo2024saullm7bpioneeringlargelanguage}
Pierre Colombo, Telmo~Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre F.~T. Martins, Fabrizio Esposito, Vera~Lúcia Raposo, Sofia Morgado, and Michael Desa.
\newblock Saul{LM}-7{B}: {A} pioneering large language model for law, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.03883}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In Jian Su, Kevin Duh, and Xavier Carreras, editors, \emph{Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics, 2016.
\newblock URL \url{https://aclanthology.org/D16-1264/}.

\bibitem[Chollet(2019)]{chollet2019ARC}
François Chollet.
\newblock On the measure of intelligence, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.01547}.

\bibitem[Eldan and Li(2023)]{eldan2023tinystories}
Ronen Eldan and Yuanzhi Li.
\newblock Tiny{S}tories: {H}ow small can language models be and still speak coherent {E}nglish?, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.07759}.

\bibitem[Gunasekar et~al.(2024)Gunasekar, Zhang, Aneja, Mendes, Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, Salim, Shah, Behl, Wang, Bubeck, Eldan, Kalai, Lee, and Li]{gunasekar2024textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio Cesar~Teodoro Mendes, Allie~Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero~Conti Kauffmann, Gustavo~Henrique de~Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam~Tauman Kalai, Yin~Tat Lee, and Yuanzhi Li.
\newblock Textbooks are all you need, 2024.
\newblock URL \url{https://openreview.net/forum?id=Fq8tKtjACC}.

\bibitem[Maini et~al.(2024)Maini, Seto, Bai, Grangier, Zhang, and Jaitly]{maini2024rephrasingweb}
Pratyush Maini, Skyler Seto, Richard Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly.
\newblock Rephrasing the web: A recipe for compute and data-efficient language modeling.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics}. Association for Computational Linguistics, 2024.
\newblock URL \url{https://aclanthology.org/2024.acl-long.757/}.

\bibitem[Tang et~al.(2023)Tang, Han, Jiang, and Hu]{tang2023doessyntheticdatageneration}
Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu.
\newblock Does synthetic data generation of {LLM}s help clinical text mining?, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.04360}.

\bibitem[Gandhi et~al.(2024)Gandhi, Gala, Viswanathan, Wu, and Neubig]{gandhi2024bettersyntheticdataretrieving}
Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig.
\newblock Better synthetic data by retrieving and transforming existing datasets.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Findings of the Association for Computational Linguistics}. Association for Computational Linguistics, 2024.
\newblock URL \url{https://aclanthology.org/2024.findings-acl.385/}.

\bibitem[Wang et~al.(2023)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2023selfinstructaligninglanguagemodels}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-{I}nstruct: {A}ligning language models with self-generated instructions.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics}. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.acl-long.754/}.

\bibitem[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{peng2023instructiontuninggpt4}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with {GPT}-4, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.03277}.

\bibitem[Yang et~al.(2025)Yang, Band, Li, Candes, and Hashimoto]{yang2025synthetic}
Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candes, and Tatsunori Hashimoto.
\newblock Synthetic continued pretraining.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.
\newblock URL \url{https://openreview.net/forum?id=07yvxWDSla}.

\bibitem[Mitchell et~al.(2022)Mitchell, Lin, Bosselut, Finn, and Manning]{mitchell2022fastmodeleditingscale}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D Manning.
\newblock Fast model editing at scale.
\newblock In \emph{The Tenth International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=0DcZxeWfOPt}.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf}.

\bibitem[Meng et~al.(2023)Meng, Sharma, Andonian, Belinkov, and Bau]{meng2023masseditingmemorytransformer}
Kevin Meng, Arnab~Sen Sharma, Alex~J Andonian, Yonatan Belinkov, and David Bau.
\newblock Mass-editing memory in a transformer.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=MkbcAHIYgyS}.

\bibitem[Yehudai et~al.(2024)Yehudai, Carmeli, Mass, Arviv, Mills, Shnarch, and Choshen]{yehudai2024genie}
Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Eyal Shnarch, and Leshem Choshen.
\newblock Achieving human parity in content-grounded datasets generation.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=RjYKTQ0L0W}.

\bibitem[Aky{\"u}rek et~al.(2024)Aky{\"u}rek, Aky{\"u}rek, Choshen, Wijaya, and Andreas]{akyurek2024deductive}
Afra~Feyza Aky{\"u}rek, Ekin Aky{\"u}rek, Leshem Choshen, Derry Wijaya, and Jacob Andreas.
\newblock Deductive closure training of language models for coherence, accuracy, and updatability.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Findings of the Association for Computational Linguistics}. Association for Computational Linguistics, 2024.
\newblock URL \url{https://aclanthology.org/2024.findings-acl.584/}.

\bibitem[Lampinen et~al.(2025)Lampinen, Chaudhry, Chan, Wild, Wan, Ku, Bornschein, Pascanu, Shanahan, and McClelland]{lampinen2025generalizationlanguagemodelsincontext}
Andrew~K. Lampinen, Arslan Chaudhry, Stephanie C.~Y. Chan, Cody Wild, Diane Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, and James~L. McClelland.
\newblock On the generalization of language models from in-context learning and finetuning: a controlled study, 2025.
\newblock URL \url{https://arxiv.org/abs/2505.00661}.

\bibitem[Park et~al.(2025)Park, Zhang, and Tanaka]{park2025textitnewnewssystem2finetuning}
Core~Francisco Park, Zechen Zhang, and Hidenori Tanaka.
\newblock $\textit{New News}$: System-2 fine-tuning for robust integration of new knowledge, 2025.
\newblock URL \url{https://arxiv.org/abs/2505.01812}.

\bibitem[Sun et~al.(2020)Sun, Wang, Liu, Miller, Efros, and Hardt]{sun2020test}
Yu~Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei~A. Efros, and Moritz Hardt.
\newblock Test-time training with self-supervision for generalization under distribution shifts.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}. {PMLR}, 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/sun20b.html}.

\bibitem[Gandelsman et~al.(2022)Gandelsman, Sun, Chen, and Efros]{gandelsman2022test}
Yossi Gandelsman, Yu~Sun, Xinlei Chen, and Alexei Efros.
\newblock Test-time training with masked autoencoders.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/bcdec1c2d60f94a93b6e36f937aa0530-Paper-Conference.pdf}.

\bibitem[Sun et~al.(2024)Sun, Li, Dalal, Hsu, Koyejo, Guestrin, Wang, Hashimoto, and Chen]{sun2024TTT}
Yu~Sun, Xinhao Li, Karan Dalal, Chloe Hsu, Sanmi Koyejo, Carlos Guestrin, Xiaolong Wang, Tatsunori Hashimoto, and Xinlei Chen.
\newblock Learning to (learn at test time), 2024.
\newblock URL \url{https://arxiv.org/abs/2310.13807}.

\bibitem[Akyürek et~al.(2025)Akyürek, Damani, Zweiger, Qiu, Guo, Pari, Kim, and Andreas]{akyurek2025TTT}
Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and Jacob Andreas.
\newblock The surprising effectiveness of test-time training for few-shot learning, 2025.
\newblock URL \url{https://arxiv.org/abs/2411.07279}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{ouyang2022rlhf}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul~F Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf}.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022starbootstrappingreasoningreasoning}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock {STaR}: {B}ootstrapping reasoning with reasoning.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/639a9a172c044fbb64175b5fad42e9a5-Paper-Conference.pdf}.

\bibitem[Singh et~al.(2024)Singh, Co-Reyes, Agarwal, Anand, Patil, Garcia, Liu, Harrison, Lee, Xu, Parisi, Kumar, Alemi, Rizkowsky, Nova, Adlam, Bohnet, Elsayed, Sedghi, Mordatch, Simpson, Gur, Snoek, Pennington, Hron, Kenealy, Swersky, Mahajan, Culp, Xiao, Bileschi, Constant, Novak, Liu, Warkentin, Bansal, Dyer, Neyshabur, Sohl-Dickstein, and Fiedel]{singh2023beyond}
Avi Singh, John~D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter~J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron~T Parisi, Abhishek Kumar, Alexander~A Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin~Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura~A Culp, Lechao Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel.
\newblock Beyond human data: {S}caling self-training for problem-solving with language models.
\newblock \emph{Transactions on Machine Learning Research}, 2024.
\newblock URL \url{https://openreview.net/forum?id=lNAyUngGFK}.

\bibitem[{DeepSeek-AI}(2025)]{deepseekai2025r1}
{DeepSeek-AI}.
\newblock Deepseek-{R}1: {I}ncentivizing reasoning capability in {LLM}s via reinforcement learning, 2025.
\newblock URL \url{https://arxiv.org/abs/2501.12948}.

\bibitem[Schmidhuber(1987)]{schmidhuber1987meta}
Jürgen Schmidhuber.
\newblock Evolutionary principles in self-referential learning, 1987.
\newblock URL \url{https://people.idsia.ch/~juergen/diploma1987ocr.pdf}.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and Conwell]{hochreiter2001learning}
Sepp Hochreiter, A.~Steven Younger, and Peter~R. Conwell.
\newblock Learning to learn using gradient descent.
\newblock In Georg Dorffner, Horst Bischof, and Kurt Hornik, editors, \emph{ICANN}. Springer Berlin Heidelberg, 2001.
\newblock URL \url{https://link.springer.com/chapter/10.1007/3-540-44668-0_13}.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017maml}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the 34th International Conference on Machine Learning}, Proceedings of Machine Learning Research. PMLR, 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/finn17a.html}.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and Abbeel]{duan2016rl}
Yan Duan, John Schulman, Xi~Chen, Peter~L. Bartlett, Ilya Sutskever, and Pieter Abbeel.
\newblock R{L}$^2$: {F}ast reinforcement learning via slow reinforcement learning, 2016.
\newblock URL \url{https://arxiv.org/abs/1611.02779}.

\bibitem[Wang et~al.(2017)Wang, Kurth-Nelson, Tirumala, Soyer, Leibo, Munos, Blundell, Kumaran, and Botvinick]{wang2016learning}
Jane~X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel~Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.
\newblock Learning to reinforcement learn, 2017.
\newblock URL \url{https://arxiv.org/abs/1611.05763}.

\bibitem[Frans et~al.(2018)Frans, Ho, Chen, Abbeel, and Schulman]{frans2017meta}
Kevin Frans, Jonathan Ho, Xi~Chen, Pieter Abbeel, and John Schulman.
\newblock Meta learning shared hierarchies.
\newblock In \emph{The Sixth International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=SyX0IeWAW}.

\bibitem[Gupta et~al.(2018)Gupta, Mendonca, Liu, Abbeel, and Levine]{gupta2018meta}
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.
\newblock Meta-reinforcement learning of structured exploration strategies.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf}.

\bibitem[Sun et~al.(2025)Sun, Cetin, and Tang]{sun2025text}
Qi~Sun, Edoardo Cetin, and Yujin Tang.
\newblock Transformer-{S}quared: {S}elf-adaptive {LLM}s, 2025.
\newblock URL \url{https://arxiv.org/abs/2501.06252}.

\bibitem[Schmidhuber(1992)]{schmidhuber1992steps}
Jurgen Schmidhuber.
\newblock Steps towards `self-referential' neural learning: A thought experiment, 1992.
\newblock URL \url{https://people.idsia.ch/~juergen/selfref1992.pdf}.

\bibitem[Irie et~al.(2022)Irie, Schlag, Csord{\'a}s, and Schmidhuber]{irie2022modern}
Kazuki Irie, Imanol Schlag, R{\'o}bert Csord{\'a}s, and J{\"u}rgen Schmidhuber.
\newblock A modern self-referential weight matrix that learns to modify itself.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/irie22b.html}.

\bibitem[Tan et~al.(2024)Tan, Zhang, and Fu]{tan2023massive}
Chenmien Tan, Ge~Zhang, and Jie Fu.
\newblock Massive editing for large language models via meta learning, 2024.
\newblock URL \url{https://arxiv.org/abs/2311.04661}.

\bibitem[Hu et~al.(2023)Hu, Mitchell, Manning, and Finn]{hu2023metalearningonlineadaptationlanguage}
Nathan Hu, Eric Mitchell, Christopher Manning, and Chelsea Finn.
\newblock Meta-learning online adaptation of language models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.268/}.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, and Kaplan]{bai2022constitutionalaiharmlessnessai}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.
\newblock Constitutional {AI}: {H}armlessness from {AI} feedback, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2212.08073}.

\bibitem[Lee et~al.(2024)Lee, Phatale, Mansoor, Mesnard, Ferret, Lu, Bishop, Hall, Carbune, Rastogi, and Prakash]{lee2024rlaif}
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash.
\newblock {RLAIF} vs. {RLHF}: {S}caling reinforcement learning from human feedback with {AI} feedback.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}, ICML '24. JMLR.org, 2024.

\bibitem[Pang et~al.(2024)Pang, Wang, Li, Chen, Xu, Zhang, and Yu]{pang2024selfimprove}
Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, and Yang Yu.
\newblock Language model self-improvement by reinforcement learning contemplation.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=38E4yUbrgr}.

\bibitem[Wang et~al.(2025)Wang, He, Liang, Zhang, Bansal, Wei, Zhang, and Yao]{wang2025cream}
Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, and Huaxiu Yao.
\newblock {CREAM}: {C}onsistency regularized self-rewarding language models.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.
\newblock URL \url{https://openreview.net/forum?id=Vf6RDObyEF}.

\bibitem[Song et~al.(2025)Song, Zhang, Eisenach, Kakade, Foster, and Ghai]{song2025mind}
Yuda Song, Hanlin Zhang, Carson Eisenach, Sham~M. Kakade, Dean Foster, and Udaya Ghai.
\newblock Mind the gap: {E}xamining the self-improvement capabilities of large language models.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.
\newblock URL \url{https://openreview.net/forum?id=mtJSMcF3ek}.

\bibitem[Huang et~al.(2023)Huang, Gu, Hou, Wu, Wang, Yu, and Han]{huang2023selfimprove}
Jiaxin Huang, Shixiang Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.
\newblock Large language models can self-improve.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.67/}.

\bibitem[Prasad et~al.(2024)Prasad, Yuan, Pang, Xu, Fazel-Zarandi, Bansal, Sukhbaatar, Weston, and Yu]{prasad2024selfconsistencypreferenceoptimization}
Archiki Prasad, Weizhe Yuan, Richard~Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, and Jane Yu.
\newblock Self-consistency preference optimization, 2024.
\newblock URL \url{https://arxiv.org/abs/2411.04109}.

\bibitem[Huang et~al.(2025)Huang, Block, Foster, Rohatgi, Zhang, Simchowitz, Ash, and Krishnamurthy]{huang2025self}
Audrey Huang, Adam Block, Dylan~J Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz, Jordan~T. Ash, and Akshay Krishnamurthy.
\newblock Self-improvement in language models: {T}he sharpening mechanism.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.
\newblock URL \url{https://openreview.net/forum?id=WJaUkwci9o}.

\bibitem[Zuo et~al.(2025)Zuo, Zhang, Sheng, Qu, Cui, Zhu, Li, Zhang, Long, Hua, Qi, Sun, Ma, Yuan, Ding, and Zhou]{zuo2025ttrltesttimereinforcementlearning}
Yuxin Zuo, Kaiyan Zhang, Li~Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, and Bowen Zhou.
\newblock {TTRL}: {T}est-time reinforcement learning, 2025.
\newblock URL \url{https://arxiv.org/abs/2504.16084}.

\bibitem[Shafayat et~al.(2025)Shafayat, Tajwar, Salakhutdinov, Schneider, and Zanette]{shafayat2025largereasoningmodelsselftrain}
Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette.
\newblock Can large reasoning models self-train?, 2025.
\newblock URL \url{https://arxiv.org/abs/2505.21444}.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Bi, Zhang, Zhang, Li, Wu, and Guo]{shao2024deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.~K. Li, Y.~Wu, and Daya Guo.
\newblock {DeepSeekMath}: {P}ushing the limits of mathematical reasoning in open language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.03300}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms, 2017.
\newblock URL \url{https://arxiv.org/abs/1707.06347}.

\bibitem[Gilks and Wild(1992)]{gilks1992rejectionsampling}
W.~R. Gilks and P.~Wild.
\newblock Adaptive rejection sampling for gibbs sampling.
\newblock \emph{Journal of the Royal Statistical Society}, 1992.
\newblock URL \url{http://www.jstor.org/stable/2347565}.

\bibitem[Kumar et~al.(2022)Kumar, Hong, Singh, and Levine]{kumar2022preferofflinereinforcementlearning}
Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine.
\newblock When should we prefer offline reinforcement learning over behavioral cloning?
\newblock In \emph{The Tenth International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=AP1MKT37rJ}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, El-Showk, Elhage, Hatfield-Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan]{bai2022traininghelpfulharmlessassistant}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2204.05862}.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Lu, Tan, Zhou, and Zhou]{yuan2023scalingrelationshiplearningmathematical}
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou.
\newblock Scaling relationship on learning mathematical reasoning with large language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2308.01825}.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distill}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network, 2015.
\newblock URL \url{https://arxiv.org/abs/1503.02531}.

\bibitem[Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lo{RA}: {L}ow-rank adaptation of large language models.
\newblock In \emph{The Tenth International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[McCloskey and Cohen(1989)]{mccloskey1989catastrophicinterference}
Michael McCloskey and Neal~J. Cohen.
\newblock Catastrophic interference in connectionist networks: {T}he sequential learning problem, 1989.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0079742108605368}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Mirza, Xiao, Courville, and Bengio]{goodfellow2015catastrophicforgetting}
Ian~J. Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio.
\newblock An empirical investigation of catastrophic forgetting in gradient-based neural networks.
\newblock In \emph{The Second International Conference on Learning Representations}, 2014.
\newblock URL \url{https://openreview.net/forum?id=oXSw7laxwUpln}.

\bibitem[Hu et~al.(2020)Hu, Wang, Jia, Wang, Chen, Hao, Wu, and Fan]{hu2020learning}
Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and Changjie Fan.
\newblock Learning to utilize shaping rewards: {A} new approach of reward shaping.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf}.

\bibitem[Xie et~al.(2024)Xie, Zhao, Wu, Liu, Luo, Zhong, Yang, and Yu]{xie2023text2reward}
Tianbao Xie, Siheng Zhao, Chen~Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu.
\newblock {Text2Reward}: {R}eward shaping with language models for reinforcement learning, 2024.
\newblock URL \url{https://arxiv.org/abs/2309.11489}.

\bibitem[Fu et~al.(2025)Fu, Zhao, Yao, Wang, Han, and Xiao]{fu2025reward}
Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi~Han, and Yanghua Xiao.
\newblock Reward shaping to mitigate reward hacking in {RLHF}, 2025.
\newblock URL \url{https://arxiv.org/abs/2502.18770}.

\bibitem[Fang et~al.(2025)Fang, Jiang, Wang, Ma, Shi, Wang, He, and Chua]{fang2025alphaeditnullspaceconstrainedknowledge}
Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Jie Shi, Xiang Wang, Xiangnan He, and Tat-Seng Chua.
\newblock Alpha{E}dit: {N}ull-space constrained model editing for language models.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.
\newblock URL \url{https://openreview.net/forum?id=HvSytvg3Jh}.

\bibitem[Cheung et~al.(2019)Cheung, Terekhov, Chen, Agrawal, and Olshausen]{cheung2019superpositionmodels}
Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen.
\newblock Superposition of many models into one.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf}.

\bibitem[Villalobos et~al.(2024)Villalobos, Ho, Sevilla, Besiroglu, Heim, and Hobbhahn]{villalobos2024rundatalimitsllm}
Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn.
\newblock Will we run out of data? {L}imits of {LLM} scaling based on human-generated data, 2024.
\newblock URL \url{https://arxiv.org/abs/2211.04325}.

\bibitem[OpenAI et~al.(2024)OpenAI, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, et~al.]{openai2024gpt4technicalreport}
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et~al.
\newblock {GPT}-4 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and He]{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock Deep{S}peed: {S}ystem optimizations enable training deep learning models with over 100 billion parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, KDD '20. Association for Computing Machinery, 2020.
\newblock URL \url{https://doi.org/10.1145/3394486.3406703}.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu, Joseph~E. Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with {P}aged{A}ttention.
\newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.
\newblock URL \url{https://dl.acm.org/doi/10.1145/3600006.3613165}.

\end{thebibliography}
