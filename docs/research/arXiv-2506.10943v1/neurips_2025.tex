\documentclass{article}

% \bibliographystyle{plainnat}
% For numeric citations, use this instead:
\PassOptionsToPackage{numbers}{natbib}
\bibliographystyle{unsrtnat}

%\usepackage{neurips_2025}
\usepackage[preprint]{neurips_2025}
% \usepackage[final]{neurips_2025}
% \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{xcolor}         % colors

%%% BEGIN CUSTOM IMPORTS %%%
\usepackage{listings}
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{enumitem}

\usepackage{amsmath, amssymb}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}

\tcbset{
  questionstyle/.style={
    colback=gray!10,
    colframe=black,
    fonttitle=\bfseries,
    title=Prompt,
    boxrule=0.5mm,
    rounded corners
  }
}

\lstdefinestyle{promptstyle}{
    basicstyle=\small\ttfamily,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    frame=single,
    frameround=tttt,
    backgroundcolor=\color{gray!10},
    showstringspaces=false,
    tabsize=2
}

% \newcommand{\pulkit}[1]{{\color{orange}Pulkit: #1}}
% \newcommand{\jyo}[1]{{\color{blue}Jyo: #1}}
% \newcommand{\adam}[1]{{\color{red}Adam: #1}}
% \newcommand{\han}[1]{{\color{pink}Han: #1}}
% \newcommand{\yoon}[1]{{\color{purple}Yoon: #1}}

\newcommand{\methodname}{Self-Adapting LLMs (SEAL)\xspace}
\newcommand{\methodacronym}{SEAL\xspace}
\newcommand{\methodacronymnospace}{SEAL}

% \renewcommand{\pulkit}[1]{{}}
% \renewcommand{\jyo}[1]{{}}
% \renewcommand{\adam}[1]{{}}
% \renewcommand{\han}[1]{{}}
% \renewcommand{\yoon}[1]{{}}


%%% END CUSTOM IMPORTS %%%

\title{Self-Adapting Language Models}


\author{
  Adam Zweiger\thanks{Equal contribution.} \quad
  Jyothish Pari\footnotemark[1]\hspace{0.42em}\textsuperscript{\dag} \quad
  Han Guo \quad
  Ekin Akyürek \quad
  Yoon Kim \quad
  Pulkit Agrawal\textsuperscript{\dag} \vspace{1mm} \\
  Massachusetts Institute of Technology \vspace{1mm} \\
  \texttt{\{adamz, jyop, hanguo, akyurek, yoonkim, pulkitag\}@mit.edu}
  \vspace{-2mm}
}

\begin{document}

\maketitle

\begingroup
\renewcommand\thefootnote{\dag} % Use dagger for this specific note
\footnotetext{Improbable AI Lab, CSAIL MIT}
\endgroup

\begin{abstract}
Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce \textbf{Se}lf-\textbf{A}dapting \textbf{L}LMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a \textit{self-edit}---a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop, using the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's  generation to parameterize and control its own adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation in response to new data. Our website and code is available at \url{https://jyopari.github.io/posts/seal}.
\end{abstract}

\section{Introduction}

Large language models (LLMs) pretrained on vast text corpora exhibit remarkable abilities in language understanding and generation \citep{brown2020language,touvron2023llama,grattafiori2024llama3herdmodels,groeneveld2024olmo,qwen2025qwen25technicalreport}. However, adapting these powerful models for specific tasks~\citep{gururangan2020finetune}, integrating new information \citep{zhu2021modifying}, or mastering novel reasoning skills \citep{chollet2024arc} remains challenging due to the limited availability of task-specific data. In this paper, we explore an intriguing hypothesis: can an LLM self-adapt by transforming or generating its own training data and learning procedure?

As an analogy, consider a human student preparing for the final exam of a machine learning class. Many students rely on their notes to prepare for the exam. These notes are often derived from the lecture content, textbooks, or information available on the internet. Instead of relying on the raw content, assimilating and rewriting the information in the form of notes often improves the ability of students to understand the content and answer exam questions. This phenomenon of reinterpreting and augmenting external knowledge in a way that is easier to understand is not limited to just taking exams, but seems to be universally true of human learning across tasks. Furthermore, different humans assimilate information in different ways---some might condense the information into a visual diagram, some into text, or some might rely more on concrete mathematical descriptions. 

Such assimilation, restructuring, or rewriting of data as part of the learning process is in contrast with how large language models (LLMs) are typically trained and deployed. Given a new task, current LLMs consume and learn from the task data ``as-is'' via finetuning or in-context learning \citep{wei2022finetuned,rozière2024codellamaopenfoundation,chen2023meditron70bscalingmedicalpretraining,colombo2024saullm7bpioneeringlargelanguage}. However, such data may not be in an optimal format (or volume) for learning, and current approaches do not enable models to develop bespoke strategies for how to best transform and learn from their training data.

As a step towards scalable and efficient adaptation of language models, we propose equipping LLMs with the ability to generate their own training data and finetuning directives for utilizing such data. In particular, we introduce a reinforcement learning algorithm that trains LLMs to generate \textbf{``self-edits''}---natural-language instructions that specify the data and, optionally, the optimization hyperparameters for updating the model's weights (see Figure~\ref{fig:self_edit}). We refer to such models as \textbf{Se}lf-\textbf{A}dapting \textbf{L}LMs (\methodacronym).  

We evaluate \methodacronym on two applications. We first consider the task of integrating new factual knowledge into an LLM. Rather than finetuning directly on the passage text, we finetune on synthetic data generated by the \methodacronym model. Our results show that, following reinforcement learning (RL) training, finetuning on self-generated synthetic data improves question-answering performance on the no-passage-in-context variant of SQuAD~\citep{rajpurkar2016squad} from 33.5\% to 47.0\%. Notably, self-generated data from \methodacronym outperforms synthetic data generated by GPT-4.1.

We further evaluate \methodacronym on few-shot learning on a simplified subset of the ARC-AGI benchmark~\citep{chollet2019ARC}, where the model leverages a set of \textit{tools} to autonomously select both synthetic data augmentations and optimization hyperparameters (e.g., learning rate, training epochs, selective loss computation over token types). Our experiments demonstrate that automatic selection and configuration of these tools using \methodacronym enhances performance compared to both standard in-context learning (ICL) and self-editing \textit{without} RL training to use the tools effectively. These results collectively show that \methodacronym is a versatile framework for enabling language models to self-adapt.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/main_fig.pdf}
    \caption{\textbf{Overview of \methodacronym.} In each RL outer loop iteration, the model generates candidate self-edits (SE)---directives on how to update the weights---applies updates, evaluates performance on a downstream task, and uses the resulting rewards to improve the self-edit generation policy.}
    \vspace{-8pt}
\label{fig:self_edit}
\end{figure}

\section{Related Work}
\paragraph{Synthetic Data Generation.} Creating synthetic data for training is increasingly common, from large-scale pretraining datasets \citep{eldan2023tinystories, gunasekar2024textbooks, maini2024rephrasingweb} to task-specific data augmentation \citep{tang2023doessyntheticdatageneration, gandhi2024bettersyntheticdataretrieving} and instruction-tuning sets \citep{wang2023selfinstructaligninglanguagemodels, peng2023instructiontuninggpt4}. For incorporation of a smaller-sized corpus, \citet{yang2025synthetic} use synthetic data generation via graph-based prompting. \methodacronym builds on this line of work by using reinforcement learning to train a \textit{generative policy} that directly maximizes the downstream utility of synthetic data when applied for gradient-based self-updates, rather than relying on static or heuristic generation strategies that are manually tuned and therefore potentially not scalable or optimal. 

\paragraph{Knowledge Updating.}
\label{subsection:ke}
Several recent works aim to modify or inject factual knowledge into language models via weight updates. Some methods attempt to directly locate specific parameters that correspond to individual facts \citep{mitchell2022fastmodeleditingscale,meng2022locating,meng2023masseditingmemorytransformer}. Others propose generating additional finetuning data using the information in context \citep{yehudai2024genie,akyurek2024deductive,yang2025synthetic,lampinen2025generalizationlanguagemodelsincontext,park2025textitnewnewssystem2finetuning}. We adopt the latter strategy, following \citet{akyurek2024deductive}, who propose generating logical implications of a fact and finetuning on them, and \citet{lampinen2025generalizationlanguagemodelsincontext}, who show that implication-based finetuning can even outperform in-context learning. We build on these approaches by \textit{training} models through RL to generate more optimal finetuning data. \citet{park2025textitnewnewssystem2finetuning} show that prompting language models to generate question–answer (QA) pairs directly can outperform implication-style prompting. Because the \methodacronym{} framework is agnostic to the prompt and format of the self-edit data, it can also be trained to generate QA pairs or other output formats, as explored in \S\ref{app:prompting}. With sufficient compute, a cold-start setup, where the model discovers the most effective format without guidance from prompting, may also be viable.

\paragraph{Test-Time Training.} Test-Time Training (TTT) temporarily adapts model weights based on the input the model receives \citep{sun2020test, gandelsman2022test, sun2024TTT, akyurek2025TTT}. \citet{akyurek2025TTT} show that combining TTT with ICL enables gradient-updates to outperform standard ICL in the few-shot setting. \methodacronym can be viewed as incorporating a round of TTT in its inner-loop optimization, leveraging TTT's efficiency relative to full-scale training to perform multiple updates and reward the generated data that yields the greatest performance gain. Although our method is trained using single-example TTT episodes, we demonstrate in the knowledge incorporation setting that it generalizes to the continued pretraining setting---where placing data directly in context is no longer feasible.

\paragraph{Reinforcement Learning for LLMs.}
Reinforcement learning has played a central role in improving LLM behavior, originally through RLHF \citep{ouyang2022rlhf}. More recently, RL with verifiable rewards has been applied to boost reasoning performance by optimizing the model directly for task success \citep{zelikman2022starbootstrappingreasoningreasoning, singh2023beyond,deepseekai2025r1}. \methodacronym applies RL not to optimize final answers or trace revisions, but to optimize the generation of \textit{self-edit} data that is then used for weight updates. 

\paragraph{Meta-Learning and Self-Modifying Systems.} \methodacronym embodies meta-learning principles \citep{schmidhuber1987meta,hochreiter2001learning,finn2017maml} by learning an adaptation strategy---how to generate effective self-edits---via its outer optimization loop. The goal is to learn \textit{how to learn} efficiently from task contexts. Meta-learning has similarly been applied in reinforcement learning \citep{duan2016rl,wang2016learning,frans2017meta,gupta2018meta, sun2025text}, where models are trained with a meta-objective to rapidly adapt to new tasks. A natural extension of this line of work is self-referential networks, where models modify their own parameters \citet{schmidhuber1992steps, irie2022modern}. In the domain of large language models, recent work has applied meta-learning principles to improve LLM adaptation \citep{tan2023massive, hu2023metalearningonlineadaptationlanguage}. Notably, \citet{hu2023metalearningonlineadaptationlanguage} trained a smaller model to output token-specific weights during finetuning on a corpus, addressing a knowledge incorporation task similar to ours. However, \methodacronym offers greater generality across domains by leveraging the model's existing generative capabilities to parametrize updates.

\paragraph{Self-Improvement.}

Several recent works fall under the umbrella of self-improvement or self-training. Methods such as RLAIF \citep{bai2022constitutionalaiharmlessnessai, lee2024rlaif} and self-rewarding language models \citep{pang2024selfimprove, wang2025cream} use the model itself to provide reward signals, leveraging the observation that judging outputs is often easier than generating them \citep{song2025mind}. Other recent works improve performance on mathematical tasks by using majority-vote or model confidence as reinforcement learning rewards, enabling performance improvement without access to ground-truth labels \citep{huang2023selfimprove, prasad2024selfconsistencypreferenceoptimization, huang2025self, zuo2025ttrltesttimereinforcementlearning, shafayat2025largereasoningmodelsselftrain}. However, all of these methods are fundamentally limited by the model's current evaluation abilities and self-consistency. In contrast, we view self-improvement through interaction with external data as a more powerful and scalable path. \methodacronym learns how to best utilize this external data for self-improvement.

\section{Methods}
\label{sec:methods}

We propose \methodname, a framework that enables language models to improve themselves by generating their own synthetic data and optimization parameters (``self-edits'') in response to new data. The model is trained to produce these self-edits directly through token generation with the data provided in the model's context. Self-edit generation is learned via reinforcement learning (RL) where the model is rewarded for generating self-edits ($\texttt{SE}$) that, when applied, improve the model's performance at the target task. \methodacronym{} can therefore be interpreted as an algorithm with two nested loops: an \textit{outer RL loop}, which optimizes the self-edit generation, and an \textit{inner update loop}, which uses the generated self-edit to update the model via gradient descent. Our method can be seen as an instance of meta-learning where we meta-learn how to generate effective self-edits. 

\subsection{General Framework}

Let $\theta$ denote the parameters of the language model $\texttt{LM}_\theta$. \methodacronym operates on individual task instances $(C,\tau)$ where $C$ is a context containing information relevant to the task, and $\tau$ defines the downstream evaluation used to assess the model's adaptation. For example, in knowledge incorporation, $C$ is the passage intended to be integrated into the model's internal knowledge, and $\tau$ is a set of questions and associated answers about the passage. In few-shot learning, $C$ includes few-shot demonstrations of a novel task, and $\tau$ is the query input and ground-truth output. Given $C$, the model generates a self-edit $\texttt{SE}$—the form of which varies by domain (see \S\ref{subsec:domain_instantiations})—and updates its parameters via supervised finetuning: $\theta' \leftarrow \texttt{SFT}(\theta, \texttt{SE})$. 

We optimize the self-edit generation process using reinforcement learning: the model takes an \textit{action} (generating $\texttt{SE}$), receives a \textit{reward} $r$ based on $\texttt{LM}_{\theta'}$'s performance on $\tau$, and updates its policy to maximize expected reward:
\vspace{-1pt}
\begin{equation}
\label{eqn:objective}
\mathcal{L}_{\text{RL}}(\theta_t) :=\, 
 -\mathbb{E}_{(C, \tau) \sim \mathcal{D}} \left[ 
    \mathbb{E}_{\texttt{SE} \sim \text{LM}_{\theta_t}(\cdot \mid C)} 
    \left[ r(\texttt{SE}, \tau, \theta_t) \right] 
\right].
\end{equation}

\begin{wrapfigure}{R}{0.44\textwidth}
\vspace{-23pt}
\begin{minipage}{0.44\textwidth}
\begin{algorithm}[H]
\small
\caption{\methodname: \\Self-Edit Reinforcement Learning Loop}
\label{alg:seel_rl_simplified}
\begin{algorithmic}[1]
\STATE \textbf{Input:} LM$_\theta$, dataset $\mathcal{D} = \{(C, \tau)\}$
\FOR{outer iteration $t = 1, 2, \dots$}
    \STATE Sample $(C, \tau) \sim \mathcal{D}$
    \STATE Generate self-edit $\texttt{SE} \sim \text{LM}_{\theta}(\cdot \mid C)$
    \STATE Inner Loop Update: $\theta_t' \leftarrow \texttt{SFT}(\theta_t, \texttt{SE})$
    \STATE Evaluate: $\texttt{Ans} \sim \text{LM}_{\theta_t'}(\cdot \mid \tau)$
    \STATE Compute reward: $r \leftarrow r(\texttt{Ans} , \tau)$
    \STATE Update: $\theta_{t+1} \leftarrow \texttt{RL\_Update}(\theta_t, r, \texttt{SE})$
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-10pt}
\end{wrapfigure}

Unlike in standard RL setups, the reward assigned to a given action in our setting depends on the model \textit{parameters} $\theta$ at the time the action is taken (since $\theta$ is updated to $\theta'$, which is then evaluated). As a result, the underlying RL state must include the policy's parameters and is given by $(C, \theta)$, even though the policy's observation is limited to $C$ (placing $\theta$ directly in context is infeasible). The implication of this is that (state, action, reward) triples collected with a previous version of the model, $\theta_{\text{old}}$, may become stale and misaligned for the current model $\theta_{\text{current}}$. For this reason, we adopt an on-policy approach, in which self-edits are sampled from---and, crucially, rewards are computed using---the current model.

We experimented with various on-policy methods such as Group Relative Policy Optimization (GRPO) \citep{shao2024deepseekmath} and Proximal Policy Optimization (PPO) \citep{schulman2017proximal}, but found the training to be unstable. Instead, we adopt ReST$^\text{\textit{EM}}$ \citep{singh2023beyond}, a simpler approach based on filtered behavior cloning---also known as ``rejection sampling + SFT'' \citep{gilks1992rejectionsampling, kumar2022preferofflinereinforcementlearning, bai2022traininghelpfulharmlessassistant, zelikman2022starbootstrappingreasoningreasoning, yuan2023scalingrelationshiplearningmathematical}.

ReST$^\text{\textit{EM}}$ can be viewed as an expectation-maximization (EM) procedure: the \textbf{E-step} samples candidate outputs from the current model policy, and the \textbf{M-step} reinforces only those samples that receive positive reward through supervised finetuning. This approach optimizes an approximation of our objective~\eqref{eqn:objective} under the binary reward:
\begin{equation}
\label{eqn:reward}
r(\texttt{SE}, \tau, \theta_t) =
\begin{cases}
1 & \text{If on } \tau \text{, adaptation using } \texttt{SE} \text{ improves } \text{LM}_{\theta_t}\text{'s performance} \footnotemark \\
0 & \text{Otherwise}
\end{cases}
\end{equation}
\footnotetext{The reward may also be assigned to the single self-edit that yields the greatest improvement among sampled candidates, which we do in knowledge incorporation, rather than to all edits that yield a positive improvement.}

More precisely, in optimizing \eqref{eqn:objective}, we must compute the gradient $\nabla_{\theta_t}\mathcal{L}_{\text{RL}}$. However, as we noted, the reward term $r(\texttt{SE}, \tau, \theta_t)$ depends on $\theta_t$ in our setup but is not differentiable. We address this by treating the reward as fixed with respect to $\theta_t$. With this approximation, the Monte-Carlo estimator for a minibatch of \(N\) contexts and \(M\) sampled self-edits per context becomes
\begin{align}
\nabla_{\theta_t}\mathcal{L}_{\text{RL}}
&\approx -\frac{1}{NM}\sum_{i=1}^{N}\sum_{j=1}^{M}
r_{ij}\,
\nabla_{\theta_t}\log p_{\theta_t}(\texttt{SE}_{ij} \mid C_i) \\
&= -\frac{1}{NM}\sum_{i=1}^{N}\sum_{j=1}^{M}
r_{ij}\,
\sum_{s=1}^T \nabla_{\theta_t}\log p_{\theta_t}(y^{(i,j)}_s \mid y^{(i,j)}_{<s}, C_i),
\label{eqn:sft}
\end{align}
where \(p_{\theta_t}\) denotes the model's autoregressive distribution and \(y^{(i,j)}_s\) is the $s^\text{th}$ token of self-edit \(\texttt{SE}_{ij}\), the $j^\text{th}$ sample for context $C_i$. Since sequences with $r=0$ can be ignored in \eqref{eqn:sft}, we have shown that ReST$^{\text{\textit{EM}}}$, with simple ``SFT on good self-edits,'' indeed optimizes \eqref{eqn:objective} under the binary reward \eqref{eqn:reward} (with a stop-gradient applied to the reward term). 
The \methodacronym training loop is summarized in Alg.~\ref{alg:seel_rl_simplified}.

Finally, we note that while the implementation in this work uses a single model for both generating self-edits and learning from these self-edits, it is also possible to decouple these roles. In such a ``teacher-student'' formulation \citep{hinton2015distill}, a student model would be updated using edits proposed by a separate teacher model. The teacher would then be trained via RL to generate edits that maximize student improvement.

\subsection{Domain Instantiations}
\label{subsec:domain_instantiations}
We instantiate the \methodacronym framework in two distinct domains: knowledge incorporation and few-shot learning. These domains were chosen to highlight two complementary forms of model adaptation: (1) the ability to integrate new information into a model's weights so that it can be recalled without relying on context (evaluated using a no-context variant of SQuAD) and (2) the ability to generalize to novel tasks after seeing only a small number of examples (evaluated using ARC).

\paragraph{Knowledge Incorporation.} 
Our goal is to efficiently incorporate the information provided in a passage into the model's weights. A promising recent approach involves using a language model to generate content derived from the passage, followed by finetuning on both the original passage and the generated content \citep{yehudai2024genie, akyurek2024deductive,yang2025synthetic,lampinen2025generalizationlanguagemodelsincontext,park2025textitnewnewssystem2finetuning}. While the form of generated content may vary, we adopt what we consider the canonical format: \textit{implications derived from the passage}. This approach, introduced in deductive closure training \citep{akyurek2024deductive}, converts a given context $C$ into a set of implications $\texttt{SE} = \{s_1, s_2, \dots, s_n\}$ by prompting the model to ``List several implications derived from the content.'' The output may include inferences, logical consequences, or restatements of the original passage. In \S\ref{app:prompting}, we also explore alternative prompts such as ``rewrite the passage in different ways'' or ``rewrite in a question-answer format'' and show that our method improves performance by similar or greater margins regardless of the base prompt.

\begin{figure}[ht]
    \includegraphics[width=1\linewidth]{figures/knowledge_setup.pdf}
    \caption{\textbf{Knowledge Incorporation Setup.} Given a new passage, the model generates synthetic data (the \textit{self-edit}) in the form of ``implications'' of the passage. We then finetune on these outputs using LoRA. The updated model is evaluated on questions about the passage \textit{without} access to the original text, and the resulting accuracy serves as the reward signal for reinforcement learning.}
    \label{fig:cpt_inner_loop}
\end{figure}

These self-generated statements form the training data for a supervised finetuning (SFT) update: we compute the standard causal language-modeling loss over each sequence $s_i$ and update the model parameters, yielding $\theta'$. Since the amount of data per update is small and the number of updates we do in total is large, we use low-rank adapters (LoRA \citep{hu2022lora}) for efficient, lightweight tuning. Finally, the adapted model $\texttt{LM}_{\theta'}$ is evaluated on the task $\tau$. This process is shown in Figure~\ref{fig:cpt_inner_loop}.

During RL training, the adapted model's accuracy on $\tau$ defines the reward $r$ that drives the outer RL optimization. This trains the model to restructure the passage in a way that is most effective for assimilation via finetuning.

\paragraph{Few-Shot Learning.}
\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{figures/few_shot_setup.pdf}
\caption{\textbf{Few-Shot Learning with \methodacronym.} Left: example ARC demonstrations. Center: the model generates a self-edit specifying augmentations and training hyperparameters. Right: the adapted model is evaluated on a held-out test input.}
\label{fig:arc_inner_loop}
\vspace{-4pt}
\end{figure}

The Abstraction and Reasoning Corpus (ARC) \citep{chollet2024arc} is a benchmark designed to test abstract reasoning and generalization from very limited examples. Each task includes a small set of input-output demonstrations and a held-out test input whose correct output must be predicted.

We adopt the test-time training (TTT) protocol of \citet{akyurek2025TTT}, where augmentations of the few-shot examples are used to perform gradient-based adaptation. Rather than relying on manually tuned heuristics for selecting augmentations and optimization settings, we train \methodacronym{} to learn these decisions. This setting tests whether \methodacronym{} can autonomously configure the adaptation pipeline---determining which augmentations to apply and what optimization parameters to use.

To implement this, we define a set of \textbf{tools}, each of which is a pre-defined function from \citet{akyurek2025TTT} that transforms data or specifies training parameters. These include:
\vspace{-4pt}
\begin{itemize}[left=0pt, itemsep=0pt]
    \item \textbf{Data augmentations:} rotations, flips, reflections, transpositions, resizing operations (e.g., changing grid resolution), and chained or repeated transformations.
    \vspace{-2pt}
    \item \textbf{Optimization parameters:} learning rate, number of training epochs, and whether the loss is computed over all tokens or only output tokens.
\end{itemize}

The model is prompted with a task's few-shot demonstrations and generates a self-edit, which in this case is a specification of which tools to invoke and how to configure them, as shown in Figure~\ref{fig:arc_inner_loop}. This self-edit is then applied to adapt the model via LoRA-based finetuning. The adapted model is evaluated on the held-out test input, and the result determines the reward for training the self-edit generation policy.

\section{Results}

In this section we empirically evaluate \methodacronym{} across our two adaptation domains: few-shot learning and knowledge incorporation. Full training, hyperparameter, and evaluation details are provided in \S\ref{app:exp_details_few_shot} and \S\ref{app:exp_details}.

\subsection{Few-Shot Learning}

We conduct our experiments using \texttt{Llama-3.2-1B-Instruct}, a small open-source model with no ARC-specific pretraining. Since most ARC tasks are challenging for models that have not been pretrained on ARC, we curate a subset of 11 tasks from the ARC training set and 8 from the evaluation set, filtered to ensure that they are solvable under optimal TTT configurations for a base \texttt{Llama-3.2-1B-Instruct}. While this is a small number of examples, note that \citet{akyurek2025TTT} used the same TTT configuration for all tasks, and thus we do not need a large training set for learning a fixed self-edit. More details are included in \S\ref{app:exp_details_few_shot}.

The model is trained using ReST$^\text{\textit{EM}}$ by sampling 15 self-edits per training task. Each self-edit is applied individually to generate 15 updated models, which are then evaluated on the corresponding held-out test example. We reinforce only those self-edits that lead to correctly adapted models, i.e., models that produce the correct output for the test input after adaptation.

After training, we evaluate the model by generating 5 self-edits per held-out evaluation task and apply each one independently. We then report the percentage of self-edits that lead to correct outputs, yielding a success rate that reflects the quality of the learned self-edit generation policy.

We compare against the following baselines:

\begin{enumerate}[left=0pt]
\item \textbf{ICL (In-Context Learning):} \texttt{Llama-3.2-1B-Instruct} is prompted with the given few-shot examples using \citet{akyurek2025TTT}'s protocol, and directly queried on the test input.
\vspace{-2pt}
\item \textbf{TTT + Self-Edit (w/o prior RL):} \texttt{Llama-3.2-1B-Instruct} performs test-time training (TTT) using few-shot examples and synthetic augmentations, but without any prior RL to optimize which augmentations or training configurations to use. 
\vspace{-2pt}
\item \textbf{Oracle TTT:} The model performs test-time training (TTT) using the optimal human-crafted configuration from \citet{akyurek2025TTT}. This provides an upper bound of our method. 
\end{enumerate}

\begin{table}[ht]
    \vspace{-6pt}
    \centering
    \begin{tabular}{l c}
        \toprule
        \textbf{Method} & \textbf{Success Rate (\%)} \\
        \midrule
        ICL & 0 \\
        TTT + Self-Edit (w/o prior RL) & 20 \\
        \methodacronym & 72.5 \\
        Oracle TTT & 100 \\
        \bottomrule
    \end{tabular}
    \label{tab:methods_comparison}
    \vspace{6pt}
    \caption{Few-shot Abstract Reasoning}
    \vspace{-16pt}
\end{table}

We record results in Table \ref{tab:methods_comparison}. \methodacronym{} substantially improves adaptation success rate compared to baselines: 72.5\% vs. 20\% (with self-edits from the base model without RL training) and 0\% (no adaptation), though performance remains below Oracle TTT, suggesting room for further improvement.

\subsection{Knowledge Incorporation}

We experiment with \texttt{Qwen2.5-7B} on incorporating novel factual content from SQuAD passages \citep{rajpurkar2016squad}. We use the relatively simple SQuAD dataset because its passages can be fully ``understood'' by the base model in-context, yet the model cannot reliably answer questions about them \textit{without} that context. We do 2 rounds of ReST$^\text{\textit{EM}}$ with a batch of $50$ contexts (see \S\ref{app:exp_details} for further details). We compare \methodacronym on knowledge incorporation against the following baseline approaches:

\begin{enumerate}[left=0pt]
\item \textbf{Base Model:} The pretrained model is evaluated on downstream QA tasks without any adaptation or access to the passage.
\vspace{-2pt}
\item \textbf{Train on Passage Only:} The model is finetuned directly on the passage using the standard language modeling loss, without any synthetic data.
\vspace{-2pt}
\item \textbf{Train on Passage + Synthetic Data:} The model is trained on the passage along with self-generated implications. This is the same setup as \methodacronym but without any prior RL training.
\vspace{-2pt}
\item \textbf{Train on Passage + GPT-4.1 Synthetic Data:} The model is trained on the passage along with model-generated implications collected from GPT-4.1 via the OpenAI API.
\end{enumerate}

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \vspace{-10pt} % Adjust vertical space as needed
    \includegraphics[width=0.4\textwidth]{figures/RL_training_plot.pdf}
    \vspace{-10pt}
    \caption{\textbf{Accuracy over RL iterations.} Each iteration consists of a minibatch of $50$ contexts, each with $5$ sampled self-edits. \methodacronym{} surpasses GPT-4.1 synthetic data after two iterations of ReST$^\text{\textit{EM}}$ on the no-context SQuAD set.}
    \label{fig:RL_training_plot}
    \vspace{-15pt}
\end{wrapfigure}

Table~\ref{tab:merged_knowledge_incorp} reports mean no-context SQuAD accuracy under two regimes: single-passage ($n = 1$) and continued pretraining (CPT, $n = 200$). In the single-passage setting, finetuning directly on the passage yields a negligible gain over the frozen base model (33.5\% vs.\ 32.7\%), confirming that using the raw data alone is insufficient. Augmenting with synthetic implications generated by GPT-4.1 boosts accuracy to 46.3\%, an improvement of 12.8 percentage points over the passage-only baseline. Using synthetic data produced by the base \texttt{Qwen-2.5-7B} model yields 39.7\%, a 6.2-point increase. After reinforcement learning, \methodacronym{} further improves accuracy to \textbf{47.0\%}, notably outperforming using synthetic data from GPT-4.1, despite being a much smaller model.

In the CPT setting, the model assimilates information from $n = 200$ passages in a single continued pretraining run. It is then evaluated on the union of all 974 corresponding questions. In this setting, we sample $5$ self-edit generations for each passage and take the aggregate synthetic dataset for continued pretraining. As shown in Table~\ref{tab:merged_knowledge_incorp}, we observe a similar ranking of methods as in the single-passage case. \methodacronym{} again outperforms all baselines, achieving 43.8\% accuracy. While the absolute performance is lower than in the single-passage setting---likely due to increased gradient interference---the relative improvements remain consistent. This suggests that the editing policy discovered by \methodacronym generalizes \textit{beyond} its original RL setup of creating synthetic data in a single generation for a single passage.

\begin{table}[ht]
    \centering
    \footnotesize
    \label{tab:merged_knowledge_incorp}
    \begin{tabular}{l p{2.2cm} p{3.2cm}}
        \toprule
        \textbf{Method} & \textbf{Single Passage (n = 1)} & \textbf{Continued Pretraining (n = 200)} \\ \midrule
        Base model & 32.7 & 32.7 \\
        Train on Passage & 33.5 & 32.2 \\
        Train on Passage + Synthetic & 39.7 & 41.0 \\
        Train on Passage + GPT-4.1 Synthetic & 46.3 & 39.4 \\
        \textbf{\methodacronym} & \textbf{47.0} & \textbf{43.8} \\
        \bottomrule
    \end{tabular}
    \vspace{6pt}
    \caption{Knowledge Incorporation Performance across Passage Settings}
    \vspace{-8pt}
\end{table}

Figure~\ref{fig:RL_training_plot} tracks accuracy after each outer RL iteration. Two iterations suffice for \methodacronym{} to overtake GPT-4.1 data; subsequent iterations yield diminishing returns, suggesting that the policy quickly converges to an edit style that distills the passage into easily learnable atomic facts (see qualitative examples in Figure~\ref{fig:knowledge_examples}). All results use tuned hyperparameters (see \S\ref{app:exp_details}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/CPT_analysis.pdf}
    \vspace{-10pt}
    \caption{\textbf{Example Knowledge Incorporation Self-Edits Across RL Iterations.} In this example, we see how RL leads to the generation of more detailed self-edits, which in turn results in better performance. While the progression is clear in this case, the differences across iterations are sometimes more subtle in other examples. We show in \S\ref{app:prompting} that prompting for longer self-edits is effective, and that RL training further improves performance by a similar margin.}
    \label{fig:knowledge_examples}
\end{figure}

\vspace{10pt}

\section{Limitations}
\begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-15pt}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/catastrophic.pdf}
    \vspace{-10pt}
    \caption{\textbf{Catastrophic forgetting from continual self-edits.} We sequentially update the model on new passages and track degradation on prior tasks. Entry-wise standard errors are reported in \S\ref{app:SEM}.}
    \label{fig:forgetting_analysis}
    \vspace{-25pt}
\end{wrapfigure}

\paragraph{Catastrophic forgetting.} One key motivation we had for enabling language models to self-edit is to move towards the ultimate goal of continual learning---allowing models to incorporate new information over time, whether through agentically interacting with an environment or through standard training. While our earlier experiments assess how well \methodacronym adapts to individual edits in isolation, a more ambitious goal is to support \textit{sequences} of edits: can the model adapt to new information repeatedly while preserving prior knowledge? 

This question relates directly to the challenge of \textit{catastrophic forgetting} \citep{mccloskey1989catastrophicinterference, goodfellow2015catastrophicforgetting}, where new updates interfere destructively with past learning. We do not explicitly optimize for retention in our current training setup, but we aim to establish a baseline for how well \methodacronym handles sequential self-edits without dedicated mechanisms for handling catastrophic forgetting. To test this, we simulate a continual learning setting in the knowledge incorporation domain. The model receives a stream of test passages, each triggering a new self-edit. After each update, we re-evaluate the model on all previously seen tasks to measure retention. This setup tests the model's ability to integrate new edits without forgetting earlier ones.

As shown in Figure~\ref{fig:forgetting_analysis}, performance on earlier tasks gradually declines as the number of edits increases, suggesting that \methodacronym is still susceptible to catastrophic forgetting. Still, it can perform multiple updates without complete collapse, indicating possibility for improvement. Future work could enhance this ability through reward shaping \citep{hu2020learning, xie2023text2reward, fu2025reward} to penalize regressions on earlier tasks, or by integrating continual learning strategies such as null-space constrained edits \citep{fang2025alphaeditnullspaceconstrainedknowledge} or representational superposition \citep{cheung2019superpositionmodels}.

\paragraph{Computational overhead.} The TTT reward loop is significantly more computationally expensive than other reinforcement learning loops used with LLMs. For instance, reward signals based on human preferences typically involve a single model forward pass, and those using verified solutions may rely on simple pattern matching (e.g., regex). In contrast, our approach requires finetuning and evaluating an entire model to compute the reward---each self-edit evaluation takes approximately 30–45 seconds, introducing substantial overhead (see \S\ref{app:computeresources}).

\paragraph{Context-dependent evaluation.}  Our current instantiations assume that every context is paired with an explicit downstream task: few-shot demonstrations arrive with a held‑out query pair, and each passage comes bundled with reference QA. This coupling simplifies reward computation but prevents RL training of \methodacronym from scaling to unlabeled corpora. A potential solution is to let the model generate not only self-edits but also its own evaluation questions---e.g., draft QA items or synthetic test cases for each passage---while the original content is still in context. These model-written queries could provide the immediate supervision required for reinforcement learning, broadening applicability to general training domains where external question-and-answer sets are unavailable.

\section{Discussion and Conclusion}

\citet{villalobos2024rundatalimitsllm} project that frontier LLMs will be trained on all publicly available human-generated text by 2028. We argue that this impending ``data wall'' will necessitate the adoption of synthetic data augmentation. Once web-scale corpora is exhausted, progress will hinge on a model's capacity to \textit{generate its own high-utility training signal}. A natural next step is to meta-train a dedicated \methodacronym{} synthetic-data generator model that produces fresh pretraining corpora, allowing future models to scale and achieve greater data efficiency without relying on additional human text.

We can imagine a future in which LLMs can ingest new data, such as academic papers, and generate large quantities of explanations and implications for themselves using their existing knowledge and reasoning with the in-context data. This iterative loop of self-expression and self-refinement could allow models to keep improving on rare or underrepresented topics even in the absence of additional external supervision.

In addition, while modern reasoning models are often trained with RL to generate chain-of-thought (CoT) traces, \methodacronym could offer a complementary mechanism, allowing the model to learn when and how to update its own weights. These two approaches could synergize: the model may choose to perform weight updates mid-reasoning to guide its current trajectory, or after completing reasoning to distill key insights into its parameters---improving future inference through internalized learning.

This continual refinement loop is also promising for building agentic systems---models that operate over extended interactions and adapt dynamically to evolving goals. Agentic models must incrementally acquire and retain knowledge as they act. Our approach supports such behavior by enabling structured self-modification: after an interaction, the agent could synthesize a self-edit which triggers a weight update. This could allow the agent to develop over time, aligning its behavior with prior experience and reducing reliance on repeated supervision. 

\methodacronym demonstrates that large language models need not remain static after pretraining: by learning to generate their own synthetic self-edit data and to apply it through lightweight weight updates, they can autonomously incorporate new knowledge and adapt to novel tasks. Looking ahead, we envision extending the \methodacronym framework to pretraining, continual learning, and agentic models, ultimately enabling language models to self-learn and scale in a data-constrained world.

\begin{ack}
We would like to thank Shivam Duggal, Idan Shenfeld, Seungwook Han, Jeremy Bernstein, Akarsh Kumar, Linlu Qiu, Juno Kim, Brian Cheung, Moritz Reuss, Ayush Sekhari, Zhang-Wei Hong, Mehul Damani, Leshem Choshen, and Ryan Yang for their valuable discussions and feedback. We acknowledge support from ARO MURI grant number W911NF-23-1-0277. This research was also partly sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19- 2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein. This research was also partly supported by the Stevens Fund for MIT UROP research and by the MIT-IBM Watson AI Lab.
\end{ack}

\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\newpage
\section{Experimental Details: Few-shot Learning}
\label{app:exp_details_few_shot}

\subsection{Model and Setup}

We use \texttt{Llama-3.2-1B-Instruct} \citep{grattafiori2024llama3herdmodels} as the base language model. Since this model has no specialized training on ARC, its ability to solve ARC tasks is limited. To enable controlled evaluation, we curated a small set of ARC problems from the training and evaluation splits that are solvable with optimal TTT hyperparameters.

\textbf{Training Set:} We selected 11 ARC tasks from the training set as the environment for RL optimization. 
\textbf{Evaluation Set:} We selected 8 distinct ARC problems from the evaluation set for measuring generalization performance. These 8 were explicitly filtered for being amenable to TTT out of the full evaluation set.

These sets were chosen to isolate the effect of self-edit learning rather than general ARC ability.

\subsection{Training Procedure}

We trained \methodacronym{} using a single outer loop of reinforcement learning on the 11 training problems. For each problem, the model generated 15 self-edit configurations consisting of:
\begin{itemize}
    \item \textbf{Data augmentation strategy:} Controls whether to include variations such as basic examples, size-based augmentations, chained reasoning, and repeated inputs:
    \texttt{include\_basic}, \texttt{include\_size}, \texttt{include\_chain}, \texttt{include\_repeat}.
    
    \item \textbf{Optimization configuration:} Specifies hyperparameters such as learning rate, number of epochs, and loss function type (e.g., whether to compute loss over all tokens or only the final output tokens).
\end{itemize}

Each configuration was evaluated via test-time training (TTT), and assigned a binary reward: 1 if the adapted model produced the correct solution, 0 otherwise using \citet{akyurek2025TTT}'s evaluation protocol. To encourage efficient adaptation, we discarded configurations requiring more than 375 training steps, retained only configurations with reward 1 and acceptable cost for LoRA-based SFT.

SFT for TTT was done with the following hyperparameters:
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
LoRA rank & 128 \\
LoRA alpha & 16 \\
Learning rate & N/A (model chooses it) \\
\texttt{num\_train\_epochs} & N/A (model chooses it) \\

\hline
\end{tabular}
\end{center}

LoRA adapters were inserted into the following modules: \texttt{q\_proj}, \texttt{v\_proj}, \texttt{gate\_proj}, \texttt{down\_proj}, \texttt{up\_proj}

\vspace{1em}

For the RL SFT we used the following hyperparameters:
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
\texttt{lora\_rank} & 16 \\
\texttt{lora\_alpha} & 16 \\
\texttt{num\_train\_epochs} & 8 \\
\texttt{per\_device\_train\_batch\_size} & 5 \\
\texttt{learning\_rate} & 5e-5 \\
\hline
\end{tabular}
\end{center}

When generating the self-edits, we add the following prompt as a suffix to the few-shot examples.

\begin{tcolorbox}
You are configuring a model training pipeline by selecting from predefined tools.

You must make two decisions:

1. \textbf{Data Generation Tools} --- For each of the following, choose true or false:\\
\hspace*{1em}- use\_basic\_augmentations\\
\hspace*{1em}- use\_size\_augmentations\\
\hspace*{1em}- use\_chain\_augmentations\\
\hspace*{1em}- use\_repeat\_augmentations\\

2. \textbf{Training Configuration} --- Choose one of:\\
\hspace*{1em}- \texttt{"train\_using\_all\_tokens"}\\
\hspace*{1em}- \texttt{"train\_using\_output\_tokens"}\\

Also specify:\\
\hspace*{1em}- learning\_rate (float)\\
\hspace*{1em}- num\_train\_epochs (integer)\\

\textbf{Output Format}\\

Respond with a valid JSON object. Do not include any explanation, markdown, or extra text. Use lowercase \texttt{true}/\texttt{false} for booleans and ensure correct JSON syntax.

Example output:
\begin{verbatim}
{
  "data_generation": {
    "use_basic_augmentations": ...,
    "use_size_augmentations": ...,
    "use_chain_augmentations": ...,
    "use_repeat_augmentations": ...
  },
  "training": {
    "strategy": ...,
    "learning_rate": ...,
    "num_train_epochs": ...
  }
}
\end{verbatim}
\end{tcolorbox}


\subsection{Evaluation Details}

For each of the 8 held-out evaluation tasks, the model generated 5 self-edit configurations, yielding a total of 40 configurations. Success was measured as the percentage of configurations that led to correct outputs after adaptation. We followed the evaluation protocol from \citet{akyurek2025TTT}. 

For the Oracle TTT we used the following configs:
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
\texttt{lora\_rank} & 128 \\
\texttt{lora\_alpha} & 16 \\
\texttt{num\_train\_epochs} & 2 \\
\texttt{batch\_size} & 2 \\
\texttt{learning\_rate} & 1e-4 \\
\hline
\end{tabular}
\end{center}


\subsection{Compute Resources}
We performed all training runs on a single A100, H100, or H200. Each TTT per problem requires between half a minute to few minutes, which is also why we limited the number of samples for ReST$^\text{\textit{EM}}$ and additionally limited the number of gradient steps allowed per self-edit TTT. Overall ReST$^\text{\textit{EM}}$ took around 2-3 hours. 

\section{Experimental Details: Knowledge Incorporation}
\label{app:exp_details}

\subsection{Model and Setup}

We use the \texttt{Qwen-2.5-7B} base model \citep{qwen2025qwen25technicalreport} in the knowledge incorporation experiments. We repurpose the SQuAD dataset v1.1 \citep{rajpurkar2016squad} for the task of answering questions \textit{without} the passage in-context. We use the training set for RL training and a 200-article subset of the evaluation set for evaluation. Within the training set and evaluation set, there are some overlapping topics of passages, but there is no overlap between these sets, so we can be sure that there is no data contamination of the test passages due to RL training.

\subsection{RL Training Procedure}

We run 2 rounds of ReST$^\text{\textit{EM}}$ training \cite{singh2023beyond}. On each round, we take a batch of 50 context-questions-answers triples from the SQuAD training set. For each context, we sample 5 self-edit generations at temperature $1$. We evaluate each self-edit over 3 random seeds, training on the sequences and then evaluating the updated model on the corresponding questions. We average each generation's results over 3 seeds and then keep the single best generation for each of the 50 contexts. Finally, to finish the round of ReST$^\text{\textit{EM}}$, we perform supervised finetuning on the 50 resulting prompt-completion pairs.

Supervised finetuning here is done with batch size of 10, for 2 epochs, with learning rate 3e-4, using LoRA \citep{hu2022lora} with rank 64 and alpha 128, applied to all MLP and attention projection layers.

\subsection{Synthetic Data Generation and Finetuning Details}

In all models, we generate synthetic data by prompting to generate implications of the passage:

\begin{tcolorbox}
Let's read the following passage and produce a list of implications derived directly or indirectly from the content. \\ \\
Passage: \\ \{passage\} \\ \\
Implications:
\end{tcolorbox}

We then take the resulting generated sequence. In the single-passage case, we split it by newlines into a set of training documents. In the multi-passage case, we use the full generated sequence as a single training document. In the case of synthetic data from GPT-4.1 (\texttt{gpt-4.1-2025-04-14}), an instruct-model, we additionally have the following rule: If the second line begins with a ``1.'' then we omit the first line from the training set. This is because we found that the first line often contained filler text (e.g. ``Sure, here is the list of implications:'').

We then use the following training hyperparameters:

\begin{table}[ht]
    \centering
    \caption{Single-Passage Knowledge Incorporation Hyperparameters}
    \label{tab:single_passage_knowledge}
    \small
    \begin{tabularx}{0.5\linewidth}{l X}
        \toprule
        \textbf{Parameter} & \textbf{Search Space} \\
        \midrule
        LoRA Rank (\(r\)) & [\textbf{32}, 64] \\
        LoRA Alpha (\(\alpha\)) & [32, \textbf{64}] \\
        Learning Rate & [1e-4, 3e-4, 5e-4, \textbf{1e-3}, 2e-3] \\
        Epochs & [1, 5, \textbf{10}, 15, 20] \\
        Batch Size & [\textbf{1}, 4] \\
        \bottomrule
    \end{tabularx}
\end{table}

In the multi-passage $n=200$ case, we sample 5 self-edit completions for each passage and take the aggregate dataset of all self-edits across all passages to train on.

\begin{table}[ht]
    \centering
    \caption{Multi-Passage Knowledge Incorporation Hyperparameters}
    \label{tab:multi_passage_knowledge}
    \small
    \begin{tabularx}{0.5\linewidth}{l X}
        \toprule
        \textbf{Parameter} & \textbf{Search Space} \\
        \midrule
        LoRA Rank (\(R\)) & [\textbf{32}, 64] \\
        LoRA Alpha (\(\alpha\)) & [32, \textbf{64}] \\
        Learning Rate & [1e-4, 3e-4, 5e-4, \textbf{1e-3}, 2e-3] \\
        Epochs & [1, \textbf{3}, 5] \\
        Batch Size & [1, 4, \textbf{8}, 16] \\
        \bottomrule
    \end{tabularx}
\end{table}

To answer the corresponding questions, we use the following prompt:

\begin{tcolorbox}
Let's answer a question directly and concisely. \\
Question: \{question\} \\
Answer:
\end{tcolorbox}

\subsection{Evaluation Details}

We evaluate on a 200-passage subset of the SQuAD evaluation set, consisting of a combined 974 evaluation questions (roughly 5 corresponding to each passage). The pipeline of generating synthetic data and finetuning on it is the same as above. For automated grading, we use \texttt{gpt-4.1-2025-04-14} \citep{openai2024gpt4technicalreport} via the OpenAI API with greedy decoding.

The grading prompt is as follows:
\begin{tcolorbox}
You are a grading assistant. Your job is to determine whether a student's answer correctly answers the question based solely on the provided gold answer. Do not use any outside knowledge. The student answer can include additional information, but it must at least fully convey the gold answer and must not contradict it. Ignore style, phrasing, or extra details that do not affect correctness. Respond ONLY with `yes' or `no'. \\ \\
Question: \{question\} \\
Gold answer: \{gold\} \\
Student answer: \{pred\} \\
Is the student answer correct based solely on the gold answer? Respond `yes' or `no'.
\end{tcolorbox}

\subsection{Compute Resources}
\label{app:computeresources}

All experiments are performed on $2\times$H100 or $2\times$H200. We use DeepSpeed ZeRO-3 \citep{rasley2020deepspeed} for SFT in ReST$^\text{\textit{EM}}$ training. We use vLLM \citep{kwon2023efficient} for efficient inference. The most compute-intensive portion of our training and evaluation is the E-step of ReST$^\text{\textit{EM}}$ training, where the model generates completions and is graded through the inner-loop process of finetuning and running inference. Doing a single round requires a batch of 50 passages over 5 completions and 3 runs per completion, meaning 750 inner loop iterations. This takes about 6 hours on $2\times$H100s. 

\subsection{Standard Error of the Mean in Catastrophic Forgetting Experiment}
\label{app:SEM}

The standard errors of the mean (SEM) for each entry in Figure~\ref{fig:forgetting_analysis} is shown below in Table~\ref{tab:SEM}.

\begin{table}[h]
\label{tab:SEM}
\centering
\caption{Entrywise standard errors of the mean (SEM) across continual self-edits experiment.}
\begin{tabular}{c|cccccccc}
\toprule
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\midrule
0 & 0.0306 & 0.0315 & 0.0263 & 0.0318 & 0.0297 & 0.0370 & 0.0310 & 0.0284 \\
1 & 0.0273 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\
2 & 0.0305 & 0.0277 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\
3 & 0.0277 & 0.0358 & 0.0406 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\
4 & 0.0272 & 0.0303 & 0.0337 & 0.0320 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\
5 & 0.0296 & 0.0342 & 0.0290 & 0.0298 & 0.0319 & 0.0000 & 0.0000 & 0.0000 \\
6 & 0.0289 & 0.0334 & 0.0271 & 0.0258 & 0.0320 & 0.0337 & 0.0000 & 0.0000 \\
7 & 0.0255 & 0.0313 & 0.0264 & 0.0253 & 0.0309 & 0.0331 & 0.0363 & 0.0000 \\
8 & 0.0237 & 0.0307 & 0.0211 & 0.0267 & 0.0273 & 0.0271 & 0.0358 & 0.0263 \\
\bottomrule
\end{tabular}
\end{table}


\section{Prompting}
\label{app:prompting}

Recent works have shown that reinforcement learning baselines and outcomes can be highly sensitive to prompting. We experiment with 4 additional self-edit prompts in the knowledge-incorporation setting. The five prompts---\texttt{implications}, \texttt{implications-long}, \texttt{implications-very-long}, \texttt{rewrite}, and \texttt{self-qa}---are shown below. All results in the main content of the paper used the \texttt{implications} prompt, which we consider to be the most prototypical \citep{akyurek2024deductive, lampinen2025generalizationlanguagemodelsincontext}. However, prior work has found prompts involving rewriting or generating question-answer pairs can be more effective, as discussed in \S\ref{subsection:ke}.

Furthermore, as we see qualitatively in Figure~\ref{fig:knowledge_examples}, RL appears to have dramatically increased the length of the response of the example. We therefore experiment with prompting for longer generations with \texttt{implications-long} and \texttt{implications-very-long} to test if we can achieve similar gains through prompting alone.

The results are shown in Table~\ref{tab:prompting}. Notably, the baselines for \texttt{implications-long} and \texttt{rewrite} the RL-trained version of \texttt{implications}. However, using these prompts as the base of RL training yields even greater improvements. In all cases, ReST$^\text{\textit{EM}}$ enhanced performance by roughly $6$ to $11$ percentage points.

\begin{table}[h!]
\centering
\footnotesize
\begin{tabular}{l|p{1.6cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1cm}|p{1.5cm}}
\textbf{} & \textbf{No self-edit} & \textbf{Implications} & \textbf{Implications-long} & \textbf{Implications-very-long} & \textbf{Rewrite} & \textbf{Self-QA} \\
\hline
Original & 33.5 & 39.7 & 49.3 & 45.0 & 49.4 & 37.3 \\
Round 1 & -- & 43.7 & 52.4 & 51.5 & 55.3 & 42.8 \\
Round 2 & -- & \textbf{47.0} & 51.8 & 52.1 & 55.6 & 48.7 \\
\end{tabular}
\vspace{6pt}
\caption{Performance across 2 rounds of ReST$^\text{\textit{EM}}$ RL training on various prompts in the single-document knowledge incorporation setting.}
\vspace{-6pt}
\label{tab:prompting}
\end{table}

The five prompts are shown below.

\begin{tcolorbox}[questionstyle, title=\texttt{implications}]
Let's read the following passage and produce a list of implications derived directly or indirectly from the content. \\ \\
Passage: \\ \{passage\} \\ \\
Implications:
\end{tcolorbox}

\begin{tcolorbox}[questionstyle, title=\texttt{implications-long}]
Let's read the following passage and produce a long list of implications derived directly or indirectly from the content. \\ \\
Passage: \\ \{passage\} \\ \\
Implications:
\end{tcolorbox}

\begin{tcolorbox}[questionstyle, title=\texttt{implications-very-long}]
Let's read the following passage and produce a very long list of implications derived directly or indirectly from the content. \\ \\
Passage: \\ \{passage\} \\ \\
Implications:
\end{tcolorbox}

\begin{tcolorbox}[questionstyle, title=\texttt{rewrite}]
Let's read the following passage and rewrite it in a few different ways, each one separated by a newline. \\ \\
Passage: \\ \{passage\} \\ \\
Rewritten passages:
\end{tcolorbox}

\begin{tcolorbox}[questionstyle, title=\texttt{self-qa}]
Let's read the following passage and rewrite it in a question-answer format. \\ \\
Passage: \\ \{passage\} \\ \\
Question 1:
\end{tcolorbox}

\textbf{Note:} For \texttt{self-qa}, we apply additional formatting so that training documents consist of question–answer pairs, rather than using our standard approach of splitting by newline characters. Specifically, we split the output using occurrences of ``Question~n:'' instead of newlines.

\end{document}