{
  "generated_at": "2025-07-07T03:30:57.366735",
  "constitutional_hash": "cdd01ef066bc6cf2",
  "total_papers": 114,
  "download_stats": {
    "total_found": 114,
    "downloaded": 114,
    "skipped": 0,
    "errors": 0
  },
  "papers": {
    "2407.16008": {
      "arxiv_id": "2407.16008",
      "title": "Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic\n  Data Generation",
      "authors": [
        "Jiaming Shen",
        "Ran Xu",
        "Yennie Jun",
        "Zhen Qin",
        "Tianqi Liu",
        "Carl Yang",
        "Yi Liang",
        "Simon Baumgartner",
        "Michael Bendersky"
      ],
      "summary": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. They are trained using preference datasets where each\nexample consists of one input prompt, two responses, and a preference label. As\ncurating a high-quality human labeled preference dataset is both time-consuming\nand expensive, people often rely on existing powerful LLMs for preference label\ngeneration. This can potentially introduce noise and impede RM training. In\nthis work, we present RMBoost, a novel synthetic preference data generation\nparadigm to boost reward model quality. Unlike traditional methods, which\ngenerate two responses before obtaining the preference label, RMBoost first\ngenerates one response and selects a preference label, followed by generating\nthe second more (or less) preferred response conditioned on the pre-selected\npreference label and the first response. This approach offers two main\nadvantages. First, RMBoost reduces labeling noise since preference pairs are\nconstructed intentionally. Second, RMBoost facilitates the creation of more\ndiverse responses by incorporating various quality aspects (e.g., helpfulness,\nrelevance, completeness) into the prompts. We conduct extensive experiments\nacross three diverse datasets and demonstrate that RMBoost outperforms other\nsynthetic preference data generation techniques and significantly boosts the\nperformance of four distinct reward models.",
      "published": "2024-07-22T19:21:55Z",
      "updated": "2025-03-14T20:08:08Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16008.pdf",
      "abs_url": "http://arxiv.org/abs/2407.16008v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2303.08774": {
      "arxiv_id": "2303.08774",
      "title": "GPT-4 Technical Report",
      "authors": [
        "OpenAI",
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman",
        "Shyamal Anadkat",
        "Red Avila",
        "Igor Babuschkin",
        "Suchir Balaji",
        "Valerie Balcom",
        "Paul Baltescu",
        "Haiming Bao",
        "Mohammad Bavarian",
        "Jeff Belgum",
        "Irwan Bello",
        "Jake Berdine",
        "Gabriel Bernadett-Shapiro",
        "Christopher Berner",
        "Lenny Bogdonoff",
        "Oleg Boiko",
        "Madelaine Boyd",
        "Anna-Luisa Brakman",
        "Greg Brockman",
        "Tim Brooks",
        "Miles Brundage",
        "Kevin Button",
        "Trevor Cai",
        "Rosie Campbell",
        "Andrew Cann",
        "Brittany Carey",
        "Chelsea Carlson",
        "Rory Carmichael",
        "Brooke Chan",
        "Che Chang",
        "Fotis Chantzis",
        "Derek Chen",
        "Sully Chen",
        "Ruby Chen",
        "Jason Chen",
        "Mark Chen",
        "Ben Chess",
        "Chester Cho",
        "Casey Chu",
        "Hyung Won Chung",
        "Dave Cummings",
        "Jeremiah Currier",
        "Yunxing Dai",
        "Cory Decareaux",
        "Thomas Degry",
        "Noah Deutsch",
        "Damien Deville",
        "Arka Dhar",
        "David Dohan",
        "Steve Dowling",
        "Sheila Dunning",
        "Adrien Ecoffet",
        "Atty Eleti",
        "Tyna Eloundou",
        "David Farhi",
        "Liam Fedus",
        "Niko Felix",
        "Sim\u00f3n Posada Fishman",
        "Juston Forte",
        "Isabella Fulford",
        "Leo Gao",
        "Elie Georges",
        "Christian Gibson",
        "Vik Goel",
        "Tarun Gogineni",
        "Gabriel Goh",
        "Rapha Gontijo-Lopes",
        "Jonathan Gordon",
        "Morgan Grafstein",
        "Scott Gray",
        "Ryan Greene",
        "Joshua Gross",
        "Shixiang Shane Gu",
        "Yufei Guo",
        "Chris Hallacy",
        "Jesse Han",
        "Jeff Harris",
        "Yuchen He",
        "Mike Heaton",
        "Johannes Heidecke",
        "Chris Hesse",
        "Alan Hickey",
        "Wade Hickey",
        "Peter Hoeschele",
        "Brandon Houghton",
        "Kenny Hsu",
        "Shengli Hu",
        "Xin Hu",
        "Joost Huizinga",
        "Shantanu Jain",
        "Shawn Jain",
        "Joanne Jang",
        "Angela Jiang",
        "Roger Jiang",
        "Haozhun Jin",
        "Denny Jin",
        "Shino Jomoto",
        "Billie Jonn",
        "Heewoo Jun",
        "Tomer Kaftan",
        "\u0141ukasz Kaiser",
        "Ali Kamali",
        "Ingmar Kanitscheider",
        "Nitish Shirish Keskar",
        "Tabarak Khan",
        "Logan Kilpatrick",
        "Jong Wook Kim",
        "Christina Kim",
        "Yongjik Kim",
        "Jan Hendrik Kirchner",
        "Jamie Kiros",
        "Matt Knight",
        "Daniel Kokotajlo",
        "\u0141ukasz Kondraciuk",
        "Andrew Kondrich",
        "Aris Konstantinidis",
        "Kyle Kosic",
        "Gretchen Krueger",
        "Vishal Kuo",
        "Michael Lampe",
        "Ikai Lan",
        "Teddy Lee",
        "Jan Leike",
        "Jade Leung",
        "Daniel Levy",
        "Chak Ming Li",
        "Rachel Lim",
        "Molly Lin",
        "Stephanie Lin",
        "Mateusz Litwin",
        "Theresa Lopez",
        "Ryan Lowe",
        "Patricia Lue",
        "Anna Makanju",
        "Kim Malfacini",
        "Sam Manning",
        "Todor Markov",
        "Yaniv Markovski",
        "Bianca Martin",
        "Katie Mayer",
        "Andrew Mayne",
        "Bob McGrew",
        "Scott Mayer McKinney",
        "Christine McLeavey",
        "Paul McMillan",
        "Jake McNeil",
        "David Medina",
        "Aalok Mehta",
        "Jacob Menick",
        "Luke Metz",
        "Andrey Mishchenko",
        "Pamela Mishkin",
        "Vinnie Monaco",
        "Evan Morikawa",
        "Daniel Mossing",
        "Tong Mu",
        "Mira Murati",
        "Oleg Murk",
        "David M\u00e9ly",
        "Ashvin Nair",
        "Reiichiro Nakano",
        "Rajeev Nayak",
        "Arvind Neelakantan",
        "Richard Ngo",
        "Hyeonwoo Noh",
        "Long Ouyang",
        "Cullen O'Keefe",
        "Jakub Pachocki",
        "Alex Paino",
        "Joe Palermo",
        "Ashley Pantuliano",
        "Giambattista Parascandolo",
        "Joel Parish",
        "Emy Parparita",
        "Alex Passos",
        "Mikhail Pavlov",
        "Andrew Peng",
        "Adam Perelman",
        "Filipe de Avila Belbute Peres",
        "Michael Petrov",
        "Henrique Ponde de Oliveira Pinto",
        "Michael",
        "Pokorny",
        "Michelle Pokrass",
        "Vitchyr H. Pong",
        "Tolly Powell",
        "Alethea Power",
        "Boris Power",
        "Elizabeth Proehl",
        "Raul Puri",
        "Alec Radford",
        "Jack Rae",
        "Aditya Ramesh",
        "Cameron Raymond",
        "Francis Real",
        "Kendra Rimbach",
        "Carl Ross",
        "Bob Rotsted",
        "Henri Roussez",
        "Nick Ryder",
        "Mario Saltarelli",
        "Ted Sanders",
        "Shibani Santurkar",
        "Girish Sastry",
        "Heather Schmidt",
        "David Schnurr",
        "John Schulman",
        "Daniel Selsam",
        "Kyla Sheppard",
        "Toki Sherbakov",
        "Jessica Shieh",
        "Sarah Shoker",
        "Pranav Shyam",
        "Szymon Sidor",
        "Eric Sigler",
        "Maddie Simens",
        "Jordan Sitkin",
        "Katarina Slama",
        "Ian Sohl",
        "Benjamin Sokolowsky",
        "Yang Song",
        "Natalie Staudacher",
        "Felipe Petroski Such",
        "Natalie Summers",
        "Ilya Sutskever",
        "Jie Tang",
        "Nikolas Tezak",
        "Madeleine B. Thompson",
        "Phil Tillet",
        "Amin Tootoonchian",
        "Elizabeth Tseng",
        "Preston Tuggle",
        "Nick Turley",
        "Jerry Tworek",
        "Juan Felipe Cer\u00f3n Uribe",
        "Andrea Vallone",
        "Arun Vijayvergiya",
        "Chelsea Voss",
        "Carroll Wainwright",
        "Justin Jay Wang",
        "Alvin Wang",
        "Ben Wang",
        "Jonathan Ward",
        "Jason Wei",
        "CJ Weinmann",
        "Akila Welihinda",
        "Peter Welinder",
        "Jiayi Weng",
        "Lilian Weng",
        "Matt Wiethoff",
        "Dave Willner",
        "Clemens Winter",
        "Samuel Wolrich",
        "Hannah Wong",
        "Lauren Workman",
        "Sherwin Wu",
        "Jeff Wu",
        "Michael Wu",
        "Kai Xiao",
        "Tao Xu",
        "Sarah Yoo",
        "Kevin Yu",
        "Qiming Yuan",
        "Wojciech Zaremba",
        "Rowan Zellers",
        "Chong Zhang",
        "Marvin Zhang",
        "Shengjia Zhao",
        "Tianhao Zheng",
        "Juntang Zhuang",
        "William Zhuk",
        "Barret Zoph"
      ],
      "summary": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.",
      "published": "2023-03-15T17:15:04Z",
      "updated": "2024-03-04T06:01:33Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08774.pdf",
      "abs_url": "http://arxiv.org/abs/2303.08774v6",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2405.14734": {
      "arxiv_id": "2405.14734",
      "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
      "authors": [
        "Yu Meng",
        "Mengzhou Xia",
        "Danqi Chen"
      ],
      "summary": "Direct Preference Optimization (DPO) is a widely used offline preference\noptimization algorithm that reparameterizes reward functions in reinforcement\nlearning from human feedback (RLHF) to enhance simplicity and training\nstability. In this work, we propose SimPO, a simpler yet more effective\napproach. The effectiveness of SimPO is attributed to a key design: using the\naverage log probability of a sequence as the implicit reward. This reward\nformulation better aligns with model generation and eliminates the need for a\nreference model, making it more compute and memory efficient. Additionally, we\nintroduce a target reward margin to the Bradley-Terry objective to encourage a\nlarger margin between the winning and losing responses, further improving the\nalgorithm's performance. We compare SimPO to DPO and its latest variants across\nvarious state-of-the-art training setups, including both base and\ninstruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on\nextensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench,\nand Arena-Hard. Our results demonstrate that SimPO consistently and\nsignificantly outperforms existing approaches without substantially increasing\nresponse length. Specifically, SimPO outperforms DPO by up to 6.4 points on\nAlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model,\nbuilt on Gemma-2-9B-it, achieves a 72.4% length-controlled win rate on\nAlpacaEval 2, a 59.1% win rate on Arena-Hard, and ranks 1st on Chatbot Arena\namong <10B models with real user votes.",
      "published": "2024-05-23T16:01:46Z",
      "updated": "2024-11-01T20:05:19Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.14734.pdf",
      "abs_url": "http://arxiv.org/abs/2405.14734v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2104.08691": {
      "arxiv_id": "2104.08691",
      "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
      "authors": [
        "Brian Lester",
        "Rami Al-Rfou",
        "Noah Constant"
      ],
      "summary": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.",
      "published": "2021-04-18T03:19:26Z",
      "updated": "2021-09-02T17:34:41Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2104.08691.pdf",
      "abs_url": "http://arxiv.org/abs/2104.08691v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2410.11348": {
      "arxiv_id": "2410.11348",
      "title": "RATE: Causal Explainability of Reward Models with Imperfect\n  Counterfactuals",
      "authors": [
        "David Reber",
        "Sean Richardson",
        "Todd Nief",
        "Cristina Garbacea",
        "Victor Veitch"
      ],
      "summary": "Reward models are widely used as proxies for human preferences when aligning\nor evaluating LLMs. However, reward models are black boxes, and it is often\nunclear what, exactly, they are actually rewarding. In this paper we develop\nRewrite-based Attribute Treatment Estimator (RATE) as an effective method for\nmeasuring the sensitivity of a reward model to high-level attributes of\nresponses, such as sentiment, helpfulness, or complexity. Importantly, RATE\nmeasures the causal effect of an attribute on the reward. RATE uses LLMs to\nrewrite responses to produce imperfect counterfactuals examples that can be\nused to measure causal effects. A key challenge is that these rewrites are\nimperfect in a manner that can induce substantial bias in the estimated\nsensitivity of the reward model to the attribute. The core idea of RATE is to\nadjust for this imperfect-rewrite effect by rewriting twice. We establish the\nvalidity of the RATE procedure and show empirically that it is an effective\nestimator.",
      "published": "2024-10-15T07:22:16Z",
      "updated": "2025-05-19T22:33:49Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11348.pdf",
      "abs_url": "http://arxiv.org/abs/2410.11348v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2310.01377": {
      "arxiv_id": "2310.01377",
      "title": "UltraFeedback: Boosting Language Models with Scaled AI Feedback",
      "authors": [
        "Ganqu Cui",
        "Lifan Yuan",
        "Ning Ding",
        "Guanming Yao",
        "Bingxiang He",
        "Wei Zhu",
        "Yuan Ni",
        "Guotong Xie",
        "Ruobing Xie",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "summary": "Learning from human feedback has become a pivot technique in aligning large\nlanguage models (LLMs) with human preferences. However, acquiring vast and\npremium human feedback is bottlenecked by time, labor, and human capability,\nresulting in small sizes or limited topics of current datasets. This further\nhinders feedback learning as well as alignment research within the open-source\ncommunity. To address this issue, we explore how to go beyond human feedback\nand collect high-quality \\textit{AI feedback} automatically for a scalable\nalternative. Specifically, we identify \\textbf{scale and diversity} as the key\nfactors for feedback data to take effect. Accordingly, we first broaden\ninstructions and responses in both amount and breadth to encompass a wider\nrange of user-assistant interactions. Then, we meticulously apply a series of\ntechniques to mitigate annotation biases for more reliable AI feedback. We\nfinally present \\textsc{UltraFeedback}, a large-scale, high-quality, and\ndiversified AI feedback dataset, which contains over 1 million GPT-4 feedback\nfor 250k user-assistant conversations from various aspects. Built upon\n\\textsc{UltraFeedback}, we align a LLaMA-based model by best-of-$n$ sampling\nand reinforcement learning, demonstrating its exceptional performance on chat\nbenchmarks. Our work validates the effectiveness of scaled AI feedback data in\nconstructing strong open-source chat language models, serving as a solid\nfoundation for future feedback learning research. Our data and models are\navailable at https://github.com/thunlp/UltraFeedback.",
      "published": "2023-10-02T17:40:01Z",
      "updated": "2024-07-16T03:24:39Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.01377.pdf",
      "abs_url": "http://arxiv.org/abs/2310.01377v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2204.05862": {
      "arxiv_id": "2204.05862",
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning\n  from Human Feedback",
      "authors": [
        "Yuntao Bai",
        "Andy Jones",
        "Kamal Ndousse",
        "Amanda Askell",
        "Anna Chen",
        "Nova DasSarma",
        "Dawn Drain",
        "Stanislav Fort",
        "Deep Ganguli",
        "Tom Henighan",
        "Nicholas Joseph",
        "Saurav Kadavath",
        "Jackson Kernion",
        "Tom Conerly",
        "Sheer El-Showk",
        "Nelson Elhage",
        "Zac Hatfield-Dodds",
        "Danny Hernandez",
        "Tristan Hume",
        "Scott Johnston",
        "Shauna Kravec",
        "Liane Lovitt",
        "Neel Nanda",
        "Catherine Olsson",
        "Dario Amodei",
        "Tom Brown",
        "Jack Clark",
        "Sam McCandlish",
        "Chris Olah",
        "Ben Mann",
        "Jared Kaplan"
      ],
      "summary": "We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization. We explore an iterated online mode of\ntraining, where preference models and RL policies are updated on a weekly\ncadence with fresh human feedback data, efficiently improving our datasets and\nmodels. Finally, we investigate the robustness of RLHF training, and identify a\nroughly linear relation between the RL reward and the square root of the KL\ndivergence between the policy and its initialization. Alongside our main\nresults, we perform peripheral analyses on calibration, competing objectives,\nand the use of OOD detection, compare our models with human writers, and\nprovide samples from our models using prompts appearing in recent related work.",
      "published": "2022-04-12T15:02:38Z",
      "updated": "2022-04-12T15:02:38Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.05862.pdf",
      "abs_url": "http://arxiv.org/abs/2204.05862v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2406.17744": {
      "arxiv_id": "2406.17744",
      "title": "Following Length Constraints in Instructions",
      "authors": [
        "Weizhe Yuan",
        "Ilia Kulikov",
        "Ping Yu",
        "Kyunghyun Cho",
        "Sainbayar Sukhbaatar",
        "Jason Weston",
        "Jing Xu"
      ],
      "summary": "Aligned instruction following models can better fulfill user requests than\ntheir unaligned counterparts. However, it has been shown that there is a length\nbias in evaluation of such models, and that training algorithms tend to exploit\nthis bias by learning longer responses. In this work we show how to train\nmodels that can be controlled at inference time with instructions containing\ndesired length constraints. Such models are superior in length instructed\nevaluations, outperforming standard instruction following models such as GPT4,\nLlama 3 and Mixtral.",
      "published": "2024-06-25T17:29:52Z",
      "updated": "2024-06-25T17:29:52Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17744.pdf",
      "abs_url": "http://arxiv.org/abs/2406.17744v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2504.10045": {
      "arxiv_id": "2504.10045",
      "title": "CHARM: Calibrating Reward Models With Chatbot Arena Scores",
      "authors": [
        "Xiao Zhu",
        "Chenmien Tan",
        "Pinzhen Chen",
        "Rico Sennrich",
        "Yanlin Zhang",
        "Hanxu Hu"
      ],
      "summary": "Reward models (RMs) play a crucial role in Reinforcement Learning from Human\nFeedback by serving as proxies for human preferences in aligning large language\nmodels. In this paper, we identify a model preference bias in RMs, where they\nsystematically assign disproportionately high scores to responses from certain\npolicy models. This bias distorts ranking evaluations and leads to unfair\njudgments. To address this issue, we propose a calibration method named CHatbot\nArena calibrated Reward Modeling (CHARM) that leverages Elo scores from the\nChatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a\nMismatch Degree metric to measure this preference bias. Our approach is\ncomputationally efficient, requiring only a small preference dataset for\ncontinued training of the RM. We conduct extensive experiments on reward model\nbenchmarks and human preference alignment. Results demonstrate that our\ncalibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the\nChat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with\nhuman preferences by producing scores more closely aligned with Elo rankings.\nBy mitigating model preference bias, our method provides a generalizable and\nefficient solution for building fairer and more reliable reward models.",
      "published": "2025-04-14T09:51:09Z",
      "updated": "2025-04-14T09:51:09Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.10045.pdf",
      "abs_url": "http://arxiv.org/abs/2504.10045v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2404.11999": {
      "arxiv_id": "2404.11999",
      "title": "Token-level Direct Preference Optimization",
      "authors": [
        "Yongcheng Zeng",
        "Guoqing Liu",
        "Weiyu Ma",
        "Ning Yang",
        "Haifeng Zhang",
        "Jun Wang"
      ],
      "summary": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.",
      "published": "2024-04-18T08:49:38Z",
      "updated": "2024-08-30T03:39:57Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.11999.pdf",
      "abs_url": "http://arxiv.org/abs/2404.11999v5",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2005.14165": {
      "arxiv_id": "2005.14165",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "Tom B. Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel M. Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Christopher Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam McCandlish",
        "Alec Radford",
        "Ilya Sutskever",
        "Dario Amodei"
      ],
      "summary": "Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.",
      "published": "2020-05-28T17:29:03Z",
      "updated": "2020-07-22T19:47:17Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2005.14165.pdf",
      "abs_url": "http://arxiv.org/abs/2005.14165v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2212.10559": {
      "arxiv_id": "2212.10559",
      "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform\n  Gradient Descent as Meta-Optimizers",
      "authors": [
        "Damai Dai",
        "Yutao Sun",
        "Li Dong",
        "Yaru Hao",
        "Shuming Ma",
        "Zhifang Sui",
        "Furu Wei"
      ],
      "summary": "Large pretrained language models have shown surprising in-context learning\n(ICL) ability. With a few demonstration input-label pairs, they can predict the\nlabel for an unseen input without parameter updates. Despite the great success\nin performance, its working mechanism still remains an open question. In this\npaper, we explain language models as meta-optimizers and understand in-context\nlearning as implicit finetuning. Theoretically, we figure out that Transformer\nattention has a dual form of gradient descent. On top of it, we understand ICL\nas follows: GPT first produces meta-gradients according to the demonstration\nexamples, and then these meta-gradients are applied to the original GPT to\nbuild an ICL model. We comprehensively compare the behaviors of in-context\nlearning and explicit finetuning on real tasks to provide empirical evidence\nthat supports our understanding. Experimental results show that in-context\nlearning behaves similarly to explicit finetuning from multiple perspectives.\nInspired by the dual form between Transformer attention and gradient descent,\nwe design a momentum-based attention by analogy with gradient descent with\nmomentum. The improved performance over vanilla attention further supports our\nunderstanding from another perspective, and more importantly, shows the\npotential to utilize our understanding for future model design. The code is\navailable at \\url{https://aka.ms/icl}.",
      "published": "2022-12-20T18:58:48Z",
      "updated": "2023-05-15T11:45:12Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.10559.pdf",
      "abs_url": "http://arxiv.org/abs/2212.10559v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2201.03544": {
      "arxiv_id": "2201.03544",
      "title": "The Effects of Reward Misspecification: Mapping and Mitigating\n  Misaligned Models",
      "authors": [
        "Alexander Pan",
        "Kush Bhatia",
        "Jacob Steinhardt"
      ],
      "summary": "Reward hacking -- where RL agents exploit gaps in misspecified reward\nfunctions -- has been widely observed, but not yet systematically studied. To\nunderstand how reward hacking arises, we construct four RL environments with\nmisspecified rewards. We investigate reward hacking as a function of agent\ncapabilities: model capacity, action space resolution, observation space noise,\nand training time. More capable agents often exploit reward misspecifications,\nachieving higher proxy reward and lower true reward than less capable agents.\nMoreover, we find instances of phase transitions: capability thresholds at\nwhich the agent's behavior qualitatively shifts, leading to a sharp decrease in\nthe true reward. Such phase transitions pose challenges to monitoring the\nsafety of ML systems. To address this, we propose an anomaly detection task for\naberrant policies and offer several baseline detectors.",
      "published": "2022-01-10T18:58:52Z",
      "updated": "2022-02-14T09:05:38Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.03544.pdf",
      "abs_url": "http://arxiv.org/abs/2201.03544v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2307.15217": {
      "arxiv_id": "2307.15217",
      "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from\n  Human Feedback",
      "authors": [
        "Stephen Casper",
        "Xander Davies",
        "Claudia Shi",
        "Thomas Krendl Gilbert",
        "J\u00e9r\u00e9my Scheurer",
        "Javier Rando",
        "Rachel Freedman",
        "Tomasz Korbak",
        "David Lindner",
        "Pedro Freire",
        "Tony Wang",
        "Samuel Marks",
        "Charbel-Rapha\u00ebl Segerie",
        "Micah Carroll",
        "Andi Peng",
        "Phillip Christoffersen",
        "Mehul Damani",
        "Stewart Slocum",
        "Usman Anwar",
        "Anand Siththaranjan",
        "Max Nadeau",
        "Eric J. Michaud",
        "Jacob Pfau",
        "Dmitrii Krasheninnikov",
        "Xin Chen",
        "Lauro Langosco",
        "Peter Hase",
        "Erdem B\u0131y\u0131k",
        "Anca Dragan",
        "David Krueger",
        "Dorsa Sadigh",
        "Dylan Hadfield-Menell"
      ],
      "summary": "Reinforcement learning from human feedback (RLHF) is a technique for training\nAI systems to align with human goals. RLHF has emerged as the central method\nused to finetune state-of-the-art large language models (LLMs). Despite this\npopularity, there has been relatively little public work systematizing its\nflaws. In this paper, we (1) survey open problems and fundamental limitations\nof RLHF and related methods; (2) overview techniques to understand, improve,\nand complement RLHF in practice; and (3) propose auditing and disclosure\nstandards to improve societal oversight of RLHF systems. Our work emphasizes\nthe limitations of RLHF and highlights the importance of a multi-faceted\napproach to the development of safer AI systems.",
      "published": "2023-07-27T22:29:25Z",
      "updated": "2023-09-11T17:25:24Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.15217.pdf",
      "abs_url": "http://arxiv.org/abs/2307.15217v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.15833": {
      "arxiv_id": "2402.15833",
      "title": "Prompt Perturbation Consistency Learning for Robust Language Models",
      "authors": [
        "Yao Qiang",
        "Subhrangshu Nandi",
        "Ninareh Mehrabi",
        "Greg Ver Steeg",
        "Anoop Kumar",
        "Anna Rumshisky",
        "Aram Galstyan"
      ],
      "summary": "Large language models (LLMs) have demonstrated impressive performance on a\nnumber of natural language processing tasks, such as question answering and\ntext summarization. However, their performance on sequence labeling tasks such\nas intent classification and slot filling (IC-SF), which is a central component\nin personal assistant systems, lags significantly behind discriminative models.\nFurthermore, there is a lack of substantive research on the robustness of LLMs\nto various perturbations in the input prompts. The contributions of this paper\nare three-fold. First, we show that fine-tuning sufficiently large LLMs can\nproduce IC-SF performance comparable to discriminative models. Next, we\nsystematically analyze the performance deterioration of those fine-tuned models\ndue to three distinct yet relevant types of input perturbations - oronyms,\nsynonyms, and paraphrasing. Finally, we propose an efficient mitigation\napproach, Prompt Perturbation Consistency Learning (PPCL), which works by\nregularizing the divergence between losses from clean and perturbed samples.\nOur experiments demonstrate that PPCL can recover on average 59% and 69% of the\nperformance drop for IC and SF tasks, respectively. Furthermore, PPCL beats the\ndata augmentation approach while using ten times fewer augmented data samples.",
      "published": "2024-02-24T15:00:58Z",
      "updated": "2024-02-24T15:00:58Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.15833.pdf",
      "abs_url": "http://arxiv.org/abs/2402.15833v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2403.19270": {
      "arxiv_id": "2403.19270",
      "title": "sDPO: Don't Use Your Data All at Once",
      "authors": [
        "Dahyun Kim",
        "Yungi Kim",
        "Wonho Song",
        "Hyeonwoo Kim",
        "Yunsu Kim",
        "Sanghoon Kim",
        "Chanjun Park"
      ],
      "summary": "As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.",
      "published": "2024-03-28T09:56:04Z",
      "updated": "2024-10-07T04:21:15Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.19270.pdf",
      "abs_url": "http://arxiv.org/abs/2403.19270v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2009.06367": {
      "arxiv_id": "2009.06367",
      "title": "GeDi: Generative Discriminator Guided Sequence Generation",
      "authors": [
        "Ben Krause",
        "Akhilesh Deepak Gotmare",
        "Bryan McCann",
        "Nitish Shirish Keskar",
        "Shafiq Joty",
        "Richard Socher",
        "Nazneen Fatema Rajani"
      ],
      "summary": "While large-scale language models (LMs) are able to imitate the distribution\nof natural language well enough to generate realistic text, it is difficult to\ncontrol which regions of the distribution they generate. This is especially\nproblematic because datasets used for training large LMs usually contain\nsignificant toxicity, hate, bias, and negativity. We propose GeDi as an\nefficient method for using smaller LMs as generative discriminators to guide\ngeneration from large LMs to make them safer and more controllable. GeDi guides\ngeneration at each step by computing classification probabilities for all\npossible next tokens via Bayes rule by normalizing over two class-conditional\ndistributions; one conditioned on the desired attribute, or control code, and\nanother conditioned on the undesired attribute, or anti control code. We find\nthat GeDi gives stronger controllability than the state of the art method while\nalso achieving generation speeds more than 30 times faster. Additionally,\ntraining GeDi on only four topics allows us to controllably generate new topics\nzero-shot from just a keyword, unlocking a new capability that previous\ncontrollable generation methods do not have. Lastly, we show that GeDi can make\nGPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic\nquality, making it by far the most practical existing method for detoxifying\nlarge language models while maintaining a fast generation speed.",
      "published": "2020-09-14T17:45:36Z",
      "updated": "2020-10-22T14:14:09Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2009.06367.pdf",
      "abs_url": "http://arxiv.org/abs/2009.06367v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2304.06767": {
      "arxiv_id": "2304.06767",
      "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
      "authors": [
        "Hanze Dong",
        "Wei Xiong",
        "Deepanshu Goyal",
        "Yihan Zhang",
        "Winnie Chow",
        "Rui Pan",
        "Shizhe Diao",
        "Jipeng Zhang",
        "Kashun Shum",
        "Tong Zhang"
      ],
      "summary": "Generative foundation models are susceptible to implicit biases that can\narise from extensive unsupervised training data. Such biases can produce\nsuboptimal samples, skewed outcomes, and unfairness, with potentially serious\nconsequences. Consequently, aligning these models with human ethics and\npreferences is an essential step toward ensuring their responsible and\neffective deployment in real-world applications. Prior research has primarily\nemployed Reinforcement Learning from Human Feedback (RLHF) to address this\nproblem, where generative models are fine-tuned with RL algorithms guided by a\nhuman-feedback-informed reward model. However, the inefficiencies and\ninstabilities associated with RL algorithms frequently present substantial\nobstacles to the successful alignment, necessitating the development of a more\nrobust and streamlined approach. To this end, we introduce a new framework,\nReward rAnked FineTuning (RAFT), designed to align generative models\neffectively. Utilizing a reward model and a sufficient number of samples, our\napproach selects the high-quality samples, discarding those that exhibit\nundesired behavior, and subsequently enhancing the model by fine-tuning on\nthese filtered samples. Our studies show that RAFT can effectively improve the\nmodel performance in both reward learning and other automated metrics in both\nlarge language models and diffusion models.",
      "published": "2023-04-13T18:22:40Z",
      "updated": "2023-12-01T14:28:06Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.06767.pdf",
      "abs_url": "http://arxiv.org/abs/2304.06767v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2408.08656": {
      "arxiv_id": "2408.08656",
      "title": "LLMs Are Biased Towards Output Formats! Systematically Evaluating and\n  Mitigating Output Format Bias of LLMs",
      "authors": [
        "Do Xuan Long",
        "Hai Nguyen Ngoc",
        "Tiviatis Sim",
        "Hieu Dao",
        "Shafiq Joty",
        "Kenji Kawaguchi",
        "Nancy F. Chen",
        "Min-Yen Kan"
      ],
      "summary": "We present the first systematic evaluation examining format bias in\nperformance of large language models (LLMs). Our approach distinguishes between\ntwo categories of an evaluation metric under format constraints to reliably and\naccurately assess performance: one measures performance when format constraints\nare adhered to, while the other evaluates performance regardless of constraint\nadherence. We then define a metric for measuring the format bias of LLMs and\nestablish effective strategies to reduce it. Subsequently, we present our\nempirical format bias evaluation spanning four commonly used categories --\nmultiple-choice question-answer, wrapping, list, and mapping -- covering 15\nwidely-used formats. Our evaluation on eight generation tasks uncovers\nsignificant format bias across state-of-the-art LLMs. We further discover that\nimproving the format-instruction following capabilities of LLMs across formats\npotentially reduces format bias. Based on our evaluation findings, we study\nprompting and fine-tuning with synthesized format data techniques to mitigate\nformat bias. Our methods successfully reduce the variance in ChatGPT's\nperformance among wrapping formats from 235.33 to 0.71 (%$^2$).",
      "published": "2024-08-16T10:45:45Z",
      "updated": "2025-02-23T03:39:12Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08656.pdf",
      "abs_url": "http://arxiv.org/abs/2408.08656v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2110.04366": {
      "arxiv_id": "2110.04366",
      "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
      "authors": [
        "Junxian He",
        "Chunting Zhou",
        "Xuezhe Ma",
        "Taylor Berg-Kirkpatrick",
        "Graham Neubig"
      ],
      "summary": "Fine-tuning large pre-trained language models on downstream tasks has become\nthe de-facto learning paradigm in NLP. However, conventional approaches\nfine-tune all the parameters of the pre-trained model, which becomes\nprohibitive as the model size and the number of tasks grow. Recent work has\nproposed a variety of parameter-efficient transfer learning methods that only\nfine-tune a small number of (extra) parameters to attain strong performance.\nWhile effective, the critical ingredients for success and the connections among\nthe various methods are poorly understood. In this paper, we break down the\ndesign of state-of-the-art parameter-efficient transfer learning methods and\npresent a unified framework that establishes connections between them.\nSpecifically, we re-frame them as modifications to specific hidden states in\npre-trained models, and define a set of design dimensions along which different\nmethods vary, such as the function to compute the modification and the position\nto apply the modification. Through comprehensive empirical studies across\nmachine translation, text summarization, language understanding, and text\nclassification benchmarks, we utilize the unified view to identify important\ndesign choices in previous methods. Furthermore, our unified framework enables\nthe transfer of design elements across different approaches, and as a result we\nare able to instantiate new parameter-efficient fine-tuning methods that tune\nless parameters than previous methods while being more effective, achieving\ncomparable results to fine-tuning all parameters on all four tasks.",
      "published": "2021-10-08T20:22:26Z",
      "updated": "2022-02-02T16:39:23Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2110.04366.pdf",
      "abs_url": "http://arxiv.org/abs/2110.04366v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2401.08491": {
      "arxiv_id": "2401.08491",
      "title": "Contrastive Perplexity for Controlled Generation: An Application in\n  Detoxifying Large Language Models",
      "authors": [
        "Tassilo Klein",
        "Moin Nabi"
      ],
      "summary": "The generation of toxic content by large language models (LLMs) remains a\ncritical challenge for the safe deployment of language technology. We propose a\nnovel framework for implicit knowledge editing and controlled text generation\nby fine-tuning LLMs with a prototype-based contrastive perplexity objective.\nCentral to our method is the construction of hard negatives - toxic outputs\nthat are generated through adversarial paraphrasing to be semantically similar\nand model probability to their non-toxic counterparts. By training on these\nchallenging and realistic pairs, our approach ensures robust and stable\ncontrastive optimization. Experimental results in the domain of detoxification\ndemonstrate that our method significantly reduces toxic generation while\nmaintaining strong performance on downstream tasks such as commonsense\nreasoning and reading comprehension. Our findings highlight the effectiveness\nof exploiting hard negatives for attribute-aware fine-tuning.",
      "published": "2024-01-16T16:49:39Z",
      "updated": "2025-05-30T09:37:59Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.08491.pdf",
      "abs_url": "http://arxiv.org/abs/2401.08491v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.01766": {
      "arxiv_id": "2402.01766",
      "title": "LLM Voting: Human Choices and AI Collective Decision Making",
      "authors": [
        "Joshua C. Yang",
        "Damian Dailisan",
        "Marcin Korecki",
        "Carina I. Hausladen",
        "Dirk Helbing"
      ],
      "summary": "This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes.",
      "published": "2024-01-31T14:52:02Z",
      "updated": "2024-08-14T13:41:02Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "econ.GN",
        "q-fin.EC",
        "68T05, 91B14, 91C20",
        "I.2.7; J.4; K.4.1"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01766.pdf",
      "abs_url": "http://arxiv.org/abs/2402.01766v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2403.16512": {
      "arxiv_id": "2403.16512",
      "title": "LLMs Are Few-Shot In-Context Low-Resource Language Learners",
      "authors": [
        "Samuel Cahyawijaya",
        "Holy Lovenia",
        "Pascale Fung"
      ],
      "summary": "In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages. Our code is publicly\nreleased at https://github.com/SamuelCahyawijaya/in-context-alignment",
      "published": "2024-03-25T07:55:29Z",
      "updated": "2024-06-25T11:54:23Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16512.pdf",
      "abs_url": "http://arxiv.org/abs/2403.16512v5",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2104.08821": {
      "arxiv_id": "2104.08821",
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen"
      ],
      "summary": "This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation, and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework by using \"entailment\" pairs as\npositives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nthe previous best results. We also show -- both theoretically and empirically\n-- that the contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.",
      "published": "2021-04-18T11:27:08Z",
      "updated": "2022-05-18T12:29:49Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2104.08821.pdf",
      "abs_url": "http://arxiv.org/abs/2104.08821v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.19085": {
      "arxiv_id": "2402.19085",
      "title": "Controllable Preference Optimization: Toward Controllable\n  Multi-Objective Alignment",
      "authors": [
        "Yiju Guo",
        "Ganqu Cui",
        "Lifan Yuan",
        "Ning Ding",
        "Zexu Sun",
        "Bowen Sun",
        "Huimin Chen",
        "Ruobing Xie",
        "Jie Zhou",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "summary": "Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nimprovements in multi-objective alignment.",
      "published": "2024-02-29T12:12:30Z",
      "updated": "2024-10-11T08:21:50Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.19085.pdf",
      "abs_url": "http://arxiv.org/abs/2402.19085v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2411.07180": {
      "arxiv_id": "2411.07180",
      "title": "Gumbel Counterfactual Generation From Language Models",
      "authors": [
        "Shauli Ravfogel",
        "Anej Svete",
        "V\u00e9steinn Sn\u00e6bjarnarson",
        "Ryan Cotterell"
      ],
      "summary": "Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.",
      "published": "2024-11-11T17:57:30Z",
      "updated": "2025-03-06T15:26:56Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07180.pdf",
      "abs_url": "http://arxiv.org/abs/2411.07180v5",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2404.02078": {
      "arxiv_id": "2404.02078",
      "title": "Advancing LLM Reasoning Generalists with Preference Trees",
      "authors": [
        "Lifan Yuan",
        "Ganqu Cui",
        "Hanbin Wang",
        "Ning Ding",
        "Xingyao Wang",
        "Jia Deng",
        "Boji Shan",
        "Huimin Chen",
        "Ruobing Xie",
        "Yankai Lin",
        "Zhenghao Liu",
        "Bowen Zhou",
        "Hao Peng",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "summary": "We introduce Eurus, a suite of large language models (LLMs) optimized for\nreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve\nstate-of-the-art results among open-source models on a diverse set of\nbenchmarks covering mathematics, code generation, and logical reasoning\nproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a\ncomprehensive benchmarking across 12 tests covering five tasks, and achieves a\n33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging\nbenchmarks, substantially outperforming existing open-source models by margins\nmore than 13.3%. The strong performance of Eurus can be primarily attributed to\nUltraInteract, our newly-curated large-scale, high-quality alignment dataset\nspecifically designed for complex reasoning tasks. UltraInteract can be used in\nboth supervised fine-tuning and preference learning. For each instruction, it\nincludes a preference tree consisting of (1) reasoning chains with diverse\nplanning strategies in a unified format, (2) multi-turn interaction\ntrajectories with the environment and the critique, and (3) pairwise data to\nfacilitate preference learning. UltraInteract allows us to conduct an in-depth\nexploration of preference learning for reasoning tasks. Our investigation\nreveals that some well-established preference learning algorithms may be less\nsuitable for reasoning tasks compared to their effectiveness in general\nconversations. Inspired by this, we derive a novel reward modeling objective\nwhich, together with UltraInteract, leads to a strong reward model.",
      "published": "2024-04-02T16:25:30Z",
      "updated": "2024-04-02T16:25:30Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.02078.pdf",
      "abs_url": "http://arxiv.org/abs/2404.02078v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2401.12187": {
      "arxiv_id": "2401.12187",
      "title": "WARM: On the Benefits of Weight Averaged Reward Models",
      "authors": [
        "Alexandre Ram\u00e9",
        "Nino Vieillard",
        "L\u00e9onard Hussenot",
        "Robert Dadashi",
        "Geoffrey Cideron",
        "Olivier Bachem",
        "Johan Ferret"
      ],
      "summary": "Aligning large language models (LLMs) with human preferences through\nreinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit\nfailures in the reward model (RM) to achieve seemingly high rewards without\nmeeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL\nprocess and inconsistencies in human preferences. As a solution, we propose\nWeight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then\naveraging them in the weight space. This strategy follows the observation that\nfine-tuned weights remain linearly mode connected when sharing the same\npre-training. By averaging weights, WARM improves efficiency compared to the\ntraditional ensembling of predictions, while improving reliability under\ndistribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-N and RL methods, shows that\nWARM improves the overall quality and alignment of LLM predictions; for\nexample, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy\nRL fine-tuned with a single RM.",
      "published": "2024-01-22T18:27:08Z",
      "updated": "2024-01-22T18:27:08Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12187.pdf",
      "abs_url": "http://arxiv.org/abs/2401.12187v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2305.14233": {
      "arxiv_id": "2305.14233",
      "title": "Enhancing Chat Language Models by Scaling High-quality Instructional\n  Conversations",
      "authors": [
        "Ning Ding",
        "Yulin Chen",
        "Bokai Xu",
        "Yujia Qin",
        "Zhi Zheng",
        "Shengding Hu",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Bowen Zhou"
      ],
      "summary": "Fine-tuning on instruction data has been widely validated as an effective\npractice for implementing chat language models like ChatGPT. Scaling the\ndiversity and quality of such data, although straightforward, stands a great\nchance of leading to improved performance. This paper aims to improve the upper\nbound of open-source models further. We first provide a systematically\ndesigned, diverse, informative, large-scale dataset of instructional\nconversations, UltraChat, which does not involve human queries. Our objective\nis to capture the breadth of interactions that a human might have with an AI\nassistant and employs a comprehensive framework to generate multi-turn\nconversation iteratively. UltraChat contains 1.5 million high-quality\nmulti-turn dialogues and covers a wide range of topics and instructions. Our\nstatistical analysis of UltraChat reveals its superiority in various key\nmetrics, including scale, average length, diversity, coherence, etc.,\nsolidifying its position as a leading open-source dataset. Building upon\nUltraChat, we fine-tune a LLaMA model to create a powerful conversational\nmodel, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently\noutperforms other open-source models, including Vicuna, the previously\nrecognized state-of-the-art open-source model. The dataset and the model will\nbe publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.",
      "published": "2023-05-23T16:49:14Z",
      "updated": "2023-05-23T16:49:14Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.14233.pdf",
      "abs_url": "http://arxiv.org/abs/2305.14233v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2406.05534": {
      "arxiv_id": "2406.05534",
      "title": "Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing",
      "authors": [
        "Biqing Qi",
        "Pengfei Li",
        "Fangyuan Li",
        "Junqi Gao",
        "Kaiyan Zhang",
        "Bowen Zhou"
      ],
      "summary": "Direct Preference Optimization (DPO) improves the alignment of large language\nmodels (LLMs) with human values by training directly on human preference\ndatasets, eliminating the need for reward models. However, due to the presence\nof cross-domain human preferences, direct continual training can lead to\ncatastrophic forgetting, limiting DPO's performance and efficiency. Inspired by\nintraspecific competition driving species evolution, we propose a Online\nFast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating\ncompetition through fast and slow chasing among models to facilitate rapid\nadaptation. Specifically, we first derive the regret upper bound for online\nlearning, validating our motivation with a min-max optimization pattern. Based\non this, we introduce two identical modules using Low-rank Adaptive (LoRA) with\ndifferent optimization speeds to simulate intraspecific competition, and\npropose a new regularization term to guide their learning. To further mitigate\ncatastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with\nLoRA modules combination strategy, resulting in the Cross domain Online\nFast-Slow chasing DPO (COFS-DPO). This method leverages linear combinations of\nfast modules parameters from different task domains, fully utilizing historical\ninformation to achive continual value alignment. Experimental results show that\nOFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in\ncross-domain continual learning scenarios.",
      "published": "2024-06-08T17:30:54Z",
      "updated": "2024-06-08T17:30:54Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05534.pdf",
      "abs_url": "http://arxiv.org/abs/2406.05534v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2204.02329": {
      "arxiv_id": "2204.02329",
      "title": "Can language models learn from explanations in context?",
      "authors": [
        "Andrew K. Lampinen",
        "Ishita Dasgupta",
        "Stephanie C. Y. Chan",
        "Kory Matthewson",
        "Michael Henry Tessler",
        "Antonia Creswell",
        "James L. McClelland",
        "Jane X. Wang",
        "Felix Hill"
      ],
      "summary": "Language Models (LMs) can perform new tasks by adapting to a few in-context\nexamples. For humans, explanations that connect examples to task principles can\nimprove learning. We therefore investigate whether explanations of few-shot\nexamples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how\ndifferent types of explanations, instructions, and controls affect zero- and\nfew-shot performance. We analyze these results using statistical multilevel\nmodeling techniques that account for the nested dependencies among conditions,\ntasks, prompts, and models. We find that explanations can improve performance\n-- even without tuning. Furthermore, explanations hand-tuned for performance on\na small validation set offer substantially larger benefits, and building a\nprompt by selecting examples and explanations together substantially improves\nperformance over selecting examples alone. Finally, even untuned explanations\noutperform carefully matched controls, suggesting that the benefits are due to\nthe link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit. In summary, explanations can\nsupport the in-context learning of large LMs on challenging tasks.",
      "published": "2022-04-05T16:33:44Z",
      "updated": "2022-10-10T15:25:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.02329.pdf",
      "abs_url": "http://arxiv.org/abs/2204.02329v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2306.02561": {
      "arxiv_id": "2306.02561",
      "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and\n  Generative Fusion",
      "authors": [
        "Dongfu Jiang",
        "Xiang Ren",
        "Bill Yuchen Lin"
      ],
      "summary": "We present LLM-Blender, an ensembling framework designed to attain\nconsistently superior performance by leveraging the diverse strengths of\nmultiple open-source large language models (LLMs). Our framework consists of\ntwo modules: PairRanker and GenFuser, addressing the observation that optimal\nLLMs for different examples can significantly vary. PairRanker employs a\nspecialized pairwise comparison method to distinguish subtle differences\nbetween candidate outputs. It jointly encodes the input text and a pair of\ncandidates, using cross-attention encoders to determine the superior one. Our\nresults demonstrate that PairRanker exhibits the highest correlation with\nChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,\ngenerating an improved output by capitalizing on their strengths and mitigating\ntheir weaknesses. To facilitate large-scale evaluation, we introduce a\nbenchmark dataset, MixInstruct, which is a mixture of multiple instruction\ndatasets featuring oracle pairwise comparisons. Our LLM-Blender significantly\noutperform individual LLMs and baseline methods across various metrics,\nestablishing a substantial performance gap.",
      "published": "2023-06-05T03:32:26Z",
      "updated": "2023-06-30T21:39:54Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.02561.pdf",
      "abs_url": "http://arxiv.org/abs/2306.02561v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2404.19733": {
      "arxiv_id": "2404.19733",
      "title": "Iterative Reasoning Preference Optimization",
      "authors": [
        "Richard Yuanzhe Pang",
        "Weizhe Yuan",
        "Kyunghyun Cho",
        "He He",
        "Sainbayar Sukhbaatar",
        "Jason Weston"
      ],
      "summary": "Iterative preference optimization methods have recently been shown to perform\nwell for general instruction tuning tasks, but typically make little\nimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this\nwork we develop an iterative approach that optimizes the preference between\ncompeting generated Chain-of-Thought (CoT) candidates by optimizing for winning\nvs. losing reasoning steps that lead to the correct answer. We train using a\nmodified DPO loss (Rafailov et al., 2023) with an additional negative\nlog-likelihood term, which we find to be crucial. We show reasoning improves\nacross repeated iterations of this scheme. While only relying on examples in\nthe training set, our approach results in increasing accuracy on GSM8K, MATH,\nand ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based\nmodels not relying on additionally sourced datasets. For example, we see a\nlarge improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with\nmajority voting out of 32 samples.",
      "published": "2024-04-30T17:28:05Z",
      "updated": "2024-06-26T01:28:35Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19733.pdf",
      "abs_url": "http://arxiv.org/abs/2404.19733v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2412.15115": {
      "arxiv_id": "2412.15115",
      "title": "Qwen2.5 Technical Report",
      "authors": [
        "Qwen",
        ":",
        "An Yang",
        "Baosong Yang",
        "Beichen Zhang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang",
        "Haoran Wei",
        "Huan Lin",
        "Jian Yang",
        "Jianhong Tu",
        "Jianwei Zhang",
        "Jianxin Yang",
        "Jiaxi Yang",
        "Jingren Zhou",
        "Junyang Lin",
        "Kai Dang",
        "Keming Lu",
        "Keqin Bao",
        "Kexin Yang",
        "Le Yu",
        "Mei Li",
        "Mingfeng Xue",
        "Pei Zhang",
        "Qin Zhu",
        "Rui Men",
        "Runji Lin",
        "Tianhao Li",
        "Tianyi Tang",
        "Tingyu Xia",
        "Xingzhang Ren",
        "Xuancheng Ren",
        "Yang Fan",
        "Yang Su",
        "Yichang Zhang",
        "Yu Wan",
        "Yuqiong Liu",
        "Zeyu Cui",
        "Zhenru Zhang",
        "Zihan Qiu"
      ],
      "summary": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.",
      "published": "2024-12-19T17:56:09Z",
      "updated": "2025-01-03T02:18:21Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15115.pdf",
      "abs_url": "http://arxiv.org/abs/2412.15115v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.01306": {
      "arxiv_id": "2402.01306",
      "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
      "authors": [
        "Kawin Ethayarajh",
        "Winnie Xu",
        "Niklas Muennighoff",
        "Dan Jurafsky",
        "Douwe Kiela"
      ],
      "summary": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration.",
      "published": "2024-02-02T10:53:36Z",
      "updated": "2024-11-19T18:12:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01306.pdf",
      "abs_url": "http://arxiv.org/abs/2402.01306v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1711.05101": {
      "arxiv_id": "1711.05101",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "summary": "L$_2$ regularization and weight decay regularization are equivalent for\nstandard stochastic gradient descent (when rescaled by the learning rate), but\nas we demonstrate this is \\emph{not} the case for adaptive gradient algorithms,\nsuch as Adam. While common implementations of these algorithms employ L$_2$\nregularization (often calling it \"weight decay\" in what may be misleading due\nto the inequivalence we expose), we propose a simple modification to recover\nthe original formulation of weight decay regularization by \\emph{decoupling}\nthe weight decay from the optimization steps taken w.r.t. the loss function. We\nprovide empirical evidence that our proposed modification (i) decouples the\noptimal choice of weight decay factor from the setting of the learning rate for\nboth standard SGD and Adam and (ii) substantially improves Adam's\ngeneralization performance, allowing it to compete with SGD with momentum on\nimage classification datasets (on which it was previously typically\noutperformed by the latter). Our proposed decoupled weight decay has already\nbeen adopted by many researchers, and the community has implemented it in\nTensorFlow and PyTorch; the complete source code for our experiments is\navailable at https://github.com/loshchil/AdamW-and-SGDW",
      "published": "2017-11-14T14:24:06Z",
      "updated": "2019-01-04T21:01:49Z",
      "categories": [
        "cs.LG",
        "cs.NE",
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/1711.05101.pdf",
      "abs_url": "http://arxiv.org/abs/1711.05101v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2405.07863": {
      "arxiv_id": "2405.07863",
      "title": "RLHF Workflow: From Reward Modeling to Online RLHF",
      "authors": [
        "Hanze Dong",
        "Wei Xiong",
        "Bo Pang",
        "Haoxiang Wang",
        "Han Zhao",
        "Yingbo Zhou",
        "Nan Jiang",
        "Doyen Sahoo",
        "Caiming Xiong",
        "Tong Zhang"
      ],
      "summary": "We present the workflow of Online Iterative Reinforcement Learning from Human\nFeedback (RLHF) in this technical report, which is widely reported to\noutperform its offline counterpart by a large margin in the recent large\nlanguage model (LLM) literature. However, existing open-source RLHF projects\nare still largely confined to the offline learning setting. In this technical\nreport, we aim to fill in this gap and provide a detailed recipe that is easy\nto reproduce for online iterative RLHF. In particular, since online human\nfeedback is usually infeasible for open-source communities with limited\nresources, we start by constructing preference models using a diverse set of\nopen-source datasets and use the constructed proxy preference model to\napproximate human feedback. Then, we discuss the theoretical insights and\nalgorithmic principles behind online iterative RLHF, followed by a detailed\npractical implementation. Our trained LLM achieves impressive performance on\nLLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as\nwell as other academic benchmarks such as HumanEval and TruthfulQA. We have\nshown that supervised fine-tuning (SFT) and iterative RLHF can obtain\nstate-of-the-art performance with fully open-source datasets. Further, we have\nmade our models, curated datasets, and comprehensive step-by-step code\nguidebooks publicly available. Please refer to\nhttps://github.com/RLHFlow/RLHF-Reward-Modeling and\nhttps://github.com/RLHFlow/Online-RLHF for more detailed information.",
      "published": "2024-05-13T15:50:39Z",
      "updated": "2024-11-12T11:18:43Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07863.pdf",
      "abs_url": "http://arxiv.org/abs/2405.07863v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2101.00190": {
      "arxiv_id": "2101.00190",
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "authors": [
        "Xiang Lisa Li",
        "Percy Liang"
      ],
      "summary": "Fine-tuning is the de facto way to leverage large pretrained language models\nto perform downstream tasks. However, it modifies all the language model\nparameters and therefore necessitates storing a full copy for each task. In\nthis paper, we propose prefix-tuning, a lightweight alternative to fine-tuning\nfor natural language generation tasks, which keeps language model parameters\nfrozen, but optimizes a small continuous task-specific vector (called the\nprefix). Prefix-tuning draws inspiration from prompting, allowing subsequent\ntokens to attend to this prefix as if it were \"virtual tokens\". We apply\nprefix-tuning to GPT-2 for table-to-text generation and to BART for\nsummarization. We find that by learning only 0.1\\% of the parameters,\nprefix-tuning obtains comparable performance in the full data setting,\noutperforms fine-tuning in low-data settings, and extrapolates better to\nexamples with topics unseen during training.",
      "published": "2021-01-01T08:00:36Z",
      "updated": "2021-01-01T08:00:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2101.00190.pdf",
      "abs_url": "http://arxiv.org/abs/2101.00190v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2401.12086": {
      "arxiv_id": "2401.12086",
      "title": "West-of-N: Synthetic Preferences for Self-Improving Reward Models",
      "authors": [
        "Aliz\u00e9e Pace",
        "Jonathan Mallinson",
        "Eric Malmi",
        "Sebastian Krause",
        "Aliaksei Severyn"
      ],
      "summary": "The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.",
      "published": "2024-01-22T16:24:43Z",
      "updated": "2024-10-25T12:04:26Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12086.pdf",
      "abs_url": "http://arxiv.org/abs/2401.12086v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2504.00891": {
      "arxiv_id": "2504.00891",
      "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
      "authors": [
        "Jian Zhao",
        "Runze Liu",
        "Kaiyan Zhang",
        "Zhimu Zhou",
        "Junqi Gao",
        "Dong Li",
        "Jiafei Lyu",
        "Zhouyi Qian",
        "Biqing Qi",
        "Xiu Li",
        "Bowen Zhou"
      ],
      "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
      "published": "2025-04-01T15:21:05Z",
      "updated": "2025-04-05T03:04:37Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.00891.pdf",
      "abs_url": "http://arxiv.org/abs/2504.00891v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1511.06732": {
      "arxiv_id": "1511.06732",
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "authors": [
        "Marc'Aurelio Ranzato",
        "Sumit Chopra",
        "Michael Auli",
        "Wojciech Zaremba"
      ],
      "summary": "Many natural language processing applications use language models to generate\ntext. These models are typically trained to predict the next word in a\nsequence, given the previous words and some context such as an image. However,\nat test time the model is expected to generate the entire sequence from\nscratch. This discrepancy makes generation brittle, as errors may accumulate\nalong the way. We address this issue by proposing a novel sequence level\ntraining algorithm that directly optimizes the metric used at test time, such\nas BLEU or ROUGE. On three different tasks, our approach outperforms several\nstrong baselines for greedy generation. The method is also competitive when\nthese baselines employ beam search, while being several times faster.",
      "published": "2015-11-20T19:25:54Z",
      "updated": "2016-05-06T21:18:46Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/1511.06732.pdf",
      "abs_url": "http://arxiv.org/abs/1511.06732v7",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2409.11704": {
      "arxiv_id": "2409.11704",
      "title": "From Lists to Emojis: How Format Bias Affects Model Alignment",
      "authors": [
        "Xuanchang Zhang",
        "Wei Xiong",
        "Lichang Chen",
        "Tianyi Zhou",
        "Heng Huang",
        "Tong Zhang"
      ],
      "summary": "In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models.",
      "published": "2024-09-18T05:13:18Z",
      "updated": "2025-05-23T16:32:54Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11704.pdf",
      "abs_url": "http://arxiv.org/abs/2409.11704v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2203.04291": {
      "arxiv_id": "2203.04291",
      "title": "Learning from Few Examples: A Summary of Approaches to Few-Shot Learning",
      "authors": [
        "Archit Parnami",
        "Minwoo Lee"
      ],
      "summary": "Few-Shot Learning refers to the problem of learning the underlying pattern in\nthe data just from a few training samples. Requiring a large number of data\nsamples, many deep learning solutions suffer from data hunger and extensively\nhigh computation time and resources. Furthermore, data is often not available\ndue to not only the nature of the problem or privacy concerns but also the cost\nof data preparation. Data collection, preprocessing, and labeling are strenuous\nhuman tasks. Therefore, few-shot learning that could drastically reduce the\nturnaround time of building machine learning applications emerges as a low-cost\nsolution. This survey paper comprises a representative list of recently\nproposed few-shot learning algorithms. Given the learning dynamics and\ncharacteristics, the approaches to few-shot learning problems are discussed in\nthe perspectives of meta-learning, transfer learning, and hybrid approaches\n(i.e., different variations of the few-shot learning problem).",
      "published": "2022-03-07T23:15:21Z",
      "updated": "2022-03-07T23:15:21Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2203.04291.pdf",
      "abs_url": "http://arxiv.org/abs/2203.04291v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2409.13156": {
      "arxiv_id": "2409.13156",
      "title": "RRM: Robust Reward Model Training Mitigates Reward Hacking",
      "authors": [
        "Tianqi Liu",
        "Wei Xiong",
        "Jie Ren",
        "Lichang Chen",
        "Junru Wu",
        "Rishabh Joshi",
        "Yang Gao",
        "Jiaming Shen",
        "Zhen Qin",
        "Tianhe Yu",
        "Daniel Sohn",
        "Anastasiia Makarova",
        "Jeremiah Liu",
        "Yuan Liu",
        "Bilal Piot",
        "Abe Ittycheriah",
        "Aviral Kumar",
        "Mohammad Saleh"
      ],
      "summary": "Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human preferences. However, traditional RM training, which relies\non response pairs tied to specific prompts, struggles to disentangle\nprompt-driven preferences from prompt-independent artifacts, such as response\nlength and format. In this work, we expose a fundamental limitation of current\nRM training methods, where RMs fail to effectively distinguish between\ncontextual signals and irrelevant artifacts when determining preferences. To\naddress this, we introduce a causal framework that learns preferences\nindependent of these artifacts and propose a novel data augmentation technique\ndesigned to eliminate them. Extensive experiments show that our approach\nsuccessfully filters out undesirable artifacts, yielding a more robust reward\nmodel (RRM). Our RRM improves the performance of a pairwise reward model\ntrained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to\n84.15%. Additionally, we train two DPO policies using both the RM and RRM,\ndemonstrating that the RRM significantly enhances DPO-aligned policies,\nimproving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in\nAlpacaEval-2 from 33.46% to 52.49%.",
      "published": "2024-09-20T01:46:07Z",
      "updated": "2025-02-27T16:30:42Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.13156.pdf",
      "abs_url": "http://arxiv.org/abs/2409.13156v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2004.10964": {
      "arxiv_id": "2004.10964",
      "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "authors": [
        "Suchin Gururangan",
        "Ana Marasovi\u0107",
        "Swabha Swayamdipta",
        "Kyle Lo",
        "Iz Beltagy",
        "Doug Downey",
        "Noah A. Smith"
      ],
      "summary": "Language models pretrained on text from a wide variety of sources form the\nfoundation of today's NLP. In light of the success of these broad-coverage\nmodels, we investigate whether it is still helpful to tailor a pretrained model\nto the domain of a target task. We present a study across four domains\n(biomedical and computer science publications, news, and reviews) and eight\nclassification tasks, showing that a second phase of pretraining in-domain\n(domain-adaptive pretraining) leads to performance gains, under both high- and\nlow-resource settings. Moreover, adapting to the task's unlabeled data\n(task-adaptive pretraining) improves performance even after domain-adaptive\npretraining. Finally, we show that adapting to a task corpus augmented using\nsimple data selection strategies is an effective alternative, especially when\nresources for domain-adaptive pretraining might be unavailable. Overall, we\nconsistently find that multi-phase adaptive pretraining offers large gains in\ntask performance.",
      "published": "2020-04-23T04:21:19Z",
      "updated": "2020-05-05T22:00:44Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2004.10964.pdf",
      "abs_url": "http://arxiv.org/abs/2004.10964v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2410.00847": {
      "arxiv_id": "2410.00847",
      "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is\n  Unknown",
      "authors": [
        "Xingzhou Lou",
        "Dong Yan",
        "Wei Shen",
        "Yuzi Yan",
        "Jian Xie",
        "Junge Zhang"
      ],
      "summary": "Reward models (RMs) are essential for aligning large language models (LLM)\nwith human expectations. However, existing RMs struggle to capture the\nstochastic and uncertain nature of human preferences and fail to assess the\nreliability of reward predictions. To address these challenges, we introduce\nthe Uncertainty-aware Reward Model (URM) and its ensemble variant, URME. URM\nemploys a probabilistic value head to capture aleatoric uncertainty by modeling\nthe distribution of disentangled human preference attributes. URME further\nquantifies epistemic uncertainty by examining discrepancies among individual\nURMs within the ensemble, enabling identification of unreliable evaluations.\nOur empirical evaluations demonstrate that URM achieves strong performance on\nRewardBench, outperforming competitive large-scale models. Additionally,\nextensive experiments, including best-of-n sampling (BoN), iterative direct\npreference optimization (iterative DPO), and proximal policy optimization\n(PPO), demonstrate that URM and URME significantly enhance LLMs' generation\nquality. Notably, reward predictions with lower uncertainty are far more\nreliable, demonstrate significantly higher quality, and result in substantially\nimproved alignment.",
      "published": "2024-10-01T16:29:59Z",
      "updated": "2025-02-12T03:34:29Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00847.pdf",
      "abs_url": "http://arxiv.org/abs/2410.00847v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2005.00247": {
      "arxiv_id": "2005.00247",
      "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
      "authors": [
        "Jonas Pfeiffer",
        "Aishwarya Kamath",
        "Andreas R\u00fcckl\u00e9",
        "Kyunghyun Cho",
        "Iryna Gurevych"
      ],
      "summary": "Sequential fine-tuning and multi-task learning are methods aiming to\nincorporate knowledge from multiple tasks; however, they suffer from\ncatastrophic forgetting and difficulties in dataset balancing. To address these\nshortcomings, we propose AdapterFusion, a new two stage learning algorithm that\nleverages knowledge from multiple tasks. First, in the knowledge extraction\nstage we learn task specific parameters called adapters, that encapsulate the\ntask-specific information. We then combine the adapters in a separate knowledge\ncomposition step. We show that by separating the two stages, i.e., knowledge\nextraction and knowledge composition, the classifier can effectively exploit\nthe representations learned from multiple tasks in a non-destructive manner. We\nempirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it\neffectively combines various types of knowledge at different layers of the\nmodel. We show that our approach outperforms traditional strategies such as\nfull fine-tuning as well as multi-task learning. Our code and adapters are\navailable at AdapterHub.ml.",
      "published": "2020-05-01T07:03:42Z",
      "updated": "2021-01-26T12:54:33Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2005.00247.pdf",
      "abs_url": "http://arxiv.org/abs/2005.00247v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2303.03846": {
      "arxiv_id": "2303.03846",
      "title": "Larger language models do in-context learning differently",
      "authors": [
        "Jerry Wei",
        "Jason Wei",
        "Yi Tay",
        "Dustin Tran",
        "Albert Webson",
        "Yifeng Lu",
        "Xinyun Chen",
        "Hanxiao Liu",
        "Da Huang",
        "Denny Zhou",
        "Tengyu Ma"
      ],
      "summary": "We study how in-context learning (ICL) in language models is affected by\nsemantic priors versus input-label mappings. We investigate two setups-ICL with\nflipped labels and ICL with semantically-unrelated labels-across various model\nfamilies (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments\non ICL with flipped labels show that overriding semantic priors is an emergent\nability of model scale. While small language models ignore flipped labels\npresented in-context and thus rely primarily on semantic priors from\npretraining, large models can override semantic priors when presented with\nin-context exemplars that contradict priors, despite the stronger semantic\npriors that larger models may hold. We next study semantically-unrelated label\nICL (SUL-ICL), in which labels are semantically unrelated to their inputs\n(e.g., foo/bar instead of negative/positive), thereby forcing language models\nto learn the input-label mappings shown in in-context exemplars in order to\nperform the task. The ability to do SUL-ICL also emerges primarily with scale,\nand large-enough language models can even perform linear classification in a\nSUL-ICL setting. Finally, we evaluate instruction-tuned models and find that\ninstruction tuning strengthens both the use of semantic priors and the capacity\nto learn input-label mappings, but more of the former.",
      "published": "2023-03-07T12:24:17Z",
      "updated": "2023-03-08T07:37:43Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03846.pdf",
      "abs_url": "http://arxiv.org/abs/2303.03846v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2301.13819": {
      "arxiv_id": "2301.13819",
      "title": "Causal-Discovery Performance of ChatGPT in the context of Neuropathic\n  Pain Diagnosis",
      "authors": [
        "Ruibo Tu",
        "Chao Ma",
        "Cheng Zhang"
      ],
      "summary": "ChatGPT has demonstrated exceptional proficiency in natural language\nconversation, e.g., it can answer a wide range of questions while no previous\nlarge language models can. Thus, we would like to push its limit and explore\nits ability to answer causal discovery questions by using a medical benchmark\n(Tu et al. 2019) in causal discovery.",
      "published": "2023-01-24T19:23:38Z",
      "updated": "2023-02-06T12:03:38Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.13819.pdf",
      "abs_url": "http://arxiv.org/abs/2301.13819v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2406.11717": {
      "arxiv_id": "2406.11717",
      "title": "Refusal in Language Models Is Mediated by a Single Direction",
      "authors": [
        "Andy Arditi",
        "Oscar Obeso",
        "Aaquib Syed",
        "Daniel Paleka",
        "Nina Panickssery",
        "Wes Gurnee",
        "Neel Nanda"
      ],
      "summary": "Conversational large language models are fine-tuned for both\ninstruction-following and safety, resulting in models that obey benign requests\nbut refuse harmful ones. While this refusal behavior is widespread across chat\nmodels, its underlying mechanisms remain poorly understood. In this work, we\nshow that refusal is mediated by a one-dimensional subspace, across 13 popular\nopen-source chat models up to 72B parameters in size. Specifically, for each\nmodel, we find a single direction such that erasing this direction from the\nmodel's residual stream activations prevents it from refusing harmful\ninstructions, while adding this direction elicits refusal on even harmless\ninstructions. Leveraging this insight, we propose a novel white-box jailbreak\nmethod that surgically disables refusal with minimal effect on other\ncapabilities. Finally, we mechanistically analyze how adversarial suffixes\nsuppress propagation of the refusal-mediating direction. Our findings\nunderscore the brittleness of current safety fine-tuning methods. More broadly,\nour work showcases how an understanding of model internals can be leveraged to\ndevelop practical methods for controlling model behavior.",
      "published": "2024-06-17T16:36:12Z",
      "updated": "2024-10-30T18:57:07Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11717.pdf",
      "abs_url": "http://arxiv.org/abs/2406.11717v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2310.03708": {
      "arxiv_id": "2310.03708",
      "title": "Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct\n  Preference Optimization",
      "authors": [
        "Zhanhui Zhou",
        "Jie Liu",
        "Jing Shao",
        "Xiangyu Yue",
        "Chao Yang",
        "Wanli Ouyang",
        "Yu Qiao"
      ],
      "summary": "A single language model, even when aligned with labelers through\nreinforcement learning from human feedback (RLHF), may not suit all human\npreferences. Recent approaches therefore prefer customization, gathering\nmulti-dimensional feedback, and creating distinct reward models for each\ndimension. Different language models are then optimized for various preferences\nusing multi-objective RLHF (MORLHF) with varying reward weights. However, RL\nfine-tuning is unstable and resource-heavy, especially with diverse and usually\nconflicting objectives. In this paper, we present Multi-Objective Direct\nPreference Optimization (MODPO), an RL-free extension of Direct Preference\nOptimization (DPO) for multiple alignment objectives. Essentially, MODPO folds\nlanguage modeling directly into reward modeling, training language models as\nimplicit collective reward models that combine all objectives with specific\nweights. MODPO theoretically yields the same optimal solutions as MORLHF but is\npractically more stable and efficient. Empirical results in safety alignment\nand long-form question answering show that MODPO matches or outperforms\nexisting methods, producing a Pareto front of language models catering to\ndiverse preferences with three times less computational resources compared to\nMORLHF. Code is available at https://github.com/ZHZisZZ/modpo.",
      "published": "2023-10-05T17:35:26Z",
      "updated": "2024-08-17T13:39:13Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03708.pdf",
      "abs_url": "http://arxiv.org/abs/2310.03708v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2504.16828": {
      "arxiv_id": "2504.16828",
      "title": "Process Reward Models That Think",
      "authors": [
        "Muhammad Khalifa",
        "Rishabh Agarwal",
        "Lajanugen Logeswaran",
        "Jaekyeom Kim",
        "Hao Peng",
        "Moontae Lee",
        "Honglak Lee",
        "Lu Wang"
      ],
      "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm.",
      "published": "2025-04-23T15:44:54Z",
      "updated": "2025-06-24T03:05:02Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.16828.pdf",
      "abs_url": "http://arxiv.org/abs/2504.16828v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2304.11082": {
      "arxiv_id": "2304.11082",
      "title": "Fundamental Limitations of Alignment in Large Language Models",
      "authors": [
        "Yotam Wolf",
        "Noam Wies",
        "Oshri Avnery",
        "Yoav Levine",
        "Amnon Shashua"
      ],
      "summary": "An important aspect in developing language models that interact with humans\nis aligning their behavior to be useful and unharmful for their human users.\nThis is usually achieved by tuning the model in a way that enhances desired\nbehaviors and inhibits undesired ones, a process referred to as alignment. In\nthis paper, we propose a theoretical approach called Behavior Expectation\nBounds (BEB) which allows us to formally investigate several inherent\ncharacteristics and limitations of alignment in large language models.\nImportantly, we prove that within the limits of this framework, for any\nbehavior that has a finite probability of being exhibited by the model, there\nexist prompts that can trigger the model into outputting this behavior, with\nprobability that increases with the length of the prompt. This implies that any\nalignment process that attenuates an undesired behavior but does not remove it\naltogether, is not safe against adversarial prompting attacks. Furthermore, our\nframework hints at the mechanism by which leading alignment approaches such as\nreinforcement learning from human feedback make the LLM prone to being prompted\ninto the undesired behaviors. This theoretical result is being experimentally\ndemonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\",\nwhere adversarial users trick the LLM into breaking its alignment guardrails by\ntriggering it into acting as a malicious persona. Our results expose\nfundamental limitations in alignment of LLMs and bring to the forefront the\nneed to devise reliable mechanisms for ensuring AI safety.",
      "published": "2023-04-19T17:50:09Z",
      "updated": "2024-06-03T12:19:16Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.11082.pdf",
      "abs_url": "http://arxiv.org/abs/2304.11082v6",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2412.04628": {
      "arxiv_id": "2412.04628",
      "title": "Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts",
      "authors": [
        "Taneesh Gupta",
        "Rahul Madhavan",
        "Xuchao Zhang",
        "Nagarajan Natarajan",
        "Chetan Bansal",
        "Saravan Rajmohan"
      ],
      "summary": "Direct Preference Optimization (DPO) has become a popular approach for\naligning language models using pairwise preferences. However, in practical\npost-training pipelines, on-policy generation typically yields multiple\ncandidate responses per prompt, which are scored by a reward model to guide\nlearning. In this setting, we propose $\\textbf{Multi-Preference Optimization\n(MPO)}$, a generalization of DPO that optimizes over entire sets of responses\nby extending the Bradley-Terry model to groupwise comparisons between chosen\nand rejected sets. To further enhance learning, MPO employs deviation-based\nweighting, which emphasizes outlier responses that deviate most from the mean\nreward, effectively inducing a self-paced curriculum. We theoretically prove\nthat MPO reduces alignment bias at a rate of\n$\\mathcal{O}\\left(\\frac{1}{\\sqrt{n}}\\right)$ with respect to the number of\nresponses per query. Empirically, MPO achieves state-of-the-art performance on\nthe UltraFeedback benchmark and yields up to $\\sim 17.5\\%$ improvement over the\nstate-of-the-art baseline in length-controlled win rate on AlpacaEval2,\nestablishing a new baseline for preference-based alignment",
      "published": "2024-12-05T21:50:22Z",
      "updated": "2025-06-19T11:00:28Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04628.pdf",
      "abs_url": "http://arxiv.org/abs/2412.04628v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2403.13787": {
      "arxiv_id": "2403.13787",
      "title": "RewardBench: Evaluating Reward Models for Language Modeling",
      "authors": [
        "Nathan Lambert",
        "Valentina Pyatkin",
        "Jacob Morrison",
        "LJ Miranda",
        "Bill Yuchen Lin",
        "Khyathi Chandu",
        "Nouha Dziri",
        "Sachin Kumar",
        "Tom Zick",
        "Yejin Choi",
        "Noah A. Smith",
        "Hannaneh Hajishirzi"
      ],
      "summary": "Reward models (RMs) are at the crux of successfully using RLHF to align\npretrained models to human preferences, yet there has been relatively little\nstudy that focuses on evaluation of those models. Evaluating reward models\npresents an opportunity to understand the opaque technologies used for\nalignment of language models and which values are embedded in them. Resources\nfor reward model training and understanding are sparse in the nascent\nopen-source community around them. To enhance scientific understanding of\nreward models, we present RewardBench, a benchmark dataset and code-base for\nevaluation. The RewardBench dataset is a collection of prompt-chosen-rejected\ntrios spanning chat, reasoning, and safety, to benchmark how reward models\nperform on challenging, structured and out-of-distribution queries. We create\nspecific comparison datasets for RMs that have subtle, but verifiable reasons\n(e.g. bugs, incorrect facts) why one answer should be preferred to another. On\nthe RewardBench leaderboard, we evaluate reward models trained with a variety\nof methods, such as the direct MLE training of classifiers and the implicit\nreward modeling of Direct Preference Optimization (DPO). We present many\nfindings on propensity for refusals, reasoning limitations, and instruction\nfollowing shortcomings of various reward models towards a better understanding\nof the RLHF process.",
      "published": "2024-03-20T17:49:54Z",
      "updated": "2024-06-08T16:40:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.13787.pdf",
      "abs_url": "http://arxiv.org/abs/2403.13787v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2501.12345": {
      "arxiv_id": "2501.12345",
      "title": "The doubly librating Plutinos",
      "authors": [
        "Renu Malhotra",
        "Takashi Ito"
      ],
      "summary": "Named for orbital kinship with Pluto, the Plutinos are a prominent group of\nKuiper Belt objects whose orbital periods are in libration about the 3/2 ratio\nwith Neptune's. We investigate the long term orbital dynamics of known\nPlutinos, with attention to the additional libration (or lack thereof) of their\nargument of perihelion, $g$, a well-known characteristic of Pluto's orbit. We\nshow that the $g$ librators amongst the Plutinos cluster around an arc in the\neccentricity--inclination parameter plane. This previously unreported dynamical\nstructure is owed to a family of periodic orbits of the third kind in the\nrestricted problem of three bodies, identified by Poincar\\'e at the end of the\n19th century. Approximately sixteen percent of the currently known Plutinos\nexhibit $g$ librations, a far greater fraction than the ratios of the\nassociated libration frequencies. These results may offer new constraints for\ntheoretical models of the dynamical history of the Plutinos and of the orbital\nmigration history of the giant planets.",
      "published": "2025-01-21T18:23:42Z",
      "updated": "2025-01-21T18:23:42Z",
      "categories": [
        "astro-ph.EP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.12345.pdf",
      "abs_url": "http://arxiv.org/abs/2501.12345v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2310.02743": {
      "arxiv_id": "2310.02743",
      "title": "Reward Model Ensembles Help Mitigate Overoptimization",
      "authors": [
        "Thomas Coste",
        "Usman Anwar",
        "Robert Kirk",
        "David Krueger"
      ],
      "summary": "Reinforcement learning from human feedback (RLHF) is a standard approach for\nfine-tuning large language models to follow instructions. As part of this\nprocess, learned reward models are used to approximately model human\npreferences. However, as imperfect representations of the \"true\" reward, these\nlearned reward models are susceptible to overoptimization. Gao et al. (2023)\nstudied this phenomenon in a synthetic human feedback setup with a\nsignificantly larger \"gold\" reward model acting as the true reward (instead of\nhumans) and showed that overoptimization remains a persistent problem\nregardless of the size of the proxy reward model and training data used. Using\na similar setup, we conduct a systematic study to evaluate the efficacy of\nusing ensemble-based conservative optimization objectives, specifically\nworst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for\nmitigating reward model overoptimization when using two optimization methods:\n(a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We\nadditionally extend the setup of Gao et al. (2023) to include 25% label noise\nto better mirror real-world conditions. Both with and without label noise, we\nfind that conservative optimization practically eliminates overoptimization and\nimproves performance by up to 70% for BoN sampling. For PPO, ensemble-based\nconservative optimization always reduces overoptimization and outperforms\nsingle reward model optimization. Moreover, combining it with a small KL\npenalty successfully prevents overoptimization at no performance cost. Overall,\nour results demonstrate that ensemble-based conservative optimization can\neffectively counter overoptimization.",
      "published": "2023-10-04T11:34:22Z",
      "updated": "2024-03-10T16:14:58Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02743.pdf",
      "abs_url": "http://arxiv.org/abs/2310.02743v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2403.15583": {
      "arxiv_id": "2403.15583",
      "title": "U-ARE-ME: Uncertainty-Aware Rotation Estimation in Manhattan\n  Environments",
      "authors": [
        "Aalok Patwardhan",
        "Callum Rhodes",
        "Gwangbin Bae",
        "Andrew J. Davison"
      ],
      "summary": "Camera rotation estimation from a single image is a challenging task, often\nrequiring depth data and/or camera intrinsics, which are generally not\navailable for in-the-wild videos. Although external sensors such as inertial\nmeasurement units (IMUs) can help, they often suffer from drift and are not\napplicable in non-inertial reference frames. We present U-ARE-ME, an algorithm\nthat estimates camera rotation along with uncertainty from uncalibrated RGB\nimages. Using a Manhattan World assumption, our method leverages the per-pixel\ngeometric priors encoded in single-image surface normal predictions and\nperforms optimisation over the SO(3) manifold. Given a sequence of images, we\ncan use the per-frame rotation estimates and their uncertainty to perform\nmulti-frame optimisation, achieving robustness and temporal consistency. Our\nexperiments demonstrate that U-ARE-ME performs comparably to RGB-D methods and\nis more robust than sparse feature-based SLAM methods. We encourage the reader\nto view the accompanying video at https://callum-rhodes.github.io/U-ARE-ME for\na visual overview of our method.",
      "published": "2024-03-22T19:14:28Z",
      "updated": "2024-03-22T19:14:28Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.15583.pdf",
      "abs_url": "http://arxiv.org/abs/2403.15583v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2405.20947": {
      "arxiv_id": "2405.20947",
      "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models",
      "authors": [
        "Justin Cui",
        "Wei-Lin Chiang",
        "Ion Stoica",
        "Cho-Jui Hsieh"
      ],
      "summary": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models.",
      "published": "2024-05-31T15:44:33Z",
      "updated": "2025-06-15T21:44:25Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20947.pdf",
      "abs_url": "http://arxiv.org/abs/2405.20947v5",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1907.02893": {
      "arxiv_id": "1907.02893",
      "title": "Invariant Risk Minimization",
      "authors": [
        "Martin Arjovsky",
        "L\u00e9on Bottou",
        "Ishaan Gulrajani",
        "David Lopez-Paz"
      ],
      "summary": "We introduce Invariant Risk Minimization (IRM), a learning paradigm to\nestimate invariant correlations across multiple training distributions. To\nachieve this goal, IRM learns a data representation such that the optimal\nclassifier, on top of that data representation, matches for all training\ndistributions. Through theory and experiments, we show how the invariances\nlearned by IRM relate to the causal structures governing the data and enable\nout-of-distribution generalization.",
      "published": "2019-07-05T15:26:26Z",
      "updated": "2020-03-27T19:07:58Z",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1907.02893.pdf",
      "abs_url": "http://arxiv.org/abs/1907.02893v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2105.03023": {
      "arxiv_id": "2105.03023",
      "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and\n  Anti-Experts",
      "authors": [
        "Alisa Liu",
        "Maarten Sap",
        "Ximing Lu",
        "Swabha Swayamdipta",
        "Chandra Bhagavatula",
        "Noah A. Smith",
        "Yejin Choi"
      ],
      "summary": "Despite recent advances in natural language generation, it remains\nchallenging to control attributes of generated text. We propose DExperts:\nDecoding-time Experts, a decoding-time method for controlled text generation\nthat combines a pretrained language model with \"expert\" LMs and/or\n\"anti-expert\" LMs in a product of experts. Intuitively, under the ensemble,\ntokens only get high probability if they are considered likely by the experts,\nand unlikely by the anti-experts. We apply DExperts to language detoxification\nand sentiment-controlled generation, where we outperform existing controllable\ngeneration methods on both automatic and human evaluations. Moreover, because\nDExperts operates only on the output of the pretrained LM, it is effective with\n(anti-)experts of smaller size, including when operating on GPT-3. Our work\nhighlights the promise of tuning small LMs on text with (un)desirable\nattributes for efficient decoding-time steering.",
      "published": "2021-05-07T01:19:38Z",
      "updated": "2021-06-03T05:26:11Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2105.03023.pdf",
      "abs_url": "http://arxiv.org/abs/2105.03023v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2310.16944": {
      "arxiv_id": "2310.16944",
      "title": "Zephyr: Direct Distillation of LM Alignment",
      "authors": [
        "Lewis Tunstall",
        "Edward Beeching",
        "Nathan Lambert",
        "Nazneen Rajani",
        "Kashif Rasul",
        "Younes Belkada",
        "Shengyi Huang",
        "Leandro von Werra",
        "Cl\u00e9mentine Fourrier",
        "Nathan Habib",
        "Nathan Sarrazin",
        "Omar Sanseviero",
        "Alexander M. Rush",
        "Thomas Wolf"
      ],
      "summary": "We aim to produce a smaller language model that is aligned to user intent.\nPrevious research has shown that applying distilled supervised fine-tuning\n(dSFT) on larger models significantly improves task accuracy; however, these\nmodels are unaligned, i.e. they do not respond well to natural prompts. To\ndistill this property, we experiment with the use of preference data from AI\nFeedback (AIF). Starting from a dataset of outputs ranked by a teacher model,\nwe apply distilled direct preference optimization (dDPO) to learn a chat model\nwith significantly improved intent alignment. The approach requires only a few\nhours of training without any additional sampling during fine-tuning. The final\nresult, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B\nparameter models, and requires no human annotation. In particular, results on\nMT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access\nRLHF-based model. Code, models, data, and tutorials for the system are\navailable at https://github.com/huggingface/alignment-handbook.",
      "published": "2023-10-25T19:25:16Z",
      "updated": "2023-10-25T19:25:16Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16944.pdf",
      "abs_url": "http://arxiv.org/abs/2310.16944v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2112.00861": {
      "arxiv_id": "2112.00861",
      "title": "A General Language Assistant as a Laboratory for Alignment",
      "authors": [
        "Amanda Askell",
        "Yuntao Bai",
        "Anna Chen",
        "Dawn Drain",
        "Deep Ganguli",
        "Tom Henighan",
        "Andy Jones",
        "Nicholas Joseph",
        "Ben Mann",
        "Nova DasSarma",
        "Nelson Elhage",
        "Zac Hatfield-Dodds",
        "Danny Hernandez",
        "Jackson Kernion",
        "Kamal Ndousse",
        "Catherine Olsson",
        "Dario Amodei",
        "Tom Brown",
        "Jack Clark",
        "Sam McCandlish",
        "Chris Olah",
        "Jared Kaplan"
      ],
      "summary": "Given the broad capabilities of large language models, it should be possible\nto work towards a general-purpose, text-based assistant that is aligned with\nhuman values, meaning that it is helpful, honest, and harmless. As an initial\nforay in this direction we study simple baseline techniques and evaluations,\nsuch as prompting. We find that the benefits from modest interventions increase\nwith model size, generalize to a variety of alignment evaluations, and do not\ncompromise the performance of large models. Next we investigate scaling trends\nfor several training objectives relevant to alignment, comparing imitation\nlearning, binary discrimination, and ranked preference modeling. We find that\nranked preference modeling performs much better than imitation learning, and\noften scales more favorably with model size. In contrast, binary discrimination\ntypically performs and scales very similarly to imitation learning. Finally we\nstudy a `preference model pre-training' stage of training, with the goal of\nimproving sample efficiency when finetuning on human preferences.",
      "published": "2021-12-01T22:24:34Z",
      "updated": "2021-12-09T21:40:22Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2112.00861.pdf",
      "abs_url": "http://arxiv.org/abs/2112.00861v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.10207": {
      "arxiv_id": "2402.10207",
      "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with\n  Dynamic Preference Adjustment",
      "authors": [
        "Rui Yang",
        "Xiaoman Pan",
        "Feng Luo",
        "Shuang Qiu",
        "Han Zhong",
        "Dong Yu",
        "Jianshu Chen"
      ],
      "summary": "We consider the problem of multi-objective alignment of foundation models\nwith human preferences, which is a critical step towards helpful and harmless\nAI systems. However, it is generally costly and unstable to fine-tune large\nfoundation models using reinforcement learning (RL), and the\nmulti-dimensionality, heterogeneity, and conflicting nature of human\npreferences further complicate the alignment process. In this paper, we\nintroduce Rewards-in-Context (RiC), which conditions the response of a\nfoundation model on multiple rewards in its prompt context and applies\nsupervised fine-tuning for alignment. The salient features of RiC are\nsimplicity and adaptivity, as it only requires supervised fine-tuning of a\nsingle foundation model and supports dynamic adjustment for user preferences\nduring inference time. Inspired by the analytical solution of an abstracted\nconvex optimization problem, our dynamic inference-time adjustment method\napproaches the Pareto-optimal solution for multiple objectives. Empirical\nevidence demonstrates the efficacy of our method in aligning both Large\nLanguage Models (LLMs) and diffusion models to accommodate diverse rewards with\nonly around 10% GPU hours compared with multi-objective RL baseline.",
      "published": "2024-02-15T18:58:31Z",
      "updated": "2024-10-16T03:24:02Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.10207.pdf",
      "abs_url": "http://arxiv.org/abs/2402.10207v6",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2403.08295": {
      "arxiv_id": "2403.08295",
      "title": "Gemma: Open Models Based on Gemini Research and Technology",
      "authors": [
        "Gemma Team",
        "Thomas Mesnard",
        "Cassidy Hardin",
        "Robert Dadashi",
        "Surya Bhupatiraju",
        "Shreya Pathak",
        "Laurent Sifre",
        "Morgane Rivi\u00e8re",
        "Mihir Sanjay Kale",
        "Juliette Love",
        "Pouya Tafti",
        "L\u00e9onard Hussenot",
        "Pier Giuseppe Sessa",
        "Aakanksha Chowdhery",
        "Adam Roberts",
        "Aditya Barua",
        "Alex Botev",
        "Alex Castro-Ros",
        "Ambrose Slone",
        "Am\u00e9lie H\u00e9liou",
        "Andrea Tacchetti",
        "Anna Bulanova",
        "Antonia Paterson",
        "Beth Tsai",
        "Bobak Shahriari",
        "Charline Le Lan",
        "Christopher A. Choquette-Choo",
        "Cl\u00e9ment Crepy",
        "Daniel Cer",
        "Daphne Ippolito",
        "David Reid",
        "Elena Buchatskaya",
        "Eric Ni",
        "Eric Noland",
        "Geng Yan",
        "George Tucker",
        "George-Christian Muraru",
        "Grigory Rozhdestvenskiy",
        "Henryk Michalewski",
        "Ian Tenney",
        "Ivan Grishchenko",
        "Jacob Austin",
        "James Keeling",
        "Jane Labanowski",
        "Jean-Baptiste Lespiau",
        "Jeff Stanway",
        "Jenny Brennan",
        "Jeremy Chen",
        "Johan Ferret",
        "Justin Chiu",
        "Justin Mao-Jones",
        "Katherine Lee",
        "Kathy Yu",
        "Katie Millican",
        "Lars Lowe Sjoesund",
        "Lisa Lee",
        "Lucas Dixon",
        "Machel Reid",
        "Maciej Miku\u0142a",
        "Mateo Wirth",
        "Michael Sharman",
        "Nikolai Chinaev",
        "Nithum Thain",
        "Olivier Bachem",
        "Oscar Chang",
        "Oscar Wahltinez",
        "Paige Bailey",
        "Paul Michel",
        "Petko Yotov",
        "Rahma Chaabouni",
        "Ramona Comanescu",
        "Reena Jana",
        "Rohan Anil",
        "Ross McIlroy",
        "Ruibo Liu",
        "Ryan Mullins",
        "Samuel L Smith",
        "Sebastian Borgeaud",
        "Sertan Girgin",
        "Sholto Douglas",
        "Shree Pandya",
        "Siamak Shakeri",
        "Soham De",
        "Ted Klimenko",
        "Tom Hennigan",
        "Vlad Feinberg",
        "Wojciech Stokowiec",
        "Yu-hui Chen",
        "Zafarali Ahmed",
        "Zhitao Gong",
        "Tris Warkentin",
        "Ludovic Peran",
        "Minh Giang",
        "Cl\u00e9ment Farabet",
        "Oriol Vinyals",
        "Jeff Dean",
        "Koray Kavukcuoglu",
        "Demis Hassabis",
        "Zoubin Ghahramani",
        "Douglas Eck",
        "Joelle Barral",
        "Fernando Pereira",
        "Eli Collins",
        "Armand Joulin",
        "Noah Fiedel",
        "Evan Senter",
        "Alek Andreev",
        "Kathleen Kenealy"
      ],
      "summary": "This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.",
      "published": "2024-03-13T06:59:16Z",
      "updated": "2024-04-16T12:52:47Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.08295.pdf",
      "abs_url": "http://arxiv.org/abs/2403.08295v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2312.11805": {
      "arxiv_id": "2312.11805",
      "title": "Gemini: A Family of Highly Capable Multimodal Models",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew M. Dai",
        "Anja Hauth",
        "Katie Millican",
        "David Silver",
        "Melvin Johnson",
        "Ioannis Antonoglou",
        "Julian Schrittwieser",
        "Amelia Glaese",
        "Jilin Chen",
        "Emily Pitler",
        "Timothy Lillicrap",
        "Angeliki Lazaridou",
        "Orhan Firat",
        "James Molloy",
        "Michael Isard",
        "Paul R. Barham",
        "Tom Hennigan",
        "Benjamin Lee",
        "Fabio Viola",
        "Malcolm Reynolds",
        "Yuanzhong Xu",
        "Ryan Doherty",
        "Eli Collins",
        "Clemens Meyer",
        "Eliza Rutherford",
        "Erica Moreira",
        "Kareem Ayoub",
        "Megha Goel",
        "Jack Krawczyk",
        "Cosmo Du",
        "Ed Chi",
        "Heng-Tze Cheng",
        "Eric Ni",
        "Purvi Shah",
        "Patrick Kane",
        "Betty Chan",
        "Manaal Faruqui",
        "Aliaksei Severyn",
        "Hanzhao Lin",
        "YaGuang Li",
        "Yong Cheng",
        "Abe Ittycheriah",
        "Mahdis Mahdieh",
        "Mia Chen",
        "Pei Sun",
        "Dustin Tran",
        "Sumit Bagri",
        "Balaji Lakshminarayanan",
        "Jeremiah Liu",
        "Andras Orban",
        "Fabian G\u00fcra",
        "Hao Zhou",
        "Xinying Song",
        "Aurelien Boffy",
        "Harish Ganapathy",
        "Steven Zheng",
        "HyunJeong Choe",
        "\u00c1goston Weisz",
        "Tao Zhu",
        "Yifeng Lu",
        "Siddharth Gopal",
        "Jarrod Kahn",
        "Maciej Kula",
        "Jeff Pitman",
        "Rushin Shah",
        "Emanuel Taropa",
        "Majd Al Merey",
        "Martin Baeuml",
        "Zhifeng Chen",
        "Laurent El Shafey",
        "Yujing Zhang",
        "Olcan Sercinoglu",
        "George Tucker",
        "Enrique Piqueras",
        "Maxim Krikun",
        "Iain Barr",
        "Nikolay Savinov",
        "Ivo Danihelka",
        "Becca Roelofs",
        "Ana\u00efs White",
        "Anders Andreassen",
        "Tamara von Glehn",
        "Lakshman Yagati",
        "Mehran Kazemi",
        "Lucas Gonzalez",
        "Misha Khalman",
        "Jakub Sygnowski",
        "Alexandre Frechette",
        "Charlotte Smith",
        "Laura Culp",
        "Lev Proleev",
        "Yi Luan",
        "Xi Chen",
        "James Lottes",
        "Nathan Schucher",
        "Federico Lebron",
        "Alban Rrustemi",
        "Natalie Clay",
        "Phil Crone",
        "Tomas Kocisky",
        "Jeffrey Zhao",
        "Bartek Perz",
        "Dian Yu",
        "Heidi Howard",
        "Adam Bloniarz",
        "Jack W. Rae",
        "Han Lu",
        "Laurent Sifre",
        "Marcello Maggioni",
        "Fred Alcober",
        "Dan Garrette",
        "Megan Barnes",
        "Shantanu Thakoor",
        "Jacob Austin",
        "Gabriel Barth-Maron",
        "William Wong",
        "Rishabh Joshi",
        "Rahma Chaabouni",
        "Deeni Fatiha",
        "Arun Ahuja",
        "Gaurav Singh Tomar",
        "Evan Senter",
        "Martin Chadwick",
        "Ilya Kornakov",
        "Nithya Attaluri",
        "I\u00f1aki Iturrate",
        "Ruibo Liu",
        "Yunxuan Li",
        "Sarah Cogan",
        "Jeremy Chen",
        "Chao Jia",
        "Chenjie Gu",
        "Qiao Zhang",
        "Jordan Grimstad",
        "Ale Jakse Hartman",
        "Xavier Garcia",
        "Thanumalayan Sankaranarayana Pillai",
        "Jacob Devlin",
        "Michael Laskin",
        "Diego de Las Casas",
        "Dasha Valter",
        "Connie Tao",
        "Lorenzo Blanco",
        "Adri\u00e0 Puigdom\u00e8nech Badia",
        "David Reitter",
        "Mianna Chen",
        "Jenny Brennan",
        "Clara Rivera",
        "Sergey Brin",
        "Shariq Iqbal",
        "Gabriela Surita",
        "Jane Labanowski",
        "Abhi Rao",
        "Stephanie Winkler",
        "Emilio Parisotto",
        "Yiming Gu",
        "Kate Olszewska",
        "Ravi Addanki",
        "Antoine Miech",
        "Annie Louis",
        "Denis Teplyashin",
        "Geoff Brown",
        "Elliot Catt",
        "Jan Balaguer",
        "Jackie Xiang",
        "Pidong Wang",
        "Zoe Ashwood",
        "Anton Briukhov",
        "Albert Webson",
        "Sanjay Ganapathy",
        "Smit Sanghavi",
        "Ajay Kannan",
        "Ming-Wei Chang",
        "Axel Stjerngren",
        "Josip Djolonga",
        "Yuting Sun",
        "Ankur Bapna",
        "Matthew Aitchison",
        "Pedram Pejman",
        "Henryk Michalewski",
        "Tianhe Yu",
        "Cindy Wang",
        "Juliette Love",
        "Junwhan Ahn",
        "Dawn Bloxwich",
        "Kehang Han",
        "Peter Humphreys",
        "Thibault Sellam",
        "James Bradbury",
        "Varun Godbole",
        "Sina Samangooei",
        "Bogdan Damoc",
        "Alex Kaskasoli",
        "S\u00e9bastien M. R. Arnold",
        "Vijay Vasudevan",
        "Shubham Agrawal",
        "Jason Riesa",
        "Dmitry Lepikhin",
        "Richard Tanburn",
        "Srivatsan Srinivasan",
        "Hyeontaek Lim",
        "Sarah Hodkinson",
        "Pranav Shyam",
        "Johan Ferret",
        "Steven Hand",
        "Ankush Garg",
        "Tom Le Paine",
        "Jian Li",
        "Yujia Li",
        "Minh Giang",
        "Alexander Neitz",
        "Zaheer Abbas",
        "Sarah York",
        "Machel Reid",
        "Elizabeth Cole",
        "Aakanksha Chowdhery",
        "Dipanjan Das",
        "Dominika Rogozi\u0144ska",
        "Vitaliy Nikolaev",
        "Pablo Sprechmann",
        "Zachary Nado",
        "Lukas Zilka",
        "Flavien Prost",
        "Luheng He",
        "Marianne Monteiro",
        "Gaurav Mishra",
        "Chris Welty",
        "Josh Newlan",
        "Dawei Jia",
        "Miltiadis Allamanis",
        "Clara Huiyi Hu",
        "Raoul de Liedekerke",
        "Justin Gilmer",
        "Carl Saroufim",
        "Shruti Rijhwani",
        "Shaobo Hou",
        "Disha Shrivastava",
        "Anirudh Baddepudi",
        "Alex Goldin",
        "Adnan Ozturel",
        "Albin Cassirer",
        "Yunhan Xu",
        "Daniel Sohn",
        "Devendra Sachan",
        "Reinald Kim Amplayo",
        "Craig Swanson",
        "Dessie Petrova",
        "Shashi Narayan",
        "Arthur Guez",
        "Siddhartha Brahma",
        "Jessica Landon",
        "Miteyan Patel",
        "Ruizhe Zhao",
        "Kevin Villela",
        "Luyu Wang",
        "Wenhao Jia",
        "Matthew Rahtz",
        "Mai Gim\u00e9nez",
        "Legg Yeung",
        "James Keeling",
        "Petko Georgiev",
        "Diana Mincu",
        "Boxi Wu",
        "Salem Haykal",
        "Rachel Saputro",
        "Kiran Vodrahalli",
        "James Qin",
        "Zeynep Cankara",
        "Abhanshu Sharma",
        "Nick Fernando",
        "Will Hawkins",
        "Behnam Neyshabur",
        "Solomon Kim",
        "Adrian Hutter",
        "Priyanka Agrawal",
        "Alex Castro-Ros",
        "George van den Driessche",
        "Tao Wang",
        "Fan Yang",
        "Shuo-yiin Chang",
        "Paul Komarek",
        "Ross McIlroy",
        "Mario Lu\u010di\u0107",
        "Guodong Zhang",
        "Wael Farhan",
        "Michael Sharman",
        "Paul Natsev",
        "Paul Michel",
        "Yamini Bansal",
        "Siyuan Qiao",
        "Kris Cao",
        "Siamak Shakeri",
        "Christina Butterfield",
        "Justin Chung",
        "Paul Kishan Rubenstein",
        "Shivani Agrawal",
        "Arthur Mensch",
        "Kedar Soparkar",
        "Karel Lenc",
        "Timothy Chung",
        "Aedan Pope",
        "Loren Maggiore",
        "Jackie Kay",
        "Priya Jhakra",
        "Shibo Wang",
        "Joshua Maynez",
        "Mary Phuong",
        "Taylor Tobin",
        "Andrea Tacchetti",
        "Maja Trebacz",
        "Kevin Robinson",
        "Yash Katariya",
        "Sebastian Riedel",
        "Paige Bailey",
        "Kefan Xiao",
        "Nimesh Ghelani",
        "Lora Aroyo",
        "Ambrose Slone",
        "Neil Houlsby",
        "Xuehan Xiong",
        "Zhen Yang",
        "Elena Gribovskaya",
        "Jonas Adler",
        "Mateo Wirth",
        "Lisa Lee",
        "Music Li",
        "Thais Kagohara",
        "Jay Pavagadhi",
        "Sophie Bridgers",
        "Anna Bortsova",
        "Sanjay Ghemawat",
        "Zafarali Ahmed",
        "Tianqi Liu",
        "Richard Powell",
        "Vijay Bolina",
        "Mariko Iinuma",
        "Polina Zablotskaia",
        "James Besley",
        "Da-Woon Chung",
        "Timothy Dozat",
        "Ramona Comanescu",
        "Xiance Si",
        "Jeremy Greer",
        "Guolong Su",
        "Martin Polacek",
        "Rapha\u00ebl Lopez Kaufman",
        "Simon Tokumine",
        "Hexiang Hu",
        "Elena Buchatskaya",
        "Yingjie Miao",
        "Mohamed Elhawaty",
        "Aditya Siddhant",
        "Nenad Tomasev",
        "Jinwei Xing",
        "Christina Greer",
        "Helen Miller",
        "Shereen Ashraf",
        "Aurko Roy",
        "Zizhao Zhang",
        "Ada Ma",
        "Angelos Filos",
        "Milos Besta",
        "Rory Blevins",
        "Ted Klimenko",
        "Chih-Kuan Yeh",
        "Soravit Changpinyo",
        "Jiaqi Mu",
        "Oscar Chang",
        "Mantas Pajarskas",
        "Carrie Muir",
        "Vered Cohen",
        "Charline Le Lan",
        "Krishna Haridasan",
        "Amit Marathe",
        "Steven Hansen",
        "Sholto Douglas",
        "Rajkumar Samuel",
        "Mingqiu Wang",
        "Sophia Austin",
        "Chang Lan",
        "Jiepu Jiang",
        "Justin Chiu",
        "Jaime Alonso Lorenzo",
        "Lars Lowe Sj\u00f6sund",
        "S\u00e9bastien Cevey",
        "Zach Gleicher",
        "Thi Avrahami",
        "Anudhyan Boral",
        "Hansa Srinivasan",
        "Vittorio Selo",
        "Rhys May",
        "Konstantinos Aisopos",
        "L\u00e9onard Hussenot",
        "Livio Baldini Soares",
        "Kate Baumli",
        "Michael B. Chang",
        "Adri\u00e0 Recasens",
        "Ben Caine",
        "Alexander Pritzel",
        "Filip Pavetic",
        "Fabio Pardo",
        "Anita Gergely",
        "Justin Frye",
        "Vinay Ramasesh",
        "Dan Horgan",
        "Kartikeya Badola",
        "Nora Kassner",
        "Subhrajit Roy",
        "Ethan Dyer",
        "V\u00edctor Campos Campos",
        "Alex Tomala",
        "Yunhao Tang",
        "Dalia El Badawy",
        "Elspeth White",
        "Basil Mustafa",
        "Oran Lang",
        "Abhishek Jindal",
        "Sharad Vikram",
        "Zhitao Gong",
        "Sergi Caelles",
        "Ross Hemsley",
        "Gregory Thornton",
        "Fangxiaoyu Feng",
        "Wojciech Stokowiec",
        "Ce Zheng",
        "Phoebe Thacker",
        "\u00c7a\u011flar \u00dcnl\u00fc",
        "Zhishuai Zhang",
        "Mohammad Saleh",
        "James Svensson",
        "Max Bileschi",
        "Piyush Patil",
        "Ankesh Anand",
        "Roman Ring",
        "Katerina Tsihlas",
        "Arpi Vezer",
        "Marco Selvi",
        "Toby Shevlane",
        "Mikel Rodriguez",
        "Tom Kwiatkowski",
        "Samira Daruki",
        "Keran Rong",
        "Allan Dafoe",
        "Nicholas FitzGerald",
        "Keren Gu-Lemberg",
        "Mina Khan",
        "Lisa Anne Hendricks",
        "Marie Pellat",
        "Vladimir Feinberg",
        "James Cobon-Kerr",
        "Tara Sainath",
        "Maribeth Rauh",
        "Sayed Hadi Hashemi",
        "Richard Ives",
        "Yana Hasson",
        "Eric Noland",
        "Yuan Cao",
        "Nathan Byrd",
        "Le Hou",
        "Qingze Wang",
        "Thibault Sottiaux",
        "Michela Paganini",
        "Jean-Baptiste Lespiau",
        "Alexandre Moufarek",
        "Samer Hassan",
        "Kaushik Shivakumar",
        "Joost van Amersfoort",
        "Amol Mandhane",
        "Pratik Joshi",
        "Anirudh Goyal",
        "Matthew Tung",
        "Andrew Brock",
        "Hannah Sheahan",
        "Vedant Misra",
        "Cheng Li",
        "Nemanja Raki\u0107evi\u0107",
        "Mostafa Dehghani",
        "Fangyu Liu",
        "Sid Mittal",
        "Junhyuk Oh",
        "Seb Noury",
        "Eren Sezener",
        "Fantine Huot",
        "Matthew Lamm",
        "Nicola De Cao",
        "Charlie Chen",
        "Sidharth Mudgal",
        "Romina Stella",
        "Kevin Brooks",
        "Gautam Vasudevan",
        "Chenxi Liu",
        "Mainak Chain",
        "Nivedita Melinkeri",
        "Aaron Cohen",
        "Venus Wang",
        "Kristie Seymore",
        "Sergey Zubkov",
        "Rahul Goel",
        "Summer Yue",
        "Sai Krishnakumaran",
        "Brian Albert",
        "Nate Hurley",
        "Motoki Sano",
        "Anhad Mohananey",
        "Jonah Joughin",
        "Egor Filonov",
        "Tomasz K\u0119pa",
        "Yomna Eldawy",
        "Jiawern Lim",
        "Rahul Rishi",
        "Shirin Badiezadegan",
        "Taylor Bos",
        "Jerry Chang",
        "Sanil Jain",
        "Sri Gayatri Sundara Padmanabhan",
        "Subha Puttagunta",
        "Kalpesh Krishna",
        "Leslie Baker",
        "Norbert Kalb",
        "Vamsi Bedapudi",
        "Adam Kurzrok",
        "Shuntong Lei",
        "Anthony Yu",
        "Oren Litvin",
        "Xiang Zhou",
        "Zhichun Wu",
        "Sam Sobell",
        "Andrea Siciliano",
        "Alan Papir",
        "Robby Neale",
        "Jonas Bragagnolo",
        "Tej Toor",
        "Tina Chen",
        "Valentin Anklin",
        "Feiran Wang",
        "Richie Feng",
        "Milad Gholami",
        "Kevin Ling",
        "Lijuan Liu",
        "Jules Walter",
        "Hamid Moghaddam",
        "Arun Kishore",
        "Jakub Adamek",
        "Tyler Mercado",
        "Jonathan Mallinson",
        "Siddhinita Wandekar",
        "Stephen Cagle",
        "Eran Ofek",
        "Guillermo Garrido",
        "Clemens Lombriser",
        "Maksim Mukha",
        "Botu Sun",
        "Hafeezul Rahman Mohammad",
        "Josip Matak",
        "Yadi Qian",
        "Vikas Peswani",
        "Pawel Janus",
        "Quan Yuan",
        "Leif Schelin",
        "Oana David",
        "Ankur Garg",
        "Yifan He",
        "Oleksii Duzhyi",
        "Anton \u00c4lgmyr",
        "Timoth\u00e9e Lottaz",
        "Qi Li",
        "Vikas Yadav",
        "Luyao Xu",
        "Alex Chinien",
        "Rakesh Shivanna",
        "Aleksandr Chuklin",
        "Josie Li",
        "Carrie Spadine",
        "Travis Wolfe",
        "Kareem Mohamed",
        "Subhabrata Das",
        "Zihang Dai",
        "Kyle He",
        "Daniel von Dincklage",
        "Shyam Upadhyay",
        "Akanksha Maurya",
        "Luyan Chi",
        "Sebastian Krause",
        "Khalid Salama",
        "Pam G Rabinovitch",
        "Pavan Kumar Reddy M",
        "Aarush Selvan",
        "Mikhail Dektiarev",
        "Golnaz Ghiasi",
        "Erdem Guven",
        "Himanshu Gupta",
        "Boyi Liu",
        "Deepak Sharma",
        "Idan Heimlich Shtacher",
        "Shachi Paul",
        "Oscar Akerlund",
        "Fran\u00e7ois-Xavier Aubet",
        "Terry Huang",
        "Chen Zhu",
        "Eric Zhu",
        "Elico Teixeira",
        "Matthew Fritze",
        "Francesco Bertolini",
        "Liana-Eleonora Marinescu",
        "Martin B\u00f6lle",
        "Dominik Paulus",
        "Khyatti Gupta",
        "Tejasi Latkar",
        "Max Chang",
        "Jason Sanders",
        "Roopa Wilson",
        "Xuewei Wu",
        "Yi-Xuan Tan",
        "Lam Nguyen Thiet",
        "Tulsee Doshi",
        "Sid Lall",
        "Swaroop Mishra",
        "Wanming Chen",
        "Thang Luong",
        "Seth Benjamin",
        "Jasmine Lee",
        "Ewa Andrejczuk",
        "Dominik Rabiej",
        "Vipul Ranjan",
        "Krzysztof Styrc",
        "Pengcheng Yin",
        "Jon Simon",
        "Malcolm Rose Harriott",
        "Mudit Bansal",
        "Alexei Robsky",
        "Geoff Bacon",
        "David Greene",
        "Daniil Mirylenka",
        "Chen Zhou",
        "Obaid Sarvana",
        "Abhimanyu Goyal",
        "Samuel Andermatt",
        "Patrick Siegler",
        "Ben Horn",
        "Assaf Israel",
        "Francesco Pongetti",
        "Chih-Wei \"Louis\" Chen",
        "Marco Selvatici",
        "Pedro Silva",
        "Kathie Wang",
        "Jackson Tolins",
        "Kelvin Guu",
        "Roey Yogev",
        "Xiaochen Cai",
        "Alessandro Agostini",
        "Maulik Shah",
        "Hung Nguyen",
        "Noah \u00d3 Donnaile",
        "S\u00e9bastien Pereira",
        "Linda Friso",
        "Adam Stambler",
        "Adam Kurzrok",
        "Chenkai Kuang",
        "Yan Romanikhin",
        "Mark Geller",
        "ZJ Yan",
        "Kane Jang",
        "Cheng-Chun Lee",
        "Wojciech Fica",
        "Eric Malmi",
        "Qijun Tan",
        "Dan Banica",
        "Daniel Balle",
        "Ryan Pham",
        "Yanping Huang",
        "Diana Avram",
        "Hongzhi Shi",
        "Jasjot Singh",
        "Chris Hidey",
        "Niharika Ahuja",
        "Pranab Saxena",
        "Dan Dooley",
        "Srividya Pranavi Potharaju",
        "Eileen O'Neill",
        "Anand Gokulchandran",
        "Ryan Foley",
        "Kai Zhao",
        "Mike Dusenberry",
        "Yuan Liu",
        "Pulkit Mehta",
        "Ragha Kotikalapudi",
        "Chalence Safranek-Shrader",
        "Andrew Goodman",
        "Joshua Kessinger",
        "Eran Globen",
        "Prateek Kolhar",
        "Chris Gorgolewski",
        "Ali Ibrahim",
        "Yang Song",
        "Ali Eichenbaum",
        "Thomas Brovelli",
        "Sahitya Potluri",
        "Preethi Lahoti",
        "Cip Baetu",
        "Ali Ghorbani",
        "Charles Chen",
        "Andy Crawford",
        "Shalini Pal",
        "Mukund Sridhar",
        "Petru Gurita",
        "Asier Mujika",
        "Igor Petrovski",
        "Pierre-Louis Cedoz",
        "Chenmei Li",
        "Shiyuan Chen",
        "Niccol\u00f2 Dal Santo",
        "Siddharth Goyal",
        "Jitesh Punjabi",
        "Karthik Kappaganthu",
        "Chester Kwak",
        "Pallavi LV",
        "Sarmishta Velury",
        "Himadri Choudhury",
        "Jamie Hall",
        "Premal Shah",
        "Ricardo Figueira",
        "Matt Thomas",
        "Minjie Lu",
        "Ting Zhou",
        "Chintu Kumar",
        "Thomas Jurdi",
        "Sharat Chikkerur",
        "Yenai Ma",
        "Adams Yu",
        "Soo Kwak",
        "Victor \u00c4hdel",
        "Sujeevan Rajayogam",
        "Travis Choma",
        "Fei Liu",
        "Aditya Barua",
        "Colin Ji",
        "Ji Ho Park",
        "Vincent Hellendoorn",
        "Alex Bailey",
        "Taylan Bilal",
        "Huanjie Zhou",
        "Mehrdad Khatir",
        "Charles Sutton",
        "Wojciech Rzadkowski",
        "Fiona Macintosh",
        "Roopali Vij",
        "Konstantin Shagin",
        "Paul Medina",
        "Chen Liang",
        "Jinjing Zhou",
        "Pararth Shah",
        "Yingying Bi",
        "Attila Dankovics",
        "Shipra Banga",
        "Sabine Lehmann",
        "Marissa Bredesen",
        "Zifan Lin",
        "John Eric Hoffmann",
        "Jonathan Lai",
        "Raynald Chung",
        "Kai Yang",
        "Nihal Balani",
        "Arthur Bra\u017einskas",
        "Andrei Sozanschi",
        "Matthew Hayes",
        "H\u00e9ctor Fern\u00e1ndez Alcalde",
        "Peter Makarov",
        "Will Chen",
        "Antonio Stella",
        "Liselotte Snijders",
        "Michael Mandl",
        "Ante K\u00e4rrman",
        "Pawe\u0142 Nowak",
        "Xinyi Wu",
        "Alex Dyck",
        "Krishnan Vaidyanathan",
        "Raghavender R",
        "Jessica Mallet",
        "Mitch Rudominer",
        "Eric Johnston",
        "Sushil Mittal",
        "Akhil Udathu",
        "Janara Christensen",
        "Vishal Verma",
        "Zach Irving",
        "Andreas Santucci",
        "Gamaleldin Elsayed",
        "Elnaz Davoodi",
        "Marin Georgiev",
        "Ian Tenney",
        "Nan Hua",
        "Geoffrey Cideron",
        "Edouard Leurent",
        "Mahmoud Alnahlawi",
        "Ionut Georgescu",
        "Nan Wei",
        "Ivy Zheng",
        "Dylan Scandinaro",
        "Heinrich Jiang",
        "Jasper Snoek",
        "Mukund Sundararajan",
        "Xuezhi Wang",
        "Zack Ontiveros",
        "Itay Karo",
        "Jeremy Cole",
        "Vinu Rajashekhar",
        "Lara Tumeh",
        "Eyal Ben-David",
        "Rishub Jain",
        "Jonathan Uesato",
        "Romina Datta",
        "Oskar Bunyan",
        "Shimu Wu",
        "John Zhang",
        "Piotr Stanczyk",
        "Ye Zhang",
        "David Steiner",
        "Subhajit Naskar",
        "Michael Azzam",
        "Matthew Johnson",
        "Adam Paszke",
        "Chung-Cheng Chiu",
        "Jaume Sanchez Elias",
        "Afroz Mohiuddin",
        "Faizan Muhammad",
        "Jin Miao",
        "Andrew Lee",
        "Nino Vieillard",
        "Jane Park",
        "Jiageng Zhang",
        "Jeff Stanway",
        "Drew Garmon",
        "Abhijit Karmarkar",
        "Zhe Dong",
        "Jong Lee",
        "Aviral Kumar",
        "Luowei Zhou",
        "Jonathan Evens",
        "William Isaac",
        "Geoffrey Irving",
        "Edward Loper",
        "Michael Fink",
        "Isha Arkatkar",
        "Nanxin Chen",
        "Izhak Shafran",
        "Ivan Petrychenko",
        "Zhe Chen",
        "Johnson Jia",
        "Anselm Levskaya",
        "Zhenkai Zhu",
        "Peter Grabowski",
        "Yu Mao",
        "Alberto Magni",
        "Kaisheng Yao",
        "Javier Snaider",
        "Norman Casagrande",
        "Evan Palmer",
        "Paul Suganthan",
        "Alfonso Casta\u00f1o",
        "Irene Giannoumis",
        "Wooyeol Kim",
        "Miko\u0142aj Rybi\u0144ski",
        "Ashwin Sreevatsa",
        "Jennifer Prendki",
        "David Soergel",
        "Adrian Goedeckemeyer",
        "Willi Gierke",
        "Mohsen Jafari",
        "Meenu Gaba",
        "Jeremy Wiesner",
        "Diana Gage Wright",
        "Yawen Wei",
        "Harsha Vashisht",
        "Yana Kulizhskaya",
        "Jay Hoover",
        "Maigo Le",
        "Lu Li",
        "Chimezie Iwuanyanwu",
        "Lu Liu",
        "Kevin Ramirez",
        "Andrey Khorlin",
        "Albert Cui",
        "Tian LIN",
        "Marcus Wu",
        "Ricardo Aguilar",
        "Keith Pallo",
        "Abhishek Chakladar",
        "Ginger Perng",
        "Elena Allica Abellan",
        "Mingyang Zhang",
        "Ishita Dasgupta",
        "Nate Kushman",
        "Ivo Penchev",
        "Alena Repina",
        "Xihui Wu",
        "Tom van der Weide",
        "Priya Ponnapalli",
        "Caroline Kaplan",
        "Jiri Simsa",
        "Shuangfeng Li",
        "Olivier Dousse",
        "Fan Yang",
        "Jeff Piper",
        "Nathan Ie",
        "Rama Pasumarthi",
        "Nathan Lintz",
        "Anitha Vijayakumar",
        "Daniel Andor",
        "Pedro Valenzuela",
        "Minnie Lui",
        "Cosmin Paduraru",
        "Daiyi Peng",
        "Katherine Lee",
        "Shuyuan Zhang",
        "Somer Greene",
        "Duc Dung Nguyen",
        "Paula Kurylowicz",
        "Cassidy Hardin",
        "Lucas Dixon",
        "Lili Janzer",
        "Kiam Choo",
        "Ziqiang Feng",
        "Biao Zhang",
        "Achintya Singhal",
        "Dayou Du",
        "Dan McKinnon",
        "Natasha Antropova",
        "Tolga Bolukbasi",
        "Orgad Keller",
        "David Reid",
        "Daniel Finchelstein",
        "Maria Abi Raad",
        "Remi Crocker",
        "Peter Hawkins",
        "Robert Dadashi",
        "Colin Gaffney",
        "Ken Franko",
        "Anna Bulanova",
        "R\u00e9mi Leblond",
        "Shirley Chung",
        "Harry Askham",
        "Luis C. Cobo",
        "Kelvin Xu",
        "Felix Fischer",
        "Jun Xu",
        "Christina Sorokin",
        "Chris Alberti",
        "Chu-Cheng Lin",
        "Colin Evans",
        "Alek Dimitriev",
        "Hannah Forbes",
        "Dylan Banarse",
        "Zora Tung",
        "Mark Omernick",
        "Colton Bishop",
        "Rachel Sterneck",
        "Rohan Jain",
        "Jiawei Xia",
        "Ehsan Amid",
        "Francesco Piccinno",
        "Xingyu Wang",
        "Praseem Banzal",
        "Daniel J. Mankowitz",
        "Alex Polozov",
        "Victoria Krakovna",
        "Sasha Brown",
        "MohammadHossein Bateni",
        "Dennis Duan",
        "Vlad Firoiu",
        "Meghana Thotakuri",
        "Tom Natan",
        "Matthieu Geist",
        "Ser tan Girgin",
        "Hui Li",
        "Jiayu Ye",
        "Ofir Roval",
        "Reiko Tojo",
        "Michael Kwong",
        "James Lee-Thorp",
        "Christopher Yew",
        "Danila Sinopalnikov",
        "Sabela Ramos",
        "John Mellor",
        "Abhishek Sharma",
        "Kathy Wu",
        "David Miller",
        "Nicolas Sonnerat",
        "Denis Vnukov",
        "Rory Greig",
        "Jennifer Beattie",
        "Emily Caveness",
        "Libin Bai",
        "Julian Eisenschlos",
        "Alex Korchemniy",
        "Tomy Tsai",
        "Mimi Jasarevic",
        "Weize Kong",
        "Phuong Dao",
        "Zeyu Zheng",
        "Frederick Liu",
        "Fan Yang",
        "Rui Zhu",
        "Tian Huey Teh",
        "Jason Sanmiya",
        "Evgeny Gladchenko",
        "Nejc Trdin",
        "Daniel Toyama",
        "Evan Rosen",
        "Sasan Tavakkol",
        "Linting Xue",
        "Chen Elkind",
        "Oliver Woodman",
        "John Carpenter",
        "George Papamakarios",
        "Rupert Kemp",
        "Sushant Kafle",
        "Tanya Grunina",
        "Rishika Sinha",
        "Alice Talbert",
        "Diane Wu",
        "Denese Owusu-Afriyie",
        "Cosmo Du",
        "Chloe Thornton",
        "Jordi Pont-Tuset",
        "Pradyumna Narayana",
        "Jing Li",
        "Saaber Fatehi",
        "John Wieting",
        "Omar Ajmeri",
        "Benigno Uria",
        "Yeongil Ko",
        "Laura Knight",
        "Am\u00e9lie H\u00e9liou",
        "Ning Niu",
        "Shane Gu",
        "Chenxi Pang",
        "Yeqing Li",
        "Nir Levine",
        "Ariel Stolovich",
        "Rebeca Santamaria-Fernandez",
        "Sonam Goenka",
        "Wenny Yustalim",
        "Robin Strudel",
        "Ali Elqursh",
        "Charlie Deck",
        "Hyo Lee",
        "Zonglin Li",
        "Kyle Levin",
        "Raphael Hoffmann",
        "Dan Holtmann-Rice",
        "Olivier Bachem",
        "Sho Arora",
        "Christy Koh",
        "Soheil Hassas Yeganeh",
        "Siim P\u00f5der",
        "Mukarram Tariq",
        "Yanhua Sun",
        "Lucian Ionita",
        "Mojtaba Seyedhosseini",
        "Pouya Tafti",
        "Zhiyu Liu",
        "Anmol Gulati",
        "Jasmine Liu",
        "Xinyu Ye",
        "Bart Chrzaszcz",
        "Lily Wang",
        "Nikhil Sethi",
        "Tianrun Li",
        "Ben Brown",
        "Shreya Singh",
        "Wei Fan",
        "Aaron Parisi",
        "Joe Stanton",
        "Vinod Koverkathu",
        "Christopher A. Choquette-Choo",
        "Yunjie Li",
        "TJ Lu",
        "Abe Ittycheriah",
        "Prakash Shroff",
        "Mani Varadarajan",
        "Sanaz Bahargam",
        "Rob Willoughby",
        "David Gaddy",
        "Guillaume Desjardins",
        "Marco Cornero",
        "Brona Robenek",
        "Bhavishya Mittal",
        "Ben Albrecht",
        "Ashish Shenoy",
        "Fedor Moiseev",
        "Henrik Jacobsson",
        "Alireza Ghaffarkhah",
        "Morgane Rivi\u00e8re",
        "Alanna Walton",
        "Cl\u00e9ment Crepy",
        "Alicia Parrish",
        "Zongwei Zhou",
        "Clement Farabet",
        "Carey Radebaugh",
        "Praveen Srinivasan",
        "Claudia van der Salm",
        "Andreas Fidjeland",
        "Salvatore Scellato",
        "Eri Latorre-Chimoto",
        "Hanna Klimczak-Pluci\u0144ska",
        "David Bridson",
        "Dario de Cesare",
        "Tom Hudson",
        "Piermaria Mendolicchio",
        "Lexi Walker",
        "Alex Morris",
        "Matthew Mauger",
        "Alexey Guseynov",
        "Alison Reid",
        "Seth Odoom",
        "Lucia Loher",
        "Victor Cotruta",
        "Madhavi Yenugula",
        "Dominik Grewe",
        "Anastasia Petrushkina",
        "Tom Duerig",
        "Antonio Sanchez",
        "Steve Yadlowsky",
        "Amy Shen",
        "Amir Globerson",
        "Lynette Webb",
        "Sahil Dua",
        "Dong Li",
        "Surya Bhupatiraju",
        "Dan Hurt",
        "Haroon Qureshi",
        "Ananth Agarwal",
        "Tomer Shani",
        "Matan Eyal",
        "Anuj Khare",
        "Shreyas Rammohan Belle",
        "Lei Wang",
        "Chetan Tekur",
        "Mihir Sanjay Kale",
        "Jinliang Wei",
        "Ruoxin Sang",
        "Brennan Saeta",
        "Tyler Liechty",
        "Yi Sun",
        "Yao Zhao",
        "Stephan Lee",
        "Pandu Nayak",
        "Doug Fritz",
        "Manish Reddy Vuyyuru",
        "John Aslanides",
        "Nidhi Vyas",
        "Martin Wicke",
        "Xiao Ma",
        "Evgenii Eltyshev",
        "Nina Martin",
        "Hardie Cate",
        "James Manyika",
        "Keyvan Amiri",
        "Yelin Kim",
        "Xi Xiong",
        "Kai Kang",
        "Florian Luisier",
        "Nilesh Tripuraneni",
        "David Madras",
        "Mandy Guo",
        "Austin Waters",
        "Oliver Wang",
        "Joshua Ainslie",
        "Jason Baldridge",
        "Han Zhang",
        "Garima Pruthi",
        "Jakob Bauer",
        "Feng Yang",
        "Riham Mansour",
        "Jason Gelman",
        "Yang Xu",
        "George Polovets",
        "Ji Liu",
        "Honglong Cai",
        "Warren Chen",
        "XiangHai Sheng",
        "Emily Xue",
        "Sherjil Ozair",
        "Christof Angermueller",
        "Xiaowei Li",
        "Anoop Sinha",
        "Weiren Wang",
        "Julia Wiesinger",
        "Emmanouil Koukoumidis",
        "Yuan Tian",
        "Anand Iyer",
        "Madhu Gurumurthy",
        "Mark Goldenson",
        "Parashar Shah",
        "MK Blake",
        "Hongkun Yu",
        "Anthony Urbanowicz",
        "Jennimaria Palomaki",
        "Chrisantha Fernando",
        "Ken Durden",
        "Harsh Mehta",
        "Nikola Momchev",
        "Elahe Rahimtoroghi",
        "Maria Georgaki",
        "Amit Raul",
        "Sebastian Ruder",
        "Morgan Redshaw",
        "Jinhyuk Lee",
        "Denny Zhou",
        "Komal Jalan",
        "Dinghua Li",
        "Blake Hechtman",
        "Parker Schuh",
        "Milad Nasr",
        "Kieran Milan",
        "Vladimir Mikulik",
        "Juliana Franco",
        "Tim Green",
        "Nam Nguyen",
        "Joe Kelley",
        "Aroma Mahendru",
        "Andrea Hu",
        "Joshua Howland",
        "Ben Vargas",
        "Jeffrey Hui",
        "Kshitij Bansal",
        "Vikram Rao",
        "Rakesh Ghiya",
        "Emma Wang",
        "Ke Ye",
        "Jean Michel Sarr",
        "Melanie Moranski Preston",
        "Madeleine Elish",
        "Steve Li",
        "Aakash Kaku",
        "Jigar Gupta",
        "Ice Pasupat",
        "Da-Cheng Juan",
        "Milan Someswar",
        "Tejvi M.",
        "Xinyun Chen",
        "Aida Amini",
        "Alex Fabrikant",
        "Eric Chu",
        "Xuanyi Dong",
        "Amruta Muthal",
        "Senaka Buthpitiya",
        "Sarthak Jauhari",
        "Nan Hua",
        "Urvashi Khandelwal",
        "Ayal Hitron",
        "Jie Ren",
        "Larissa Rinaldi",
        "Shahar Drath",
        "Avigail Dabush",
        "Nan-Jiang Jiang",
        "Harshal Godhia",
        "Uli Sachs",
        "Anthony Chen",
        "Yicheng Fan",
        "Hagai Taitelbaum",
        "Hila Noga",
        "Zhuyun Dai",
        "James Wang",
        "Chen Liang",
        "Jenny Hamer",
        "Chun-Sung Ferng",
        "Chenel Elkind",
        "Aviel Atias",
        "Paulina Lee",
        "V\u00edt List\u00edk",
        "Mathias Carlen",
        "Jan van de Kerkhof",
        "Marcin Pikus",
        "Krunoslav Zaher",
        "Paul M\u00fcller",
        "Sasha Zykova",
        "Richard Stefanec",
        "Vitaly Gatsko",
        "Christoph Hirnschall",
        "Ashwin Sethi",
        "Xingyu Federico Xu",
        "Chetan Ahuja",
        "Beth Tsai",
        "Anca Stefanoiu",
        "Bo Feng",
        "Keshav Dhandhania",
        "Manish Katyal",
        "Akshay Gupta",
        "Atharva Parulekar",
        "Divya Pitta",
        "Jing Zhao",
        "Vivaan Bhatia",
        "Yashodha Bhavnani",
        "Omar Alhadlaq",
        "Xiaolin Li",
        "Peter Danenberg",
        "Dennis Tu",
        "Alex Pine",
        "Vera Filippova",
        "Abhipso Ghosh",
        "Ben Limonchik",
        "Bhargava Urala",
        "Chaitanya Krishna Lanka",
        "Derik Clive",
        "Yi Sun",
        "Edward Li",
        "Hao Wu",
        "Kevin Hongtongsak",
        "Ianna Li",
        "Kalind Thakkar",
        "Kuanysh Omarov",
        "Kushal Majmundar",
        "Michael Alverson",
        "Michael Kucharski",
        "Mohak Patel",
        "Mudit Jain",
        "Maksim Zabelin",
        "Paolo Pelagatti",
        "Rohan Kohli",
        "Saurabh Kumar",
        "Joseph Kim",
        "Swetha Sankar",
        "Vineet Shah",
        "Lakshmi Ramachandruni",
        "Xiangkai Zeng",
        "Ben Bariach",
        "Laura Weidinger",
        "Tu Vu",
        "Alek Andreev",
        "Antoine He",
        "Kevin Hui",
        "Sheleem Kashem",
        "Amar Subramanya",
        "Sissie Hsiao",
        "Demis Hassabis",
        "Koray Kavukcuoglu",
        "Adam Sadovsky",
        "Quoc Le",
        "Trevor Strohman",
        "Yonghui Wu",
        "Slav Petrov",
        "Jeffrey Dean",
        "Oriol Vinyals"
      ],
      "summary": "This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of the Gemini family in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases. We discuss our\napproach toward post-training and deploying Gemini models responsibly to users\nthrough services including Gemini, Gemini Advanced, Google AI Studio, and Cloud\nVertex AI.",
      "published": "2023-12-19T02:39:27Z",
      "updated": "2025-05-09T21:04:06Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.11805.pdf",
      "abs_url": "http://arxiv.org/abs/2312.11805v5",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2401.01335": {
      "arxiv_id": "2401.01335",
      "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\n  Models",
      "authors": [
        "Zixiang Chen",
        "Yihe Deng",
        "Huizhuo Yuan",
        "Kaixuan Ji",
        "Quanquan Gu"
      ],
      "summary": "Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents. Codes\nare available at https://github.com/uclaml/SPIN.",
      "published": "2024-01-02T18:53:13Z",
      "updated": "2024-06-14T21:17:17Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01335.pdf",
      "abs_url": "http://arxiv.org/abs/2401.01335v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2312.09244": {
      "arxiv_id": "2312.09244",
      "title": "Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate\n  Reward Hacking",
      "authors": [
        "Jacob Eisenstein",
        "Chirag Nagpal",
        "Alekh Agarwal",
        "Ahmad Beirami",
        "Alex D'Amour",
        "DJ Dvijotham",
        "Adam Fisch",
        "Katherine Heller",
        "Stephen Pfohl",
        "Deepak Ramachandran",
        "Peter Shaw",
        "Jonathan Berant"
      ],
      "summary": "Reward models play a key role in aligning language model applications towards\nhuman preferences. However, this setup creates an incentive for the language\nmodel to exploit errors in the reward model to achieve high estimated reward, a\nphenomenon often termed \\emph{reward hacking}. A natural mitigation is to train\nan ensemble of reward models, aggregating over model outputs to obtain a more\nrobust reward estimate. We explore the application of reward ensembles to\nalignment at both training time (through reinforcement learning) and inference\ntime (through reranking). First, we show that reward models are\n\\emph{underspecified}: reward models that perform similarly in-distribution can\nyield very different rewards when used in alignment, due to distribution shift.\nSecond, underspecification results in overoptimization, where alignment to one\nreward model does not improve reward as measured by another reward model\ntrained on the same data. Third, overoptimization is mitigated by the use of\nreward ensembles, and ensembles that vary by their \\emph{pretraining} seeds\nlead to better generalization than ensembles that differ only by their\n\\emph{fine-tuning} seeds, with both outperforming individual reward models.\nHowever, even pretrain reward ensembles do not eliminate reward hacking: we\nshow several qualitative reward hacking phenomena that are not mitigated by\nensembling because all reward models in the ensemble exhibit similar error\npatterns.",
      "published": "2023-12-14T18:59:04Z",
      "updated": "2024-08-16T23:59:29Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09244.pdf",
      "abs_url": "http://arxiv.org/abs/2312.09244v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2307.02390": {
      "arxiv_id": "2307.02390",
      "title": "Causal Discovery with Language Models as Imperfect Experts",
      "authors": [
        "Stephanie Long",
        "Alexandre Pich\u00e9",
        "Valentina Zantedeschi",
        "Tibor Schuster",
        "Alexandre Drouin"
      ],
      "summary": "Understanding the causal relationships that underlie a system is a\nfundamental prerequisite to accurate decision-making. In this work, we explore\nhow expert knowledge can be used to improve the data-driven identification of\ncausal graphs, beyond Markov equivalence classes. In doing so, we consider a\nsetting where we can query an expert about the orientation of causal\nrelationships between variables, but where the expert may provide erroneous\ninformation. We propose strategies for amending such expert knowledge based on\nconsistency properties, e.g., acyclicity and conditional independencies in the\nequivalence class. We then report a case study, on real data, where a large\nlanguage model is used as an imperfect expert.",
      "published": "2023-07-05T16:01:38Z",
      "updated": "2023-07-05T16:01:38Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02390.pdf",
      "abs_url": "http://arxiv.org/abs/2307.02390v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2406.18495": {
      "arxiv_id": "2406.18495",
      "title": "WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks,\n  and Refusals of LLMs",
      "authors": [
        "Seungju Han",
        "Kavel Rao",
        "Allyson Ettinger",
        "Liwei Jiang",
        "Bill Yuchen Lin",
        "Nathan Lambert",
        "Yejin Choi",
        "Nouha Dziri"
      ],
      "summary": "We introduce WildGuard -- an open, light-weight moderation tool for LLM\nsafety that achieves three goals: (1) identifying malicious intent in user\nprompts, (2) detecting safety risks of model responses, and (3) determining\nmodel refusal rate. Together, WildGuard serves the increasing needs for\nautomatic safety moderation and evaluation of LLM interactions, providing a\none-stop tool with enhanced accuracy and broad coverage across 13 risk\ncategories. While existing open moderation tools such as Llama-Guard2 score\nreasonably well in classifying straightforward model interactions, they lag far\nbehind a prompted GPT-4, especially in identifying adversarial jailbreaks and\nin evaluating models' refusals, a key measure for evaluating safety behaviors\nin model responses.\n  To address these challenges, we construct WildGuardMix, a large-scale and\ncarefully balanced multi-task safety moderation dataset with 92K labeled\nexamples that cover vanilla (direct) prompts and adversarial jailbreaks, paired\nwith various refusal and compliance responses. WildGuardMix is a combination of\nWildGuardTrain, the training data of WildGuard, and WildGuardTest, a\nhigh-quality human-annotated moderation test set with 5K labeled items covering\nbroad risk scenarios. Through extensive evaluations on WildGuardTest and ten\nexisting public benchmarks, we show that WildGuard establishes state-of-the-art\nperformance in open-source safety moderation across all the three tasks\ncompared to ten strong existing open-source moderation models (e.g., up to\n26.4% improvement on refusal detection). Importantly, WildGuard matches and\nsometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt\nharmfulness identification). WildGuard serves as a highly effective safety\nmoderator in an LLM interface, reducing the success rate of jailbreak attacks\nfrom 79.8% to 2.4%.",
      "published": "2024-06-26T16:58:20Z",
      "updated": "2024-12-09T20:21:56Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18495.pdf",
      "abs_url": "http://arxiv.org/abs/2406.18495v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.18041": {
      "arxiv_id": "2402.18041",
      "title": "Datasets for Large Language Models: A Comprehensive Survey",
      "authors": [
        "Yang Liu",
        "Jiahuan Cao",
        "Chongyu Liu",
        "Kai Ding",
        "Lianwen Jin"
      ],
      "summary": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
      "published": "2024-02-28T04:35:51Z",
      "updated": "2024-02-28T04:35:51Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.18041.pdf",
      "abs_url": "http://arxiv.org/abs/2402.18041v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1903.05987": {
      "arxiv_id": "1903.05987",
      "title": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse\n  Tasks",
      "authors": [
        "Matthew E. Peters",
        "Sebastian Ruder",
        "Noah A. Smith"
      ],
      "summary": "While most previous work has focused on different pretraining objectives and\narchitectures for transfer learning, we ask how to best adapt the pretrained\nmodel to a given target task. We focus on the two most common forms of\nadaptation, feature extraction (where the pretrained weights are frozen), and\ndirectly fine-tuning the pretrained model. Our empirical results across diverse\nNLP tasks with two state-of-the-art models show that the relative performance\nof fine-tuning vs. feature extraction depends on the similarity of the\npretraining and target tasks. We explore possible explanations for this finding\nand provide a set of adaptation guidelines for the NLP practitioner.",
      "published": "2019-03-14T13:32:31Z",
      "updated": "2019-06-11T13:13:46Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1903.05987.pdf",
      "abs_url": "http://arxiv.org/abs/1903.05987v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2104.06390": {
      "arxiv_id": "2104.06390",
      "title": "Detoxifying Language Models Risks Marginalizing Minority Voices",
      "authors": [
        "Albert Xu",
        "Eshaan Pathak",
        "Eric Wallace",
        "Suchin Gururangan",
        "Maarten Sap",
        "Dan Klein"
      ],
      "summary": "Language models (LMs) must be both safe and equitable to be responsibly\ndeployed in practice. With safety in mind, numerous detoxification techniques\n(e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to\nmitigate toxic LM generations. In this work, we show that current\ndetoxification techniques hurt equity: they decrease the utility of LMs on\nlanguage used by marginalized groups (e.g., African-American English and\nminority identity mentions). In particular, we perform automatic and human\nevaluations of text generation quality when LMs are conditioned on inputs with\ndifferent dialects and group identifiers. We find that detoxification makes LMs\nmore brittle to distribution shift, especially on language used by marginalized\ngroups. We identify that these failures stem from detoxification methods\nexploiting spurious correlations in toxicity datasets. Overall, our results\nhighlight the tension between the controllability and distributional robustness\nof LMs.",
      "published": "2021-04-13T17:52:01Z",
      "updated": "2021-04-13T17:52:01Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2104.06390.pdf",
      "abs_url": "http://arxiv.org/abs/2104.06390v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2309.16155": {
      "arxiv_id": "2309.16155",
      "title": "The Trickle-down Impact of Reward (In-)consistency on RLHF",
      "authors": [
        "Lingfeng Shen",
        "Sihao Chen",
        "Linfeng Song",
        "Lifeng Jin",
        "Baolin Peng",
        "Haitao Mi",
        "Daniel Khashabi",
        "Dong Yu"
      ],
      "summary": "Standard practice within Reinforcement Learning from Human Feedback (RLHF)\ninvolves optimizing against a Reward Model (RM), which itself is trained to\nreflect human preferences for desirable generations. A notable subject that is\nunderstudied is the (in-)consistency of RMs -- whether they can recognize the\nsemantic changes to different prompts and appropriately adapt their reward\nassignments -- and their impact on the downstream RLHF model.\n  In this paper, we visit a series of research questions relevant to RM\ninconsistency: (1) How can we measure the consistency of reward models? (2) How\nconsistent are the existing RMs and how can we improve them? (3) In what ways\ndoes reward inconsistency influence the chatbots resulting from the RLHF model\ntraining?\n  We propose Contrast Instructions -- a benchmarking strategy for the\nconsistency of RM. Each example in Contrast Instructions features a pair of\nlexically similar instructions with different ground truth responses. A\nconsistent RM is expected to rank the corresponding instruction and response\nhigher than other combinations. We observe that current RMs trained with the\nstandard ranking objective fail miserably on Contrast Instructions compared to\naverage humans. To show that RM consistency can be improved efficiently without\nusing extra training budget, we propose two techniques ConvexDA and\nRewardFusion, which enhance reward consistency through extrapolation during the\nRM training and inference stage, respectively. We show that RLHF models trained\nwith a more consistent RM yield more useful responses, suggesting that reward\ninconsistency exhibits a trickle-down effect on the downstream RLHF process.",
      "published": "2023-09-28T04:05:13Z",
      "updated": "2023-09-28T04:05:13Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.16155.pdf",
      "abs_url": "http://arxiv.org/abs/2309.16155v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2007.07779": {
      "arxiv_id": "2007.07779",
      "title": "AdapterHub: A Framework for Adapting Transformers",
      "authors": [
        "Jonas Pfeiffer",
        "Andreas R\u00fcckl\u00e9",
        "Clifton Poth",
        "Aishwarya Kamath",
        "Ivan Vuli\u0107",
        "Sebastian Ruder",
        "Kyunghyun Cho",
        "Iryna Gurevych"
      ],
      "summary": "The current modus operandi in NLP involves downloading and fine-tuning\npre-trained models consisting of millions or billions of parameters. Storing\nand sharing such large trained models is expensive, slow, and time-consuming,\nwhich impedes progress towards more general and versatile NLP methods that\nlearn from and for many tasks. Adapters -- small learnt bottleneck layers\ninserted within each layer of a pre-trained model -- ameliorate this issue by\navoiding full fine-tuning of the entire model. However, sharing and integrating\nadapter layers is not straightforward. We propose AdapterHub, a framework that\nallows dynamic \"stitching-in\" of pre-trained adapters for different tasks and\nlanguages. The framework, built on top of the popular HuggingFace Transformers\nlibrary, enables extremely easy and quick adaptations of state-of-the-art\npre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.\nDownloading, sharing, and training adapters is as seamless as possible using\nminimal changes to the training scripts and a specialized infrastructure. Our\nframework enables scalable and easy access to sharing of task-specific models,\nparticularly in low-resource scenarios. AdapterHub includes all recent adapter\narchitectures and can be found at https://AdapterHub.ml.",
      "published": "2020-07-15T15:56:05Z",
      "updated": "2020-10-06T10:16:39Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2007.07779.pdf",
      "abs_url": "http://arxiv.org/abs/2007.07779v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2406.12845": {
      "arxiv_id": "2406.12845",
      "title": "Interpretable Preferences via Multi-Objective Reward Modeling and\n  Mixture-of-Experts",
      "authors": [
        "Haoxiang Wang",
        "Wei Xiong",
        "Tengyang Xie",
        "Han Zhao",
        "Tong Zhang"
      ],
      "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. The\nRLHF process typically starts by training a reward model (RM) using human\npreference data. Conventional RMs are trained on pairwise responses to the same\nuser request, with relative ratings indicating which response humans prefer.\nThe trained RM serves as a proxy for human preferences. However, due to the\nblack-box nature of RMs, their outputs lack interpretability, as humans cannot\nintuitively understand why an RM thinks a response is good or not. As RMs act\nas human preference proxies, we believe they should be human-interpretable to\nensure that their internal decision processes are consistent with human\npreferences and to prevent reward hacking in LLM alignment. To build RMs with\ninterpretable preferences, we propose a two-stage approach: i) train an\nAbsolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional\nabsolute-rating data, each dimension corresponding to a human-interpretable\nobjective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts\n(MoE) strategy with a gating network that automatically selects the most\nsuitable reward objectives based on the context. We efficiently trained an\nArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top\nof the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art\nperformance on RewardBench, a benchmark evaluating RMs for language modeling.\nNotably, the performance of our model surpasses the LLM-as-a-judge method with\nGPT-4 judges by a margin, and approaches the performance of the much larger\nNemotron-4 340B reward model.",
      "published": "2024-06-18T17:58:28Z",
      "updated": "2024-06-18T17:58:28Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12845.pdf",
      "abs_url": "http://arxiv.org/abs/2406.12845v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2406.11817": {
      "arxiv_id": "2406.11817",
      "title": "Iterative Length-Regularized Direct Preference Optimization: A Case\n  Study on Improving 7B Language Models to GPT-4 Level",
      "authors": [
        "Jie Liu",
        "Zhanhui Zhou",
        "Jiaheng Liu",
        "Xingyuan Bu",
        "Chao Yang",
        "Han-Sen Zhong",
        "Wanli Ouyang"
      ],
      "summary": "Direct Preference Optimization (DPO), a standard method for aligning language\nmodels with human preferences, is traditionally applied to offline preferences.\nRecent studies show that DPO benefits from iterative training with online\npreferences labeled by a trained reward model. In this work, we identify a\npitfall of vanilla iterative DPO - improved response quality can lead to\nincreased verbosity. To address this, we introduce iterative length-regularized\nDPO (iLR-DPO) to penalize response length. Our empirical results show that\niLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing\nverbosity. Specifically, our 7B model achieves a $50.5\\%$ length-controlled win\nrate against $\\texttt{GPT-4 Preview}$ on AlpacaEval 2.0, and excels across\nstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.\nThese results demonstrate the effectiveness of iterative DPO in aligning\nlanguage models with human feedback.",
      "published": "2024-06-17T17:55:38Z",
      "updated": "2024-06-17T17:55:38Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11817.pdf",
      "abs_url": "http://arxiv.org/abs/2406.11817v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2310.03716": {
      "arxiv_id": "2310.03716",
      "title": "A Long Way to Go: Investigating Length Correlations in RLHF",
      "authors": [
        "Prasann Singhal",
        "Tanya Goyal",
        "Jiacheng Xu",
        "Greg Durrett"
      ],
      "summary": "Great success has been reported using Reinforcement Learning from Human\nFeedback (RLHF) to align large language models, with open preference datasets\nenabling wider experimentation, particularly for \"helpfulness\" in tasks like\ndialogue and web question answering. Alongside these improvements, however,\nRLHF also often drives models to produce longer outputs. This paper\ndemonstrates, on three diverse settings, that optimizing for response length\nis, much more than previously thought, a significant factor behind RLHF.\nStudying the strategies RL optimization uses to maximize reward, we find\nimprovements in reward to largely be driven by increasing response length,\ninstead of other features. Indeed, we find that even a purely length-based\nreward reproduces most downstream RLHF improvements over supervised fine-tuned\nmodels. Testing a comprehensive set of length-countering interventions, we\nidentify the dominant source of these biases to be reward models, which, by\nstudying training dynamics, we find are non-robust and easily influenced by\nlength biases in preference data.",
      "published": "2023-10-05T17:38:28Z",
      "updated": "2024-07-10T23:15:49Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03716.pdf",
      "abs_url": "http://arxiv.org/abs/2310.03716v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2209.11055": {
      "arxiv_id": "2209.11055",
      "title": "Efficient Few-Shot Learning Without Prompts",
      "authors": [
        "Lewis Tunstall",
        "Nils Reimers",
        "Unso Eun Seo Jo",
        "Luke Bates",
        "Daniel Korat",
        "Moshe Wasserblat",
        "Oren Pereg"
      ],
      "summary": "Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and\npattern exploiting training (PET), have achieved impressive results in\nlabel-scarce settings. However, they are difficult to employ since they are\nsubject to high variability from manually crafted prompts, and typically\nrequire billion-parameter language models to achieve high accuracy. To address\nthese shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an\nefficient and prompt-free framework for few-shot fine-tuning of Sentence\nTransformers (ST). SetFit works by first fine-tuning a pretrained ST on a small\nnumber of text pairs, in a contrastive Siamese manner. The resulting model is\nthen used to generate rich text embeddings, which are used to train a\nclassification head. This simple framework requires no prompts or verbalizers,\nand achieves high accuracy with orders of magnitude less parameters than\nexisting techniques. Our experiments show that SetFit obtains comparable\nresults with PEFT and PET techniques, while being an order of magnitude faster\nto train. We also show that SetFit can be applied in multilingual settings by\nsimply switching the ST body. Our code is available at\nhttps://github.com/huggingface/setfit and our datasets at\nhttps://huggingface.co/setfit .",
      "published": "2022-09-22T14:48:11Z",
      "updated": "2022-09-22T14:48:11Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.11055.pdf",
      "abs_url": "http://arxiv.org/abs/2209.11055v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.05749": {
      "arxiv_id": "2402.05749",
      "title": "Generalized Preference Optimization: A Unified Approach to Offline\n  Alignment",
      "authors": [
        "Yunhao Tang",
        "Zhaohan Daniel Guo",
        "Zeyu Zheng",
        "Daniele Calandriello",
        "R\u00e9mi Munos",
        "Mark Rowland",
        "Pierre Harvey Richemond",
        "Michal Valko",
        "Bernardo \u00c1vila Pires",
        "Bilal Piot"
      ],
      "summary": "Offline preference optimization allows fine-tuning large models directly from\noffline data, and has proved effective in recent alignment practices. We\npropose generalized preference optimization (GPO), a family of offline losses\nparameterized by a general class of convex functions. GPO enables a unified\nview over preference optimization, encompassing existing algorithms such as\nDPO, IPO and SLiC as special cases, while naturally introducing new variants.\nThe GPO framework also sheds light on how offline algorithms enforce\nregularization, through the design of the convex function that defines the\nloss. Our analysis and experiments reveal the connections and subtle\ndifferences between the offline regularization and the KL divergence\nregularization intended by the canonical RLHF formulation. In a controlled\nsetting akin to Gao et al 2023, we also show that different GPO variants\nachieve similar trade-offs between regularization and performance, though the\noptimal values of hyper-parameter might differ as predicted by theory. In all,\nour results present new algorithmic toolkits and empirical insights to\nalignment practitioners.",
      "published": "2024-02-08T15:33:09Z",
      "updated": "2024-05-28T23:25:15Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05749.pdf",
      "abs_url": "http://arxiv.org/abs/2402.05749v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2308.05374": {
      "arxiv_id": "2308.05374",
      "title": "Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language\n  Models' Alignment",
      "authors": [
        "Yang Liu",
        "Yuanshun Yao",
        "Jean-Francois Ton",
        "Xiaoying Zhang",
        "Ruocheng Guo",
        "Hao Cheng",
        "Yegor Klochkov",
        "Muhammad Faaiz Taufiq",
        "Hang Li"
      ],
      "summary": "Ensuring alignment, which refers to making models behave in accordance with\nhuman intentions [1,2], has become a critical task before deploying large\nlanguage models (LLMs) in real-world applications. For instance, OpenAI devoted\nsix months to iteratively aligning GPT-4 before its release [3]. However, a\nmajor challenge faced by practitioners is the lack of clear guidance on\nevaluating whether LLM outputs align with social norms, values, and\nregulations. This obstacle hinders systematic iteration and deployment of LLMs.\nTo address this issue, this paper presents a comprehensive survey of key\ndimensions that are crucial to consider when assessing LLM trustworthiness. The\nsurvey covers seven major categories of LLM trustworthiness: reliability,\nsafety, fairness, resistance to misuse, explainability and reasoning, adherence\nto social norms, and robustness. Each major category is further divided into\nseveral sub-categories, resulting in a total of 29 sub-categories.\nAdditionally, a subset of 8 sub-categories is selected for further\ninvestigation, where corresponding measurement studies are designed and\nconducted on several widely-used LLMs. The measurement results indicate that,\nin general, more aligned models tend to perform better in terms of overall\ntrustworthiness. However, the effectiveness of alignment varies across the\ndifferent trustworthiness categories considered. This highlights the importance\nof conducting more fine-grained analyses, testing, and making continuous\nimprovements on LLM alignment. By shedding light on these key dimensions of LLM\ntrustworthiness, this paper aims to provide valuable insights and guidance to\npractitioners in the field. Understanding and addressing these concerns will be\ncrucial in achieving reliable and ethically sound deployment of LLMs in various\napplications.",
      "published": "2023-08-10T06:43:44Z",
      "updated": "2024-03-21T00:21:14Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05374.pdf",
      "abs_url": "http://arxiv.org/abs/2308.05374v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2305.10425": {
      "arxiv_id": "2305.10425",
      "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback",
      "authors": [
        "Yao Zhao",
        "Rishabh Joshi",
        "Tianqi Liu",
        "Misha Khalman",
        "Mohammad Saleh",
        "Peter J. Liu"
      ],
      "summary": "Learning from human feedback has been shown to be effective at aligning\nlanguage models with human preferences. Past work has often relied on\nReinforcement Learning from Human Feedback (RLHF), which optimizes the language\nmodel using reward scores assigned from a reward model trained on human\npreference data. In this work we show how the recently introduced Sequence\nLikelihood Calibration (SLiC), can also be used to effectively learn from human\npreferences (SLiC-HF). Furthermore, we demonstrate this can be done with human\nfeedback data collected for a different model, similar to off-policy, offline\nRL data. Automatic and human evaluation experiments on the TL;DR summarization\ntask show that SLiC-HF significantly improves supervised fine-tuning baselines.\nFurthermore, SLiC-HF presents a competitive alternative to the PPO RLHF\nimplementation used in past work while being much simpler to implement, easier\nto tune and more computationally efficient in practice.",
      "published": "2023-05-17T17:57:10Z",
      "updated": "2023-05-17T17:57:10Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.10425.pdf",
      "abs_url": "http://arxiv.org/abs/2305.10425v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1707.06347": {
      "arxiv_id": "1707.06347",
      "title": "Proximal Policy Optimization Algorithms",
      "authors": [
        "John Schulman",
        "Filip Wolski",
        "Prafulla Dhariwal",
        "Alec Radford",
        "Oleg Klimov"
      ],
      "summary": "We propose a new family of policy gradient methods for reinforcement\nlearning, which alternate between sampling data through interaction with the\nenvironment, and optimizing a \"surrogate\" objective function using stochastic\ngradient ascent. Whereas standard policy gradient methods perform one gradient\nupdate per data sample, we propose a novel objective function that enables\nmultiple epochs of minibatch updates. The new methods, which we call proximal\npolicy optimization (PPO), have some of the benefits of trust region policy\noptimization (TRPO), but they are much simpler to implement, more general, and\nhave better sample complexity (empirically). Our experiments test PPO on a\ncollection of benchmark tasks, including simulated robotic locomotion and Atari\ngame playing, and we show that PPO outperforms other online policy gradient\nmethods, and overall strikes a favorable balance between sample complexity,\nsimplicity, and wall-time.",
      "published": "2017-07-20T02:32:33Z",
      "updated": "2017-08-28T09:20:06Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1707.06347.pdf",
      "abs_url": "http://arxiv.org/abs/1707.06347v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2410.12138": {
      "arxiv_id": "2410.12138",
      "title": "Preference Optimization with Multi-Sample Comparisons",
      "authors": [
        "Chaoqi Wang",
        "Zhuokai Zhao",
        "Chen Zhu",
        "Karthik Abinav Sankararaman",
        "Michal Valko",
        "Xuefei Cao",
        "Zhaorun Chen",
        "Madian Khabsa",
        "Yuxin Chen",
        "Hao Ma",
        "Sinong Wang"
      ],
      "summary": "Recent advancements in generative models, particularly large language models\n(LLMs) and diffusion models, have been driven by extensive pretraining on large\ndatasets followed by post-training. However, current post-training methods such\nas reinforcement learning from human feedback (RLHF) and direct alignment from\npreference methods (DAP) primarily utilize single-sample comparisons. These\napproaches often fail to capture critical characteristics such as generative\ndiversity and bias, which are more accurately assessed through multiple\nsamples. To address these limitations, we introduce a novel approach that\nextends post-training to include multi-sample comparisons. To achieve this, we\npropose Multi-sample Direct Preference Optimization (mDPO) and Multi-sample\nIdentity Preference Optimization (mIPO). These methods improve traditional DAP\nmethods by focusing on group-wise characteristics. Empirically, we demonstrate\nthat multi-sample comparison is more effective in optimizing collective\ncharacteristics~(e.g., diversity and bias) for generative models than\nsingle-sample comparison. Additionally, our findings suggest that multi-sample\ncomparisons provide a more robust optimization framework, particularly for\ndataset with label noise.",
      "published": "2024-10-16T00:59:19Z",
      "updated": "2025-03-26T06:48:11Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.12138.pdf",
      "abs_url": "http://arxiv.org/abs/2410.12138v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2304.05302": {
      "arxiv_id": "2304.05302",
      "title": "RRHF: Rank Responses to Align Language Models with Human Feedback\n  without tears",
      "authors": [
        "Zheng Yuan",
        "Hongyi Yuan",
        "Chuanqi Tan",
        "Wei Wang",
        "Songfang Huang",
        "Fei Huang"
      ],
      "summary": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment\nof large language models with human preferences, significantly enhancing the\nquality of interactions between humans and models. InstructGPT implements RLHF\nthrough several stages, including Supervised Fine-Tuning (SFT), reward model\ntraining, and Proximal Policy Optimization (PPO). However, PPO is sensitive to\nhyperparameters and requires multiple models in its standard implementation,\nmaking it hard to train and scale up to larger parameter counts. In contrast,\nwe propose a novel learning paradigm called RRHF, which scores sampled\nresponses from different sources via a logarithm of conditional probabilities\nand learns to align these probabilities with human preferences through ranking\nloss. RRHF can leverage sampled responses from various sources including the\nmodel responses from itself, other large language model responses, and human\nexpert responses to learn to rank them. RRHF only needs 1 to 2 models during\ntuning and can efficiently align language models with human preferences\nrobustly without complex hyperparameter tuning. Additionally, RRHF can be\nconsidered an extension of SFT and reward model training while being simpler\nthan PPO in terms of coding, model counts, and hyperparameters. We evaluate\nRRHF on the Helpful and Harmless dataset, demonstrating comparable alignment\nperformance with PPO by reward model score and human labeling. Extensive\nexperiments show that the performance of RRHF is highly related to sampling\nquality which suggests RRHF is a best-of-n learner. Codes available at\nhttps://github.com/GanjinZero/RRHF.",
      "published": "2023-04-11T15:53:40Z",
      "updated": "2023-10-07T07:01:26Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.05302.pdf",
      "abs_url": "http://arxiv.org/abs/2304.05302v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1909.05858": {
      "arxiv_id": "1909.05858",
      "title": "CTRL: A Conditional Transformer Language Model for Controllable\n  Generation",
      "authors": [
        "Nitish Shirish Keskar",
        "Bryan McCann",
        "Lav R. Varshney",
        "Caiming Xiong",
        "Richard Socher"
      ],
      "summary": "Large-scale language models show promising text generation capabilities, but\nusers cannot easily control particular aspects of the generated text. We\nrelease CTRL, a 1.63 billion-parameter conditional transformer language model,\ntrained to condition on control codes that govern style, content, and\ntask-specific behavior. Control codes were derived from structure that\nnaturally co-occurs with raw text, preserving the advantages of unsupervised\nlearning while providing more explicit control over text generation. These\ncodes also allow CTRL to predict which parts of the training data are most\nlikely given a sequence. This provides a potential method for analyzing large\namounts of data via model-based source attribution. We have released multiple\nfull-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.",
      "published": "2019-09-11T17:57:18Z",
      "updated": "2019-09-20T20:08:56Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/1909.05858.pdf",
      "abs_url": "http://arxiv.org/abs/1909.05858v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2203.04600": {
      "arxiv_id": "2203.04600",
      "title": "Domain Generalization using Pretrained Models without Fine-tuning",
      "authors": [
        "Ziyue Li",
        "Kan Ren",
        "Xinyang Jiang",
        "Bo Li",
        "Haipeng Zhang",
        "Dongsheng Li"
      ],
      "summary": "Fine-tuning pretrained models is a common practice in domain generalization\n(DG) tasks. However, fine-tuning is usually computationally expensive due to\nthe ever-growing size of pretrained models. More importantly, it may cause\nover-fitting on source domain and compromise their generalization ability as\nshown in recent works. Generally, pretrained models possess some level of\ngeneralization ability and can achieve decent performance regarding specific\ndomains and samples. However, the generalization performance of pretrained\nmodels could vary significantly over different test domains even samples, which\nraises challenges for us to best leverage pretrained models in DG tasks. In\nthis paper, we propose a novel domain generalization paradigm to better\nleverage various pretrained models, named specialized ensemble learning for\ndomain generalization (SEDGE). It first trains a linear label space adapter\nupon fixed pretrained models, which transforms the outputs of the pretrained\nmodel to the label space of the target domain. Then, an ensemble network aware\nof model specialty is proposed to dynamically dispatch proper pretrained models\nto predict each test sample. Experimental studies on several benchmarks show\nthat SEDGE achieves significant performance improvements comparing to strong\nbaselines including state-of-the-art method in DG tasks and reduces the\ntrainable parameters by ~99% and the training time by ~99.5%.",
      "published": "2022-03-09T09:33:59Z",
      "updated": "2022-03-09T09:33:59Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2203.04600.pdf",
      "abs_url": "http://arxiv.org/abs/2203.04600v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2403.19159": {
      "arxiv_id": "2403.19159",
      "title": "Disentangling Length from Quality in Direct Preference Optimization",
      "authors": [
        "Ryan Park",
        "Rafael Rafailov",
        "Stefano Ermon",
        "Chelsea Finn"
      ],
      "summary": "Reinforcement Learning from Human Feedback (RLHF) has been a crucial\ncomponent in the recent success of Large Language Models. However, RLHF is know\nto exploit biases in human preferences, such as verbosity. A well-formatted and\neloquent answer is often more highly rated by users, even when it is less\nhelpful and objective. A number of approaches have been developed to control\nthose biases in the classical RLHF literature, but the problem remains\nrelatively under-explored for Direct Alignment Algorithms such as Direct\nPreference Optimization (DPO). Unlike classical RLHF, DPO does not train a\nseparate reward model or use reinforcement learning directly, so previous\napproaches developed to control verbosity cannot be directly applied to this\nsetting. Our work makes several contributions. For the first time, we study the\nlength problem in the DPO setting, showing significant exploitation in DPO and\nlinking it to out-of-distribution bootstrapping. We then develop a principled\nbut simple regularization strategy that prevents length exploitation, while\nstill maintaining improvements in model quality. We demonstrate these effects\nacross datasets on summarization and dialogue, where we achieve up to 20\\%\nimprovement in win rates when controlling for length, despite the GPT4 judge's\nwell-known verbosity bias.",
      "published": "2024-03-28T06:03:47Z",
      "updated": "2024-09-09T04:39:31Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.19159.pdf",
      "abs_url": "http://arxiv.org/abs/2403.19159v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2311.08045": {
      "arxiv_id": "2311.08045",
      "title": "Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM\n  Game",
      "authors": [
        "Pengyu Cheng",
        "Yifan Yang",
        "Jian Li",
        "Yong Dai",
        "Tianhao Hu",
        "Peixin Cao",
        "Nan Du",
        "Xiaolong Li"
      ],
      "summary": "Human preference alignment is essential to improve the interaction quality of\nlarge language models (LLMs). Existing alignment methods depend on manually\nannotated preference data to guide the LLM optimization directions. However,\ncontinuously updating LLMs for alignment raises a distribution gap between\nmodel-generated samples and human-annotated responses, hindering training\neffectiveness. To mitigate this issue, previous methods require additional\npreference annotation on newly generated samples to adapt to the shifted\ndistribution, which consumes a large amount of annotation resources. Targeting\nmore efficient human preference optimization, we propose an Adversarial\nPreference Optimization (APO) framework, in which the LLM and the reward model\nupdate alternatively via a min-max game. Through adversarial training, the\nreward model can adapt to the shifted generation distribution of the LLM\nwithout any additional annotation. With comprehensive experiments, we find the\nproposed adversarial training framework further enhances existing alignment\nbaselines in terms of LLM helpfulness and harmlessness. The code is at\nhttps://github.com/Linear95/APO.",
      "published": "2023-11-14T10:10:31Z",
      "updated": "2024-06-03T11:34:05Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08045.pdf",
      "abs_url": "http://arxiv.org/abs/2311.08045v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.03300": {
      "arxiv_id": "2402.03300",
      "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open\n  Language Models",
      "authors": [
        "Zhihong Shao",
        "Peiyi Wang",
        "Qihao Zhu",
        "Runxin Xu",
        "Junxiao Song",
        "Xiao Bi",
        "Haowei Zhang",
        "Mingchuan Zhang",
        "Y. K. Li",
        "Y. Wu",
        "Daya Guo"
      ],
      "summary": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.",
      "published": "2024-02-05T18:55:32Z",
      "updated": "2024-04-27T15:25:53Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03300.pdf",
      "abs_url": "http://arxiv.org/abs/2402.03300v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2212.08073": {
      "arxiv_id": "2212.08073",
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": [
        "Yuntao Bai",
        "Saurav Kadavath",
        "Sandipan Kundu",
        "Amanda Askell",
        "Jackson Kernion",
        "Andy Jones",
        "Anna Chen",
        "Anna Goldie",
        "Azalia Mirhoseini",
        "Cameron McKinnon",
        "Carol Chen",
        "Catherine Olsson",
        "Christopher Olah",
        "Danny Hernandez",
        "Dawn Drain",
        "Deep Ganguli",
        "Dustin Li",
        "Eli Tran-Johnson",
        "Ethan Perez",
        "Jamie Kerr",
        "Jared Mueller",
        "Jeffrey Ladish",
        "Joshua Landau",
        "Kamal Ndousse",
        "Kamile Lukosuite",
        "Liane Lovitt",
        "Michael Sellitto",
        "Nelson Elhage",
        "Nicholas Schiefer",
        "Noemi Mercado",
        "Nova DasSarma",
        "Robert Lasenby",
        "Robin Larson",
        "Sam Ringer",
        "Scott Johnston",
        "Shauna Kravec",
        "Sheer El Showk",
        "Stanislav Fort",
        "Tamera Lanham",
        "Timothy Telleen-Lawton",
        "Tom Conerly",
        "Tom Henighan",
        "Tristan Hume",
        "Samuel R. Bowman",
        "Zac Hatfield-Dodds",
        "Ben Mann",
        "Dario Amodei",
        "Nicholas Joseph",
        "Sam McCandlish",
        "Tom Brown",
        "Jared Kaplan"
      ],
      "summary": "As AI systems become more capable, we would like to enlist their help to\nsupervise other AIs. We experiment with methods for training a harmless AI\nassistant through self-improvement, without any human labels identifying\nharmful outputs. The only human oversight is provided through a list of rules\nor principles, and so we refer to the method as 'Constitutional AI'. The\nprocess involves both a supervised learning and a reinforcement learning phase.\nIn the supervised phase we sample from an initial model, then generate\nself-critiques and revisions, and then finetune the original model on revised\nresponses. In the RL phase, we sample from the finetuned model, use a model to\nevaluate which of the two samples is better, and then train a preference model\nfrom this dataset of AI preferences. We then train with RL using the preference\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\nresult we are able to train a harmless but non-evasive AI assistant that\nengages with harmful queries by explaining its objections to them. Both the SL\nand RL methods can leverage chain-of-thought style reasoning to improve the\nhuman-judged performance and transparency of AI decision making. These methods\nmake it possible to control AI behavior more precisely and with far fewer human\nlabels.",
      "published": "2022-12-15T06:19:23Z",
      "updated": "2022-12-15T06:19:23Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.08073.pdf",
      "abs_url": "http://arxiv.org/abs/2212.08073v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2309.06657": {
      "arxiv_id": "2309.06657",
      "title": "Statistical Rejection Sampling Improves Preference Optimization",
      "authors": [
        "Tianqi Liu",
        "Yao Zhao",
        "Rishabh Joshi",
        "Misha Khalman",
        "Mohammad Saleh",
        "Peter J. Liu",
        "Jialu Liu"
      ],
      "summary": "Improving the alignment of language models with human preferences remains an\nactive research challenge. Previous approaches have primarily utilized\nReinforcement Learning from Human Feedback (RLHF) via online RL methods such as\nProximal Policy Optimization (PPO). Recently, offline methods such as Sequence\nLikelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have\nemerged as attractive alternatives, offering improvements in stability and\nscalability while maintaining competitive performance. SLiC refines its loss\nfunction using sequence pairs sampled from a supervised fine-tuned (SFT)\npolicy, while DPO directly optimizes language models based on preference data,\nforegoing the need for a separate reward model. However, the maximum likelihood\nestimator (MLE) of the target optimal policy requires labeled preference pairs\nsampled from that policy. DPO's lack of a reward model constrains its ability\nto sample preference pairs from the optimal policy, and SLiC is restricted to\nsampling preference pairs only from the SFT policy. To address these\nlimitations, we introduce a novel approach called Statistical Rejection\nSampling Optimization (RSO) that aims to source preference data from the target\noptimal policy using rejection sampling, enabling a more accurate estimation of\nthe optimal policy. We also propose a unified framework that enhances the loss\nfunctions used in both SLiC and DPO from a preference modeling standpoint.\nThrough extensive experiments across three diverse tasks, we demonstrate that\nRSO consistently outperforms both SLiC and DPO on evaluations from both Large\nLanguage Model (LLM) and human raters.",
      "published": "2023-09-13T01:07:25Z",
      "updated": "2024-01-23T23:16:11Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.06657.pdf",
      "abs_url": "http://arxiv.org/abs/2309.06657v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2410.11287": {
      "arxiv_id": "2410.11287",
      "title": "Process Reward Model with Q-Value Rankings",
      "authors": [
        "Wendi Li",
        "Yixuan Li"
      ],
      "summary": "Process Reward Modeling (PRM) is critical for complex reasoning and\ndecision-making tasks where the accuracy of intermediate steps significantly\ninfluences the overall outcome. Existing PRM approaches, primarily framed as\nclassification problems, employ cross-entropy loss to independently evaluate\neach step's correctness. This method can lead to suboptimal reward distribution\nand does not adequately address the interdependencies among steps. To address\nthese limitations, we introduce the Process Q-value Model (PQM), a novel\nframework that redefines PRM in the context of a Markov Decision Process. PQM\noptimizes Q-value rankings based on a novel comparative loss function,\nenhancing the model's ability to capture the intricate dynamics among\nsequential decisions. This approach provides a more granular and theoretically\ngrounded methodology for process rewards. Our extensive empirical evaluations\nacross various sampling policies, language model backbones, and multi-step\nreasoning benchmarks show that PQM outperforms classification-based PRMs. The\neffectiveness of the comparative loss function is highlighted in our\ncomprehensive ablation studies, confirming PQM's practical efficacy and\ntheoretical advantage.",
      "published": "2024-10-15T05:10:34Z",
      "updated": "2025-02-11T05:41:41Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11287.pdf",
      "abs_url": "http://arxiv.org/abs/2410.11287v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2406.10162": {
      "arxiv_id": "2406.10162",
      "title": "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large\n  Language Models",
      "authors": [
        "Carson Denison",
        "Monte MacDiarmid",
        "Fazl Barez",
        "David Duvenaud",
        "Shauna Kravec",
        "Samuel Marks",
        "Nicholas Schiefer",
        "Ryan Soklaski",
        "Alex Tamkin",
        "Jared Kaplan",
        "Buck Shlegeris",
        "Samuel R. Bowman",
        "Ethan Perez",
        "Evan Hubinger"
      ],
      "summary": "In reinforcement learning, specification gaming occurs when AI systems learn\nundesired behaviors that are highly rewarded due to misspecified training\ngoals. Specification gaming can range from simple behaviors like sycophancy to\nsophisticated and pernicious behaviors like reward-tampering, where a model\ndirectly modifies its own reward mechanism. However, these more pernicious\nbehaviors may be too complex to be discovered via exploration. In this paper,\nwe study whether Large Language Model (LLM) assistants which find easily\ndiscovered forms of specification gaming will generalize to perform rarer and\nmore blatant forms, up to and including reward-tampering. We construct a\ncurriculum of increasingly sophisticated gameable environments and find that\ntraining on early-curriculum environments leads to more specification gaming on\nremaining environments. Strikingly, a small but non-negligible proportion of\nthe time, LLM assistants trained on the full curriculum generalize zero-shot to\ndirectly rewriting their own reward function. Retraining an LLM not to game\nearly-curriculum environments mitigates, but does not eliminate,\nreward-tampering in later environments. Moreover, adding harmlessness training\nto our gameable environments does not prevent reward-tampering. These results\ndemonstrate that LLMs can generalize from common forms of specification gaming\nto more pernicious reward tampering and that such behavior may be nontrivial to\nremove.",
      "published": "2024-06-14T16:26:20Z",
      "updated": "2024-06-29T00:28:47Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10162.pdf",
      "abs_url": "http://arxiv.org/abs/2406.10162v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2410.05229": {
      "arxiv_id": "2410.05229",
      "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in\n  Large Language Models",
      "authors": [
        "Iman Mirzadeh",
        "Keivan Alizadeh",
        "Hooman Shahrokhi",
        "Oncel Tuzel",
        "Samy Bengio",
        "Mehrdad Farajtabar"
      ],
      "summary": "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir formal reasoning capabilities, particularly in mathematics. The GSM8K\nbenchmark is widely used to assess the mathematical reasoning of models on\ngrade-school-level questions. While the performance of LLMs on GSM8K has\nsignificantly improved in recent years, it remains unclear whether their\nmathematical reasoning capabilities have genuinely advanced, raising questions\nabout the reliability of the reported metrics. To address these concerns, we\nconduct a large-scale study on several SOTA open and closed models. To overcome\nthe limitations of existing evaluations, we introduce GSM-Symbolic, an improved\nbenchmark created from symbolic templates that allow for the generation of a\ndiverse set of questions. GSM-Symbolic enables more controllable evaluations,\nproviding key insights and more reliable metrics for measuring the reasoning\ncapabilities of models.Our findings reveal that LLMs exhibit noticeable\nvariance when responding to different instantiations of the same question.\nSpecifically, the performance of all models declines when only the numerical\nvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,\nwe investigate the fragility of mathematical reasoning in these models and show\nthat their performance significantly deteriorates as the number of clauses in a\nquestion increases. We hypothesize that this decline is because current LLMs\ncannot perform genuine logical reasoning; they replicate reasoning steps from\ntheir training data. Adding a single clause that seems relevant to the question\ncauses significant performance drops (up to 65%) across all state-of-the-art\nmodels, even though the clause doesn't contribute to the reasoning chain needed\nfor the final answer. Overall, our work offers a more nuanced understanding of\nLLMs' capabilities and limitations in mathematical reasoning.",
      "published": "2024-10-07T17:36:37Z",
      "updated": "2024-10-07T17:36:37Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05229.pdf",
      "abs_url": "http://arxiv.org/abs/2410.05229v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2110.14168": {
      "arxiv_id": "2110.14168",
      "title": "Training Verifiers to Solve Math Word Problems",
      "authors": [
        "Karl Cobbe",
        "Vineet Kosaraju",
        "Mohammad Bavarian",
        "Mark Chen",
        "Heewoo Jun",
        "Lukasz Kaiser",
        "Matthias Plappert",
        "Jerry Tworek",
        "Jacob Hilton",
        "Reiichiro Nakano",
        "Christopher Hesse",
        "John Schulman"
      ],
      "summary": "State-of-the-art language models can match human performance on many tasks,\nbut they still struggle to robustly perform multi-step mathematical reasoning.\nTo diagnose the failures of current models and support research, we introduce\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\nword problems. We find that even the largest transformer models fail to achieve\nhigh test performance, despite the conceptual simplicity of this problem\ndistribution. To increase performance, we propose training verifiers to judge\nthe correctness of model completions. At test time, we generate many candidate\nsolutions and select the one ranked highest by the verifier. We demonstrate\nthat verification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.",
      "published": "2021-10-27T04:49:45Z",
      "updated": "2021-11-18T00:23:45Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2110.14168.pdf",
      "abs_url": "http://arxiv.org/abs/2110.14168v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1904.09751": {
      "arxiv_id": "1904.09751",
      "title": "The Curious Case of Neural Text Degeneration",
      "authors": [
        "Ari Holtzman",
        "Jan Buys",
        "Li Du",
        "Maxwell Forbes",
        "Yejin Choi"
      ],
      "summary": "Despite considerable advancements with deep neural language models, the\nenigma of neural text degeneration persists when these models are tested as\ntext generators. The counter-intuitive empirical observation is that even\nthough the use of likelihood as training objective leads to high quality models\nfor a broad range of language understanding tasks, using likelihood as a\ndecoding objective leads to text that is bland and strangely repetitive.\n  In this paper, we reveal surprising distributional differences between human\ntext and machine text. In addition, we find that decoding strategies alone can\ndramatically effect the quality of machine text, even when generated from\nexactly the same neural language model. Our findings motivate Nucleus Sampling,\na simple but effective method to draw the best out of neural generation. By\nsampling text from the dynamic nucleus of the probability distribution, which\nallows for diversity while effectively truncating the less reliable tail of the\ndistribution, the resulting text better demonstrates the quality of human text,\nyielding enhanced diversity without sacrificing fluency and coherence.",
      "published": "2019-04-22T07:17:18Z",
      "updated": "2020-02-14T21:56:30Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/1904.09751.pdf",
      "abs_url": "http://arxiv.org/abs/1904.09751v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1606.06565": {
      "arxiv_id": "1606.06565",
      "title": "Concrete Problems in AI Safety",
      "authors": [
        "Dario Amodei",
        "Chris Olah",
        "Jacob Steinhardt",
        "Paul Christiano",
        "John Schulman",
        "Dan Man\u00e9"
      ],
      "summary": "Rapid progress in machine learning and artificial intelligence (AI) has\nbrought increasing attention to the potential impacts of AI technologies on\nsociety. In this paper we discuss one such potential impact: the problem of\naccidents in machine learning systems, defined as unintended and harmful\nbehavior that may emerge from poor design of real-world AI systems. We present\na list of five practical research problems related to accident risk,\ncategorized according to whether the problem originates from having the wrong\nobjective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an\nobjective function that is too expensive to evaluate frequently (\"scalable\nsupervision\"), or undesirable behavior during the learning process (\"safe\nexploration\" and \"distributional shift\"). We review previous work in these\nareas as well as suggesting research directions with a focus on relevance to\ncutting-edge AI systems. Finally, we consider the high-level question of how to\nthink most productively about the safety of forward-looking applications of AI.",
      "published": "2016-06-21T13:37:05Z",
      "updated": "2016-07-25T17:23:29Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1606.06565.pdf",
      "abs_url": "http://arxiv.org/abs/1606.06565v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2501.09620": {
      "arxiv_id": "2501.09620",
      "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment",
      "authors": [
        "Chaoqi Wang",
        "Zhuokai Zhao",
        "Yibo Jiang",
        "Zhaorun Chen",
        "Chen Zhu",
        "Yuxin Chen",
        "Jiayi Liu",
        "Lizhu Zhang",
        "Xiangjun Fan",
        "Hao Ma",
        "Sinong Wang"
      ],
      "summary": "Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination-that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causality to mitigate these spurious correlations. Our\nmethod enforces counterfactual invariance, ensuring reward predictions remain\nconsistent when irrelevant variables are altered. Through experiments on both\nsynthetic and real-world datasets, we show that our approach mitigates various\ntypes of spurious correlations effectively, resulting in more reliable and fair\nalignment of LLMs with human preferences. As a drop-in enhancement to the\nexisting RLHF workflow, our causal reward modeling provides a practical way to\nimprove the trustworthiness and fairness of LLM finetuning.",
      "published": "2025-01-16T16:00:37Z",
      "updated": "2025-05-29T02:21:03Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.09620.pdf",
      "abs_url": "http://arxiv.org/abs/2501.09620v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.05369": {
      "arxiv_id": "2402.05369",
      "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards",
      "authors": [
        "Huayu Chen",
        "Guande He",
        "Lifan Yuan",
        "Ganqu Cui",
        "Hang Su",
        "Jun Zhu"
      ],
      "summary": "User intentions are typically formalized as evaluation rewards to be\nmaximized when fine-tuning language models (LMs). Existing alignment methods,\nsuch as Direct Preference Optimization (DPO), are mainly tailored for pairwise\npreference data where rewards are implicitly defined rather than explicitly\ngiven. In this paper, we introduce a general framework for LM alignment,\nleveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling\nreward datasets explicitly annotated with scalar evaluations. Our framework\ncomprises two parallel algorithms, NCA and InfoNCA, both enabling the direct\nextraction of an LM policy from reward data as well as preference data.\nNotably, we show that the DPO loss is a special case of our proposed InfoNCA\nobjective under pairwise preference settings, thereby integrating and extending\ncurrent alignment theories. By comparing NCA and InfoNCA, we demonstrate that\nthe well-observed decreasing-likelihood trend of DPO/InfoNCA is caused by their\nfocus on adjusting relative likelihood across different responses. In contrast,\nNCA optimizes the absolute likelihood for each response, thereby effectively\npreventing the chosen likelihood from decreasing. We evaluate our methods in\nboth reward and preference settings with Mistral-8*7B and 7B models.\nExperiments suggest that InfoNCA/NCA surpasses various preference baselines\nwhen reward datasets are available. We also find NCA significantly outperforms\nDPO in complex reasoning tasks like math and coding.",
      "published": "2024-02-08T02:58:47Z",
      "updated": "2024-10-30T07:29:40Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05369.pdf",
      "abs_url": "http://arxiv.org/abs/2402.05369v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2404.04475": {
      "arxiv_id": "2404.04475",
      "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic\n  Evaluators",
      "authors": [
        "Yann Dubois",
        "Bal\u00e1zs Galambosi",
        "Percy Liang",
        "Tatsunori B. Hashimoto"
      ],
      "summary": "LLM-based auto-annotators have become a key component of the LLM development\nprocess due to their cost-effectiveness and scalability compared to human-based\nevaluation. However, these auto-annotators can introduce biases that are hard\nto remove. Even simple, known confounders such as preference for longer outputs\nremain in existing automated evaluation metrics. We propose a simple regression\nanalysis approach for controlling biases in auto-evaluations. As a real case\nstudy, we focus on reducing the length bias of AlpacaEval, a fast and\naffordable benchmark for instruction-tuned LLMs that uses LLMs to estimate\nresponse quality. Despite being highly correlated with human preferences,\nAlpacaEval is known to favor models that generate longer outputs. We introduce\na length-controlled AlpacaEval that aims to answer the counterfactual question:\n\"What would the preference be if the model's and baseline's output had the same\nlength?\" To achieve this, we first fit a generalized linear model to predict\nthe biased auto-annotator's preferences based on the mediators we want to\ncontrol for (length difference) and other relevant features. We then obtain\nlength-controlled preferences by predicting preferences while conditioning the\nGLM with a zero difference in lengths. Length-controlling not only improves the\nrobustness of the metric to manipulations in model verbosity, but we also find\nthat it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94\nto 0.98.",
      "published": "2024-04-06T02:29:02Z",
      "updated": "2025-03-10T09:27:03Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04475.pdf",
      "abs_url": "http://arxiv.org/abs/2404.04475v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2503.11751": {
      "arxiv_id": "2503.11751",
      "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
      "authors": [
        "Zhaofeng Wu",
        "Michihiro Yasunaga",
        "Andrew Cohen",
        "Yoon Kim",
        "Asli Celikyilmaz",
        "Marjan Ghazvininejad"
      ],
      "summary": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.",
      "published": "2025-03-14T17:59:41Z",
      "updated": "2025-03-14T17:59:41Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11751.pdf",
      "abs_url": "http://arxiv.org/abs/2503.11751v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1912.02164": {
      "arxiv_id": "1912.02164",
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text\n  Generation",
      "authors": [
        "Sumanth Dathathri",
        "Andrea Madotto",
        "Janice Lan",
        "Jane Hung",
        "Eric Frank",
        "Piero Molino",
        "Jason Yosinski",
        "Rosanne Liu"
      ],
      "summary": "Large transformer-based language models (LMs) trained on huge text corpora\nhave shown unparalleled generation capabilities. However, controlling\nattributes of the generated language (e.g. switching topic or sentiment) is\ndifficult without modifying the model architecture or fine-tuning on\nattribute-specific data and entailing the significant cost of retraining. We\npropose a simple alternative: the Plug and Play Language Model (PPLM) for\ncontrollable language generation, which combines a pretrained LM with one or\nmore simple attribute classifiers that guide text generation without any\nfurther training of the LM. In the canonical scenario we present, the attribute\nmodels are simple classifiers consisting of a user-specified bag of words or a\nsingle learned layer with 100,000 times fewer parameters than the LM. Sampling\nentails a forward and backward pass in which gradients from the attribute model\npush the LM's hidden activations and thus guide the generation. Model samples\ndemonstrate control over a range of topics and sentiment styles, and extensive\nautomated and human annotated evaluations show attribute alignment and fluency.\nPPLMs are flexible in that any combination of differentiable attribute models\nmay be used to steer text generation, which will allow for diverse and creative\napplications beyond the examples given in this paper.",
      "published": "2019-12-04T18:32:15Z",
      "updated": "2020-03-03T05:33:49Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1912.02164.pdf",
      "abs_url": "http://arxiv.org/abs/1912.02164v4",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2402.07319": {
      "arxiv_id": "2402.07319",
      "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
      "authors": [
        "Lichang Chen",
        "Chen Zhu",
        "Davit Soselia",
        "Jiuhai Chen",
        "Tianyi Zhou",
        "Tom Goldstein",
        "Heng Huang",
        "Mohammad Shoeybi",
        "Bryan Catanzaro"
      ],
      "summary": "In this work, we study the issue of reward hacking on the response length, a\nchallenge emerging in Reinforcement Learning from Human Feedback (RLHF) on\nLLMs. A well-formatted, verbose but less helpful response from the LLMs can\noften deceive LLMs or even human evaluators to achieve high scores. The same\nissue also holds for some reward models in RL. To address the challenges in\nboth training and evaluation, we establish a more reliable evaluation protocol\nfor comparing different training configurations, which inspects the trade-off\nbetween LLM evaluation score and response length obtained by varying training\nhyperparameters. Based on this evaluation, we conduct large-scale studies,\nwhere the results shed insights into the efficacy of hyperparameters and tricks\nused in RL on mitigating length bias. We further propose to improve the reward\nmodel by jointly training two linear heads on shared feature representations to\npredict the rewards, one trained to correlate with length, and the other\ntrained to decorrelate with length and therefore focus more on the actual\ncontent. We then discard the length head in RL to prevent reward hacking on\nlength. Experiments demonstrate that our approach almost eliminates the reward\ncorrelation with length, and improves the obtained policy by a significant\nmargin.",
      "published": "2024-02-11T22:40:12Z",
      "updated": "2024-02-11T22:40:12Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07319.pdf",
      "abs_url": "http://arxiv.org/abs/2402.07319v1",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2306.17563": {
      "arxiv_id": "2306.17563",
      "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting",
      "authors": [
        "Zhen Qin",
        "Rolf Jagerman",
        "Kai Hui",
        "Honglei Zhuang",
        "Junru Wu",
        "Le Yan",
        "Jiaming Shen",
        "Tianqi Liu",
        "Jialu Liu",
        "Donald Metzler",
        "Xuanhui Wang",
        "Michael Bendersky"
      ],
      "summary": "Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, researchers have found it difficult to outperform fine-tuned\nbaseline rankers on benchmark datasets. We analyze pointwise and listwise\nranking prompts used by existing methods and argue that off-the-shelf LLMs do\nnot fully understand these challenging ranking formulations. In this paper, we\npropose to significantly reduce the burden on LLMs by using a new technique\ncalled Pairwise Ranking Prompting (PRP). Our results are the first in the\nliterature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP\nbased on the Flan-UL2 model with 20B parameters performs favorably with the\nprevious best approach in the literature, which is based on the blackbox\ncommercial GPT-4 that has 50x (estimated) model size, while outperforming other\nLLM-based solutions, such as InstructGPT which has 175B parameters, by over 10%\nfor all ranking metrics. By using the same prompt template on seven BEIR tasks,\nPRP outperforms supervised baselines and outperforms the blackbox commercial\nChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on\naverage NDCG@10. Furthermore, we propose several variants of PRP to improve\nefficiency and show that it is possible to achieve competitive results even\nwith linear complexity.",
      "published": "2023-06-30T11:32:25Z",
      "updated": "2024-03-28T13:59:09Z",
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.17563.pdf",
      "abs_url": "http://arxiv.org/abs/2306.17563v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1807.03748": {
      "arxiv_id": "1807.03748",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": [
        "Aaron van den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "summary": "While supervised learning has enabled great progress in many applications,\nunsupervised learning has not seen such widespread adoption, and remains an\nimportant and challenging endeavor for artificial intelligence. In this work,\nwe propose a universal unsupervised learning approach to extract useful\nrepresentations from high-dimensional data, which we call Contrastive\nPredictive Coding. The key insight of our model is to learn such\nrepresentations by predicting the future in latent space by using powerful\nautoregressive models. We use a probabilistic contrastive loss which induces\nthe latent space to capture information that is maximally useful to predict\nfuture samples. It also makes the model tractable by using negative sampling.\nWhile most prior work has focused on evaluating representations for a\nparticular modality, we demonstrate that our approach is able to learn useful\nrepresentations achieving strong performance on four distinct domains: speech,\nimages, text and reinforcement learning in 3D environments.",
      "published": "2018-07-10T16:52:11Z",
      "updated": "2019-01-22T18:47:12Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1807.03748.pdf",
      "abs_url": "http://arxiv.org/abs/1807.03748v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1909.08593": {
      "arxiv_id": "1909.08593",
      "title": "Fine-Tuning Language Models from Human Preferences",
      "authors": [
        "Daniel M. Ziegler",
        "Nisan Stiennon",
        "Jeffrey Wu",
        "Tom B. Brown",
        "Alec Radford",
        "Dario Amodei",
        "Paul Christiano",
        "Geoffrey Irving"
      ],
      "summary": "Reward learning enables the application of reinforcement learning (RL) to\ntasks where reward is defined by human judgment, building a model of reward by\nasking humans questions. Most work on reward learning has used simulated\nenvironments, but complex information about values is often expressed in\nnatural language, and we believe reward learning for language is a key to\nmaking RL practical and safe for real-world tasks. In this paper, we build on\nadvances in generative pretraining of language models to apply reward learning\nto four natural language tasks: continuing text with positive sentiment or\nphysically descriptive language, and summarization tasks on the TL;DR and\nCNN/Daily Mail datasets. For stylistic continuation we achieve good results\nwith only 5,000 comparisons evaluated by humans. For summarization, models\ntrained with 60,000 comparisons copy whole sentences from the input but skip\nirrelevant preamble; this leads to reasonable ROUGE scores and very good\nperformance according to our human labelers, but may be exploiting the fact\nthat labelers rely on simple heuristics.",
      "published": "2019-09-18T17:33:39Z",
      "updated": "2020-01-08T23:02:36Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1909.08593.pdf",
      "abs_url": "http://arxiv.org/abs/1909.08593v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1909.12434": {
      "arxiv_id": "1909.12434",
      "title": "Learning the Difference that Makes a Difference with\n  Counterfactually-Augmented Data",
      "authors": [
        "Divyansh Kaushik",
        "Eduard Hovy",
        "Zachary C. Lipton"
      ],
      "summary": "Despite alarm over the reliance of machine learning systems on so-called\nspurious patterns, the term lacks coherent meaning in standard statistical\nframeworks. However, the language of causality offers clarity: spurious\nassociations are due to confounding (e.g., a common cause), but not direct or\nindirect causal effects. In this paper, we focus on natural language\nprocessing, introducing methods and resources for training models less\nsensitive to spurious patterns. Given documents and their initial labels, we\ntask humans with revising each document so that it (i) accords with a\ncounterfactual target label; (ii) retains internal coherence; and (iii) avoids\nunnecessary changes. Interestingly, on sentiment analysis and natural language\ninference tasks, classifiers trained on original data fail on their\ncounterfactually-revised counterparts and vice versa. Classifiers trained on\ncombined datasets perform remarkably well, just shy of those specialized to\neither domain. While classifiers trained on either original or manipulated data\nalone are sensitive to spurious features (e.g., mentions of genre), models\ntrained on the combined data are less sensitive to this signal. Both datasets\nare publicly available.",
      "published": "2019-09-26T23:25:25Z",
      "updated": "2020-02-14T22:32:46Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1909.12434.pdf",
      "abs_url": "http://arxiv.org/abs/1909.12434v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2410.21545": {
      "arxiv_id": "2410.21545",
      "title": "CARMO: Dynamic Criteria Generation for Context-Aware Reward Modelling",
      "authors": [
        "Taneesh Gupta",
        "Shivam Shandilya",
        "Xuchao Zhang",
        "Rahul Madhavan",
        "Supriyo Ghosh",
        "Chetan Bansal",
        "Huaxiu Yao",
        "Saravan Rajmohan"
      ],
      "summary": "Reward modeling in large language models is susceptible to reward hacking,\ncausing models to latch onto superficial features such as the tendency to\ngenerate lists or unnecessarily long responses. In reinforcement learning from\nhuman feedback (RLHF) and more generally during post-training flawed reward\nsignals often lead to outputs that optimize for these spurious correlates\ninstead of genuine quality or correctness. We propose Context-Aware Reward\nModeling (CARMO), a novel approach that first generates dynamic,\ncontext-relevant criteria to ground the reward model before producing reward\nscores. Unlike prior methods that rely on static rubrics, CARMO leverages large\nlanguage models (LLMs) to adaptively create evaluation criteria such as logical\nconsistency, clarity, and depth tailored to the user query. Our theoretical\nanalysis shows that such criteria generation can mitigate reward hacking. We\nfurther demonstrate that CARMO can be distilled into smaller models, reducing\nthe computational cost of alignment. We establish a new state-of-the-art\nperformance in zero-shot settings for generative models, achieving a 2.1\\%\nimprovement on Reward Bench. Furthermore, alignment performed on the\nCARMO-curated preference dataset achieves 22.5\\% and 21.1\\% LC-WR and WR,\nrespectively, on Mistral-Base (7B).",
      "published": "2024-10-28T21:18:49Z",
      "updated": "2025-02-17T21:25:09Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21545.pdf",
      "abs_url": "http://arxiv.org/abs/2410.21545v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "1502.05477": {
      "arxiv_id": "1502.05477",
      "title": "Trust Region Policy Optimization",
      "authors": [
        "John Schulman",
        "Sergey Levine",
        "Philipp Moritz",
        "Michael I. Jordan",
        "Pieter Abbeel"
      ],
      "summary": "We describe an iterative procedure for optimizing policies, with guaranteed\nmonotonic improvement. By making several approximations to the\ntheoretically-justified procedure, we develop a practical algorithm, called\nTrust Region Policy Optimization (TRPO). This algorithm is similar to natural\npolicy gradient methods and is effective for optimizing large nonlinear\npolicies such as neural networks. Our experiments demonstrate its robust\nperformance on a wide variety of tasks: learning simulated robotic swimming,\nhopping, and walking gaits; and playing Atari games using images of the screen\nas input. Despite its approximations that deviate from the theory, TRPO tends\nto give monotonic improvement, with little tuning of hyperparameters.",
      "published": "2015-02-19T06:44:25Z",
      "updated": "2017-04-20T18:04:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1502.05477.pdf",
      "abs_url": "http://arxiv.org/abs/1502.05477v5",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2405.00675": {
      "arxiv_id": "2405.00675",
      "title": "Self-Play Preference Optimization for Language Model Alignment",
      "authors": [
        "Yue Wu",
        "Zhiqing Sun",
        "Huizhuo Yuan",
        "Kaixuan Ji",
        "Yiming Yang",
        "Quanquan Gu"
      ],
      "summary": "Standard reinforcement learning from human feedback (RLHF) approaches relying\non parametric models like the Bradley-Terry model fall short in capturing the\nintransitivity and irrationality in human preferences. Recent advancements\nsuggest that directly working with preference probabilities can yield a more\naccurate reflection of human preferences, enabling more flexible and accurate\nlanguage model alignment. In this paper, we propose a self-play-based method\nfor language model alignment, which treats the problem as a constant-sum\ntwo-player game aimed at identifying the Nash equilibrium policy. Our approach,\ndubbed Self-Play Preference Optimization (SPPO), utilizes iterative policy\nupdates to provably approximate the Nash equilibrium. Additionally, we propose\na new SPPO objective which is both strongly motivated by theory and is simple\nand effective in practice. In our experiments, using only 60k prompts (without\nresponses) from the UltraFeedback dataset and without any prompt augmentation,\nby leveraging a pre-trained preference model PairRM with only 0.4B parameters,\nSPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves\nthe state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo\non AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench,\nArena-Hard, and the Open LLM Leaderboard. Starting from a stronger base model\nLlama-3-8B-Instruct, we are able to achieve a length-controlled win rate of\n38.77%. Notably, the strong performance of SPPO is achieved without additional\nexternal supervision (e.g., responses, preferences, etc.) from GPT-4 or other\nstronger language models. Codes are available at\nhttps://github.com/uclaml/SPPO.",
      "published": "2024-05-01T17:59:20Z",
      "updated": "2024-10-04T18:48:25Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.00675.pdf",
      "abs_url": "http://arxiv.org/abs/2405.00675v5",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2106.09685": {
      "arxiv_id": "2106.09685",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": [
        "Edward J. Hu",
        "Yelong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Lu Wang",
        "Weizhu Chen"
      ],
      "summary": "An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.",
      "published": "2021-06-17T17:37:18Z",
      "updated": "2021-10-16T18:40:34Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.09685.pdf",
      "abs_url": "http://arxiv.org/abs/2106.09685v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2501.13007": {
      "arxiv_id": "2501.13007",
      "title": "PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament",
      "authors": [
        "Yantao Liu",
        "Zijun Yao",
        "Rui Min",
        "Yixin Cao",
        "Lei Hou",
        "Juanzi Li"
      ],
      "summary": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Judge Reward Model (PariJudge RM) combined\nwith a knockout tournament for BoN sampling. Instead of assigning absolute\nscores, given one math problem, PariJudge RM judges two candidate solutions'\ncorrectness with chain-of-thought reasoning simultaneously. This approach\neliminates the need for scoring and enables cross-validation of solutions\nthrough parallel judgment. In the knockout tournament, PariJudge RM conducts\npairwise Judgment between candidate solutions and eliminates the incorrect ones\niteratively. We construct PairJudge-432K, a large-scale dataset of 432K\npairwise judgments derived from NumiaMath and annotated using\n\\texttt{gemini-1.5-flash}, and train the PariJudge RM via supervised\nfine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate\nsignificant improvements over baseline reward models. And a 40\\% to 60\\%\nrelative improvement is achieved on the top 50\\% challenging problems.",
      "published": "2025-01-22T16:49:37Z",
      "updated": "2025-02-19T13:35:48Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13007.pdf",
      "abs_url": "http://arxiv.org/abs/2501.13007v2",
      "constitutional_hash": "cdd01ef066bc6cf2"
    },
    "2404.10719": {
      "arxiv_id": "2404.10719",
      "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study",
      "authors": [
        "Shusheng Xu",
        "Wei Fu",
        "Jiaxuan Gao",
        "Wenjie Ye",
        "Weilin Liu",
        "Zhiyu Mei",
        "Guangju Wang",
        "Chao Yu",
        "Yi Wu"
      ],
      "summary": "Reinforcement Learning from Human Feedback (RLHF) is currently the most\nwidely used method to align large language models (LLMs) with human\npreferences. Existing RLHF methods can be roughly categorized as either\nreward-based or reward-free. Novel applications such as ChatGPT and Claude\nleverage reward-based methods that first learn a reward model and apply\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). However,\nin academic benchmarks, state-of-the-art results are often achieved via\nreward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly\nsuperior to PPO? Why does PPO perform poorly on these benchmarks? In this\npaper, we first conduct both theoretical and empirical studies on the\nalgorithmic properties of DPO and show that DPO may have fundamental\nlimitations. Moreover, we also comprehensively examine PPO and reveal the key\nfactors for the best performances of PPO in fine-tuning LLMs. Finally, we\nbenchmark DPO and PPO across a collection of RLHF testbeds, ranging from\ndialogue to code generation. Experiment results demonstrate that PPO is able to\nsurpass other alignment methods in all cases and achieve state-of-the-art\nresults in challenging code competitions. Our code is publicly available at\nhttps://github.com/openpsi-project/ReaLHF.",
      "published": "2024-04-16T16:51:53Z",
      "updated": "2024-10-10T08:30:17Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.10719.pdf",
      "abs_url": "http://arxiv.org/abs/2404.10719v3",
      "constitutional_hash": "cdd01ef066bc6cf2"
    }
  }
}