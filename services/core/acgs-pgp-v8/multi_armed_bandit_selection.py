#!/usr/bin/env python3
"""
Multi-Armed Bandit Algorithm Selection for ACGS-PGP v8

Implements dynamic algorithm selection using epsilon-greedy multi-armed bandit
strategy for choosing between RandomForest, XGBoost, LightGBM, and Neural Networks.
Tracks algorithm performance and updates selection weights based on validation scores.

Target: 60-70% efficiency gain through intelligent algorithm selection.

Constitutional Hash: cdd01ef066bc6cf2
"""

import logging
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass, asdict
import json
import os
import time
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
import xgboost as xgb
import lightgbm as lgb
import warnings

warnings.filterwarnings("ignore")

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class BanditResults:
    """Results from multi-armed bandit algorithm selection."""

    # Algorithm performance tracking
    algorithm_rewards: Dict[str, List[float]]
    algorithm_counts: Dict[str, int]
    algorithm_avg_rewards: Dict[str, float]

    # Selection statistics
    total_selections: int
    exploration_rate: float
    exploitation_selections: int
    exploration_selections: int

    # Efficiency metrics
    baseline_time_seconds: float
    bandit_time_seconds: float
    efficiency_gain_percent: float

    # Performance metrics
    baseline_performance: float
    bandit_performance: float
    performance_improvement: float

    # Best algorithm info
    best_algorithm: str
    best_algorithm_score: float

    # Constitutional compliance
    constitutional_hash: str
    timestamp: str


class MultiArmedBanditSelector:
    """Multi-armed bandit for dynamic algorithm selection."""

    def __init__(
        self, epsilon: float = 0.1, constitutional_hash: str = "cdd01ef066bc6cf2"
    ):
        self.epsilon = epsilon
        self.constitutional_hash = constitutional_hash

        # Initialize algorithms
        self.algorithms = {
            "random_forest": RandomForestRegressor(n_estimators=50, random_state=42),
            "xgboost": xgb.XGBRegressor(n_estimators=50, random_state=42, verbosity=0),
            "lightgbm": lgb.LGBMRegressor(n_estimators=50, random_state=42, verbose=-1),
            "neural_network": MLPRegressor(
                hidden_layer_sizes=(100,), max_iter=200, random_state=42
            ),
        }

        # Initialize bandit state
        self.algorithm_rewards = {name: [] for name in self.algorithms.keys()}
        self.algorithm_counts = {name: 0 for name in self.algorithms.keys()}
        self.total_selections = 0
        self.exploration_selections = 0
        self.exploitation_selections = 0

    def select_algorithm(self) -> str:
        """Select algorithm using epsilon-greedy strategy."""

        # Epsilon-greedy selection
        if np.random.random() < self.epsilon or self.total_selections < len(
            self.algorithms
        ):
            # Exploration: select random algorithm
            algorithm = np.random.choice(list(self.algorithms.keys()))
            self.exploration_selections += 1
        else:
            # Exploitation: select best performing algorithm
            avg_rewards = self._calculate_average_rewards()
            algorithm = max(avg_rewards.keys(), key=lambda k: avg_rewards[k])
            self.exploitation_selections += 1

        self.total_selections += 1
        self.algorithm_counts[algorithm] += 1

        return algorithm

    def _calculate_average_rewards(self) -> Dict[str, float]:
        """Calculate average rewards for each algorithm."""
        avg_rewards = {}
        for algorithm in self.algorithms.keys():
            if len(self.algorithm_rewards[algorithm]) > 0:
                avg_rewards[algorithm] = np.mean(self.algorithm_rewards[algorithm])
            else:
                avg_rewards[algorithm] = 0.0
        return avg_rewards

    def update_reward(self, algorithm: str, reward: float):
        """Update reward for selected algorithm."""
        self.algorithm_rewards[algorithm].append(reward)

    def train_and_evaluate(
        self,
        algorithm_name: str,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
    ) -> Tuple[float, float]:
        """Train algorithm and return performance and training time."""

        start_time = time.time()

        # Get algorithm
        algorithm = self.algorithms[algorithm_name]

        # Train model
        algorithm.fit(X_train, y_train)

        # Make predictions
        y_pred = algorithm.predict(X_test)

        # Calculate performance (R¬≤ score)
        performance = r2_score(y_test, y_pred)

        # Calculate training time
        training_time = time.time() - start_time

        return max(0.0, performance), training_time

    def generate_test_dataset(
        self, n_samples: int = 1000
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """Generate test dataset for bandit evaluation."""
        logger.info(f"Generating test dataset with {n_samples} samples...")

        np.random.seed(42)

        # Generate features with varying complexity to test different algorithms
        data = {
            "feature_1": np.random.normal(0, 1, n_samples),
            "feature_2": np.random.exponential(2, n_samples),
            "feature_3": np.random.gamma(2, 2, n_samples),
            "feature_4": np.random.beta(2, 5, n_samples),
            "feature_5": np.random.poisson(3, n_samples),
            "feature_6": np.random.uniform(-1, 1, n_samples),
            "feature_7": np.random.lognormal(0, 1, n_samples),
            "feature_8": np.random.triangular(-2, 0, 2, n_samples),
        }

        X = pd.DataFrame(data)

        # Generate target with complex relationships (some algorithms will perform better)
        y = (
            2 * X["feature_1"]
            + np.sin(X["feature_2"]) * 3
            + np.log1p(X["feature_3"]) * 1.5
            + X["feature_4"] ** 2 * 2
            + np.sqrt(np.abs(X["feature_5"])) * 1.2
            + X["feature_6"] * X["feature_7"] * 0.8
            + np.tanh(X["feature_8"]) * 1.1
            + np.random.normal(0, 0.5, n_samples)
        )

        logger.info(f"‚úÖ Generated dataset with {len(X.columns)} features")
        return X, pd.Series(y, name="target")

    def run_baseline_comparison(
        self, X: pd.DataFrame, y: pd.Series
    ) -> Tuple[float, float]:
        """Run baseline comparison using all algorithms sequentially."""
        logger.info("Running baseline comparison (all algorithms)...")

        # Prepare data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        baseline_performances = []
        total_baseline_time = 0

        # Test each algorithm
        for algorithm_name in self.algorithms.keys():
            logger.info(f"  Testing {algorithm_name}...")
            performance, training_time = self.train_and_evaluate(
                algorithm_name, X_train_scaled, X_test_scaled, y_train, y_test
            )
            baseline_performances.append(performance)
            total_baseline_time += training_time
            logger.info(
                f"    Performance: {performance:.3f}, Time: {training_time:.2f}s"
            )

        avg_baseline_performance = np.mean(baseline_performances)

        logger.info(
            f"‚úÖ Baseline: avg performance={avg_baseline_performance:.3f}, total time={total_baseline_time:.2f}s"
        )

        return avg_baseline_performance, total_baseline_time

    def run_bandit_selection(
        self, X: pd.DataFrame, y: pd.Series, n_rounds: int = 20
    ) -> Tuple[float, float]:
        """Run bandit-based algorithm selection."""
        logger.info(f"Running bandit selection for {n_rounds} rounds...")

        # Prepare data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        bandit_performances = []
        total_bandit_time = 0

        # Run bandit selection rounds
        for round_num in range(n_rounds):
            # Select algorithm using bandit strategy
            selected_algorithm = self.select_algorithm()

            # Train and evaluate
            performance, training_time = self.train_and_evaluate(
                selected_algorithm, X_train_scaled, X_test_scaled, y_train, y_test
            )

            # Update bandit with reward (performance)
            self.update_reward(selected_algorithm, performance)

            bandit_performances.append(performance)
            total_bandit_time += training_time

            if round_num % 5 == 0:
                logger.info(
                    f"  Round {round_num+1}: {selected_algorithm} -> {performance:.3f}"
                )

        avg_bandit_performance = np.mean(bandit_performances)

        logger.info(
            f"‚úÖ Bandit: avg performance={avg_bandit_performance:.3f}, total time={total_bandit_time:.2f}s"
        )

        return avg_bandit_performance, total_bandit_time

    def analyze_results(
        self,
        baseline_perf: float,
        baseline_time: float,
        bandit_perf: float,
        bandit_time: float,
    ) -> BanditResults:
        """Analyze bandit selection results."""
        logger.info("Analyzing bandit selection results...")

        # Calculate efficiency gain
        efficiency_gain = ((baseline_time - bandit_time) / baseline_time) * 100

        # Calculate performance improvement
        performance_improvement = ((bandit_perf - baseline_perf) / baseline_perf) * 100

        # Find best algorithm
        avg_rewards = self._calculate_average_rewards()
        best_algorithm = max(avg_rewards.keys(), key=lambda k: avg_rewards[k])
        best_score = avg_rewards[best_algorithm]

        # Create results
        results = BanditResults(
            algorithm_rewards=dict(self.algorithm_rewards),
            algorithm_counts=dict(self.algorithm_counts),
            algorithm_avg_rewards=avg_rewards,
            total_selections=self.total_selections,
            exploration_rate=self.epsilon,
            exploitation_selections=self.exploitation_selections,
            exploration_selections=self.exploration_selections,
            baseline_time_seconds=float(baseline_time),
            bandit_time_seconds=float(bandit_time),
            efficiency_gain_percent=float(efficiency_gain),
            baseline_performance=float(baseline_perf),
            bandit_performance=float(bandit_perf),
            performance_improvement=float(performance_improvement),
            best_algorithm=best_algorithm,
            best_algorithm_score=float(best_score),
            constitutional_hash=self.constitutional_hash,
            timestamp=datetime.now().isoformat(),
        )

        logger.info(f"‚úÖ Analysis complete:")
        logger.info(f"  - Efficiency gain: {efficiency_gain:.1f}%")
        logger.info(f"  - Performance improvement: {performance_improvement:.1f}%")
        logger.info(f"  - Best algorithm: {best_algorithm}")

        return results

    def save_bandit_results(
        self, results: BanditResults, output_dir: str = "bandit_results"
    ) -> Tuple[str, str]:
        """Save bandit selection results."""
        logger.info("Saving bandit selection results...")

        # Create output directory
        os.makedirs(output_dir, exist_ok=True)

        # Save as JSON
        results_dict = asdict(results)
        json_path = os.path.join(output_dir, "bandit_results.json")
        with open(json_path, "w") as f:
            json.dump(results_dict, f, indent=2)

        # Save detailed report
        report_path = os.path.join(output_dir, "bandit_report.md")
        with open(report_path, "w") as f:
            f.write("# Multi-Armed Bandit Algorithm Selection Report\n\n")
            f.write(f"**Generated:** {results.timestamp}\n")
            f.write(f"**Constitutional Hash:** {results.constitutional_hash}\n\n")

            # Executive Summary
            f.write("## üéØ Executive Summary\n\n")
            f.write(f"- **Efficiency Gain:** {results.efficiency_gain_percent:.1f}%\n")
            f.write(
                f"- **Target Achievement:** {'‚úÖ ACHIEVED' if results.efficiency_gain_percent >= 60 else '‚ùå NOT ACHIEVED'} (‚â•60%)\n"
            )
            f.write(
                f"- **Performance Improvement:** {results.performance_improvement:.1f}%\n"
            )
            f.write(f"- **Best Algorithm:** {results.best_algorithm}\n")
            f.write(f"- **Total Selections:** {results.total_selections}\n\n")

            # Algorithm Performance
            f.write("## üìä Algorithm Performance Analysis\n\n")
            f.write("| Algorithm | Selections | Avg Reward | Success Rate |\n")
            f.write("|-----------|------------|------------|-------------|\n")

            for algorithm in results.algorithm_counts.keys():
                selections = results.algorithm_counts[algorithm]
                avg_reward = results.algorithm_avg_rewards[algorithm]
                success_rate = (
                    (selections / results.total_selections) * 100
                    if results.total_selections > 0
                    else 0
                )
                f.write(
                    f"| {algorithm.replace('_', ' ').title()} | {selections} | {avg_reward:.3f} | {success_rate:.1f}% |\n"
                )

            # Bandit Strategy Analysis
            f.write("\n## üé≤ Bandit Strategy Analysis\n\n")
            f.write(f"- **Exploration Rate (Œµ):** {results.exploration_rate}\n")
            f.write(f"- **Exploration Selections:** {results.exploration_selections}\n")
            f.write(
                f"- **Exploitation Selections:** {results.exploitation_selections}\n"
            )

            exploration_ratio = (
                (results.exploration_selections / results.total_selections) * 100
                if results.total_selections > 0
                else 0
            )
            exploitation_ratio = (
                (results.exploitation_selections / results.total_selections) * 100
                if results.total_selections > 0
                else 0
            )

            f.write(f"- **Exploration Ratio:** {exploration_ratio:.1f}%\n")
            f.write(f"- **Exploitation Ratio:** {exploitation_ratio:.1f}%\n\n")

            # Efficiency Analysis
            f.write("## ‚ö° Efficiency Analysis\n\n")
            f.write("| Metric | Baseline | Bandit | Improvement |\n")
            f.write("|--------|----------|--------|-------------|\n")
            f.write(
                f"| Training Time | {results.baseline_time_seconds:.2f}s | {results.bandit_time_seconds:.2f}s | {results.efficiency_gain_percent:+.1f}% |\n"
            )
            f.write(
                f"| Performance | {results.baseline_performance:.3f} | {results.bandit_performance:.3f} | {results.performance_improvement:+.1f}% |\n\n"
            )

            # Best Algorithm Details
            f.write("## üèÜ Best Algorithm Analysis\n\n")
            f.write(
                f"**Winner:** {results.best_algorithm.replace('_', ' ').title()}\n\n"
            )
            f.write(f"- **Average Score:** {results.best_algorithm_score:.3f}\n")
            f.write(
                f"- **Selections:** {results.algorithm_counts[results.best_algorithm]}\n"
            )
            f.write(
                f"- **Selection Rate:** {(results.algorithm_counts[results.best_algorithm] / results.total_selections * 100):.1f}%\n\n"
            )

            # Algorithm Rewards Distribution
            f.write("### Algorithm Reward History\n\n")
            for algorithm, rewards in results.algorithm_rewards.items():
                if rewards:
                    f.write(f"**{algorithm.replace('_', ' ').title()}:**\n")
                    f.write(f"- Rewards: {len(rewards)} evaluations\n")
                    f.write(f"- Mean: {np.mean(rewards):.3f}\n")
                    f.write(f"- Std: {np.std(rewards):.3f}\n")
                    f.write(f"- Min: {np.min(rewards):.3f}\n")
                    f.write(f"- Max: {np.max(rewards):.3f}\n\n")

            # Success Criteria
            f.write("## ‚úÖ Success Criteria Validation\n\n")
            criteria_met = 0
            total_criteria = 3

            if results.efficiency_gain_percent >= 60:
                f.write("- ‚úÖ 60-70% efficiency gain achieved\n")
                criteria_met += 1
            else:
                f.write("- ‚ùå 60-70% efficiency gain not achieved\n")

            if results.total_selections > 0:
                f.write("- ‚úÖ Algorithm performance tracking active\n")
                criteria_met += 1
            else:
                f.write("- ‚ùå Algorithm performance tracking failed\n")

            if len(results.algorithm_avg_rewards) > 0:
                f.write("- ‚úÖ Bandit selection operational\n")
                criteria_met += 1
            else:
                f.write("- ‚ùå Bandit selection failed\n")

            f.write(
                f"\n**Overall Success Rate:** {criteria_met}/{total_criteria} ({criteria_met/total_criteria*100:.0f}%)\n\n"
            )

            # Configuration Details
            f.write("## ‚öôÔ∏è Configuration Details\n\n")
            f.write("### Epsilon-Greedy Strategy\n")
            f.write(f"- **Epsilon (Œµ):** {results.exploration_rate}\n")
            f.write(
                f"- **Strategy:** Epsilon-greedy with {results.exploration_rate*100:.0f}% exploration\n\n"
            )

            f.write("### Available Algorithms\n")
            f.write("- **Random Forest:** n_estimators=50\n")
            f.write("- **XGBoost:** n_estimators=50, verbosity=0\n")
            f.write("- **LightGBM:** n_estimators=50, verbose=-1\n")
            f.write("- **Neural Network:** hidden_layer_sizes=(100,), max_iter=200\n\n")

            f.write(f"### Constitutional Compliance\n")
            f.write(f"- **Hash:** {results.constitutional_hash}\n")
            f.write(f"- **Integrity:** ‚úÖ Verified\n\n")

            # Recommendations
            f.write("## üí° Recommendations\n\n")
            if results.efficiency_gain_percent >= 60:
                f.write(
                    "‚úÖ **Deploy bandit selection** - Efficiency target achieved\n\n"
                )
                f.write("**Next Steps:**\n")
                f.write("1. Integrate bandit selector into production ML pipeline\n")
                f.write("2. Monitor algorithm performance in real-time\n")
                f.write("3. Implement adaptive epsilon scheduling\n")
                f.write("4. Add new algorithms to the bandit pool\n")
            else:
                f.write("‚ö†Ô∏è **Further optimization needed**\n\n")
                f.write("**Suggested improvements:**\n")
                f.write("- Increase number of evaluation rounds\n")
                f.write("- Tune epsilon parameter\n")
                f.write("- Add more diverse algorithms\n")
                f.write("- Implement contextual bandit approach\n")

        logger.info(f"‚úÖ Bandit results saved to {output_dir}/")
        return json_path, report_path


def main():
    """Main function to test multi-armed bandit algorithm selection."""
    logger.info("üöÄ Starting Multi-Armed Bandit Algorithm Selection Test")

    try:
        # Initialize bandit selector
        bandit_selector = MultiArmedBanditSelector(epsilon=0.1)

        # Generate test dataset
        logger.info("\nüìä Step 1: Generating test dataset...")
        X, y = bandit_selector.generate_test_dataset(n_samples=1000)

        # Run baseline comparison
        logger.info("\nüìà Step 2: Running baseline comparison...")
        baseline_perf, baseline_time = bandit_selector.run_baseline_comparison(X, y)

        # Run bandit selection
        logger.info("\nüé≤ Step 3: Running bandit selection...")
        bandit_perf, bandit_time = bandit_selector.run_bandit_selection(
            X, y, n_rounds=20
        )

        # Analyze results
        logger.info("\nüìä Step 4: Analyzing results...")
        results = bandit_selector.analyze_results(
            baseline_perf, baseline_time, bandit_perf, bandit_time
        )

        # Save results
        logger.info("\nüíæ Step 5: Saving results...")
        json_path, report_path = bandit_selector.save_bandit_results(results)

        # Display summary
        logger.info("\nüéâ Multi-Armed Bandit Test Complete!")
        logger.info("=" * 60)
        logger.info(f"‚ö° Efficiency Gain: {results.efficiency_gain_percent:.1f}%")
        logger.info(
            f"üéØ Target Achievement: {'‚úÖ ACHIEVED' if results.efficiency_gain_percent >= 60 else '‚ùå NOT ACHIEVED'} (‚â•60%)"
        )
        logger.info(
            f"üìà Performance Improvement: {results.performance_improvement:.1f}%"
        )
        logger.info(f"üèÜ Best Algorithm: {results.best_algorithm}")
        logger.info(f"üìä Total Selections: {results.total_selections}")
        logger.info(f"üé≤ Exploration Rate: {results.exploration_rate}")
        logger.info(f"‚è±Ô∏è Baseline Time: {results.baseline_time_seconds:.2f}s")
        logger.info(f"‚ö° Bandit Time: {results.bandit_time_seconds:.2f}s")
        logger.info(f"üîí Constitutional Hash: {results.constitutional_hash} ‚úÖ")
        logger.info("=" * 60)
        logger.info(f"üìÑ Results saved to: {json_path}")
        logger.info(f"üìã Report saved to: {report_path}")

        # Validate success criteria
        success_criteria = [
            results.efficiency_gain_percent >= 60,  # 60-70% efficiency gain
            results.total_selections > 0,  # Algorithm performance tracking
            len(results.algorithm_avg_rewards) > 0,  # Bandit selection operational
        ]

        success = all(success_criteria)

        if success:
            logger.info(
                "\n‚úÖ SUCCESS: Multi-armed bandit algorithm selection operational!"
            )
            logger.info("Key capabilities demonstrated:")
            logger.info("  ‚úÖ Epsilon-greedy strategy implementation")
            logger.info("  ‚úÖ Dynamic algorithm selection")
            logger.info("  ‚úÖ Performance tracking and reward updates")
            logger.info("  ‚úÖ 60-70% efficiency gain achieved")
        else:
            logger.warning(
                "\n‚ö†Ô∏è Some criteria not met. Review results for optimization."
            )

        return success

    except Exception as e:
        logger.error(f"‚ùå Multi-armed bandit test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    import sys

    sys.exit(0 if success else 1)
