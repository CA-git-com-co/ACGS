\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
\label{sec:experimental_setup} 
\textbf{Models.} To demonstrate the effectiveness of \algacro{} and ensure coverage across different model families and sizes, we provide our results on four models: Qwen-2.5-7B~\citep{qwen2024qwen25}, Llama-2-7B~\citep{touvron2023llama}, Llama-3-8B~\citep{dubey2024llama}, and Phi-4-14B~\citep{abdin2024phi}.

\textbf{Data.} We use the Alpaca dataset~\citep{taori2023alpaca} to construct hidden states distribution and compute thresholds for each layer. The Alpaca dataset is an instruction-following dataset for fine-tuning language models, released by a research team from Stanford University with the aim of building and sharing an LLaMA model that follows instructions. The dataset contains 52,000 instructions and demonstrations generated by OpenAI’s text-davinci-003 engine. 

\textbf{Evaluation.}
We use lm-evaluation-harness pipeline~\citep{eval-harness} for our evaluations on an extensive suite of downstream tasks, including PIQA ~\citep{bisk2020piqa},  WinoGrande~\citep{sakaguchi2019winogrande}, HellaSwag~\citep{zellers2019hellaswag}, Arc Challenge~\citep{allenai:arc}, MMLU~\citep{hendrycks2020measuring}, and GSM8K~\citep{cobbe2021training}. 

\textbf{Baselines.} {In practice, the gating strategy can be either top-k-based or threshold-based (\eg, TEAL~\citep{liu2024trainingfreeactivationsparsitylarge} and CATS~\citep{lee2024catscontextuallyawarethresholdingsparsity}). Threshold-based approaches typically determine gating thresholds by statistically analyzing hidden state distributions from a general-purpose dataset. However, directly applying these thresholds during evaluation may cause a mismatch between the actual and target sparsity levels, due to potential distributional shifts between the training and evaluation datasets. To avoid this issue and ensure a fair comparison across methods, we adopt the top-$k$ based gating strategy in our experiments. 

To eliminate the potential effect introduced by the transformation process, we introduce an additional baseline, TEAL-Transform. In this variant, the TEAL approach is applied to the transformed model, retaining the $k$ elements with the largest absolute values $|x|$. This controlled baseline enables a fair comparison of different sparse activation strategies.

To further improve performance, we assign layer-specific sparsity ratios instead of a uniform sparsity across the model. Given a global sparsity target, we leverage the greedy algorithm proposed in TEAL to iteratively configure per-layer sparsity levels such that the aggregate sparsity meets the global budget. This adaptive allocation enables prioritization of computational resources for more critical parameter groups, improving overall performance.


\subsection{Controlled Sparsity Experiments.}
Here, we provide an empirical comparison of \algacro{} against TEAL-based baselines (\eg, TEAL and TEAL-transform) across different sparsity levels (25\% to 65\%) to demonstrate the effectiveness of our proposed algorithm under various experimental settings.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{qwen_2.5_7b.png}
        \caption{QWen-2.5-7B}
        \label{fig:qwen-2.5-7b}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth}
        \centering        \includegraphics[width=\linewidth]{llama_2_7b.png}
        \caption{Llama-2-7B}
        \label{fig:llama-2-7b}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{llama_3_8b.png}
        \caption{Llama-3-8B}
        \label{fig:llama-3-8b}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{phi_4_14b.png}
        % \caption{Example caption for Qwen 2.5 7B}
        \caption{Phi-4-14B}
        \label{fig:phi-4-14b}
    \end{subfigure}
    \caption{\textbf{Sparsity-performance frontiers.} Sparsity-performance across Qwen-2.5-7B, Llama-2-7B, Llama-3-8B, and Phi-4-14B.}
    \label{fig:qwen-overview}
\end{figure}

\input{tab_qwen2.5_7b}
\paragraph{Qwen-2.5-7B.} {We evaluate \algacro{} on Qwen2.5-7B \citep{yang2024qwen2} across various sparsity levels (i.e, 25\% -- 65\%) under the controlled sparsity setting. As shown in \autoref{tab:qwen25_7b_sparsity}, \algacro{} consistently matches or outperforms both TEAL and TEAL -transform across all sparsity levels. Notably, as sparsity increases, the performance gap between \algacro{} and the baselines becomes more pronounced. For instance, at 65\% sparsity, \algacro{} outperforms TEAL by 2.94\% and TEAL-transform by 1.41\% on average. This trend indicates that \algacro{} is more robust under high sparsity, likely due to its ability to retain the most influential activations by jointly considering hidden state magnitudes and weight norms. Particularly on harder tasks such as GSM8K and HellaSwag, \algacro{} maintains relatively strong performance even when aggressive sparsification is applied.}

\input{tab_llama2_7b}
\paragraph{Llama-2-7B.} {On Llama-2-7B, \algacro{} again shows strong performance under various sparsity constraints. As shown in \autoref{tab:llama2_7b_sparsity}, \algacro{} achieves the highest average accuracy at 25\% sparsity, outperforming both TEAL-based baselines and the full model. While performance naturally degrades at the extreme 65\% sparsity level, \algacro{} still offers the best accuracy, suggesting its robustness under aggressive pruning.}

\input{tab_llama3_8b}
\paragraph{Llama-3-8B.} The results on Llama-3-8B further emphasize \algacro{}’s resilience to pruning, as summarized in \autoref{tab:llama3_8b_sparsity}. While TEAL slightly outperforms at the 25\% level, \algacro{} leads in all remaining sparsity configurations, culminating in +1.06\% and +2.41\% over TEAL at 50\% sparsity and 65\% sparsity, respectively. Notably, \algacro{} sustains particularly strong performance on reasoning-intensive tasks like GSM8K and ARC Challenge, where other methods show significant drops under compression. These patterns suggest that \algacro{} is not only compression-friendly but also capable of preserving complex decision-making abilities under tight computational budgets.

\input{tab_phi4_14b}
\paragraph{Phi-4-14B.} \algacro{} also delivers robust performance on Phi-4-14B across all tested sparsity levels, as detailed in \autoref{tab:phi4_14b_sparsity}. It consistently either matches or exceeds the accuracy of both TEAL and TEAL-transform, and achieves the top average score at every sparsity setting. At the highest sparsity of 65\%, for instance, \algacro{} improves upon TEAL and TEAL-transform by +2.01\% and +0.86\%, respectively. Its ability to retain high performance on complex benchmarks such as GSM8K and MMLU, even under severe pruning, highlights its stability. These outcomes demonstrate that \algacro{} can effectively preserve key reasoning mechanisms in large-scale models, making it well-suited for sparsity-constrained deployments.

\begin{table}[H]
\centering
\caption{(G)FLOPs over different sparsity across diffrent model architecture.}\label{tab:gflops}
\begin{tabular}{lcccc}
\toprule
Sparsity & QWen2.5-7B & Llama-2-7B & Llama-3-8B & Phi-4\\
\midrule
Baseline & 7.07 & 6.61 & 7.50 & 14.15  \\
\midrule
0.25 & 5.44 ($\downarrow 23.1\%$) & 4.99 ($\downarrow 24.5\%$) & 5.76 ($\downarrow 23.2\%$) & 10.74($\downarrow 24.1\%$)\\
0.4 &  4.46 ($\downarrow36.9\%$) & 4.02 ($\downarrow39.2\%$) & 4.71 ($\downarrow37.2\%$) & 8.69 ($\downarrow38.6\%$)\\
0.5 & 3.81 ($\downarrow46.1\%$) & 3.37 ($\downarrow49.0\%$) & 4.01 ($\downarrow46.5\%$) & 7.33 ($\downarrow48.2\%$)\\
0.65 & 2.83 ($\downarrow60.0\%$) & 2.40 ($\downarrow63.7\%$) & 2.97 ($\downarrow60.4\%$) & 5.28 ($\downarrow62.7\%$)\\
% \midrule
% {\algacro{} Avg Acc.} & 73.02 & 54.42 & 66.00 & 77.57 \\
\bottomrule
\end{tabular}
\end{table}
\paragraph{Acceleration.} In addition to performance gains, \algacro{} yields substantial computational acceleration across all evaluated LLMs. As shown in~\autoref{tab:gflops}, \algacro{} reduces the overall (G)FLOPs by up to 60.0\% on Qwen-2.5-7B, 63.7\% on Llama-2-7B, 60.4\% on Llama-3-8B, and 62.7\% on Phi-4-14B at the 65\% sparsity level. These consistent reductions in floating point operations could translate to faster inference speeds and lower computational costs, validating \algacro{}’s effectiveness as a practical solution for deployment under tight resource constraints. 
% Notably, these accelerations are achieved without requiring any model retraining, highlighting the plug-and-play nature of our training-free framework.





% \input{src/tab_llama2_13b}
% all results are buged.
% \input{src/tab_mistral_7b}



% \subsection{Empirical Sparsity Analysis.}