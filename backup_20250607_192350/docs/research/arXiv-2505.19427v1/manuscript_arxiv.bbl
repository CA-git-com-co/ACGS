\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[sea(2025)]{seamless2025joint}
Joint speech and text machine translation for up to 100 languages.
\newblock \emph{Nature}, 637\penalty0 (8046):\penalty0 587--593, 2025.

\bibitem[Abdin et~al.(2024)Abdin, Aneja, Behl, Bubeck, Eldan, Gunasekar,
  Harrison, Hewett, Javaheripi, Kauffmann, et~al.]{abdin2024phi}
M.~Abdin, J.~Aneja, H.~Behl, S.~Bubeck, R.~Eldan, S.~Gunasekar, M.~Harrison,
  R.~J. Hewett, M.~Javaheripi, P.~Kauffmann, et~al.
\newblock Phi-4 technical report.
\newblock \emph{arXiv preprint arXiv:2412.08905}, 2024.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,
  Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida,
  J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Alizadeh et~al.(2023)Alizadeh, Mirzadeh, Belenko, Khatamifard, Cho,
  Del~Mundo, Rastegari, and Farajtabar]{alizadeh2023llm}
K.~Alizadeh, I.~Mirzadeh, D.~Belenko, K.~Khatamifard, M.~Cho, C.~C. Del~Mundo,
  M.~Rastegari, and M.~Farajtabar.
\newblock Llm in a flash: Efficient large language model inference with limited
  memory.
\newblock \emph{arXiv preprint arXiv:2312.11514}, 2023.

\bibitem[Ashkboos et~al.(2024)Ashkboos, Croci, do~Nascimento, Hoefler, and
  Hensman]{ashkboos2024slicegptcompresslargelanguage}
S.~Ashkboos, M.~L. Croci, M.~G. do~Nascimento, T.~Hoefler, and J.~Hensman.
\newblock Slicegpt: Compress large language models by deleting rows and
  columns, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.15024}.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang,
  Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu,
  Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan,
  Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{bai2023qwen}
J.~Bai, S.~Bai, Y.~Chu, Z.~Cui, K.~Dang, X.~Deng, Y.~Fan, W.~Ge, Y.~Han,
  F.~Huang, B.~Hui, L.~Ji, M.~Li, J.~Lin, R.~Lin, D.~Liu, G.~Liu, C.~Lu, K.~Lu,
  J.~Ma, R.~Men, X.~Ren, X.~Ren, C.~Tan, S.~Tan, J.~Tu, P.~Wang, S.~Wang,
  W.~Wang, S.~Wu, B.~Xu, J.~Xu, A.~Yang, H.~Yang, J.~Yang, S.~Yang, Y.~Yao,
  B.~Yu, H.~Yuan, Z.~Yuan, J.~Zhang, X.~Zhang, Y.~Zhang, Z.~Zhang, C.~Zhou,
  J.~Zhou, X.~Zhou, and T.~Zhu.
\newblock Qwen technical report, 2023.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Y.~Bisk, R.~Zellers, J.~Gao, Y.~Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem[Chang et~al.(2024)Chang, Wang, Wang, Wu, Yang, Zhu, Chen, Yi, Wang,
  Wang, et~al.]{chang2024survey}
Y.~Chang, X.~Wang, J.~Wang, Y.~Wu, L.~Yang, K.~Zhu, H.~Chen, X.~Yi, C.~Wang,
  Y.~Wang, et~al.
\newblock A survey on evaluation of large language models.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  15\penalty0 (3):\penalty0 1--45, 2024.

\bibitem[Chen et~al.(2017)Chen, Curtis, and Robinson]{chen2017reduced}
T.~Chen, F.~E. Curtis, and D.~P. Robinson.
\newblock A reduced-space algorithm for minimizing $\ell_1$-regularized convex
  functions.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (3):\penalty0
  1583--1610, 2017.

\bibitem[Chen et~al.(2020)Chen, Ji, Shi, Ding, Fang, Yi, and
  Tu]{chen2020neural}
T.~Chen, B.~Ji, Y.~Shi, T.~Ding, B.~Fang, S.~Yi, and X.~Tu.
\newblock Neural network compression via sparse optimization.
\newblock \emph{arXiv preprint arXiv:2011.04868}, 2020.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Ding, Ji, Wang, Shi, Tian, Yi,
  Tu, and Zhu]{chen2021orthant}
T.~Chen, T.~Ding, B.~Ji, G.~Wang, Y.~Shi, J.~Tian, S.~Yi, X.~Tu, and Z.~Zhu.
\newblock Orthant based proximal stochastic gradient method for $\ell_1$-regularized optimization.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD 2020, Ghent, Belgium, September 14--18, 2020,
  Proceedings, Part III}, pages 57--73. Springer, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Ji, Ding, Fang, Wang, Zhu, Liang,
  Shi, Yi, and Tu]{chen2021oto}
T.~Chen, B.~Ji, T.~Ding, B.~Fang, G.~Wang, Z.~Zhu, L.~Liang, Y.~Shi, S.~Yi, and
  X.~Tu.
\newblock Only train once: A one-shot neural network training and pruning
  framework.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Ding, Yadav, Zharkov, and
  Liang]{chen2023lorashear}
T.~Chen, T.~Ding, B.~Yadav, I.~Zharkov, and L.~Liang.
\newblock Lorashear: Efficient large language model structured pruning and
  knowledge recovery.
\newblock \emph{arXiv preprint arXiv:2310.18356}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Ding, Zhu, Chen, Wu, Zharkov, and
  Liang]{chen2023otov3}
T.~Chen, T.~Ding, Z.~Zhu, Z.~Chen, H.~Wu, I.~Zharkov, and L.~Liang.
\newblock Otov3: Automatic architecture-agnostic neural network training and
  compression from structured pruning to erasing operators.
\newblock \emph{arXiv preprint arXiv:2312.09411}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Liang, Ding, Zhu, and
  Zharkov]{chen2023otov2}
T.~Chen, L.~Liang, T.~Ding, Z.~Zhu, and I.~Zharkov.
\newblock Otov2: Automatic, generic, user-friendly.
\newblock \emph{arXiv preprint arXiv:2303.06862}, 2023{\natexlab{c}}.

\bibitem[Chen et~al.(2024)Chen, Qu, Aponte, Banbury, Ko, Ding, Ma, Lyapunov,
  Zharkov, and Liang]{chen2024hesso}
T.~Chen, X.~Qu, D.~Aponte, C.~Banbury, J.~Ko, T.~Ding, Y.~Ma, V.~Lyapunov,
  I.~Zharkov, and L.~Liang.
\newblock Hesso: Towards automatic efficient and user friendly any neural
  network training and pruning.
\newblock \emph{arXiv preprint arXiv:2409.09085}, 2024.

\bibitem[Cheng et~al.(2025)Cheng, Blodgett, DeVrio, Egede, and
  Olteanu]{cheng2025dehumanizing}
M.~Cheng, S.~L. Blodgett, A.~DeVrio, L.~Egede, and A.~Olteanu.
\newblock Dehumanizing machines: Mitigating anthropomorphic behaviors in text
  generation systems.
\newblock \emph{arXiv preprint arXiv:2502.14019}, 2025.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{allenai:arc}
P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and
  O.~Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{arXiv:1803.05457v1}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021training}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert,
  J.~Tworek, J.~Hilton, R.~Nakano, C.~Hesse, and J.~Schulman.
\newblock Training verifiers to solve math word problems, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[Dong et~al.(2024)Dong, Liu, Xu, Cui, Che, Sun, and
  Liu]{qwen2024qwen25}
Y.~Dong, Z.~Liu, Y.~Xu, Y.~Cui, W.~Che, T.~Sun, and T.~Liu.
\newblock Qwen2: Scaling up language models with data mixture of expert
  quality, 2024.
\newblock URL \url{https://huggingface.co/Qwen/Qwen2-7B}.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman,
  Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
A.~Dubey, A.~Jauhri, A.~Pandey, A.~Kadian, A.~Al-Dahle, A.~Letman, A.~Mathur,
  A.~Schelten, A.~Yang, A.~Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
W.~Fedus, B.~Zoph, and N.~Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (120):\penalty0 1--39, 2022.

\bibitem[Frankle and Carbin(2018)]{frankle2018lottery}
J.~Frankle and M.~Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frantar and Alistarh(2023)]{frantar2023sparsegpt}
E.~Frantar and D.~Alistarh.
\newblock Sparsegpt: Massive language models can be accurately pruned in
  one-shot.
\newblock In \emph{International Conference on Machine Learning}, pages
  10323--10337. PMLR, 2023.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster,
  Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds,
  Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and
  Zou]{eval-harness}
L.~Gao, J.~Tow, B.~Abbasi, S.~Biderman, S.~Black, A.~DiPofi, C.~Foster,
  L.~Golding, J.~Hsu, A.~Le~Noac'h, H.~Li, K.~McDonell, N.~Muennighoff,
  C.~Ociepa, J.~Phang, L.~Reynolds, H.~Schoelkopf, A.~Skowron, L.~Sutawika,
  E.~Tang, A.~Thite, B.~Wang, K.~Wang, and A.~Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Mao, and Dally]{han2015deep}
S.~Han, H.~Mao, and W.~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015{\natexlab{a}}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{han2015learnweightsconnect}
S.~Han, J.~Pool, J.~Tran, and W.~J. Dally.
\newblock Learning both weights and connections for efficient neural networks,
  2015{\natexlab{b}}.

\bibitem[He et~al.(2018)He, Kang, Dong, Fu, and Yang]{he2018soft}
Y.~He, G.~Kang, X.~Dong, Y.~Fu, and Y.~Yang.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1808.06866}, 2018.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2020measuring}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and
  J.~Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hendy et~al.(2023)Hendy, Abdelrehim, Sharaf, Raunak, Gabr, Matsushita,
  Kim, Afify, and Awadalla]{hendy2023goodgptmodelsmachine}
A.~Hendy, M.~Abdelrehim, A.~Sharaf, V.~Raunak, M.~Gabr, H.~Matsushita, Y.~J.
  Kim, M.~Afify, and H.~H. Awadalla.
\newblock How good are gpt models at machine translation? a comprehensive
  evaluation, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.09210}.

\bibitem[Hui et~al.(2025)Hui, Li, Chen, Banbury, Koishida,
  et~al.]{hui2025winclick}
Z.~Hui, Y.~Li, T.~Chen, C.~Banbury, K.~Koishida, et~al.
\newblock Winclick: Gui grounding with multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2503.04730}, 2025.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and
  Hinton]{jacobs1991adaptive}
R.~A. Jacobs, M.~I. Jordan, S.~J. Nowlan, and G.~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 79--87, 1991.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,
  Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~d.~l.
  Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Ko et~al.(2024)Ko, Kim, Chen, and Yun]{ko2024distillm}
J.~Ko, S.~Kim, T.~Chen, and S.-Y. Yun.
\newblock Distillm: Towards streamlined distillation for large language models.
\newblock \emph{arXiv preprint arXiv:2402.03898}, 2024.

\bibitem[Ko et~al.(2025)Ko, Chen, Kim, Ding, Liang, Zharkov, and
  Yun]{ko2025distillm}
J.~Ko, T.~Chen, S.~Kim, T.~Ding, L.~Liang, I.~Zharkov, and S.-Y. Yun.
\newblock Distillm-2: A contrastive approach boosts the distillation of llms.
\newblock \emph{arXiv preprint arXiv:2503.07067}, 2025.

\bibitem[Lee et~al.(2024)Lee, Lee, Zhang, Tiwari, and
  Mirhoseini]{lee2024catscontextuallyawarethresholdingsparsity}
D.~Lee, J.-Y. Lee, G.~Zhang, M.~Tiwari, and A.~Mirhoseini.
\newblock Cats: Contextually-aware thresholding for sparsity in large language
  models, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.08763}.

\bibitem[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{lepikhin2020GShard}
D.~Lepikhin, H.~Lee, Y.~Xu, D.~Chen, O.~Firat, Y.~Huang, M.~Krikun, N.~Shazeer,
  and Z.~Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock \emph{arXiv preprint arXiv:2006.16668}, 2020.

\bibitem[Li et~al.(2024)Li, Tang, Zhao, Nie, and Wen]{li2024pre}
J.~Li, T.~Tang, W.~X. Zhao, J.-Y. Nie, and J.-R. Wen.
\newblock Pre-trained language models for text generation: A survey.
\newblock \emph{ACM Computing Surveys}, 56\penalty0 (9):\penalty0 1--39, 2024.

\bibitem[Li et~al.(2020)Li, Gu, Mayer, Gool, and Timofte]{li2020group}
Y.~Li, S.~Gu, C.~Mayer, L.~V. Gool, and R.~Timofte.
\newblock Group sparsity: The hinge between filter pruning and decomposition
  for network compression.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 8018--8027, 2020.

\bibitem[Lin et~al.(2019)Lin, Ji, Li, Deng, and Li]{lin2019toward}
S.~Lin, R.~Ji, Y.~Li, C.~Deng, and X.~Li.
\newblock Toward compact convnets via structure-sparsity regularized filter
  pruning.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  31\penalty0 (2):\penalty0 574--588, 2019.

\bibitem[Liu et~al.(2024)Liu, Ponnusamy, Cai, Guo, Kim, and
  Athiwaratkun]{liu2024trainingfreeactivationsparsitylarge}
J.~Liu, P.~Ponnusamy, T.~Cai, H.~Guo, Y.~Kim, and B.~Athiwaratkun.
\newblock Training-free activation sparsity in large language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.14690}.

\bibitem[Qu et~al.(2025)Qu, Aponte, Banbury, Robinson, Ding, Koishida, Zharkov,
  and Chen]{qu2025automatic}
X.~Qu, D.~Aponte, C.~Banbury, D.~P. Robinson, T.~Ding, K.~Koishida, I.~Zharkov,
  and T.~Chen.
\newblock Automatic joint structured pruning and quantization for efficient
  neural network training and compression.
\newblock \emph{arXiv preprint arXiv:2502.16638}, 2025.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0
  (140):\penalty0 1--67, 2020.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and
  Choi]{sakaguchi2019winogrande}
K.~Sakaguchi, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{arXiv preprint arXiv:1907.10641}, 2019.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and J.~Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{taori2023alpaca}
R.~Taori, I.~Gulrajani, T.~Zhang, Y.~Dubois, X.~Li, C.~Guestrin, P.~Liang, and
  T.~B. Hashimoto.
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock \emph{Stanford Center for Research on Foundation Models.
  https://crfm. stanford. edu/2023/03/13/alpaca. html}, 3\penalty0
  (6):\penalty0 7, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Lacroix, Rozière,
  Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, T.~Lacroix, B.~Rozière, N.~Goyal,
  E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin, E.~Grave, and G.~Lample.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Tschannen et~al.(2025)Tschannen, Gritsenko, Wang, Naeem,
  Alabdulmohsin, Parthasarathy, Evans, Beyer, Xia, Mustafa,
  et~al.]{tschannen2025siglip}
M.~Tschannen, A.~Gritsenko, X.~Wang, M.~F. Naeem, I.~Alabdulmohsin,
  N.~Parthasarathy, T.~Evans, L.~Beyer, Y.~Xia, B.~Mustafa, et~al.
\newblock Siglip 2: Multilingual vision-language encoders with improved
  semantic understanding, localization, and dense features.
\newblock \emph{arXiv preprint arXiv:2502.14786}, 2025.

\bibitem[Wang et~al.(2024)Wang, Ma, Wang, and Wei]{wang2024q}
H.~Wang, S.~Ma, R.~Wang, and F.~Wei.
\newblock Q-sparse: All large language models can be fully sparsely-activated.
\newblock \emph{arXiv preprint arXiv:2407.10969}, 2024.

\bibitem[Wen et~al.(2016{\natexlab{a}})Wen, Wu, Wang, Chen, and
  Li]{wen2016learning}
W.~Wen, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1608.03665}, 2016{\natexlab{a}}.

\bibitem[Wen et~al.(2016{\natexlab{b}})Wen, Wu, Wang, Chen, and
  Li]{wen2016learnstructsparse}
W.~Wen, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Learning structured sparsity in deep neural networks,
  2016{\natexlab{b}}.

\bibitem[Xue et~al.(2022)Xue, Shi, Wei, Lou, Liu, and You]{xue2022go}
F.~Xue, Z.~Shi, F.~Wei, Y.~Lou, Y.~Liu, and Y.~You.
\newblock Go wider instead of deeper.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 8779--8787, 2022.

\bibitem[Yang et~al.(2024)Yang, Yang, Zhang, Hui, Zheng, Yu, Li, Liu, Huang,
  Wei, et~al.]{yang2024qwen2}
A.~Yang, B.~Yang, B.~Zhang, B.~Hui, B.~Zheng, B.~Yu, C.~Li, D.~Liu, F.~Huang,
  H.~Wei, et~al.
\newblock Qwen2. 5 technical report.
\newblock \emph{arXiv preprint arXiv:2412.15115}, 2024.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, 2019.

\bibitem[Zhao et~al.(2025)Zhao, Chen, and Wang]{zhao2025robustness}
H.~Zhao, T.~Chen, and Z.~Wang.
\newblock On the robustness of gui grounding models against image attacks.
\newblock \emph{arXiv preprint arXiv:2504.04716}, 2025.

\bibitem[Zhuang et~al.(2020)Zhuang, Zhang, Huang, Zeng, Shuang, and
  Li]{zhuang2020neuron}
T.~Zhuang, Z.~Zhang, Y.~Huang, X.~Zeng, K.~Shuang, and X.~Li.
\newblock Neuron-level structured pruning using polarization regularizer.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zuo et~al.(2022)Zuo, Zhang, Liang, He, Zhao, and Chen]{zuo2022moebert}
S.~Zuo, Q.~Zhang, C.~Liang, P.~He, T.~Zhao, and W.~Chen.
\newblock Moebert: from bert to mixture-of-experts via importance-guided
  adaptation.
\newblock \emph{arXiv preprint arXiv:2204.07675}, 2022.

\end{thebibliography}
