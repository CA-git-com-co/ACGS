version: '3.8'
services:
  nano-vllm-reasoning:
    build:
      context: ../../services/reasoning-models
      dockerfile: Dockerfile.nano-vllm
    container_name: acgs_nano_vllm_reasoning
    environment:
    - NANO_VLLM_MODE=enabled
    - FALLBACK_TO_VLLM=true
    - NVIDIA_MODEL_PATH=nvidia/Llama-3.1-Nemotron-70B-Instruct-HF
    - MICROSOFT_MODEL_PATH=microsoft/Phi-4
    - ENABLE_FALLBACK=true
    - LOG_LEVEL=INFO
    - PYTHONPATH=/app
    - TENSOR_PARALLEL_SIZE=1
    - GPU_MEMORY_UTILIZATION=0.9
    - MAX_MODEL_LEN=32768
    - CONSTITUTIONAL_MODE=enabled
    - REASONING_DEPTH=standard
    ports:
    - 8000:8000
    - 8001:8001
    - 8002:8002
    volumes:
    - ${HOME:-~}/.cache/huggingface:/root/.cache/huggingface
    - nano_vllm_models:/app/models
    - ../../config/nano-vllm:/app/config:ro
    - ../../logs:/app/logs
    - ../../config/constitutional:/app/constitutional:ro
    networks:
    - acgs_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: json-file
      options:
        max-size: 50m
        max-file: '3'
        labels: service=nano-vllm,environment=${ENVIRONMENT:-development}
  nano-vllm-metrics:
    image: prom/node-exporter:latest
    container_name: acgs_nano_vllm_metrics
    command:
    - --path.rootfs=/host
    - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    ports:
    - 9100:9100
    volumes:
    - /:/host:ro,rslave
    networks:
    - acgs_network
    restart: unless-stopped
networks:
  acgs_network:
    external: true
volumes:
  nano_vllm_models:
    driver: local
constitutional_hash: cdd01ef066bc6cf2
