version: '3.8'

# ACGS Nano-vLLM Service - Production Deployment
# Lightweight vLLM alternative with comparable performance

services:
  # =============================================================================
  # Nano-vLLM Reasoning Service (Replaces vLLM containers)
  # =============================================================================
  nano-vllm-reasoning:
    build:
      context: ../../services/reasoning-models
      dockerfile: Dockerfile.nano-vllm
    container_name: acgs_nano_vllm_reasoning
    environment:
      - NANO_VLLM_MODE=enabled
      - FALLBACK_TO_VLLM=true
      # Model Configuration
      - NVIDIA_MODEL_PATH=nvidia/Llama-3.1-Nemotron-70B-Instruct-HF
      - MICROSOFT_MODEL_PATH=microsoft/Phi-4
      - ENABLE_FALLBACK=true
      - LOG_LEVEL=INFO
      - PYTHONPATH=/app

      # Performance Settings
      - TENSOR_PARALLEL_SIZE=1
      - GPU_MEMORY_UTILIZATION=0.9
      - MAX_MODEL_LEN=32768

      # Constitutional AI Settings
      - CONSTITUTIONAL_MODE=enabled
      - REASONING_DEPTH=standard

    ports:
      - '8000:8000' # NVIDIA AceReason equivalent
      - '8001:8001' # Microsoft Phi-4 equivalent
      - '8002:8002' # Multimodal equivalent

    volumes:
      # Model cache
      - ${HOME:-~}/.cache/huggingface:/root/.cache/huggingface
      - nano_vllm_models:/app/models

      # Configuration
      - ../../config/nano-vllm:/app/config:ro

      # Logs
      - ../../logs:/app/logs

      # Constitutional principles
      - ../../config/constitutional:/app/constitutional:ro

    networks:
      - acgs_network

    restart: unless-stopped

    # Resource limits (much lower than vLLM)
    deploy:
      resources:
        limits:
          memory: 8G # Reduced from 32G
          cpus: '4.0' # Reduced from 8.0
        reservations:
          memory: 4G
          cpus: '2.0'

    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8000/health']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s # Faster startup than vLLM

    logging:
      driver: 'json-file'
      options:
        max-size: '50m' # Smaller logs
        max-file: '3'
        labels: 'service=nano-vllm,environment=${ENVIRONMENT:-development}'

  # =============================================================================
  # Monitoring and Metrics (Optional)
  # =============================================================================
  nano-vllm-metrics:
    image: prom/node-exporter:latest
    container_name: acgs_nano_vllm_metrics
    command:
      - '--path.rootfs=/host'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - '9100:9100'
    volumes:
      - /:/host:ro,rslave
    networks:
      - acgs_network
    restart: unless-stopped

# =============================================================================
# Networks and Volumes
# =============================================================================
networks:
  acgs_network:
    external: true

volumes:
  nano_vllm_models:
    driver: local
