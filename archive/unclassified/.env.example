import os
# =============================================================================
# ACGS-PGP Centralized Configuration
# =============================================================================

# Environment Configuration
ENVIRONMENT=development                                # development, production, testing
DEBUG=false                                           # Enable debug mode
TEST_MODE=false                                       # Enable test mode

# Database Configuration
DATABASE_URL=postgresql+asyncpg://acgs_user:acgs_password@postgres_db:5432/acgs_pgp_db
TEST_DATABASE_URL=postgresql+asyncpg://test_user:test_password@localhost:5433/test_acgs_db
DB_ECHO_LOG=false                                     # Enable SQL query logging

# Service URLs (External/Development)
AUTH_SERVICE_URL=http://localhost:8000
AC_SERVICE_URL=http://localhost:8001
INTEGRITY_SERVICE_URL=http://localhost:8002
FV_SERVICE_URL=http://localhost:8003
GS_SERVICE_URL=http://localhost:8004
PGC_SERVICE_URL=http://localhost:8005

# Internal Service URLs (Docker Container Communication)
AUTH_SERVICE_INTERNAL_URL=http://auth_service:8000
AC_SERVICE_INTERNAL_URL=http://ac_service:8001
INTEGRITY_SERVICE_INTERNAL_URL=http://integrity_service:8002
FV_SERVICE_INTERNAL_URL=http://fv_service:8003
GS_SERVICE_INTERNAL_URL=http://gs_service:8004
PGC_SERVICE_INTERNAL_URL=http://pgc_service:8005

# API Configuration
API_VERSION=v1
BACKEND_CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Security Configuration
# CRITICAL: Generate strong random keys for production deployment
# Use: python -c "import secrets; print(secrets.token_urlsafe(64))" to generate secure keys
JWT_SECRET_KEY=your-secret-key-change-in-production-please-use-strong-random-key-minimum-64-chars
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# CSRF Protection
CSRF_SECRET_KEY=your-csrf-secret-key-change-in-production-minimum-32-chars

# Database Security
POSTGRES_PASSWORD=your-strong-postgres-password-change-in-production

# HTTPS/TLS Configuration
ENABLE_HTTPS=true
TLS_CERT_PATH=/etc/ssl/certs/acgs.crt
TLS_KEY_PATH=/etc/ssl/private/acgs.key
FORCE_HTTPS_REDIRECT=true

# Security Headers Configuration
ENABLE_SECURITY_HEADERS=true
ENABLE_HSTS=true
HSTS_MAX_AGE=31536000
ENABLE_CSP=true
CSP_REPORT_URI=/api/v1/security/csp-report

# Rate Limiting Configuration
ENABLE_RATE_LIMITING=true
RATE_LIMIT_REQUESTS_PER_MINUTE=100
RATE_LIMIT_BURST_SIZE=20
RATE_LIMIT_BLOCK_DURATION_MINUTES=15
ENABLE_IP_BLOCKING=true
MAX_FAILED_ATTEMPTS=5

# Input Validation Configuration
MAX_REQUEST_SIZE_MB=10
ENABLE_REQUEST_VALIDATION=true
ENABLE_SQL_INJECTION_PROTECTION=true
ENABLE_XSS_PROTECTION=true

# Security Monitoring
ENABLE_SECURITY_LOGGING=true
SECURITY_LOG_LEVEL=INFO
ENABLE_INTRUSION_DETECTION=true
SECURITY_ALERT_WEBHOOK_URL=your-security-webhook-url-here

# External Services
LLM_API_ENDPOINT=http://mock_llm_service/generate

# AI Model Configuration for ACGS-PGP Operations
# Primary model for policy synthesis and governance operations
ACGS_PRIMARY_LLM_MODEL=claude-3-7-sonnet-20250219
ACGS_RESEARCH_LLM_MODEL=sonar-pro
ACGS_FALLBACK_LLM_MODEL=claude-3-5-sonnet

# Model-specific settings
ACGS_LLM_MAX_TOKENS=64000
ACGS_LLM_TEMPERATURE=0.2
ACGS_RESEARCH_TEMPERATURE=0.1

# Enable specific AI models for testing and research
ENABLE_GEMINI_2_5_FLASH=true                          # Enable Google Gemini 2.5 Flash for testing
ENABLE_DEEPSEEK_R1=true                               # Enable DeepSeek-R1 for research operations
ENABLE_BIAS_DETECTION_LLM=true                        # Enable LLM-based bias detection
ENABLE_GROQ_LLAMA_MODELS=true                         # Enable Groq-hosted Llama models for testing

# Groq Model Configuration for ACGS-PGP Testing
GROQ_MODEL_NAME=llama-3.3-70b-versatile              # Default Groq model for testing
GROQ_TESTING_MODEL_VERSATILE=llama-3.3-70b-versatile # Large versatile model for comprehensive testing
GROQ_TESTING_MODEL_MAVERICK=meta-llama/llama-4-maverick-17b-128e-instruct  # Mid-size model with extended context
GROQ_TESTING_MODEL_SCOUT=meta-llama/llama-4-scout-17b-16e-instruct         # Efficient model for rapid testing

# Monitoring and Logging
LOG_LEVEL=INFO                                        # DEBUG, INFO, WARNING, ERROR, CRITICAL
METRICS_ENABLED=true
PROMETHEUS_PORT=9090

# =============================================================================
# TaskMaster AI API Keys (Required to enable respective provider)
# =============================================================================
ANTHROPIC_api_key = os.getenv("API_KEY")       # Required: Format: sk-ant-api03-...
PERPLEXITY_api_key = os.getenv("API_KEY")     # Optional: Format: pplx-...
OPENAI_api_key = os.getenv("API_KEY")             # Optional, for OpenAI/OpenRouter models. Format: sk-proj-...
GOOGLE_api_key = os.getenv("API_KEY")             # Required for Google Gemini 2.5 Flash model
MISTRAL_api_key = os.getenv("API_KEY")               # Optional, for Mistral AI models.
XAI_api_key = os.getenv("API_KEY")                       # Optional, for xAI AI models.
AZURE_OPENAI_api_key = os.getenv("API_KEY")            # Optional, for Azure OpenAI models (requires endpoint in .taskmasterconfig).
OLLAMA_api_key = os.getenv("API_KEY")             # Optional: For remote Ollama servers that require authentication.

# HuggingFace Configuration for DeepSeek-R1
HUGGINGFACE_api_key = os.getenv("API_KEY")   # Required for DeepSeek-R1 model access
HUGGINGFACE_API_ENDPOINT="https://api-inference.huggingface.co/models"  # HuggingFace Inference API endpoint

# OpenRouter Configuration (Alternative for DeepSeek models)
OPENROUTER_api_key = os.getenv("API_KEY")     # Optional: For OpenRouter DeepSeek models

# Groq Configuration for Llama Models
GROQ_api_key = os.getenv("API_KEY")                 # Required for Groq-hosted Llama models (llama-3.3-70b-versatile, llama-4-maverick-17b, llama-4-scout-17b)

# Ollama Configuration for Local Model Deployment
OLLAMA_BASE_URL="http://127.0.0.1:11434"             # Ollama server base URL
OLLAMA_API_KEY=""                                     # Optional: For remote Ollama servers requiring authentication
OLLAMA_DEFAULT_MODEL="hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL"  # Default Ollama model for local deployment
OLLAMA_TIMEOUT_SECONDS=60                             # Timeout for Ollama API calls
OLLAMA_MAX_RETRIES=3                                  # Maximum retry attempts for Ollama calls
ENABLE_OLLAMA_MODELS=true                             # Enable Ollama local model integration

# =============================================================================
# LangGraph Workflow Configuration
# =============================================================================

# LangGraph Infrastructure
LANGGRAPH_REDIS_URL=redis://langgraph_redis:6379      # Redis URL for LangGraph state management
LANGGRAPH_POSTGRES_URL=postgresql://acgs_user:acgs_password@postgres_db:5432/acgs_pgp_db  # PostgreSQL for workflow persistence

# Multi-Model LLM Configuration for LangGraph
GEMINI_api_key = os.getenv("API_KEY")             # Required for Gemini models in LangGraph workflows
GROQ_api_key = os.getenv("API_KEY")                 # Required for Groq Llama models in LangGraph workflows
XAI_api_key = os.getenv("API_KEY")                   # Required for xAI Grok models in LangGraph workflows
NVIDIA_api_key = os.getenv("API_KEY")             # Required for NVIDIA API Qwen models with reasoning capabilities and LLM Router

# NVIDIA LLM Router Service Configuration
LLM_ROUTER_LOG_LEVEL="INFO"                      # Log level for LLM Router services
LLM_ROUTER_DEBUG_MODE="false"                    # Enable debug mode for troubleshooting
DEFAULT_MODEL_TIER="standard"                    # Default model tier (efficient, standard, premium)
FALLBACK_MODEL="nvidia/llama-3.1-8b-instruct"   # Fallback model when preferred models unavailable
MAX_CONCURRENT_REQUESTS="100"                    # Maximum concurrent requests to router
REQUEST_TIMEOUT_SECONDS="30"                     # Request timeout in seconds

# NVIDIA LLM Router ACGS Integration
ENABLE_CONSTITUTIONAL_ROUTING="true"             # Enable constitutional governance routing
CONSTITUTIONAL_MODEL_TIER="premium"              # Model tier for constitutional tasks
POLICY_SYNTHESIS_MODEL="nvidia/llama-3.1-nemotron-70b-instruct"  # Model for policy synthesis
API_KEY_ENCRYPTION_KEY="your-encryption-key-change-in-production"  # Key for API key encryption

# NVIDIA LLM Router URLs
NVIDIA_API_BASE_URL="https://integrate.api.nvidia.com/v1"  # NVIDIA API base URL
LLM_ROUTER_URL="http://nvidia_llm_router_server:8081"      # Internal LLM Router URL
LLM_ROUTER_EXTERNAL_URL="http://localhost:8081"            # External LLM Router URL for development

# Constitutional Governance Thresholds
CONSTITUTIONAL_FIDELITY_THRESHOLD=0.85                # Minimum constitutional compliance score (0.0-1.0)
POLICY_QUALITY_THRESHOLD=0.80                         # Minimum policy quality score (0.0-1.0)
BIAS_DETECTION_THRESHOLD=0.15                         # Maximum acceptable bias score (0.0-1.0)

# Constitutional Violation Detection Thresholds
CONSTITUTIONAL_FIDELITY_GREEN_THRESHOLD=0.85          # Green threshold for constitutional fidelity score
CONSTITUTIONAL_FIDELITY_AMBER_THRESHOLD=0.70          # Amber threshold for constitutional fidelity score
CONSTITUTIONAL_FIDELITY_RED_THRESHOLD=0.55            # Red threshold for constitutional fidelity score
VIOLATION_COUNT_GREEN_THRESHOLD=2                     # Green threshold for violation count per hour
VIOLATION_COUNT_AMBER_THRESHOLD=5                     # Amber threshold for violation count per hour
VIOLATION_COUNT_RED_THRESHOLD=10                      # Red threshold for violation count per hour

# Violation Detection Configuration
VIOLATION_SCAN_INTERVAL_SECONDS=30                    # Interval for real-time violation scanning
VIOLATION_BATCH_SIZE=100                              # Batch size for violation processing
VIOLATION_DETECTION_TIMEOUT_SECONDS=60               # Timeout for violation detection operations
ENABLE_REAL_TIME_VIOLATION_SCANNING=true             # Enable real-time violation scanning
ENABLE_BATCH_VIOLATION_ANALYSIS=true                 # Enable batch violation analysis
ENABLE_HISTORICAL_VIOLATION_ANALYSIS=true            # Enable historical violation analysis
MAX_VIOLATIONS_PER_SCAN=1000                         # Maximum violations to process per scan
VIOLATION_CACHE_THRESHOLD_SECONDS=300                # Cache timeout for violation thresholds

# Violation Escalation Configuration
ENABLE_AUTOMATIC_ESCALATION=true                     # Enable automatic violation escalation
ENABLE_TIMEOUT_ESCALATION=true                       # Enable timeout-based escalation
MAX_ESCALATION_LEVEL=emergency_response              # Maximum escalation level
DEFAULT_ESCALATION_RESPONSE_TIME_MINUTES=30          # Default response time for escalations
ESCALATION_NOTIFICATION_RETRY_ATTEMPTS=3             # Number of notification retry attempts
ESCALATION_NOTIFICATION_RETRY_DELAY_SECONDS=60       # Delay between notification retries

# Workflow Iteration Limits
MAX_SYNTHESIS_LOOPS=3                                 # Maximum policy synthesis iterations
MAX_REFINEMENT_ITERATIONS=5                           # Maximum amendment refinement iterations
MAX_CORRECTION_ATTEMPTS=3                             # Maximum error correction attempts

# LangGraph Monitoring and Alerting
ENABLE_PERFORMANCE_MONITORING=true                    # Enable LangGraph performance monitoring
ENABLE_CONSTITUTIONAL_ALERTS=true                     # Enable constitutional compliance alerts
ALERT_WEBHOOK_URL="your_webhook_url_here"             # Webhook URL for critical alerts

# LangGraph Debug and Development
LANGGRAPH_DEBUG_MODE=false                            # Enable LangGraph debug mode
LANGGRAPH_LOG_LEVEL=INFO                              # Log level for LangGraph workflows
ENABLE_WORKFLOW_TRACING=true                          # Enable detailed workflow tracing

# =============================================================================
# WINA (Weight Informed Neuron Activation) Configuration
# =============================================================================

# WINA Core Configuration
WINA_ENABLED=true                                     # Enable WINA optimization
WINA_TARGET_SPARSITY=0.6                             # Target sparsity ratio (0.0-1.0)
WINA_GFLOPS_REDUCTION_TARGET=0.5                     # Target GFLOPs reduction ratio (0.0-1.0)
WINA_ACCURACY_THRESHOLD=0.95                         # Minimum acceptable accuracy (0.0-1.0)
WINA_MODE=inference_only                             # WINA operation mode (inference_only, training_aware, constitutional_guided)
WINA_SPARSITY_STRATEGY=layer_specific                # Sparsity strategy (global_uniform, layer_specific, adaptive_dynamic)

# WINA Feature Toggles
WINA_ENABLE_SVD=true                                 # Enable SVD-based transformation
WINA_ENABLE_GATING=true                              # Enable dynamic runtime gating
WINA_ENABLE_MONITORING=true                          # Enable performance monitoring
WINA_ENABLE_CONSTITUTIONAL=true                      # Enable constitutional compliance checking

# WINA Integration Configuration
WINA_GS_ENGINE_OPT=true                              # Enable GS Engine LLM optimization
WINA_EC_LAYER_OPT=true                               # Enable EC Layer oversight optimization
WINA_PGC_ENHANCEMENT=true                            # Enable PGC enforcement enhancement
WINA_CONSTITUTIONAL_UPDATES=true                     # Enable constitutional principle updates

# WINA Performance Monitoring
WINA_METRICS_INTERVAL=60                             # Metrics collection interval in seconds
WINA_PROMETHEUS_METRICS=true                         # Enable Prometheus metrics export
WINA_DETAILED_LOGGING=true                           # Enable detailed performance logging

# WINA Advanced Configuration
WINA_SVD_RANK_REDUCTION=0.8                         # SVD rank reduction ratio (0.0-1.0)
WINA_GATING_THRESHOLD=0.1                            # Gating threshold for neuron activation
WINA_CACHE_TRANSFORMED_WEIGHTS=true                  # Cache transformed weights for efficiency
WINA_PARALLEL_PROCESSING=true                        # Enable parallel processing
WINA_BATCH_OPTIMIZATION=true                         # Enable batch optimization

# =============================================================================
# MCP Server Configuration (for development tools)
# =============================================================================

# GitHub Integration
GITHUB_PERSONAL_ACCESS_TOKEN="your_github_pat_here"  # Required for GitHub MCP server integration

# Search and Browser Tools
BRAVE_api_key = os.getenv("API_KEY")              # Required for Brave Search MCP server
POWER_api_key = os.getenv("API_KEY")              # Required for Power MCP server