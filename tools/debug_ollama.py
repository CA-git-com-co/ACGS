#!/usr/bin/env python3
"""
Debug Ollama Integration Issues
"""

import asyncio
import json
import os
import sys

import aiohttp

# Add the backend directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src", "backend"))


async def test_direct_ollama():
    """Test direct Ollama API call."""
    print("üîç Testing Direct Ollama API Call")

    try:
        async with aiohttp.ClientSession() as session:
            payload = {
                "model": "hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL",
                "prompt": "What is AI governance?",
                "stream": False,
                "options": {"temperature": 0.3, "num_predict": 100},
            }

            print("Sending request to: http://127.0.0.1:11434/api/generate")
            print(f"Payload: {json.dumps(payload, indent=2)}")

            async with session.post(
                "http://127.0.0.1:11434/api/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=30),
            ) as response:
                print(f"Response status: {response.status}")
                print(f"Response headers: {dict(response.headers)}")

                if response.status == 200:
                    data = await response.json()
                    print(f"Response data keys: {list(data.keys())}")
                    print(
                        f"Response: {data.get('response', 'No response field')[:200]}..."
                    )
                    return True
                error_text = await response.text()
                print(f"Error response: {error_text}")
                return False

    except Exception as e:
        print(f"Direct API test failed: {e}")
        return False


async def test_ollama_client():
    """Test our Ollama client implementation."""
    print("\nüîç Testing Ollama Client Implementation")

    try:
        from gs_service.app.core.ollama_client import OllamaLLMClient

        client = OllamaLLMClient()

        # Test health check first
        print("1. Testing health check...")
        is_healthy = await client.health_check()
        print(f"   Health check result: {is_healthy}")

        if not is_healthy:
            print("   Health check failed, skipping generation test")
            await client.close()
            return False

        # Test simple generation
        print("2. Testing simple generation...")
        try:
            response = await client.generate_text(
                prompt="Hello, how are you?", temperature=0.3, max_tokens=50
            )
            print(f"   Generation successful: {len(response)} characters")
            print(f"   Response preview: {response[:100]}...")
            await client.close()
            return True

        except Exception as gen_error:
            print(f"   Generation failed: {gen_error}")
            await client.close()
            return False

    except Exception as e:
        print(f"Client test failed: {e}")
        return False


async def test_model_availability():
    """Test model availability."""
    print("\nüîç Testing Model Availability")

    try:
        async with aiohttp.ClientSession() as session:
            async with session.get("http://127.0.0.1:11434/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    models = data.get("models", [])
                    print(f"Available models: {len(models)}")

                    target_model = (
                        "hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL"
                    )
                    model_names = [model["name"] for model in models]

                    print("Model names:")
                    for name in model_names:
                        print(f"  - {name}")

                    if target_model in model_names:
                        print(f"‚úÖ Target model '{target_model}' is available")
                        return True
                    print(f"‚ùå Target model '{target_model}' not found")
                    return False
                print(f"Failed to get models: {response.status}")
                return False

    except Exception as e:
        print(f"Model availability test failed: {e}")
        return False


async def main():
    """Main debug execution."""
    print("üöÄ Ollama Integration Debug")
    print("=" * 40)

    results = []

    # Run debug tests
    results.append(await test_model_availability())
    results.append(await test_direct_ollama())
    results.append(await test_ollama_client())

    # Summary
    print("\n" + "=" * 40)
    print("üìä Debug Summary")
    print("=" * 40)

    passed = sum(results)
    total = len(results)

    print(f"Tests passed: {passed}/{total}")

    if passed == total:
        print("‚úÖ All debug tests passed - Ollama integration should work")
    else:
        print("‚ùå Some debug tests failed - check the issues above")

        if not results[0]:
            print(
                "üí° Model not available - run: ollama pull hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL"
            )
        if not results[1]:
            print("üí° Direct API failed - check Ollama server status")
        if not results[2]:
            print("üí° Client implementation issue - check error handling")


if __name__ == "__main__":
    asyncio.run(main())
